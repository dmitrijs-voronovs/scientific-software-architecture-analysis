quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7743,ERROR,ERROR,7743,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['ERROR'],['ERROR']
Availability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2137,ERROR,ERROR,2137,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['ERROR'],['ERROR']
Availability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2482,ERROR,ERROR,2482,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['ERROR'],['ERROR']
Availability,"nux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ```bash; ulimit -u 10000; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output; ```. @Carl-labhub mentioned ""When I run it, I’m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ```bash; singul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:3118,Down,Download,3118,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['Down'],['Download']
Availability,"o docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0; I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s; user 0m0.037s; sys 0m0.013s; ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:2326,checkpoint,checkpoint,2326,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,2,['checkpoint'],['checkpoint']
Availability,"o see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3321,error,error,3321,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['error'],['error']
Availability,ocal > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]; I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]; I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 ba,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:5177,error,error,5177,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['error'],['error']
Availability,"om @pichuan above, but includes instructions for installing docker, nvidia-docker, and singularity, as well as parameters specific to singularity v3.1.1. ```; # install docker; sudo yum check-update; curl -fsSL https://get.docker.com/ | sh. # install nvidia-docker; distribution=$(. /etc/os-release;echo $ID$VERSION_ID); curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \; sudo tee /etc/yum.repos.d/nvidia-docker.repo; sudo yum install -y nvidia-docker2; semanage fcontext -a -f f -t container_runtime_exec_t -s system_u /usr/bin/nvidia-docker; sudo restorecon -v /usr/bin/nvidia-docker. # start docker; sudo systemctl start docker; sudo systemctl status docker; sudo systemctl enable docker. # install deps; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y epel-release && \; sudo yum install -y golang openssl-devel libuuid-devel libseccomp-devel squashfs-tools; echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \; echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \; source ~/.bashrc. # install singularity; mkdir -p ${GOPATH}/src/github.com/sylabs && \; cd ${GOPATH}/src/github.com/sylabs && \; git clone https://github.com/sylabs/singularity.git && \; cd singularity; git checkout v3.1.1; cd ${GOPATH}/src/github.com/sylabs/singularity && \; ./mconfig && \; cd ./builddir && \; make && \; sudo make install; ; DVVER=0.8.0; # make deepvariant CPU image; sudo docker pull gcr.io/deepvariant-docker/deepvariant:${DVVER}; sudo docker tag gcr.io/deepvariant-docker/deepvariant:${DVVER} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; singularity build --nohttps deepvariant.${DVVER}.simg docker://localhost:5000/deepvariant:latest; ; # make deepvariant GPU image; sudo nvidia-docker pull gcr.io/deepvariant-docker/deepvariant_gpu:${DVVER}; sudo nvidia-docker tag gcr.io/deepvariant-docker/deepvariant_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482761898:1244,echo,echo,1244,,https://github.com/google/deepvariant/issues/132#issuecomment-482761898,1,['echo'],['echo']
Availability,"ombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying to show the randomization of variation. This is a very different goal than trying to show the preservation of gene function under variation. Their focus was more on the linkage between SNPs, and chose to carefully look at how differences progress across lineages. If you look at one of the gene specific analysis was done for the COX1 gene as a phylogenetic tree -- presented in the Supplementary Materials -- it illustrated high similarity among the lineages:. ![image](https://github.com/google/deepvariant/assets/6555937/a119536e-cdc4-4ea3-a06a-fbf32b2f3d8e). In my experience when nature finds a strong model, it usually continues to fight entropy to preserve itself. To show that, one would have to explore the functional analysis of the lineages, showing that the variation (SNPs) was eith",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:1819,repair,repair,1819,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342,1,['repair'],['repair']
Availability,"ompiled to use: AVX2 FMA; I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt; 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants; model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint; [self.n_classes_model_variable]); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes; reader = tf.train.NewCheckpointReader(checkpoint_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader; return CheckpointReader(compat.as_bytes(filepattern), status); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__; c_api.TF_GetCode(self.status.status)); tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; ; real 0m5.561s; user 0m6.116s; sys 0m0.810s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-364502198:2199,Checkpoint,CheckpointReader,2199,,https://github.com/google/deepvariant/issues/46#issuecomment-364502198,1,['Checkpoint'],['CheckpointReader']
Availability,"on ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_nwff5xo0/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; warning: /tmp/Bazel.runfiles_4ji1hg9j/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so: write error (disk full?). Continue? (y/n/^C) ; warning: /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:2617,error,error,2617,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300,1,['error'],['error']
Availability,"on()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the direc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/268#issuecomment-586584341:2304,checkpoint,checkpoint,2304,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341,1,['checkpoint'],['checkpoint']
Availability,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:1266,down,download,1266,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044,1,['down'],['download']
Availability,"ored_session.py:222] Graph was finalized.; W1217 09:08:47.838197 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.pytho; n.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I1217 09:08:47.839556 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt; I1217 09:08:52.319547 139680301201152 session_manager.py:491] Running local_init_op.; I1217 09:08:52.393850 139680301201152 session_manager.py:493] Done running local_init_op.; I1217 09:08:53.199553 139680301201152 modeling.py:410] Reloading EMA...; I1217 09:08:53.200850 139680301201152 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt; I1217 09:09:35.048069 139680301201152 call_variants.py:399] Processed 1 examples in 1 batches [5258.970 sec per 100]; real 2m5.783s; user 0m49.664s; sys 0m7.008s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tm; p_output/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 247. I checked $OUTPUT_DIR after this, but it was empty. Let me know how to proceed,. Thanks,; Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-749313156:7021,checkpoint,checkpoint,7021,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156,1,['checkpoint'],['checkpoint']
Availability,"orflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session; config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore; err, ""a Variable name or other graph key that is missing""); tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory; 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:16141,checkpoint,checkpoint,16141,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['checkpoint'],['checkpoint']
Availability,"ose. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: ; 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords.; 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately.; 3) Run `model_train` on shuffled training set shuffled data.; 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files.; 5) Pick best model listed in the `best_checkpoint.txt` file.; 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. ; 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study.; 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081:1222,checkpoint,checkpoints,1222,,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081,1,['checkpoint'],['checkpoints']
Availability,"oticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371293506:1128,error,error,1128,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506,1,['error'],['error']
Availability,"output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; INFO:tensorflow:Running local_init_op.; I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op.; 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2); INFO:tensorflow:Reloading EMA...; I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]; ```. ```; real	0m53.331s; user	0m35.346s; sys	0m22.537s; ```; It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917442547:3047,error,error,3047,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547,1,['error'],['error']
Availability,"ow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480642492:1661,Down,Downloaded,1661,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492,1,['Down'],['Downloaded']
Availability,"ow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal; ret = Operation(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 69, in get_tensor; return CheckpointReader.CheckpointReader_GetTensor(; RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1314, in restore; names_to_keys = object_graph_key_mapping(save_path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1632, in object_graph_key_mapping; object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor; error_translator(e); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator; raise errors_impl.NotFoundError(None, None, error_message); tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJEC",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:12923,Checkpoint,CheckpointReader,12923,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,1,['Checkpoint'],['CheckpointReader']
Availability,"ownloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # #####################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3810,down,downgrades,3810,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,"['down', 'echo']","['downgrades', 'echo']"
Availability,"package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:2267,Avail,Available,2267,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,1,['Avail'],['Available']
Availability,"partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:3171,recover,recovery,3171,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['recover'],['recovery']
Availability,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1540,error,error-free,1540,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431,2,['error'],"['error-free', 'errors']"
Availability,"please see the error has:. ```bash; --ref is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/783#issuecomment-2010181773:15,error,error,15,,https://github.com/google/deepvariant/issues/783#issuecomment-2010181773,1,['error'],['error']
Availability,"ples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:58.408862 140044150212416 errors.py:61] ref argument is required. **(newenv) fci@fci-V530-15ICR:~$ echo $(pwd); /home/fci**; and I will attach screen of my home that ref file is found ; **https:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:1745,error,errors,1745,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562,1,['error'],['errors']
Availability,"pplications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------------------; #include <features.h>. /* Defined to 1 if the current compiler invocation provides a; floating-point type with the IEEE 754 binary128 format, and this glibc; includes corresponding *f128 interfaces for it. */; #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \; && defined __FLOAT128__; # define __HAVE_FLOAT128 1; #else; # define __HAVE_FLOAT128 0; #endif. /* add the following block of fix tensorflow build error */; #if CUDART_VERSION; #undef __HAVE_FLOAT128; #define __HAVE_FLOAT128 0; #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct; from the default float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:10570,error,error,10570,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['error'],['error']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12449,error,errors,12449,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2969,error,errors,2969,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11891,error,errors,11891,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4364,error,errors,4364,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4643,error,errors,4643,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4085,error,errors,4085,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3527,error,errors,3527,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3806,error,errors,3806,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12170,error,errors,12170,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11333,error,errors,11333,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4922,error,errors,4922,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6595,error,errors,6595,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11612,error,errors,11612,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2690,error,errors,2690,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3248,error,errors,3248,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:5201,error,errors,5201,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"put ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""、""F1_haplotagged.bam""、""M1_haplotagged.bam""; --reference ${ref}; ${outdir2}/${sample}_deepvariant1.phased.vcf.gz; ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. ④The final result of variant calling; #DeepTrio version:1.2.0; /opt/deepvariant/bin/deeptrio/run_deeptrio; --model_type PACBIO; --ref ${ref}; --reads_child ${outdir2}/C1_haplotagged.bam; --reads_parent1 ${outdir2}/F1_haplotagged.bam; --reads_parent2 ${outdir2}/M1_haplotagged.bam; --output_vcf_child ${outdir4}/C1.output.vcf.gz; --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz; --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz; --sample_name_child 'C1'; --sample_name_parent1 'F1'; --sample_name_parent2 'M1'; --num_shards 8; --output_gvcf_child ${outdir4}/C1.g.vcf.gz; --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz; --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:1618,error,errors,1618,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,2,['error'],"['error', 'errors']"
Availability,"pvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292#issuecomment-608553986:1753,error,error,1753,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986,1,['error'],['error']
Availability,"python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1319, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:17566,checkpoint,checkpoint,17566,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,1,['checkpoint'],['checkpoint']
Availability,"quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujiang",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:1061,echo,echo,1061,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['echo'],['echo']
Availability,"r each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more important.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765#issuecomment-1906964538:1722,down,downsides,1722,,https://github.com/google/deepvariant/issues/765#issuecomment-1906964538,1,['down'],['downsides']
Availability,"r/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; DV_GPU_BUILD=0; export PATH DV_GPU_BUILD. %apprun download_testdata; BUCKET=""gs://deepvariant""; DATA_BUCKET=""${BUCKET}/quickstart-testdata/*""; mkdir -p input; gsutil cp -R ""${DATA_BUCKET}"" input/. %apprun make_examples; exec /opt/deepvariant/bin/make_examples \; --mode calling \; --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; --reads /dv2/input/NA12878_S1.chr20.10_10p1mb.bam \; --examples output.examples.tfrecord \; --regions ""chr20:10,000,000-10,010,000"". %apprun call_variants; exec /opt/deepvariant/bin/call_variants \; --outfile call_variants_output.tfrecord \; --examples output.examples.tfrecord \; --checkpoint /models/wgs/model.ckpt. %apprun postprocess_variants; exec /opt/deepvariant/bin/postprocess_variants \; --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; --infile call_variants_output.tfrecord \; --outfile output.vcf. %runscript; if [ $# -eq 0 ]; then; echo '''Example Usage:. # download data to input and models; singularity run --app download_testdata deepvariant-custom.simg. # make the examples, mapping inputs; singularity run --bind input:/dv2/input/ --app make_examples deepvariant-custom.simg. # call variants, mapping models; singularity run --app call_variants deepvariant-custom.simg. # postprocess variants; singularity run --bind input:/dv2/input/ --app postprocess_variants deepvariant-custom.simg. # https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md; '''; else; exec ""$@""; fi. %post; export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)""; echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -; apt-get -y update && apt-get install -y google-cloud-sdk parallel wget; rm -rf /var/lib/apt/lists/*. # https://github.com/google/deepvariant/blob/r0.5/deepvariant/docker/Dockerfile; BASH_HEADER='#!/bin/bash' && \; printf ""%s\n%s\n"" \; ""${BASH",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-458208323:2074,down,download,2074,,https://github.com/google/deepvariant/issues/132#issuecomment-458208323,1,['down'],['download']
Availability,"r1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real' /tmp/openvino.log; real 7m26.887s; real 20m40.889s; real 6m25.257s; ```. ---. # Machine details. I got the machine with this command:. ```; gcloud compute instances create ""${USER}-openvino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:4158,echo,echo,4158,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['echo'],['echo']
Availability,"r20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python bin/make_examples.zip \; --mode calling \; --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; --channels ""insert_size""; ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step.; If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:3536,mainten,maintenance,3536,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,1,['mainten'],['maintenance']
Availability,ream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:47.638081: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error. Do you have any idea?. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993:2665,error,error,2665,,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993,6,['error'],['error']
Availability,"ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa""; path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:; input:; expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):; return. rule extract_bam:; input:; bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:1776,echo,echo,1776,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792,1,['echo'],['echo']
Availability,"rence you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728; I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags.; 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz; 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_paralle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:4494,checkpoint,checkpoint,4494,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['checkpoint'],['checkpoint']
Availability,"rg)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: ‘STDOUT’. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10865,down,download,10865,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['down'],['download']
Availability,"riant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details.; With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:; ----. # Commands. Tested on the same machine:. All below were done with command like:; ```; scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log; ```; with some code diffs below:. 1. Use your Docker image, use_openvino=true; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:1113,echo,echo,1113,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['echo'],['echo']
Availability,"riant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3562,failure,failure,3562,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3841,failure,failure,3841,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4678,failure,failure,4678,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:5236,failure,failure,5236,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12484,failure,failure,12484,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:12205,failure,failure,12205,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4120,failure,failure,4120,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11368,failure,failure,11368,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:2725,failure,failure,2725,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0430 18:57:45.239560 139714767980352 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_v3df_cxj/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '17']"".; E0430 18:57:45.262542 140488697198400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_1k7ckjbn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:6630,failure,failure,6630,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0430 18:57:45.303462 139795832272704 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_sfy61tb_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '29']"".; E0430 18:57:45.331091 140537432155968 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_y60taw80/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '31']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4957,failure,failure,4957,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11647,failure,failure,11647,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E0430 18:57:45.354358 139648338528064 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3baynv6p/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '47']"".; E0430 18:57:45.249558 140521800120128 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_hpf7iban/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; E0430 18:57:45.245847 140159308461888 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_0ekpfvin/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '24']"".; E0430 18:57:45.283955 140581469652800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_brxc6_sa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '7']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:4399,failure,failure,4399,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E0430 18:57:45.269598 140672023549760 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_prpwm4tu/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '41']"".; E0430 18:57:45.343238 140414174025536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:11926,failure,failure,11926,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3004,failure,failure,3004,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"riant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E0430 18:57:45.278714 140080891017024 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__lr697ii/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '20']"".; E0430 18:57:45.366031 140648053425984 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9e_e2zvn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '38']"".; E0430 18:57:45.197047 140371481966400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9fc41k4h/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '14']"".; E0430 18:57:45.279598 140654497855296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_rybr_dmc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '34']"".; E0430 18:57:45.291190 140144580835136 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xwtzfol0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '15']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:3283,failure,failure,3283,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"rl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4046,echo,echo,4046,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['echo'],['echo']
Availability,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-598179709:2046,error,error,2046,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709,1,['error'],['error']
Availability,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3752,ERROR,ERROR,3752,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['ERROR'],['ERROR']
Availability,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4778,ERROR,ERROR,4778,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['ERROR'],['ERROR']
Availability,"rror mesages, but it still finished SNP calling and all the output files seemed fine. == CUDA ==; CUDA Version 11.3.1; 2023-10-30 00:30:39.544727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.958146: E tensorflow/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993:1561,error,error,1561,,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993,1,['error'],['error']
Availability,"run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_8twzrz82/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 0m2.302s; user 0m1.215s; sys 0m0.687s. ## command-line plan B:; /share/app/singularity/3.8.1/bin/singularity exec \; --bind /usr/lib/locale/:/usr/lib/locale/ \; --bind $ccsbam:$ccsbam \; --bind $ccsbam.bai:$ccsbam.bai \; --bind $fasta:$fasta \; --bind $fasta.fai:$fasta.fai \; --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output \; /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be wr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:3867,Error,Error,3867,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300,1,['Error'],['Error']
Availability,"ry Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1086,Down,Download,1086,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,1,['Down'],['Download']
Availability,"s WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1967,recover,recovery,1967,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['recover'],['recovery']
Availability,"s protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5212,ERROR,ERROR,5212,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['ERROR'],['ERROR']
Availability,"s well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2104,recover,recovery,2104,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['recover'],['recovery']
Availability,"s will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994:3350,checkpoint,checkpoint,3350,,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994,1,['checkpoint'],['checkpoint']
Availability,s.log` . ```{bash}; I0330 15:47:21.754682 140654028756736 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756700 140654028756736 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@8.tsv --gvcf /out,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435#issuecomment-810383024:1133,error,errors,1133,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024,1,['error'],['errors']
Availability,"s_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:08:49.859084 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz; I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz; I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]; I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_jqt5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:4681,error,error,4681,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['error'],['error']
Availability,"se are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:2977,error,error,2977,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,1,['error'],['error']
Availability,"sh; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:2492,down,download,2492,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['down'],['download']
Availability,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3814,reliab,reliably,3814,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,"['down', 'reliab']","['downstream', 'reliably']"
Availability,"sing CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2019-12-13 13:07:00.397901: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I1213 13:07:01.327224 139885760198400 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1213 13:07:01.414663 139885760198400 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1213 13:07:02.310832 139885760198400 make_examples.py:1363] Task 0: 0 candidates (0 examples) [1.93s elapsed]; I1213 13:07:05.401592 139885760198400 make_examples.py:1380] Found 28 candidate variants; I1213 13:07:05.402002 139885760198400 make_examples.py:1381] Created 28 examples. real	0m10.204s; user	0m5.490s; sys	0m3.310s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 13:07:08.439639 140638419556096 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 13:07:08.488881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 13:07:08.491470: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x58612b0 executing computations on platform Host. Devices:; 2019-12-13 13:07:08.491562: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 13:07:08.495160: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 13:07:08.525229 140638419556096 modeling.py:560] Initializing model with random parameters; W1213 13:07:08.527353 140638419556096 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpr4M5u5; I1213 13:07:08.528274 140638419556096 estimator.py:201] Using config: {'_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249#issuecomment-565441661:2987,checkpoint,checkpoint,2987,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661,1,['checkpoint'],['checkpoint']
Availability,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ ~/bin/deepvariant/deepvariant_1.0.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=reference.fasta --reads=input.bam --regions=regions.bed --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz --num_shards=16. I also tried with the -e flag, and removing -B. Same error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/385#issuecomment-730789300:336,error,error,336,,https://github.com/google/deepvariant/issues/385#issuecomment-730789300,1,['error'],['error']
Availability,"singularity version 3.7.0-1.el8; CentOS 8. ```; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s; user	0m2.686s; sys	0m4.872s; I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-767207902:58,Error,Error,58,,https://github.com/google/deepvariant/issues/296#issuecomment-767207902,3,"['Error', 'error']","['Error', 'error']"
Availability,sion await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clini,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4308,error,error,4308,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['error'],['error']
Availability,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/43#issuecomment-361020869:46,error,errors,46,,https://github.com/google/deepvariant/issues/43#issuecomment-361020869,1,['error'],['errors']
Availability,"ss you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verificat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1562,error,error,1562,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312,2,['error'],['error']
Availability,"su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances reset --zone us-west1-b pichuan-gpu2; ```. and then ssh back to the machine. ```; BIN_VERSION=1.5.0; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:1334,error,error,1334,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355,1,['error'],['error']
Availability,"suming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python; is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}; is_transversion = not( is_transition ); else is_transition = is_transversion = False; ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the align",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196:1015,down,downsample,1015,,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196,1,['down'],['downsample']
Availability,"t all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4787,ERROR,ERROR,4787,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['ERROR'],['ERROR']
Availability,"t give you a reason, but for some cases I can share observations. In all cases, I will list the call followed by observation. ```; chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:5:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:..; ```. This looks clean. DeepTrio's GQ is low probably because it is a clear de novo and it has learned such events are rare. ```; chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:27,1,0:18:0,18,45,990,990,990:II; ```; One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. ```; chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:28,0:50:0,90,899:..; ```. This is interesting, since the evidence reported by DeepTrio doesn't match the view in the BAM. DeepTrio is saying that both parents don't have evidence for variant reads at this position. It is possible the after re-alignment (which DeepTrio does) there isn't evidence for a variant. However, the child is still called and has similar evidence. Is it possible for you to share snippets of this BAM file (e.g. a ~1000 bp window around thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-823860823:1397,down,downstream,1397,,https://github.com/google/deepvariant/issues/440#issuecomment-823860823,1,['down'],['downstream']
Availability,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1412,Error,Error,1412,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,4,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"t you see:. ```C; #define _GNU_SOURCE; #include <pthread.h>; #include <stdio.h>; #include <stdlib.h>; #include <errno.h>. #define handle_error_en(en, msg) \; do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int; main(int argc, char *argv[]); {; int s;; cpu_set_t cpuset;; pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);; for (int j = 0; j < 8; j++); CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");; for (int j = 0; j < CPU_SETSIZE; j++); if (CPU_ISSET(j, &cpuset)); printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);; }; ```. You should see something like this:. ```Bash; $ ./affinity; Set returned by pthread_getaffinity_np() contained:; CPU 0; CPU 1; CPU 2; CPU 3; CPU 4; CPU 5; CPU 6; CPU 7; $; ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`; Command 2: `lscpu`; Command 3: `cat /etc/os-release` ; Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/497#issuecomment-993147588:2332,avail,available,2332,,https://github.com/google/deepvariant/issues/497#issuecomment-993147588,1,['avail'],['available']
Availability,"t):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2625,Error,Error,2625,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,3,['Error'],['Error']
Availability,"t--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 638, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 266, in _run_make_examples; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxxx: no space left on device']].; (exit status 1); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70#issuecomment-386838731:2275,error,error,2275,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731,2,"['Error', 'error']","['Error', 'error']"
Availability,"t.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6811,down,download,6811,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['down'],['download']
Availability,"t.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open; ```. I also get the same error when hosting the model (renamed model.ckpt) in my personal GS bucket -- I have made the storage bucket read accessible to all users so the TPU should have access:; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://tpu-bwb/analysis-files/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. I0527 21:26:03.381308 140127359940416 run_deepvariant.py:341] Creating a directory for intermediate results in /output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:2326,error,error,2326,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,1,['error'],['error']
Availability,"t/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8273,error,errors,8273,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['error'],['errors']
Availability,"t2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errors：**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:2481,error,error,2481,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,1,['error'],['error']
Availability,"t_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994:3945,checkpoint,checkpoint,3945,,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994,1,['checkpoint'],['checkpoint']
Availability,"t_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/deepvariant_MITO60.vcf.gz; --model_type ONT_R104. On Wed, Jun 12, 2024 at 1:02 PM JYOTI MRIDHA ***@***.***> wrote:. > Hi,; > Thanks a lot for your immediate response, i have followed the above; > instructions given by you, now the docker command is running fine, but I; > have come across a new error.; >; > *[E::hts_open_format] Failed to open file; > ""/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result""; > : Is a directory*; >; > *ValueError: UNKNOWN: Could not open variants_path:; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result*; >; > here i have used the same path for docker run (-v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result); > and also same path for run_deepvariant (--output_vcf; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > ) , Here i am not able to rectify what minor error in my command, i am; > following the same pattern mentioned in the link shared, i might be; > missing out something minute.; >; > Here is my command which is used.; >; > sudo docker run -v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; > -v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508:2631,error,error,2631,,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508,1,['error'],['error']
Availability,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10104,ERROR,ERROR,10104,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"tal_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}"" --no-install-recommends \; wget \; unzip; ; +apt-get install ""${APT_ARGS[@]}"" python3-apt; +cd ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:3051,down,download,3051,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['down'],['download']
Availability,"tall Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:2319,echo,echo,2319,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['echo'],['echo']
Availability,"te ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. Version:. ```; $ uname -a; Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. Install conda:. ```bash; curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda; eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. It completed without any error messages. I see:. ```; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/; bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0; call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh; deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip; ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405:1373,error,error,1373,,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405,1,['error'],['error']
Availability,thank you @pgrosu please could it had been same vpn problem with this docker pulling command?. Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp 64.233.187.82:443: i/o timeout,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-417210803:95,Error,Error,95,,https://github.com/google/deepvariant/issues/89#issuecomment-417210803,1,['Error'],['Error']
Availability,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2074662859:296,error,errors,296,,https://github.com/google/deepvariant/issues/806#issuecomment-2074662859,1,['error'],['errors']
Availability,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/111#issuecomment-432491512:118,error,error,118,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512,1,['error'],['error']
Availability,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1179,checkpoint,checkpoint,1179,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733,1,['checkpoint'],['checkpoint']
Availability,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:2997,ERROR,ERROR,2997,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['ERROR'],['ERROR']
Availability,"ting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: ‘STDOUT’. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10930,down,download,10930,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['down'],['download']
Availability,too slow to download the main soft,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-481555098:12,down,download,12,,https://github.com/google/deepvariant/issues/137#issuecomment-481555098,1,['down'],['download']
Availability,"tput:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unless you explicitly specify; that.; ```. Do you have any idea what is causing the problem and how to proceed, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:3726,avail,available,3726,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,1,['avail'],['available']
Availability,"true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir. the error ; ***** Running the command:*****; time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:1539,error,errors,1539,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562,1,['error'],['errors']
Availability,"u have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is inc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3319,ERROR,ERROR,3319,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['ERROR'],['ERROR']
Availability,"u"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1477,Down,Download,1477,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,1,['Down'],['Download']
Availability,"uch closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU train",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1395,checkpoint,checkpoint,1395,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,1,['checkpoint'],['checkpoint']
Availability,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1603,error,error-free,1603,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853,2,['error'],"['error-free', 'errors']"
Availability,"unds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: ; Warning: skipping import of repository 'com_google_protobuf' because it already exists.; (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured).; (05:40:23) INFO: Found 141 targets and 48 test targets...; (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \; exec env - \; bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:10307,ERROR,ERROR,10307,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['ERROR'],['ERROR']
Availability,"uplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994:3647,checkpoint,checkpoint,3647,,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994,1,['checkpoint'],['checkpoint']
Availability,"usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------------------; #include <features.h>. /* Defined to 1 if the current compiler invocation provides a; floating-point type with the IEEE 754 binary128 format, and this glibc; includes corresponding *f128 interfaces for it. */; #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \; && defined __FLOAT128__; # define __HAVE_FLOAT128 1; #else; # define __HAVE_FLOAT128 0; #endif. /* add the following block of fix tensorflow build error */; #if CUDART_VERSION; #undef __HAVE_FLOAT128; #define __HAVE_FLOAT128 0; #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct; from the default float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package; bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install; pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification; python -c ""import tensorflow as tf; print(tf.__version__)""; ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:11087,error,error,11087,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['error'],['error']
Availability,"ut you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:; ```; ## Run `call_variants`; ( time \; /opt/deepvariant/bin/call_variants \; --outfile ""HG002.cvo.tfrecord.gz"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""model.ckpt"" \; ) 2>&1 | tee ""call_variants.log"" &; ```; When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:; ```; ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz""; ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:5056,error,errors,5056,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,2,['error'],['errors']
Availability,utor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:40.075465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:45.488357: E tensorflow/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993:1745,error,error,1745,,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993,1,['error'],['error']
Availability,utor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:40.617831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.029023: E tensorflow/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993:1929,error,error,1929,,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993,1,['error'],['error']
Availability,utor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.161964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.569218: E tensorflow/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993:2113,error,error,2113,,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993,1,['error'],['error']
Availability,utor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:41.707151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:47.101524: E tensorflow/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993:2297,error,error,2297,,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993,1,['error'],['error']
Availability,utor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.254657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:42.796956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.322127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:43.879132: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.404755: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:44.958146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:45.488357: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.029023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:46.569218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:47.101524: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; 2023-10-30 00:30:47.638081: E tensorflow/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993:2481,error,error,2481,,https://github.com/google/deepvariant/issues/722#issuecomment-1784889993,1,['error'],['error']
Availability,"v,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: ‘STDOUT’. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10249,down,download,10249,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['down'],['download']
Availability,"v37.fasta --reads /data/input/c6c4c1db-4328-4aa9-b038-074c9a453117.dedup.bam --examples make_examples.tfrecord@12.gz. showing error:. E0430 18:57:45.160124 140015706085184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_9oblbsyi/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1296,error,errors,1296,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['error'],['errors']
Availability,"variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 414",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2195,recover,recovery,2195,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['recover'],['recovery']
Availability,"variant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E0430 18:57:45.190006 140494428116800 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gsw00kpo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '12']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1888,failure,failure,1888,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"variant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E0430 18:57:45.281981 139640920373056 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_087sqql0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '18']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:10531,failure,failure,10531,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"variant/deepvariant/make_examples.py', '30']"".; E0430 18:57:45.312847 140640366352192 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_3bfx8qjn/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3']"".; E0430 18:57:45.285222 140677590058816 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_pbohkh7d/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '40']"".; E0430 18:57:45.251970 140221024618304 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_r241voor/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '0']"".; E0430 18:57:45.291264 139938218350400 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_q2sdkm67/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '23']"".; E0430 18:57:45.246095 140479227369280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_uphv_tdy/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '37']"".; E0430 18:57:45.317537 139819477477184 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4750gtic/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '43']"".; E0430 18:57:45.318566 140493027030848 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_mw222qvc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '19']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:10252,failure,failure,10252,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"variant/deepvariant/make_examples.py', '33']"".; E0430 18:57:45.128387 140717112878912 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_gd9fj22_/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '2']"".; E0430 18:57:45.149224 139802704738112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_2hbfxd92/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '16']"".; E0430 18:57:45.196900 140351706515264 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles__ytoy0a6/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '4']"".; E0430 18:57:45.304501 140371551012672 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_w7jwxlqo/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '28']"".; E0430 18:57:45.337900 139621533210432 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_22p2hp0z/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '45']"".; E0430 18:57:45.424449 139874966882112 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_wv1oakms/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '46']"".; E0430 18:57:45.268234 140085723301696 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_g62feq4g/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '10']"".; E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:1609,failure,failure,1609,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"variant/deepvariant/make_examples.py', '44']"".; E0430 18:57:45.264230 139873562601280 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_qtxrg7dl/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7465,failure,failure,7465,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"variant/deepvariant/make_examples.py', '6']"".; E0430 18:57:45.193969 139692519348032 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_05qe3qax/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '36']"".; E0430 18:57:45.197033 140638292031296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_75j2ynwd/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '5']"".; E0430 18:57:45.264279 139811152422720 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i060jaf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '27']"".; E0430 18:57:45.313135 139973217675072 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_p96gjj9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '11']"".; E0430 18:57:45.286854 139660079433536 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_fhgr5den/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '26']"".; E0430 18:57:45.266479 139743940249408 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_09rk_ns4/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '9']"".; E0430 18:57:45.246133 140107102431040 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_i98_1ues/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '13']"".; E0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:7744,failure,failure,7744,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['failure'],['failure']
Availability,"vironment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:58 CEST] Stage 'build-prereq.sh complete' starting`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:13110,ERROR,ERROR,13110,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['ERROR'],['ERROR']
Availability,"w 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2925,echo,echo,2925,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['echo'],['echo']
Availability,"w/python/training/monitored_session.py"", line 1236, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session; config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore; err, ""a Variable name or other graph key that is missing""); tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory; 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:15951,checkpoint,checkpoint,15951,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['checkpoint'],['checkpoint']
Availability,"when I build_and_test.sh, there showing error:; bazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist. where is this directory?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/448#issuecomment-827135295:40,error,error,40,,https://github.com/google/deepvariant/issues/448#issuecomment-827135295,1,['error'],['error']
Availability,"which will label exon regions like this (including their start and end sites):. ```; chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";; ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1695957189:1329,down,downsample,1329,,https://github.com/google/deepvariant/issues/701#issuecomment-1695957189,1,['down'],['downsample']
Availability,"with you.; > ; > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least?. In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering?. > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data?. Please see:; https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results?. If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?. I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1.; The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186:2108,error,errors,2108,,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186,2,['error'],"['error', 'errors']"
Availability,"wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1673,recover,recovery,1673,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['recover'],['recovery']
Availability,"wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](ht",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2011,recover,recovery,2011,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['recover'],['recovery']
Availability,"xamples.tfrecord@${N_SHARDS}.gz"" \; --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \; --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:; ```; ls HG002.examples.tfrecord*.gz | wc -l; ```; I see 64 of them here.; A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:; ```; ## Run `call_variants`; ( time \; /opt/deepvariant/bin/call_variants \; --outfile ""HG002.cvo.tfrecord.gz"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""model.ckpt"" \; ) 2>&1 | tee ""call_variants.log"" &; ```; When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:; ```; ValueError: Cannot find matching files with th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:4294,error,errors,4294,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,1,['error'],['errors']
Availability,xport HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4009,failover,failovermethod,4009,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['failover'],['failovermethod']
Availability,"y"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: /usr/local/cuda-10.0/lib64/libcublas.so.9.0: version `libcublas.so.9.0' not found (required by /root/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. However:. 1. I specified CUDA 10 in `settings.sh`; 1. CUDA 9 is not required for TensorFlow `r1.12`. Additionally:. 1. CUDA 9 is not available for my system, Ubuntu 18; 1. Symlinking to the correct file names as suggested [elsewhere](https://github.com/tensorflow/tensorflow/issues/15604) did not work; 1. I have built TensorFlow `r1.12` (and master) for CUDA 10 (and 9) in my environment previously. Questions:. 1. Does deepvariant have a requirement for CUDA 9(.0?)?; 1. How would you recommend proceeding?. _________. ```; Linux localhost 4.15.0-1032-aws #34-Ubuntu SMP Thu Jan 17 15:18:09 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; ```. Full log:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=1; ++ TF_ENABLE_XLA=1; ++ export TF_NEED_CUDA=1; ++ TF_NEED_CUDA=1; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=1; ++ TF_NEED_MKL=1; ++ export TF_NEED_MPI=0; ++ TF_NEED_M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:4215,avail,available,4215,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['avail'],['available']
Availability,"y; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4306,recover,recovery,4306,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['recover'],['recovery']
Availability,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:1300,down,down,1300,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,1,['down'],['down']
Availability,"ython3.6/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1524, in restore_v2; name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper; attrs=attr_protos, op_def=op_def); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3485, in _create_op_internal; op_def=op_def); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__; self._traceback = tf_stack.extract_stack(). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 70, in get_tensor; self, compat.as_bytes(tensor_str)); RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1309, in restore; names_to_keys = object_graph_key_mapping(save_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1627, in object_graph_key_mapping; object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 74, in get_tensor; error_translator(e); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 35, in error_translator; raise errors_impl.NotFoundError(None, None, error_message); tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, anothe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:12365,checkpoint,checkpoint,12365,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['checkpoint'],['checkpoint']
Availability,"zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:2212,echo,echo,2212,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761,1,['echo'],['echo']
Availability,"}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2814,down,downloads,2814,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['down'],['downloads']
Availability,"~; | |; | PyObject * {aka struct _object *}; /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarrayobject.h:139:62: note: in definition of macro ‘PyArray_GETPTR1’; 139 | (i)*PyArray_STRIDES(obj)[0])); | ^~~; In file included from /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarrayobject.h:12,; from /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/arrayobject.h:5,; from pandas/_libs/src/ujson/python/JSONtoObj.c:42:; /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarraytypes.h:1526:38: note: expected ‘const PyArrayObject *’ {aka ‘const struct tagPyArrayObject_fields *’} but argument is of type ‘PyObject *’ {aka ‘struct _object *’}; 1526 | PyArray_STRIDES(const PyArrayObject *arr); | ~~~~~~~~~~~~~~~~~~~~~^~~; pandas/_libs/src/ujson/python/JSONtoObj.c:319:31: warning: passing argument 1 of ‘PyArray_SETITEM’ from incompatible pointer type [-Wincompatible-pointer-types]; 319 | PyArray_SETITEM(npyarr->ret, item, value) == -1) {; | ~~~~~~^~~~~; | |; | PyObject * {aka struct _object *}; In file included from /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/arrayobject.h:5,; from pandas/_libs/src/ujson/python/JSONtoObj.c:42:; /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarrayobject.h:292:32: note: expected ‘PyArrayObject *’ {aka ‘struct tagPyArrayObject_fields *’} but argument is of type ‘PyObject *’ {aka ‘struct _object *’}; 292 | PyArray_SETITEM(PyArrayObject *arr, char *itemptr, PyObject *v); | ~~~~~~~~~~~~~~~^~~; error: command '/usr/bin/gcc' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for pandas; ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859#issuecomment-2264852415:6571,error,error,6571,,https://github.com/google/deepvariant/issues/859#issuecomment-2264852415,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"“complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 4145",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2148,recover,recovery,2148,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['recover'],['recovery']
Deployability," # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2160,install,install,2160,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,4,['install'],['install']
Deployability," ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4270,update,update,4270,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,4,"['install', 'update', 'upgrade']","['install', 'update', 'upgrade']"
Deployability," +export PYTHON_VERSION=3.9; # shellcheck disable=SC2155; export PYTHON_BIN_PATH=""$(which python${PYTHON_VERSION})""; export PYTHON_LIB_PATH=""/usr/local/lib/python${PYTHON_VERSION}/dist-packages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:2378,Install,Install,2378,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['Install'],['Install']
Deployability," -89,18 +89,18 @@ export DV_GPU_BUILD=""${DV_GPU_BUILD:-0}""; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; -export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; +export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-0}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow""; -export DV_TF_NUMPY_VERSION=""1.19.2"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; +export DV_TF_NUMPY_VERSION=""1.24.1"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; ; # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}""; ; -export PYTHON_VERSION=3.8; +export PYTHON_VERSION=3.9; # shellcheck disable=SC2155; export PYTHON_BIN_PATH=""$(which python${PYTHON_VERSION})""; export PYTHON_LIB_PATH=""/usr/local/lib/python${PYTHON_VERSION}/dist-packages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:1202,install,installed,1202,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['install'],['installed']
Deployability," ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4268,install,install,4268,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability," /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14272,upgrade,upgrade,14272,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['upgrade'],['upgrade']
Deployability," 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: ‘STDOUT’. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10470,update,updates,10470,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['update'],['updates']
Deployability," 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:1488,Install,Install,1488,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Install'],['Install']
Deployability," > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10539,Install,Installing,10539,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,"['Install', 'install']","['Installing', 'installed']"
Deployability," > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1531,install,installed,1531,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['install'],['installed']
Deployability," EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2902,install,installed,2902,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installed']
Deployability," ImportError(msg); ImportError: Traceback (most recent call last):; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: /opt/software/GCCcore/6.4.0/lib64/libstdc++.so.6: version `CXXABI_1.3.11' not found (required by /mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. I upgraded to a higher version of GNU and re-ran but I got a nother error . ```; module load GNU/7.3.0-2.30. python $HOME/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/binaries/DeepVariant/0.7.2/DeepVariant-0.7.2+cl-225213413/make_examples.zip \; --mode training --reads ""${BAM}"" --ref ""${REF}"" --examples ""$training.tfrecord.gz"" \; --truth_variants ""${TRUTH_VCF}"" --confident_regions ""${TRUTH_BED}"" \; --exclude_regions ""chr20:14000000-15000000"" --sample_name ""train"" ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 41, in <module>; from deepvariant import pileup_image; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 42, in <module>; from third_party.nucleus.util import ranges; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 42, in <module>; from third_party.nucleus.io import bed; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453685106:2571,upgrade,upgraded,2571,,https://github.com/google/deepvariant/issues/137#issuecomment-453685106,1,['upgrade'],['upgraded']
Deployability," UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1876,install,installed,1876,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['install'],['installed']
Deployability," Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7619,release,release,7619,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['release'],['release']
Deployability," a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6637,Install,Install,6637,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Install'],['Install']
Deployability," again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:2719,release,releases,2719,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,2,"['install', 'release']","['installer-linux-', 'releases']"
Deployability," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:1443,pipeline,pipeline,1443,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872,1,['pipeline'],['pipeline']
Deployability," already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14112,upgrade,upgrade,14112,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['upgrade'],['upgrade']
Deployability," are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1532,install,installed,1532,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612,1,['install'],['installed']
Deployability," been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-561996442:1138,install,install,1138,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442,1,['install'],['install']
Deployability," caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3000,Install,Installing,3000,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Install'],['Installing']
Deployability," cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:15469,update,update,15469,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['update'],['update']
Deployability," etc. Juan Pablo Aguilar. ________________________________; From: Pi-Chuan Chang ***@***.***>; Sent: Friday, September 20, 2024 6:52:36 PM; To: google/deepvariant ***@***.***>; Cc: Aguilar Cabezas, Juan Pablo ***@***.***>; Mention ***@***.***>; Subject: [External] Re: [google/deepvariant] Retraining DeepVariant without trios data? (Issue #878). Use caution with links and attachments. Hi @desmodus1984<https://github.com/desmodus1984> ,. Fundamentally, training a DeepVariant requires truth data (truth variants and confident regions).; The core question here is: Would you be able to get truth data for the bats you're studying?. I quickly looked through your recent discussion with @kishwarshafin<https://github.com/kishwarshafin> .; I believe @kishwarshafin<https://github.com/kishwarshafin> has been trying to give you some tips on some ways to construct truth. Note that this is an advanced topic. We don't expect most of our users to train DeepVariant models, or to construct truth data. However, if you do have truth data (truth variants and confident regions), you should be able to follow the documentation https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md to try to train a model. From your description above, I still think the best way to proceed is to directly use DeepVariant release models. Once you have the callsets, try to evaluate the calls first. Even if you plan to train a model, it'll be good to have those baseline metrics available, so you know whether your trained model is working or not. Does this help? If I'm misunderstanding your question, let me know. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/878#issuecomment-2364726373>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AJWD2VJ75MSKTWY7ILOFVM3ZXSRLBAVCNFSM6AAAAABNXT6B26VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNRUG4ZDMMZXGM>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195:1848,release,release,1848,,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195,1,['release'],['release']
Deployability," help! thank you; ```; sh test0215.sh; I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****; time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory; Traceback (most recent call last):; File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/613#issuecomment-1431702886:2406,install,installed,2406,,https://github.com/google/deepvariant/issues/613#issuecomment-1431702886,1,['install'],['installed']
Deployability," numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 whi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3698,Install,Installing,3698,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Install'],['Installing']
Deployability," numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 whi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4724,Install,Installing,4724,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Install'],['Installing']
Deployability," on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461426712:1251,configurat,configurations,1251,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712,1,['configurat'],['configurations']
Deployability," popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}"" --no-install-recommends \; wget \; unzip; ; +apt-get install ""${APT_ARGS[@]}"" python3-apt; +cd /usr/lib/python3/dist-packages; +if [ -e apt_pkg.so ]; then; + rm apt_pkg.so; +fi; +ln -s apt_pkg.cpython-38-aarch64-linux-gnu.so apt_pkg.so; +cd -; +; +export PATH=/root/.local/bin/:$PATH; +apt-get install ""${APT_ARGS[@]}"" libcairo2-dev; +pip install pygobject; +apt-get install ""${APT_ARGS[@]}"" libgirepository1.0-dev; +pip install --upgrade pygobject; +sed -i 's/isAlive/is_alive/g' /usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py ; +; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_ARGS[@]}"" \; libllvm11 \; llvm-11 \; llvm-11-dev \; - llvm-11-linker-tools \; python3-dev \; zlib1g-dev; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:3922,install,install,3922,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,3,['install'],"['install', 'install-recommends']"
Deployability," required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3597,update,update,3597,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['update'],['update']
Deployability," response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1035,release,releases,1035,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['release'],['releases']
Deployability," results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****; # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed.; # perl: warning: Please check that your locale settings:; # 	LANGUAGE = (unset),; # 	LC_ALL = (unset),; # 	LC_CTYPE = ""C.UTF-8"",; # 	LANG = ""en_US.UTF-8""; # are supported and installed on your system.; # perl: warning: Falling back to the standard locale (""C"").; # perl: warning: Setting locale failed.; # perl: warning: Please check that your locale settings:; # 	LANGUAGE = (unset),; # 	LC_ALL = (unset),; # 	LC_CTYPE = ""C.UTF-8"",; # 	LANG = ""en_US.UTF-8""; # are supported and installed on your system.; # perl: warning: Falling back to the standard locale (""C"").; # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader; # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs; # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_alig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946:2583,install,installed,2583,,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946,1,['install'],['installed']
Deployability," running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1663,Install,Installing,1663,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,"['Install', 'install']","['Installing', 'installation']"
Deployability," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4344,Install,Install,4344,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,"['Install', 'install']","['Install', 'installed']"
Deployability," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5370,Install,Install,5370,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,"['Install', 'install']","['Install', 'installed']"
Deployability," the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```; ...; bazel-bin/deepvariant/postprocess_variants; bazel-bin/deepvariant/postprocess_variants.zip; bazel-bin/deepvariant/runtime_by_region_vis; bazel-bin/deepvariant/runtime_by_region_vis.zip; bazel-bin/deepvariant/show_examples; bazel-bin/deepvariant/show_examples.zip; b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1595,install,installation,1595,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104,1,['install'],['installation']
Deployability," this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1636,install,installation,1636,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612,1,['install'],['installation']
Deployability," this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1031,Install,Install,1031,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['Install'],['Install']
Deployability," we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2689,Install,Installing,2689,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,"['Install', 'install']","['Installing', 'installation']"
Deployability," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480642492:2285,Update,Update,2285,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492,1,['Update'],['Update']
Deployability,"""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json""; {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```. ```bash; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took `5m25.905s`. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. # This parts starts shuffling... ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:4248,install,install,4248,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,1,['install'],['install']
Deployability,"""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4963,Install,Install,4963,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Install'],['Install']
Deployability,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash; gcloud compute ssh pichuan-test --zone ""us-west1-b""; ```. Check the Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:739,release,release,739,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['release'],['release']
Deployability,"# Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16488,install,install,16488,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,"# share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8220,install,install,8220,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,"############################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash; # check out source code; git clone https://github.com/google/deepvariant.git; cd deepvariant; # fetch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16954,install,install,16954,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,"#####################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archiv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:20887,patch,patch,20887,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['patch'],['patch']
Deployability,$VERSION_ID); curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \; sudo tee /etc/yum.repos.d/nvidia-docker.repo; sudo yum install -y nvidia-docker2; semanage fcontext -a -f f -t container_runtime_exec_t -s system_u /usr/bin/nvidia-docker; sudo restorecon -v /usr/bin/nvidia-docker. # start docker; sudo systemctl start docker; sudo systemctl status docker; sudo systemctl enable docker. # install deps; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y epel-release && \; sudo yum install -y golang openssl-devel libuuid-devel libseccomp-devel squashfs-tools; echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \; echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \; source ~/.bashrc. # install singularity; mkdir -p ${GOPATH}/src/github.com/sylabs && \; cd ${GOPATH}/src/github.com/sylabs && \; git clone https://github.com/sylabs/singularity.git && \; cd singularity; git checkout v3.1.1; cd ${GOPATH}/src/github.com/sylabs/singularity && \; ./mconfig && \; cd ./builddir && \; make && \; sudo make install; ; DVVER=0.8.0; # make deepvariant CPU image; sudo docker pull gcr.io/deepvariant-docker/deepvariant:${DVVER}; sudo docker tag gcr.io/deepvariant-docker/deepvariant:${DVVER} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; singularity build --nohttps deepvariant.${DVVER}.simg docker://localhost:5000/deepvariant:latest; ; # make deepvariant GPU image; sudo nvidia-docker pull gcr.io/deepvariant-docker/deepvariant_gpu:${DVVER}; sudo nvidia-docker tag gcr.io/deepvariant-docker/deepvariant_gpu:${DVVER} localhost:5000/deepvariant_gpu:latest; sudo nvidia-docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo nvidia-docker push localhost:5000/deepvariant_gpu:latest; singularity build --nohttps deepvariant_gpu.${DVVER}.simg docker://localhost:5000/deepvariant_gpu:latest; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482761898:1656,install,install,1656,,https://github.com/google/deepvariant/issues/132#issuecomment-482761898,1,['install'],['install']
Deployability,"${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1157,Update,Update,1157,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Update'],['Update']
Deployability,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:10142,integrat,integrate,10142,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,1,['integrat'],['integrate']
Deployability,"(1) Have you considered using docker instead of the prebuilt binaries? In this version, we updated the quick start and case studies to use docker. I personally find that much more convenient. ; (2) For bazel, can you try the official instructions: https://docs.bazel.build/versions/master/install-ubuntu.html; Last time I tried our script on a Ubuntu 16 machine on GCP, it worked. But maybe I should try one not on GCP.; If you find out a more robust installation for bazel, please share and I can update our instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/92#issuecomment-418171330:91,update,updated,91,,https://github.com/google/deepvariant/issues/92#issuecomment-418171330,4,"['install', 'update']","['install-ubuntu', 'installation', 'update', 'updated']"
Deployability,(We'll add @kishwarshafin 's answer in our clarification in our FAQ in the next release!),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/828#issuecomment-2163501191:80,release,release,80,,https://github.com/google/deepvariant/issues/828#issuecomment-2163501191,1,['release'],['release']
Deployability,"(modified from earlier response); 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file; * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai; ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149#issuecomment-461157985:785,configurat,configuration,785,,https://github.com/google/deepvariant/issues/149#issuecomment-461157985,3,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################################################; # //deepvariant/labeler:haplotype_labeler_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ##########################################################################; # fail due to mock data, open an issue in github; https://github.com/google/deepvariant/issues/154. ##########################################################################; # //deepvariant:make_examples_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21715,install,install,21715,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared librar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5779,install,install,5779,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6418,Update,Update,6418,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Update'],['Update']
Deployability,"*** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader; W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs; I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:3325,install,installed,3325,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113,1,['install'],['installed']
Deployability,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:15,Install,Installation,15,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,4,"['Install', 'install']","['Installation', 'install', 'installed']"
Deployability,"++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_TF_NUMPY_VERSION=1.19.2; ++ DV_TF_NUMPY_VERSION=1.19.2; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; ++ export PYTHON_VERSION=3.8; ++ PYTHON_VERSION=3.8; +++ which python3.8; ++ export PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8; ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; + bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Ins",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:3407,release,release,3407,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['release'],['release']
Deployability,"+1 for work being done. Thanks!. I cannot use the binaries:. ```; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so); ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-390375949:313,release,release,313,,https://github.com/google/deepvariant/issues/29#issuecomment-390375949,1,['release'],['release']
Deployability,", I changed these lines:; https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194; to:; ```; parsed = tf.io.parse_example(serialized=tf_example,; features=self.feature_extraction_spec); image = parsed['image/encoded']; if self.tensor_shape:; image = tf.io.decode_raw(image, tf.uint8); image = tf.reshape(image, [-1]+self.tensor_shape); ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:; (1) Accuracy is the same for 4 models - this is expected.; (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina); ```; real 114m10.723s; real 193m19.324s; real 74m39.826s; ```; ## WES (Illumina); ```; real 7m26.571s; real 1m24.083s; real 1m3.679s; ```. ## PacBio (HiFi); ```; real 126m28.198s; real 175m40.960s; real 67m49.753s; ```; ## Hybrid (Illumina + PacBio HiFi); ```; real 161m32.681s; real 200m26.225s; real 63m20.731s; ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:; (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers.; (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479#issuecomment-905684335:2117,update,update,2117,,https://github.com/google/deepvariant/issues/479#issuecomment-905684335,1,['update'],['update']
Deployability,", in case it's useful to anyone else. The instructions were based on [the notes](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) from @pichuan above, but includes instructions for installing docker, nvidia-docker, and singularity, as well as parameters specific to singularity v3.1.1. ```; # install docker; sudo yum check-update; curl -fsSL https://get.docker.com/ | sh. # install nvidia-docker; distribution=$(. /etc/os-release;echo $ID$VERSION_ID); curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \; sudo tee /etc/yum.repos.d/nvidia-docker.repo; sudo yum install -y nvidia-docker2; semanage fcontext -a -f f -t container_runtime_exec_t -s system_u /usr/bin/nvidia-docker; sudo restorecon -v /usr/bin/nvidia-docker. # start docker; sudo systemctl start docker; sudo systemctl status docker; sudo systemctl enable docker. # install deps; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y epel-release && \; sudo yum install -y golang openssl-devel libuuid-devel libseccomp-devel squashfs-tools; echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \; echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \; source ~/.bashrc. # install singularity; mkdir -p ${GOPATH}/src/github.com/sylabs && \; cd ${GOPATH}/src/github.com/sylabs && \; git clone https://github.com/sylabs/singularity.git && \; cd singularity; git checkout v3.1.1; cd ${GOPATH}/src/github.com/sylabs/singularity && \; ./mconfig && \; cd ./builddir && \; make && \; sudo make install; ; DVVER=0.8.0; # make deepvariant CPU image; sudo docker pull gcr.io/deepvariant-docker/deepvariant:${DVVER}; sudo docker tag gcr.io/deepvariant-docker/deepvariant:${DVVER} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; singularity build --nohttps deepvariant.${DVVER}.simg docker://localhost:5000/deepvariant:latest; ; # make deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482761898:1001,install,install,1001,,https://github.com/google/deepvariant/issues/132#issuecomment-482761898,4,"['install', 'release']","['install', 'release']"
Deployability,"- description: Started pulling ""google/cloud-sdk:alpine""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: google/cloud-sdk:alpine; timestamp: '2018-11-08T14:28:05.897359Z'; - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:28:05.747135Z'; - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:27:34.961215Z'; - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7""; assigned in ""us-west1-b""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {};",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:7409,pipeline,pipelines-worker-,7409,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['pipeline'],['pipelines-worker-']
Deployability,"--------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:1864,install,install,1864,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553,1,['install'],['install']
Deployability,-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbi,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:3467,install,installation,3467,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['installation']
Deployability,"-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4724,install,installed,4724,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['installed']
Deployability,"-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1153,install,install,1153,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236,1,['install'],['install']
Deployability,"-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unless you explicitly specify; that.; ```. Do you have any idea what is causing the problem and how t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:2966,install,install,2966,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,1,['install'],['install']
Deployability,".19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dyna",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6886,install,installed,6886,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installed']
Deployability,".42.02) on the server is using cuda version 12.5. ```; nvidia-smi; +-----------------------------------------------------------------------------------------+; | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 |; |-----------------------------------------+------------------------+----------------------+; | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |=========================================+========================+======================|; | 0 Tesla V100-PCIE-16GB Off | 00000000:00:07.0 Off | 0 |; | N/A 28C P0 35W / 250W | 1MiB / 16384MiB | 0% Default |; | | | N/A |; +-----------------------------------------+------------------------+----------------------+; ; +-----------------------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=========================================================================================|; | No running processes found |; +-----------------------------------------------------------------------------------------+. ```. Should I update the driver, cuda or both? Also, installed on the server is cuda version 12.3, so I'm feeling a bit confused as to where the 12.5 comes from. ```; nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Wed_Nov_22_10:17:15_PST_2023; Cuda compilation tools, release 12.3, V12.3.107; Build cuda_12.3.r12.3/compiler.33567101_0; ```; It's a right mess! Sometimes call_variants uses gpu and at other times it stalls. That's why I'm now running the commands separately, I have a lot of outputs from make_examples. Our IT support say they're happy the gpu works part of the time :) It's adding a lot of extra work, and I'm trying to come up with a solution. Since it's completely unrelated to the topic, perhaps I should create a new issue instead?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2222332558:1352,update,update,1352,,https://github.com/google/deepvariant/issues/849#issuecomment-2222332558,3,"['install', 'release', 'update']","['installed', 'release', 'update']"
Deployability,".list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4987,upgrade,upgraded,4987,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; #",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4677,install,install,4677,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['install']
Deployability,"/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolcha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2455,install,install-compile-source,2455,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install-compile-source']
Deployability,"/deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```; ...; bazel-bin/deepvariant/postprocess_variants; bazel-bin/deepvariant/postprocess_variants.zip; bazel-bin/deepvariant/runtime_by_region_vis; bazel-bin/deepvariant/runtime_by_region_vis.zip; bazel-bin/deepvariant/show_examples; bazel-bin/deepvariant/show_examples.zip; bazel-bin/deepvariant/vcf_stats_report; bazel-bin/deepvariant/vcf_stats_report.zip; (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s; (09:13:54) INFO: 47 processes: 1 internal, 46 local.; (09:13:54) INFO: Build completed successfully, 47 total actions; $; ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```; $ ls bazel-bin/deepvariant/ ; call_variants; call_variants_keras; call_variants_keras.temp; call_variants_keras.zip; call_variants_keras.zip-0.params; call_variants.temp; call_variants.zip; call_variants.zip-0.params; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:2155,patch,patching,2155,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104,1,['patch'],['patching']
Deployability,"/home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4464,install,install,4464,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['install'],['install']
Deployability,/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:3547,install,installation,3547,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['installation']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:99858,install,install,99858,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:13943,install,install,13943,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running); (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log); (06:29:09) INFO: From Testing //deepvariant:call_variants_test:; ==================== Test output for //deepvariant:call_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>; import tensorflow as ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:16401,install,install,16401,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:18638,install,install,18638,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running); (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_trai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:20793,install,install,20793,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>; from third_party.nucleus.io",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:32375,install,install,32375,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_roo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:25160,install,install,25160,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log); (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:27976,install,install,27976,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log); (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:; ==================== Test output for //deepvariant:tf_utils_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disabl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:23085,install,install,23085,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running); (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log); (06:29:10) INFO: From Testing //deepvariant:data_providers_test:; ==================== Test output for //deepvariant:data_providers_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>; import tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:30131,install,install,30131,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:35255,install,install,35255,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:37410,install,install,37410,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:41851,install,install,41851,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:44260,install,install,44260,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running); (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:39565,install,install,39565,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:50843,install,install,50843,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:48694,install,install,48694,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running); (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:46409,install,install,46409,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:57430,install,install,57430,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:55281,install,install,55281,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running); (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:52996,install,install,52996,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:59579,install,install,59579,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:64020,install,install,64020,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running); (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_trai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:61728,install,install,61728,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:66175,install,install,66175,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:68330,install,install,68330,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:70485,install,install,70485,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/mode",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:77029,install,install,77029,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:74592,install,install,74592,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:; ==================== Test output for //deepvariant/python:allelecounter_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:82352,install,install,82352,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:80013,install,install,80013,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:; ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>; from third_part",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:84846,install,install,84846,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:; ==================== Test output for //deepvariant/realigner:window_selector_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:89697,install,install,89697,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running); (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:; ==================== Test output for //deepvariant/realigner:realigner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:87417,install,install,87417,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log); (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:; ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:94594,install,install,94594,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:97088,install,install,97088,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log); (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:; ==================== Test output for //deepvariant:pileup_image_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:92191,install,install,92191,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>; from third_party.nucleus.io import vcf; File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:103032,install,install,103032,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log); (06:29:20) INFO: From Testing //deepvariant:modeling_test:; ==================== Test output for //deepvariant:modeling_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disabl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:109972,install,install,109972,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:105888,install,install,105888,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log); (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:; ==================== Test output for //deepvariant/python:variant_calling_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:112047,install,install,112047,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log); (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:; ==================== Test output for //deepvariant:postprocess_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:114555,install,install,114555,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:116702,install,install,116702,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['install']
Deployability,"/python3; + cd; + rm -rf clif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820:1843,INSTALL,INSTALL,1843,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820,1,['INSTALL'],['INSTALL']
Deployability,"/usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7522,install,install,7522,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,"/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4364,install,installed,4364,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installed']
Deployability,"0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14573,upgrade,upgrade,14573,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['upgrade'],['upgrade']
Deployability,"0:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires nu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3085,install,installed,3085,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installed']
Deployability,"0W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --version; singularity version 3.7.0; ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```; # Pull the image.; BIN_VERSION=1.5.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:2455,release,release,2455,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553,1,['release'],['release']
Deployability,"0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:9480,install,install,9480,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead).; In the `WORKSPACE` file of DeepVariant, you can see this at the bottom.; I tried changing the path:; ```; new_local_repository(; name = ""clif"",; build_file = ""third_party/clif.BUILD"",; path = ""/home/pichuan"",; ); ```. And I make sure the two files are there:; ```; $ ls /home/pichuan/clif/bin/; pyclif pyclif_proto; ```; After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:; ```; # Import tensorflow. Note path.; local_repository(; name = ""org_tensorflow"",; path = ""../tensorflow"",; ); ```; So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386752411:748,install,installing,748,,https://github.com/google/deepvariant/issues/29#issuecomment-386752411,2,['install'],"['install', 'installing']"
Deployability,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-749327121:10,install,installation,10,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121,5,"['configurat', 'install', 'release']","['configuration', 'install', 'install-using-the-repository', 'installation', 'released']"
Deployability,"1. The type of the data is ONT_R9 simplex.; 2. The basecaller is guppy. ; 3. Threads are set to 48.; I also provide the code as below. Thank you for your help. ; `BIN_VERSION=""1.6.1""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; INPUT_DIR=""/home/user/Masaüstü/BEYZA/input"" OUTPUT_DIR=""/home/user/Masaüstü/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104""; sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" –output_vcf=""/output/deep.vcf"" --threads=48; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814#issuecomment-2089912166:196,update,update,196,,https://github.com/google/deepvariant/issues/814#issuecomment-2089912166,2,"['install', 'update']","['install', 'update']"
Deployability,"10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2849,release,releases,2849,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['release'],['releases']
Deployability,"100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7303,patch,patch-,7303,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,3,"['install', 'patch', 'release']","['installation', 'patch-', 'release']"
Deployability,"120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:9388,install,install,9388,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,"1:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1714,install,installed,1714,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['install'],['installed']
Deployability,"1:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7320,install,installed,7320,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['installed']
Deployability,"1}""; +export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-0}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow""; -export DV_TF_NUMPY_VERSION=""1.19.2"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; +export DV_TF_NUMPY_VERSION=""1.24.1"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; ; # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}""; ; -export PYTHON_VERSION=3.8; +export PYTHON_VERSION=3.9; # shellcheck disable=SC2155; export PYTHON_BIN_PATH=""$(which python${PYTHON_VERSION})""; export PYTHON_LIB_PATH=""/usr/local/lib/python${PYTHON_VERSION}/dist-packages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_ve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:1750,upgrade,upgraded,1750,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['upgrade'],['upgraded']
Deployability,"22 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4689,Install,Install,4689,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,['Install'],"['Install', 'Installing']"
Deployability,"362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.ta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1512,update,update,1512,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,2,"['install', 'update']","['install', 'update']"
Deployability,"3:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires nu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2059,install,installed,2059,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['install'],['installed']
Deployability,"4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9698,upgrade,upgraded,9698,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"6_64-linux-gnu/libdl.so.2 (0x0000155553032000); 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000); 	libcublas.so.12 => not found; 	libcublasLt.so.12 => not found; 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000); 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000); 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000); 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000); 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000); 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000); 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000); ```. When I grep for `libcublas` in the container:; ```stdout; [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}""; *** bunch more omitted output. I just wanted to show above versions ***; ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you!. Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,; Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060:3570,install,install,3570,,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060,1,['install'],['install']
Deployability,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3701,Configurat,Configuration,3701,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['Configurat'],['Configuration']
Deployability,"8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10381,install,installed,10381,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installed']
Deployability,"81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dyna",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6992,install,installed,6992,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installed']
Deployability,"8_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 1; exitStatus: 1; stderr: |+; /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:4053,pipeline,pipeline,4053,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['pipeline'],['pipeline']
Deployability,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:1764,update,update,1764,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040,1,['update'],['update']
Deployability,":1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8881,upgrade,upgraded,8881,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,":31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7142,release,release,7142,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['release'],['release']
Deployability,"; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7137,install,installed,7137,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['installed']
Deployability,"; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 memory:90000000-90ffffff memory:91800000-91803fff memory:91000000-917fffff memory:c0000-dffff; ```. thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:2797,configurat,configuration,2797,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205,1,['configurat'],['configuration']
Deployability,"; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), don",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5374,Install,Install,5374,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Install'],['Install']
Deployability,"; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; Distributor ID: RedHatEnterpriseServer; Description: Red Hat Enterprise Linux Server release 7.9 (Maipo); Release: 7.9; Codename: Maipo; ```. 3. I was running it on the interactive mode of HPC with 2 threads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/497#issuecomment-993155459:1459,release,release,1459,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459,3,"['Release', 'release']","['Release', 'release']"
Deployability,"; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6294,release,releases,6294,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['release'],['releases']
Deployability,"; mock: 3.0.5-py27_0 conda-forge; ncurses: 6.1-hf484d3e_1002 conda-forge; numpy: 1.14.6-py27h95a1406_1201 conda-forge; oauth2client: 1.5.2-py27_0 bioconda ; openjdk: 8.0.192-h14c3975_1003 conda-forge; openssl: 1.1.1d-h516909a_0 conda-forge; parallel: 20160622-1 bioconda ; pbr: 5.4.2-py_0 conda-forge; perl: 5.26.2-h516909a_1006 conda-forge; perl-threaded: 5.26.0-0 bioconda ; pip: 19.3.1-py27_0 conda-forge; prettytable: 0.7.2-py_3 conda-forge; protobuf: 3.11.1-py27he1b5a44_0 conda-forge; psutil: 5.6.7-py27h516909a_0 conda-forge; pyasn1: 0.4.8-py_0 conda-forge; pyasn1-modules: 0.2.7-py_0 conda-forge; pycparser: 2.19-py27_1 conda-forge; pyopenssl: 19.1.0-py27_0 conda-forge; pyparsing: 2.4.5-py_0 conda-forge; pyperclip: 1.7.0-py_0 conda-forge; pysocks: 1.7.0-py27_0 conda-forge; python: 2.7.15-h5a48372_1009 conda-forge; pyyaml: 5.2-py27h516909a_0 conda-forge; readline: 8.0-hf8c457e_0 conda-forge; requests: 2.22.0-py27_1 conda-forge; rsa: 3.1.4-py27_0 bioconda ; scipy: 1.2.1-py27h921218d_2 conda-forge; setuptools: 42.0.2-py27_0 conda-forge; six: 1.13.0-py27_0 conda-forge; sortedcontainers: 2.1.0-py_0 conda-forge; sqlite: 3.30.1-hcee41ef_0 conda-forge; stevedore: 1.30.1-py_0 conda-forge; subprocess32: 3.5.4-py27h516909a_0 conda-forge; tensorboard: 1.12.0-py27_1000 conda-forge; tensorflow: 1.12.0-gpu_py27h2a0f108_0 ; tensorflow-base: 1.12.0-gpu_py27had579c0_0 ; tensorflow-estimator: 1.13.0-py_0 ; termcolor: 1.1.0-py_2 conda-forge; tk: 8.6.10-hed695b0_0 conda-forge; traceback2: 1.4.0-py27_0 conda-forge; unicodecsv: 0.14.1-py_1 conda-forge; unittest2: 1.1.0-py_0 conda-forge; urllib3: 1.25.7-py27_0 conda-forge; wcwidth: 0.1.7-py_1 conda-forge; werkzeug: 0.16.0-py_0 conda-forge; wheel: 0.33.6-py27_0 conda-forge; xz: 5.2.4-h14c3975_1001 conda-forge; yaml: 0.2.2-h516909a_1 conda-forge; zlib: 1.2.11-h516909a_1006 conda-forge; ```; I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566427577:4257,install,installed,4257,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577,3,['install'],"['install', 'installed']"
Deployability,"==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash; # check out source code; git clone https://github.com/google/deepvariant.git; cd deepvariant; # fetch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third_party/clif.bzl. # Build and test; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; export BAZEL_PYTHON=/home/qilibj/inst/bin/python; export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11""; # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:17403,install,install,17403,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"===== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7248,release,release,7248,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['release'],['release']
Deployability,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; > ; > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault""; > ; > **Setup**; > ; > * Operating system: Ubuntu 22.04.2 LTS; > * DeepVariant version: 1.6.1; > * Installation method (Docker, built from source, etc.): Docker; > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format.; > ; > **Steps to reproduce:**; > ; > * Command: sudo docker run ; > -v ""${INPUT_DIR}"":""/input"" ; > -v ""${OUTPUT_DIR}"":""/output"" ; > google/deepvariant:""${BIN_VERSION}"" ; > /opt/deepvariant/bin/run_deepvariant ; > --model_type=PACBIO ; > --ref=/input/RILWLs1.fasta ; > --reads=/input/Out.fastq ; > --output_vcf=/output/output.vcf.gz ; > --output_gvcf=/output/output.g.vcf.gz ; > --intermediate_results_dir /output/intermediate_results_dir ; > --num_shards=15; > * Error trace: (if applicable); > ; > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/807#issuecomment-2060514080:291,Install,Installation,291,,https://github.com/google/deepvariant/issues/807#issuecomment-2060514080,1,['Install'],['Installation']
Deployability,> . Thanks for answering. I forgot to say that the samtools are already installed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/579#issuecomment-1293113502:72,install,installed,72,,https://github.com/google/deepvariant/issues/579#issuecomment-1293113502,1,['install'],['installed']
Deployability,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment.; > ; > ```; > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0; > binaries models ; > ```; > ; > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:; > ; > ```; > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>; > ```; > ; > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions!; > ; > @prabal97 could you share the error you are seeing?. I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-568349835:21,install,installing,21,,https://github.com/google/deepvariant/issues/252#issuecomment-568349835,1,['install'],['installing']
Deployability,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-737655909:17,update,update,17,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909,2,"['release', 'update']","['release', 'update']"
Deployability,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:; > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735450709:414,Update,Updated,414,,https://github.com/google/deepvariant/pull/363#issuecomment-735450709,1,['Update'],['Updated']
Deployability,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:38,install,install,38,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761,1,['install'],['install']
Deployability,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work.; I need to debug some parts of this project to better understand it, so I have to build it from source.; Do you mean there is a route to build DV using Docker or Singularity?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277:38,install,install,38,,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277,1,['install'],['install']
Deployability,"> @pichuan Thank you for the quick reply. That's a good tip to bypass the insert_size channel by default. Looking forward to your update, much appreciated.; > ; > If I understand correctly, each additional channel requires building a model using examples containing those channels. As a hypothetical example, to use `insert_size` + `allele_frequency` + `avg_base_quality` during variant calling would require re-training, correct?. Yes. The make_examples stage will need to create examples that are consistent with the model.ckpt used in call_variants. Because the model.ckpt was already trained with a specific list of channels and shape. So, if you create different examples - even if it's just to remove a channel, you're suppose to retrain on examples that are made with that channel removed as well. > ; > I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:130,update,update,130,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213,1,['update'],['update']
Deployability,"> @pioneer-pi ,; > ; > The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker.; > ; > If you are starting a container then DeepVariant binaries should be in:; > ; > ```shell; > /opt/deepvariant/bin/; > ```. I know it, but in /opt/deepvariant/bin/, The source code is different from the code in github. I want to change the source of [pileup](https://github.com/google/deepvariant/blob/r1.6/deepvariant/pileup_image_native.cc), but in container, this code have been a .so file. So I need to build it. The reason why I want to build it in docker container is that I have no root in my local machine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818125107:34,install,installation,34,,https://github.com/google/deepvariant/issues/737#issuecomment-1818125107,4,['install'],"['install', 'installation', 'installed', 'installing']"
Deployability,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8.; > ; > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727#issuecomment-2307619528:50,install,installed,50,,https://github.com/google/deepvariant/issues/727#issuecomment-2307619528,2,['install'],"['install', 'installed']"
Deployability,"> Do you recommend read trimming before alignment using tools such as fastp?. No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/791#issuecomment-1997884080:767,update,updated,767,,https://github.com/google/deepvariant/issues/791#issuecomment-1997884080,1,['update'],['updated']
Deployability,> From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually.; > ; > To generate a VCF:; > ; > ```; > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa; > ```; > ; > To generate a g.VCF:; > ; > ```; > /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz; > ```; > ; > These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run. This suggestion works for me. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/431#issuecomment-808287669:156,pipeline,pipeline,156,,https://github.com/google/deepvariant/issues/431#issuecomment-808287669,1,['pipeline'],['pipeline']
Deployability,"> Hello,; > ; > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docum",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:570,install,install,570,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,3,['install'],"['install', 'installed']"
Deployability,"> Hi @aderzelle; > ; > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position.; > ; > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); > ; > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? ; Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/257#issuecomment-577247342:547,release,release,547,,https://github.com/google/deepvariant/issues/257#issuecomment-577247342,1,['release'],['release']
Deployability,"> Hi @gambalab,; > ; > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts.; > ; > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:; > . thank you! this is a great solution for me.; you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1604068307:98,install,installed,98,,https://github.com/google/deepvariant/issues/669#issuecomment-1604068307,3,['install'],"['install', 'installation', 'installed']"
Deployability,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position).; > ; > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/).; > ; > May I ask what is your use case that prefers every non-variant position to be written?; > ; > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,; David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/282#issuecomment-966408955:922,configurat,configuration-and-analysis,922,,https://github.com/google/deepvariant/issues/282#issuecomment-966408955,1,['configurat'],['configuration-and-analysis']
Deployability,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in.; > ; > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1661420506:63,update,update,63,,https://github.com/google/deepvariant/issues/689#issuecomment-1661420506,3,"['continuous', 'update']","['continuous', 'update', 'updates']"
Deployability,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840:1253,release,release,1253,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840,2,['release'],['release']
Deployability,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:881,install,installed,881,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240,1,['install'],['installed']
Deployability,"> Hi @themkdemiiir,; > ; > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker.; > ; > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard.; > * call_variants will be run with the same number of shards.; > * postprocess_variants has to be run in a single process.; > ; > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:; > ; > ```; > bin/make_examples \; > --examples /tmn/your_examples.tfrecord@200.gz \; > --mode calling \; > --reads /tmp/your_input_bam.bam \; > --realign_reads \; > --ref=/tmp/your_reference.fna \; > --task=11; > ; > # Input for each instance of call_variants is the output of one instance of make_examples:; > bin/call_variants.par \; > --batch_size=32 \; > --checkpoint <Path to the model checkpoint or saved model>.ckpt \; > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz; > ; > # Input for for postprocess would be the output of all instances of call_variants:; > /tmp/your_call_variants_output.cvo.tfrecord@200.gz; > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/744#issuecomment-1855569624:151,pipeline,pipeline,151,,https://github.com/google/deepvariant/issues/744#issuecomment-1855569624,1,['pipeline'],['pipeline']
Deployability,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-614524936:16,install,install,16,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936,4,['install'],"['install', 'installation', 'installed']"
Deployability,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```; sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; ```; It would be safer to replace with something like this:; ```patch; diff --git a/Dockerfile b/Dockerfile; index 0432fd8..a57364d 100644; --- a/Dockerfile; +++ b/Dockerfile; @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*; # Convert model to OpenVINO format; RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; python3 -m pip install networkx defusedxml test-generator==0.1.1; \; - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \; for model in wgs wes pacbio hybrid_pacbio_illumina; do \; cd /opt/models/${model}; \; @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; --scale 128; \; rm model.pb; \; done \; + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; fi; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736278644:142,release,release,142,,https://github.com/google/deepvariant/pull/363#issuecomment-736278644,3,"['install', 'patch', 'release']","['install', 'patch', 'release']"
Deployability,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?. Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:; For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though.; Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either.; I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735915820:654,release,release,654,,https://github.com/google/deepvariant/pull/363#issuecomment-735915820,1,['release'],['release']
Deployability,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896293199:268,update,updated,268,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199,6,"['install', 'update']","['installed', 'installs', 'update', 'updated']"
Deployability,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this.; > ; > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,; Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1781774874:545,release,released,545,,https://github.com/google/deepvariant/issues/722#issuecomment-1781774874,1,['release'],['released']
Deployability,"@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}"" --no-install-recommends \; wget \; unzip; ; +apt-get install ""${APT_ARGS[@]}"" python3-apt; +cd /usr/lib/python3/dist-packages; +if [ -e apt_pkg.so ]; then; + rm apt_pkg.so; +fi; +ln -s apt_pkg.cpython-38-aarch64-linux-gnu.so apt_pkg.so; +cd -; +; +export PATH=/root/.local/bin/:$PATH; +apt-get install ""${APT_ARGS[@]}"" libcairo2-dev; +pip install pygobject; +apt-get install ""${APT_ARGS[@]}"" libgirepository1.0-dev; +pip install --upgrade pygobject; +sed -i 's/isAlive/is_alive/g' /usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py ; +; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_ARGS[@]}"" \; libllvm11 \; llvm-11 \; llvm-11-dev \; - llvm-11-linker-tools \; python3-dev \; zlib1g-dev; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/cmake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <27>(char const (&) [27])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10L",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:4368,install,install,4368,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct?. Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-471716420:227,Update,Update,227,,https://github.com/google/deepvariant/pull/159#issuecomment-471716420,1,['Update'],['Update']
Deployability,"@A-Tsai In the next release, we plan to add a flag to `call_variants` that will allow users to pass in any desired configuration options for the TensorFlow session config. We won't hard code any options, but you will be able to pass in all of the options included in this pull request. The code will be slightly different from what you have specified as the session configuration will be passed to [the estimator](https://github.com/google/deepvariant/blob/5e6fe205b984c6be116dcacafdfd83ce1df4d2e9/deepvariant/call_variants.py#L344), rather than [this code block](https://github.com/google/deepvariant/blob/5e6fe205b984c6be116dcacafdfd83ce1df4d2e9/deepvariant/call_variants.py#L330). Thanks for the suggested changes!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-472215526:20,release,release,20,,https://github.com/google/deepvariant/pull/159#issuecomment-472215526,3,"['configurat', 'release']","['configuration', 'release']"
Deployability,"@A-Tsai Of course you can especially with `pipe()`, which can take a script/command as input. At the least, you have the following two options to play with:. 1. If you use pipe to launch a script, then you can launch the application through `taskset` to limit the number of cores at launch-time. Here's the options for launching with an example: . _*Options to launch a program*_: `taskset [options] mask command [argument...]`. _*Example to use only 2 specific cores*_: `taskset -c 0,2 python ~/loop.py`. The `pipe(...)` command [as defined in the RDD base-class](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) takes an external command, which in this case would be `taskset -c core-list your-program`:. ![rdd-pipe](https://user-images.githubusercontent.com/6555937/44763439-4c73f500-ab19-11e8-99d7-99adac28c913.png). 2. In Spark, you can also limit the number of cores per task in Spark through the `spark.task.cpus` setting. You probably want to set `spark.cores.max`, and not change the `spark.executor.cores` and `spark.driver.cores`. The Spark config page explains everything in more detail here: . https://spark.apache.org/docs/latest/configuration.html. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-416812290:1179,configurat,configuration,1179,,https://github.com/google/deepvariant/issues/90#issuecomment-416812290,1,['configurat'],['configuration']
Deployability,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-417185546:974,configurat,configurations-spark-application,974,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546,1,['configurat'],['configurations-spark-application']
Deployability,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/748#issuecomment-1853175383:116,patch,patch,116,,https://github.com/google/deepvariant/issues/748#issuecomment-1853175383,1,['patch'],['patch']
Deployability,"@ASLeonard , I have pushed the updated code in these two docker:. ```bash; google/deepvariant:CL590726281; google/deepvariant:CL590726281-gpu; ```. Hopefully this will unblock you for now. I will assess the feasibility of updating the current docker or providing a v1.6.1 with this fix. Thank you for reporting this issue, it was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/748#issuecomment-1856940006:31,update,updated,31,,https://github.com/google/deepvariant/issues/748#issuecomment-1856940006,1,['update'],['updated']
Deployability,"@ASLeonard . The observed warning is not correct and is an issue we are working on fixing. Currently, DeepTrio will report that warning for all models except for WES. However, it does not impact DeepTrio inference. We are also planning on adding OpenVINO support to the next release of DeepTrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/416#issuecomment-772180895:275,release,release,275,,https://github.com/google/deepvariant/issues/416#issuecomment-772180895,1,['release'],['release']
Deployability,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-1281192222:70,update,update,70,,https://github.com/google/deepvariant/issues/115#issuecomment-1281192222,2,"['release', 'update']","['released', 'update']"
Deployability,"@AndrewCarroll ; Thanks for the answer.; We're in a bit of a time crunch now (operations), but knowing how to run that experiment will be super helpful!. @amwenger ; Right, `pbmm2` (used in our pipeline) doesn't provide the `BQ:Z` tag.; However, we also run `samtools calmd` on the BAM to generate the `MD:Z` tag. And `calmd` allows one to compute the BAQ by turning on the `-r` flag (off in our pipeline now). But as you can imagine, it will not be negligible compute.; Hence we are interested in doing some experiments to see if DV can benefit from this tag. Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/565#issuecomment-1252802626:194,pipeline,pipeline,194,,https://github.com/google/deepvariant/issues/565#issuecomment-1252802626,2,['pipeline'],['pipeline']
Deployability,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:492,release,release,492,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,1,['release'],['release']
Deployability,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/634#issuecomment-1595873180:137,update,update,137,,https://github.com/google/deepvariant/issues/634#issuecomment-1595873180,3,"['install', 'update']","['install', 'update', 'updated']"
Deployability,"@Carl-labhub, in this case, you are installing `nucleus` as user but DeepVariant is installed differently? Can you provide the full command for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2073722594:36,install,installing,36,,https://github.com/google/deepvariant/issues/812#issuecomment-2073722594,2,['install'],"['installed', 'installing']"
Deployability,@FarmOmics we are planning on a release very soon that will enable RNA-seq variant calling. I can ping you on this issue once that release is out.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/572#issuecomment-1275106892:32,release,release,32,,https://github.com/google/deepvariant/issues/572#issuecomment-1275106892,2,['release'],['release']
Deployability,@FarmOmics we have released an RNA-seq model and case study. Let me know if you have any questions. https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/572#issuecomment-1280173477:19,release,released,19,,https://github.com/google/deepvariant/issues/572#issuecomment-1280173477,1,['release'],['released']
Deployability,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it?. Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836:792,install,installed,792,,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836,1,['install'],['installed']
Deployability,@HagenC We have internally added both `min_base_quality` and `min_mapping_quality` as flags in `make_examples` and these will come out in the next DeepVariant release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/135#issuecomment-471021061:159,release,release,159,,https://github.com/google/deepvariant/issues/135#issuecomment-471021061,1,['release'],['release']
Deployability,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:200,release,released,200,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106,1,['release'],['released']
Deployability,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/490#issuecomment-948982282:370,pipeline,pipeline,370,,https://github.com/google/deepvariant/issues/490#issuecomment-948982282,1,['pipeline'],['pipeline']
Deployability,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-352087449:19,update,updated,19,,https://github.com/google/deepvariant/issues/9#issuecomment-352087449,1,['update'],['updated']
Deployability,"@MiWitt , I am closing the issue due to inactivity. Please feel free to reopen if you have any updates.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2158914413:95,update,updates,95,,https://github.com/google/deepvariant/issues/818#issuecomment-2158914413,1,['update'],['updates']
Deployability,"@MiWitt . Hi, do you have any updates on this issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2130314707:30,update,updates,30,,https://github.com/google/deepvariant/issues/818#issuecomment-2130314707,1,['update'],['updates']
Deployability,@Mipsology Thanks for the update. I'll keep this issue closed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342#issuecomment-696230691:26,update,update,26,,https://github.com/google/deepvariant/issues/342#issuecomment-696230691,1,['update'],['update']
Deployability,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1613598284:1311,pipeline,pipeline,1311,,https://github.com/google/deepvariant/issues/666#issuecomment-1613598284,1,['pipeline'],['pipeline']
Deployability,"@Phillip-a-richmond Thanks for checking.; I can take a look today. Before I made the release, I'm pretty sure I checked Singularity+GPU worked, but I should check again. I'll get a GPU machine and see if I can reproduce the errors you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1035263880:85,release,release,85,,https://github.com/google/deepvariant/issues/514#issuecomment-1035263880,1,['release'],['release']
Deployability,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/253#issuecomment-567578821:151,release,release,151,,https://github.com/google/deepvariant/issues/253#issuecomment-567578821,1,['release'],['release']
Deployability,"@Redmar-van-den-Berg this issue has been fixed internally, and the fix will be out with the next release. When gVCF records are present, we will try to extract the sample name from those, instead of using 'default'. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/354#issuecomment-698468546:97,release,release,97,,https://github.com/google/deepvariant/issues/354#issuecomment-698468546,1,['release'],['release']
Deployability,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```; $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0; binaries models ; ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```; $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>; ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions!. @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-568129639:19,install,installing,19,,https://github.com/google/deepvariant/issues/252#issuecomment-568129639,1,['install'],['installing']
Deployability,"@SHuang-Broad , yes, I'd suggest subsampling the BAM file with mapping quality of Q10 or Q20 if coverage is that high and then try again. I think the pipeline is getting stuck at the centromere. Do you happen to know the read N50?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-960110290:150,pipeline,pipeline,150,,https://github.com/google/deepvariant/issues/491#issuecomment-960110290,1,['pipeline'],['pipeline']
Deployability,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414:31,release,release,31,,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414,1,['release'],['release']
Deployability,"@Stikus ; Update: @gunjanbaid will start woking on this, and we'll get back to you soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/289#issuecomment-605291997:10,Update,Update,10,,https://github.com/google/deepvariant/issues/289#issuecomment-605291997,1,['Update'],['Update']
Deployability,"@Stikus A quick update for you: In our next release (v1.4.0), we'll update TensorFlow to v2.7.0.; Our Bazel version is still going to be 3.7.2 though (see: https://www.tensorflow.org/install/source#tested_build_configurations). But we'll keep in mind to update bazel version when we do future TensorFlow version updates!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511#issuecomment-1136617644:16,update,update,16,,https://github.com/google/deepvariant/issues/511#issuecomment-1136617644,6,"['install', 'release', 'update']","['install', 'release', 'update', 'updates']"
Deployability,"@Stikus Hi, I was able to reproduce the issue on Ubuntu 14.; We don't officially support building on Ubuntu 14, but I'll give it a try and see if I'm able to get it to work. I can give an update later. One question for you - Can you tell us the reason why you're not using our Docker image, and instead building your own binaries? It'll be useful for us to understand what our users need so we can prioritize the right things. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236#issuecomment-557244618:188,update,update,188,,https://github.com/google/deepvariant/issues/236#issuecomment-557244618,1,['update'],['update']
Deployability,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394#issuecomment-742111917:646,Release,Releases,646,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917,3,"['Release', 'release', 'update']","['Releases', 'releases', 'update']"
Deployability,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611#issuecomment-1513714977:81,update,updated,81,,https://github.com/google/deepvariant/issues/611#issuecomment-1513714977,1,['update'],['updated']
Deployability,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365#issuecomment-716869195:323,patch,patch,323,,https://github.com/google/deepvariant/pull/365#issuecomment-716869195,3,"['patch', 'release']","['patch', 'release']"
Deployability,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image.; * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372515026:386,pipeline,pipeline,386,,https://github.com/google/deepvariant/issues/6#issuecomment-372515026,1,['pipeline'],['pipeline']
Deployability,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:588,Install,Install,588,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,6,"['Install', 'install', 'update']","['Install', 'install', 'update']"
Deployability,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:; ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794#issuecomment-2035806347:954,release,release,954,,https://github.com/google/deepvariant/issues/794#issuecomment-2035806347,1,['release'],['release']
Deployability,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/766#issuecomment-1915376024:236,release,released,236,,https://github.com/google/deepvariant/issues/766#issuecomment-1915376024,1,['release'],['released']
Deployability,"@bilgehannevruz okay great, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292#issuecomment-610586044:43,update,update,43,,https://github.com/google/deepvariant/issues/292#issuecomment-610586044,1,['update'],['update']
Deployability,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it.; Adding it to our Docker image should be quite straightforward. I have one more question:; Can you provide a specific example usage you have in mind? ; You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script.; In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/414#issuecomment-768599474:395,integrat,integrated,395,,https://github.com/google/deepvariant/issues/414#issuecomment-768599474,1,['integrat'],['integrated']
Deployability,@brentp what version of singularity are you using? I have had issues with older versions of Singularity in the past and was able to get things to work by using 3+. I can experiment with this for a bit and try to put together a Dockerfile that install Samtools and Bcftools and runs on singularity.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445#issuecomment-822620636:243,install,install,243,,https://github.com/google/deepvariant/issues/445#issuecomment-822620636,1,['install'],['install']
Deployability,@chapmanb ; Thank you for your reply!. I checked the container PATH.; ```; ~$ echo $PATH; /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; ```. and I tried to find out if unzip exists in the bin directory.; ```; ~$ ls -l /opt/conda/envs/deepvariant/bin/; .; .; .; -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*; -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*; -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*; lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*; lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*; -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*; lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*; lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*; -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*; -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*; lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*; .; .; .; ```. The unzip itself doesn't seem to be installed during `conda install` .; Is this a problem that only happens to me?. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314#issuecomment-637888585:955,install,installed,955,,https://github.com/google/deepvariant/issues/314#issuecomment-637888585,2,['install'],"['install', 'installed']"
Deployability,"@chapmanb ; Thank you for your reply. ; I have seen the conda recipi. It is indeed strange. Maybe there's something wrong with my running environment.; OK, I'll deal with it by installing `unzip` directly in `apt` or `conda`.; Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314#issuecomment-638128799:177,install,installing,177,,https://github.com/google/deepvariant/issues/314#issuecomment-638128799,1,['install'],['installing']
Deployability,@chapmanb Thanks for giving it a try. ; Before I built I did something like:; ```; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable; ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. ; Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386075552:85,Install,Install,85,,https://github.com/google/deepvariant/issues/29#issuecomment-386075552,4,"['Install', 'install', 'release']","['Install', 'install', 'release-SCL']"
Deployability,"@chrisfleisch ; We have released v0.8.0 today :). This time, I made sure to try out Singularity as well. Here are some my personal notes when I tried to pull our Docker image and build Singularity image, based on the suggestion on your comment above. I'm still new to Singularity, so I'd really appreciate more feedback if any of the following doesn't make sense, or if any of these can be improved to be more similar to what users are used to. Once this becomes more mature (and if they become a popular use case), our team can consider adding them for official support in the future as well. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU. If you don't have singularity on your computer, install it first:; https://singularity.lbl.gov/install-linux. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.8.0; sudo docker tag gcr.io/deepvariant-docker/deepvariant:0.8.0 localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```; (The extra flags for singularity was added because of the locale issue: https://github.com/BioContainers/containers/issues/206#issuecomment-448698033). ## GPU image; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482430728:24,release,released,24,,https://github.com/google/deepvariant/issues/132#issuecomment-482430728,3,"['install', 'release']","['install', 'install-linux', 'released']"
Deployability,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/283#issuecomment-1281193477:77,release,released,77,,https://github.com/google/deepvariant/issues/283#issuecomment-1281193477,1,['release'],['released']
Deployability,"@crazysummerW . As Pi-Chuan mentioned, DeepConsensus is integrated into the Revio system; so, you will get DeepConsensus reads directly from that system. The `n1000.subreads.bam` demo dataset being discussed here is from Sequel II. It is a small number of reads from the human genome. You should be able to push it through the mechanical steps of alignment and variant calling, but the results will be limited by coverage. To figure out which mechanical step is broken here, I would recommend to pass the FASTQ directly rather than through a `fofn` to be more explicit. The pbmm2 alignment should have input of HiFi reads, not subreads. So, a typical aligned file name would be `REF.hifi_reads.bam` without a mention of subreads. > pbmm2 align hs37d5.fasta ${shard_id}.output.fastq aligned.bam --sort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1616079783:56,integrat,integrated,56,,https://github.com/google/deepvariant/issues/672#issuecomment-1616079783,1,['integrat'],['integrated']
Deployability,"@crazysummerW Make sure your're using `BIN_VERSION=""1.4.0""` with the command above, which was what's in the documentation you pointed to. This is because: In r1.5, we didn't update the DeepTrio PacBio version. So you'll need to use the 1.4.0 version. That said, we're about to update to r1.6! And this time, DeepTrio PacBio will get an update as well. So, @crazysummerW instead of trying to get 1.4.0 to work, I recommend that you wait a little bit. The new version is coming out soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/718#issuecomment-1766457667:174,update,update,174,,https://github.com/google/deepvariant/issues/718#issuecomment-1766457667,3,['update'],['update']
Deployability,"@cwarden45 ; I got a `t1.micro` machine on AWS , and I basically repeated what I did in https://github.com/google/deepvariant/issues/167#issuecomment-480150395. With the same command:; ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.7.2. time sudo docker run \; -v ${PWD}:/data \; gcr.io/deepvariant-docker/deepvariant:0.7.2 \; /opt/deepvariant/bin/postprocess_variants \; --ref /data/ucsc.hg19.fasta \; --infile /data/call_variants_output.tfrecord.gz \; --outfile /data/output.vcf.gz; ```. It took me:; ```; real 0m24.779s; user 0m0.033s; sys 0m0.022s; ```; on a t1.micro AWS instance. A few differences from my GCP experience is:; (1) I had to use yum (instead of apt) to install docker.; (2) I had to first run `sudo service docker start` before I pull and run the docker image. Other than these, everything seems mostly the same. At this point I have one more question for you -- where are your files located? In your command in the original post, they're from `/mnt`. Where are these files mounted from?. In my setting, I wget all the files first. So that could be one difference that I can think of now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480563774:688,install,install,688,,https://github.com/google/deepvariant/issues/167#issuecomment-480563774,1,['install'],['install']
Deployability,"@danielecook ; Ok; I try the command that you write it runs successfully; the updated command is; BIN_VERSION=""1.4.0""; nproc=8; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref= Homo_sapiens.GRCh38.dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir. the error ; ***** Running the command:*****; time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:78,update,updated,78,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562,1,['update'],['updated']
Deployability,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```; cat /etc/lsb-release; cat /etc/redhat-release; cat /etc/os-release; ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case?. 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1595877014:352,release,release,352,,https://github.com/google/deepvariant/issues/664#issuecomment-1595877014,3,['release'],['release']
Deployability,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:369,Install,Install,369,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Install'],['Install']
Deployability,"@depristo In fact, without touching any setup sh scripts, that same build_and_test attempt failed with a missing header of this prepend:; `--prepend clif/python/types.h`; That is why I ended up modifying `clif.bzl` in` third_party` to include the absolute path of this include, not sure if this is the culprit. I will try to move the clif installation to /usr/local to give it a shot, also with this new 0.4.1 release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351271203:339,install,installation,339,,https://github.com/google/deepvariant/issues/12#issuecomment-351271203,2,"['install', 'release']","['installation', 'release']"
Deployability,@depristo RE docker docs: I can update the README in https://github.com/google/deepvariant/tree/master/deepvariant/docker to include a link to https://github.com/google/deepvariant/tree/master/docs/deepvariant-docker.md (note that this README is included inside the prebuild image as well). I think that page has all the instructions needed to use our prebuilt image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/25#issuecomment-355050387:32,update,update,32,,https://github.com/google/deepvariant/issues/25#issuecomment-355050387,1,['update'],['update']
Deployability,"@depristo Thanks a lot for the clarification. ; In my case, I am trying to have a standalone version to test with, without GCP at this stage.; ; In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, ; since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:; ```; export DV_PLATFORM=""ubuntu-16""; cd ..; git clone https://github.com/google/clif ; cd clif; ./INSTALL.sh; python setup.py install; sudo ldconfig # Reload shared libraries.; ```; To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ?. `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351255520:298,Install,Install,298,,https://github.com/google/deepvariant/issues/12#issuecomment-351255520,3,"['INSTALL', 'Install', 'install']","['INSTALL', 'Install', 'install']"
Deployability,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```; (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s; (18:20:33) INFO: Build completed successfully, 2 total actions; ```; Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351350738:411,install,installation,411,,https://github.com/google/deepvariant/issues/12#issuecomment-351350738,1,['install'],['installation']
Deployability,"@depristo This might be happening because I'd aliased the ""python3"" command as ""python"", which is supposed to be for python 2.7 by default. Any idea as to how I could reset everything such that it installs only for python 2.7 (I've already removed the alias)?; ; EDIT: The closing and reopening of the issue was a mistake, please ignore.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30#issuecomment-355047368:197,install,installs,197,,https://github.com/google/deepvariant/issues/30#issuecomment-355047368,1,['install'],['installs']
Deployability,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-351577610:132,install,install,132,,https://github.com/google/deepvariant/issues/9#issuecomment-351577610,2,['install'],"['install', 'installing']"
Deployability,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-737626363:15,update,update,15,,https://github.com/google/deepvariant/pull/363#issuecomment-737626363,1,['update'],['update']
Deployability,"@dkurt FYI , the OpenVINO changes are in https://github.com/google/deepvariant/releases/tag/v1.1.0; Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-740139289:79,release,releases,79,,https://github.com/google/deepvariant/pull/363#issuecomment-740139289,1,['release'],['releases']
Deployability,@dkurt FYI: This is now the merged internally and will come out in the next release.; Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/442#issuecomment-823513740:76,release,release,76,,https://github.com/google/deepvariant/pull/442#issuecomment-823513740,1,['release'],['release']
Deployability,"@dkurt Question for you: I'm working an updates to update to Ubuntu20.04, TF2.5, Python3.8, etc.; Want to check in with you on whether we need to update any OpenVINO version before the next release.; Let me know! If it's easier through email, please email me at pichuan@google.com. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/442#issuecomment-850907219:40,update,updates,40,,https://github.com/google/deepvariant/pull/442#issuecomment-850907219,4,"['release', 'update']","['release', 'update', 'updates']"
Deployability,"@dkurt Thanks. Another update for you - I am now trying to incorporate your on-the-fly conversion code:; https://github.com/google/deepvariant/pull/363/commits/f0ed01891c3e612d4c7093e5e844f855beae707a. I think it'll be cleaner, and also removes the need for https://github.com/google/deepvariant/pull/363#issuecomment-736278644. I do have a question for the code. I'll comment inline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736694275:23,update,update,23,,https://github.com/google/deepvariant/pull/363#issuecomment-736694275,1,['update'],['update']
Deployability,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s.; So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines?; Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735510277:361,release,release,361,,https://github.com/google/deepvariant/pull/363#issuecomment-735510277,4,['release'],"['release', 'releases']"
Deployability,"@drtamermansour You don't have to install glibc, you just need to compile it in a local directory. Basically you just need to run `./configure` and `make` without running `make install`. Then just update `LD_LIBRARY_PATH` to include the local directory of the compiled glibc .so file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-454185297:34,install,install,34,,https://github.com/google/deepvariant/issues/137#issuecomment-454185297,3,"['install', 'update']","['install', 'update']"
Deployability,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461426712:333,install,installing,333,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712,2,['install'],['installing']
Deployability,"@esraaelmligy ,. The RNAseq model is trained and released for DeepVariant 1.4.0, please see [this documentation](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-rnaseq-case-study.md#running-deepvariant-rna-seq-on-a-cpu-only-machine) on how to run it. Please change `deepvariant:latest` to `deepvariant:1.4.0`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845#issuecomment-2209468899:49,release,released,49,,https://github.com/google/deepvariant/issues/845#issuecomment-2209468899,1,['release'],['released']
Deployability,"@esraaelmligy ,. The non updated version will not affect the results much. Any updates on RNASeq model is currently unplanned but I can give you an update closer to the next release date. Ideally, yes, we would want to train a new model. I just haven't assessed how much time and effort is required there vs other priorities. I have taken a note and will update you when we make a decision. . Will close this bug for now. Thank you for confirming that it worked.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845#issuecomment-2223323891:25,update,updated,25,,https://github.com/google/deepvariant/issues/845#issuecomment-2223323891,5,"['release', 'update']","['release', 'update', 'updated', 'updates']"
Deployability,"@fellen31 ,. version 1.6.1 had a very small incremental change and the docker version was not updated. I can confirm that 1.6.1 docker has the code change required but the version update for printing the version wasn't changed. Is this a blocking issue for you? Otherwise we will update it the next version appropriately in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2166494135:94,update,updated,94,,https://github.com/google/deepvariant/issues/830#issuecomment-2166494135,4,"['release', 'update']","['release', 'update', 'updated']"
Deployability,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-475486437:45,update,update,45,,https://github.com/google/deepvariant/pull/152#issuecomment-475486437,5,"['configurat', 'release', 'update']","['configuration', 'release', 'update']"
Deployability,"@frapaport ; an update on Singularity - I've tested our latest setting (which will come out in the next release) by converting it in to a Singularity image. It seems to work fine for me. So, if you would be able to install singularity, that will be an easier way forward once our next release is out.; I'll still come back and revisit the usability of our bioconda installation. But might take a while.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-480563989:16,update,update,16,,https://github.com/google/deepvariant/issues/137#issuecomment-480563989,5,"['install', 'release', 'update']","['install', 'installation', 'release', 'update']"
Deployability,"@gevro Given the error information, can you try installing setuptools?; For example, https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/385#issuecomment-728239211:48,install,installing,48,,https://github.com/google/deepvariant/issues/385#issuecomment-728239211,1,['install'],['installing']
Deployability,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first.; If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2069997953:655,release,releases,655,,https://github.com/google/deepvariant/issues/810#issuecomment-2069997953,1,['release'],['releases']
Deployability,"@gunjanbaid @pgrosu an update:. 1. I've used blasr to align the BAM file of chr20 (the file generated from `samtools view -h sorted_final_merged.bam chr20 -o chr20.bam`) and the reference `chr20.fa`.; The command:; `user@deepvariant:~/data$ blasr chr20.bam chr20.fa --bam --out alignments20.bam`; This command ran for about 13.5 hours on my local machine, and produced a file with size of 6.3GB, while the original `chr20.bam` file size was only 2.3GB . 2. I've sorted the produced alignments20.bam file using the samtools.; The command:; `user@deepvariant:~/data$ samtools sort -f alignments20.bam alignments20_sorted.bam`; This command produced a sorted file with size of 7.75GB. 3. I tried running the `make_examples` script again with the new `alignments20_sorted.bam` file.; The command:; ```; python bin/make_examples.zip \; --mode training \; --ref ""data/chr20.fa"" \; --reads ""data/alignments20_sorted.bam"" \; --examples ""training-examples/training_set.with_label.tfrecord.gz"" \; --confident_regions ""data/NA12878.sorted.bed"" \; --truth_variants ""data/NA12878.sorted.vcf.gz"" \; --regions ""chr20"" \; --norealign_reads; ```; And The output: (receiving the same QUAL field missing error); ```; 2019-01-29 11:46:16.329383: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.333216: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.334961 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.337215 140471159555840 make_examples.py:1024] Preparing inputs; 2019-01-29 11:46:16.340804: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.344462: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.346041 140471159555840 genomics_reader.py:174] R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458487916:23,update,update,23,,https://github.com/google/deepvariant/issues/138#issuecomment-458487916,1,['update'],['update']
Deployability,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-471842177:311,configurat,configuration,311,,https://github.com/google/deepvariant/pull/159#issuecomment-471842177,1,['configurat'],['configuration']
Deployability,@gunjanbaid Even better. Looking forward to the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-472252775:53,release,release,53,,https://github.com/google/deepvariant/pull/159#issuecomment-472252775,1,['release'],['release']
Deployability,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149#issuecomment-461160956:50,configurat,configuration,50,,https://github.com/google/deepvariant/issues/149#issuecomment-461160956,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"@hamidqaedi Just update the architecture to `--copt=-march=native` in [settings.sh](https://github.com/google/deepvariant/blob/r0.8/settings.sh#L102), and use the same option when compiling Tensorflow from source and you should be fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/212#issuecomment-526443293:17,update,update,17,,https://github.com/google/deepvariant/issues/212#issuecomment-526443293,1,['update'],['update']
Deployability,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746:83,update,updated,83,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746,1,['update'],['updated']
Deployability,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:69,Install,Install,69,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935,5,"['Install', 'configurat', 'install']","['Install', 'configuration', 'configurations', 'install']"
Deployability,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-1281200289:62,release,released,62,,https://github.com/google/deepvariant/issues/412#issuecomment-1281200289,2,"['release', 'update']","['released', 'updated']"
Deployability,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1546497696:44,release,release,44,,https://github.com/google/deepvariant/issues/649#issuecomment-1546497696,2,['release'],"['release', 'releases']"
Deployability,"@kishwarshafin ,. Thank you so much, I changed the version and the run finished well. I just have to ask whether using a non-updated version will dramatically affect the results, and if you are planning to do any updates on the RNAseq model soon so it could be suitable for later versions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845#issuecomment-2216709297:125,update,updated,125,,https://github.com/google/deepvariant/issues/845#issuecomment-2216709297,2,['update'],"['updated', 'updates']"
Deployability,"@kishwarshafin ; Thanks a lot for clarifying that, we were making a benchmark study on variant callers to optimize our pipeline. We also used PEPPER-DeepVariant and it worked with high precision. Again, thanks a lot! It was a helpful communication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814#issuecomment-2092651619:119,pipeline,pipeline,119,,https://github.com/google/deepvariant/issues/814#issuecomment-2092651619,1,['pipeline'],['pipeline']
Deployability,@kishwarshafin I will try this and update you on the results I get. Thank you so much for the support!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101275755:35,update,update,35,,https://github.com/google/deepvariant/issues/819#issuecomment-2101275755,1,['update'],['update']
Deployability,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724#issuecomment-1818289082:384,update,update,384,,https://github.com/google/deepvariant/issues/724#issuecomment-1818289082,1,['update'],['update']
Deployability,"@kokyriakidis currently we do not have a way of doing this, but we are working on an option to output candidate alleles in an upcoming release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/467#issuecomment-870947313:135,release,release,135,,https://github.com/google/deepvariant/issues/467#issuecomment-870947313,1,['release'],['release']
Deployability,"@ksw9 ; Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):; `/home/${USER}` should be `${HOME}` to be more general.; And, note that in order for docker to access your file system, you do need the `-v` path.; So you probably want something like:. ```; OUTPUT_DIR=${HOME}/quickstart-output; mkdir -p ""${OUTPUT_DIR}""; REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```. and ; `-v ${HOME}:${HOME} `; in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-439585565:56,update,updated,56,,https://github.com/google/deepvariant/issues/104#issuecomment-439585565,2,"['release', 'update']","['release', 'updated']"
Deployability,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517:36,install,install,36,,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517,1,['install'],['install']
Deployability,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/357#issuecomment-698738740:111,release,released,111,,https://github.com/google/deepvariant/issues/357#issuecomment-698738740,1,['release'],['released']
Deployability,@liambai Python version will be updated in the next release. ; You can see a newer (not officially release) version in the `dev` branch: https://github.com/google/deepvariant/tree/dev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727#issuecomment-2309122994:32,update,updated,32,,https://github.com/google/deepvariant/issues/727#issuecomment-2309122994,3,"['release', 'update']","['release', 'updated']"
Deployability,"@liukeweiaway ,. I see, can you please update to 1.6.1? I think you are getting stuck in the bug of 1.6.0 that we fixed in 1.6.1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855#issuecomment-2261172632:39,update,update,39,,https://github.com/google/deepvariant/issues/855#issuecomment-2261172632,1,['update'],['update']
Deployability,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.5.0 \; --zone=us-central1-c; ```. And then with that TPU, this worked:. ```; OUTPUT_GCS_BUCKET=<OURBUCKET>; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \; -v /home/${USER}:/home/${USER} \; deepvariant:latest \; /opt/deepvariant/bin/model_train \; --use_tpu \; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint=""""; ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:; ```; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-880959171:42,update,update,42,,https://github.com/google/deepvariant/issues/469#issuecomment-880959171,3,"['release', 'update']","['release', 'update']"
Deployability,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105#issuecomment-430711053:25,update,update,25,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053,2,"['release', 'update']","['release', 'update']"
Deployability,"@melkerdawy Thanks for checking back.; I talked to @nmousavi a while ago and he didn't recall any changes, but said that he'll look into it. I'll ping him again. I can also spend some time later (I'm currently on vacation). We're not familiar with Singularity container, and don't currently plan to provide that yet. But we can certainly look into why the installation location changes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-457984083:356,install,installation,356,,https://github.com/google/deepvariant/issues/132#issuecomment-457984083,1,['install'],['installation']
Deployability,"@mosh305 The data @AndrewCarroll mentioned above is mapped to the hs37d5 reference genome, as mentioned in ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb/alignment/README.txt. . The corresponding truth variants and confident regions for this data would come from the Genome in a Bottle (GIAB) files for HG002 mapped to the GRCh37 reference genome. There is no specific set of GIAB files for data mapped to the hs37d5 reference genome, as hs37d5 is mostly the same as GRCh37, but with some additional sequences. . You can find the relevant BED and VCF files here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/latest/GRCh37/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-461152701:654,release,release,654,,https://github.com/google/deepvariant/issues/138#issuecomment-461152701,1,['release'],['release']
Deployability,"@nlopez94 ,. We also observe these warnings, but, DeepVariant does not use any TensorRT apis for training or inference. So these warning are usually non actionable for the deepvariant pipeline. Are you running inference and seeing the machine's GPU is not being utilized?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2100924126:184,pipeline,pipeline,184,,https://github.com/google/deepvariant/issues/819#issuecomment-2100924126,1,['pipeline'],['pipeline']
Deployability,"@nmousavi I made a readonly public bucket you can access here:; http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:; https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437189603:205,update,update,205,,https://github.com/google/deepvariant/issues/116#issuecomment-437189603,1,['update'],['update']
Deployability,"@nurmians Still looking through the code - and looked at the [release notes](https://github.com/google/deepvariant/releases) - though still reasoning through it in case it might have been implemented via some alternate logic. In any case, here's a link to the script for converting genotypes of regions to haploid calls:. https://github.com/Ultimagen/VariantCalling/blob/master/ugvc/pipelines/convert_haploid_regions.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1691796096:62,release,release,62,,https://github.com/google/deepvariant/issues/518#issuecomment-1691796096,3,"['pipeline', 'release']","['pipelines', 'release', 'releases']"
Deployability,"@pgrosu ,. Thank you for all the help, however, I found the issue to be in `ONNX` quantization step. I patched and uploaded a new version and provided that to @myonaung. It fixed the issue on their end. As the issue has been resolved, I requested to close this thread. Your query may be unanswered here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/497#issuecomment-996960107:103,patch,patched,103,,https://github.com/google/deepvariant/issues/497#issuecomment-996960107,1,['patch'],['patched']
Deployability,@pgrosu ; I am sorry if I was not clear enough. I tried to say that I could not install glibc locally on my system. ; I started the contact with the system administrator to see what they can do.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-454145983:80,install,install,80,,https://github.com/google/deepvariant/issues/137#issuecomment-454145983,1,['install'],['install']
Deployability,"@pgrosu ; Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. ; It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. ; This might take a while though. (More than what we'll need to answer usual github issues :)); But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/109#issuecomment-431172105:691,update,update,691,,https://github.com/google/deepvariant/issues/109#issuecomment-431172105,1,['update'],['update']
Deployability,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:279,release,release,279,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,"['configurat', 'release']","['configurations', 'release']"
Deployability,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:812,Update,Update,812,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,2,"['Install', 'Update']","['Install', 'Update']"
Deployability,"@pgrosu I could not compile the library on my server . I followed the suggestion [here](https://stackoverflow.com/questions/847179/multiple-glibc-libraries-on-a-single-host/851229#851229). I added CFLAGS=""-O2"" to address an optimization request error but still the make command fails to compile; ```; mkdir glibc && cd glibc; wget https://ftp.gnu.org/gnu/glibc/glibc-2.23.tar.gz; tar xvzf glibc-2.23.tar.gz; mkdir glibc-build && cd glibc-build; mkdir ../install; ../glibc-2.23/configure CFLAGS=""-O2"" --prefix $HOME/glibc/install; make -j `nproc`; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453764783:454,install,install,454,,https://github.com/google/deepvariant/issues/137#issuecomment-453764783,2,['install'],['install']
Deployability,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:107,install,installation,107,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['install'],['installation']
Deployability,"@pgrosu Thank you for the information. Your suggestion might be a way to port DeepVariant on Spark, but it's not fit on dynamic allocation since the available resource is dynamic changed. ; I did port several popular bioinformatics tools (e.g. BWA, GATK, DELLY2, Samtools, ...) on Spark and they run well. I think the fundamental problem is that the resource configuration (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) of TensorFlow doesn't work. If there is no way to limit CPU resource on DeepVariant, should I submit this issue to TensorFlow GitHub?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-417995073:359,configurat,configuration,359,,https://github.com/google/deepvariant/issues/90#issuecomment-417995073,1,['configurat'],['configuration']
Deployability,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:60,install,install,60,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623,3,['install'],"['install', 'installation']"
Deployability,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:181,release,releases,181,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056,7,"['install', 'release']","['installed', 'installer-linux-', 'releases']"
Deployability,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:119,Install,Install,119,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,12,"['Install', 'install', 'release', 'upgrade']","['Install', 'Installing', 'install', 'installation', 'installed', 'release', 'released', 'upgrade']"
Deployability,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:234,install,installed,234,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888,2,['install'],['installed']
Deployability,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:896,Install,Install,896,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Install'],['Install']
Deployability,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:223,install,installation,223,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461,5,"['Install', 'configurat', 'install', 'release']","['Installs', 'configurations', 'install', 'installation', 'release']"
Deployability,@pichuan @danielecook Thank you guys for such a quick response. It is something I feared. The Cromwell dispatcher is a black box to me. I will try your configurations and some others too to see if it solves the problems.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446#issuecomment-827022298:152,configurat,configurations,152,,https://github.com/google/deepvariant/issues/446#issuecomment-827022298,1,['configurat'],['configurations']
Deployability,"@pichuan As you can see in my related issue in `wheel` - `--no-binary=numpy` will work even in current situation due to not installing `wheel` from source (what causing the problem) and if understand correctly ""Because of an issue with pypi's numpy on Ubuntu 14.04, we need to compile from source."" - it will fix this issue too. So, if you're planning to leave support for different Ubuntu versions in the script (which is great - for now I use a slightly modified version of your script and not my own), I suggest to change `--no-binary=:all:` to `--no-binary=numpy` and all will be ok. Actually, while you're modifying scripts - I suggest decreasing the verbosity of `curl` and `wget` in lines 71 and 110 in `build-prereq.sh` and in lines 93 and 201 in `run-prereq.sh` with `-Ss` and `-q`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394#issuecomment-743131852:124,install,installing,124,,https://github.com/google/deepvariant/issues/394#issuecomment-743131852,1,['install'],['installing']
Deployability,@pichuan Thank you for taking the time to help on this ; The super computer has an OS CentOS Linux release 7.6.1810 (LSB Version: core-4.1-amd64:core-4.1-noarch) and singularity version 2.5.2; I created the image on Amazon instance with Ubuntu 16.04. I tried using singularity version 2.5.2 & 2.6.0 but both did not help,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-483551683:99,release,release,99,,https://github.com/google/deepvariant/issues/132#issuecomment-483551683,1,['release'],['release']
Deployability,@pichuan Thank you for the quick reply. I will wait for any new updates.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-457986908:64,update,updates,64,,https://github.com/google/deepvariant/issues/132#issuecomment-457986908,1,['update'],['updates']
Deployability,"@pichuan Thank you for the quick reply. That's a good tip to bypass the insert_size channel by default. Looking forward to your update, much appreciated. If I understand correctly, each additional channel requires building a model using examples containing those channels. As a hypothetical example, to use `insert_size` + `allele_frequency` + `avg_base_quality` during variant calling would require re-training, correct? . I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568#issuecomment-1251646879:128,update,update,128,,https://github.com/google/deepvariant/issues/568#issuecomment-1251646879,1,['update'],['update']
Deployability,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). ; I run the code:; ```; cd /root/clif/build; cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; ```; and the problem is also:; ```; -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```; I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823759004:956,install,installed,956,,https://github.com/google/deepvariant/issues/739#issuecomment-1823759004,1,['install'],['installed']
Deployability,@pichuan Thanks so much! When do you plan to do the next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727#issuecomment-2310504590:57,release,release,57,,https://github.com/google/deepvariant/issues/727#issuecomment-2310504590,1,['release'],['release']
Deployability,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:485,Release,Release,485,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,"['Release', 'update']","['Release', 'updates']"
Deployability,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error ; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazel; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'. And i tried the installation and got this:. solokopi@solokopi-All-Series:~$ sudo apt-get install bazel; Reading package lists... Done; Building dependency tree ; Reading state information... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; E: Unable to locate package bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415936477:300,install,installation,300,,https://github.com/google/deepvariant/issues/89#issuecomment-415936477,2,['install'],"['install', 'installation']"
Deployability,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415936563:296,install,installation,296,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563,5,['install'],"['install', 'installation', 'installed']"
Deployability,"@pichuan thank you so much for your help on this, running it via docker is easier but i experienced some challenges pulling the deepvariant container from the docker hub, which i presume is a proxy issue, i think i need a vpn because i am in china and google domains are blocked. trying to install a vpn on my ubuntu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416270251:290,install,install,290,,https://github.com/google/deepvariant/issues/89#issuecomment-416270251,1,['install'],['install']
Deployability,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```; rm model.pb;; ```; However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735606528:258,install,installation,258,,https://github.com/google/deepvariant/pull/363#issuecomment-735606528,1,['install'],['installation']
Deployability,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome!. I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736009433:317,configurat,configuration,317,,https://github.com/google/deepvariant/pull/363#issuecomment-736009433,1,['configurat'],['configuration']
Deployability,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393#issuecomment-742247654:591,update,update,591,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654,3,"['install', 'update']","['install', 'install-recommends', 'update']"
Deployability,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permiss",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-723242914:993,pipeline,pipeline,993,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914,1,['pipeline'],['pipeline']
Deployability,"@pichuan, Sure, I don't mind if you just copy the changes. Yes, this way is valid because OpenVINO from pip uses manylinux toolchain so it's compatible with Ubuntu 16.04 and allows to upgrade to newer versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/442#issuecomment-821781022:184,upgrade,upgrade,184,,https://github.com/google/deepvariant/pull/442#issuecomment-821781022,1,['upgrade'],['upgrade']
Deployability,"@pioneer-pi , the deepvariant manuscript from six years ago. Since then, the benchmarking methods, tools and data all has been updated. The latest version of bechmarking is GIAB v4.2.1 for HG002 that you can find [here](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/). I would suggest reading the following manuscripts if you are interested to learn about these benchmarks:. 1) https://cell.com/cell-genomics/fulltext/S2666-979X(22)00058-1; 2) https://www.cell.com/cell-genomics/fulltext/S2666-979X-2200057-X; 3) https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract. The latest data can be found in: . https://storage.googleapis.com/brain-genomics-public/research/sequencing/fastq/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch37/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683:127,update,updated,127,,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683,2,"['release', 'update']","['release', 'updated']"
Deployability,"@pioneer-pi ,. The source installation instructions are for installing on your local machine. If you can run docker, you do not need to install DeepVariant. If you are starting a container with the provided docker then DeepVariant is already installed within it. Please see the quickstart: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md to see how to run DeepVariant through docker. If you are starting a container then DeepVariant binaries should be in:; ```bash; /opt/deepvariant/bin/; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818109139:26,install,installation,26,,https://github.com/google/deepvariant/issues/737#issuecomment-1818109139,4,['install'],"['install', 'installation', 'installed', 'installing']"
Deployability,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727#issuecomment-1802304427:48,install,installed,48,,https://github.com/google/deepvariant/issues/727#issuecomment-1802304427,2,['install'],"['install', 'installed']"
Deployability,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/37#issuecomment-529713917:375,release,release,375,,https://github.com/google/deepvariant/issues/37#issuecomment-529713917,1,['release'],['release']
Deployability,"@s-batalov, I will take a look soon on how difficult it is to update the CUDA version on our end. However, it's difficult to rebuild the 1.6.1 version with an updated CUDA version, it may have to be the next release where we update the CUDA version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844#issuecomment-2207040623:62,update,update,62,,https://github.com/google/deepvariant/issues/844#issuecomment-2207040623,4,"['release', 'update']","['release', 'update', 'updated']"
Deployability,"@saliksyed ; Hopefully this is resolved with v0.7.2 release, and with the workaround of putting the BED file in a public bucket.; If you encounter more issues, let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-447731903:52,release,release,52,,https://github.com/google/deepvariant/issues/116#issuecomment-447731903,1,['release'],['release']
Deployability,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353510712:222,release,release,222,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712,1,['release'],['release']
Deployability,"@serge2016 @PlatonB Thank you for raising this issue again. We're now aiming for an earlier release, in March. I'll also update to the other issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-585527464:92,release,release,92,,https://github.com/google/deepvariant/issues/252#issuecomment-585527464,2,"['release', 'update']","['release', 'update']"
Deployability,"@snakesch can you please send the file to shafin@google.com. It's unclear from the output what exactly is happening, are you certain make_examples finished properly? Anyways, if you send me the file, I can run on my end to see if there's an issue with the pipeline or the file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833#issuecomment-2183584969:256,pipeline,pipeline,256,,https://github.com/google/deepvariant/issues/833#issuecomment-2183584969,1,['pipeline'],['pipeline']
Deployability,"@solokopi since you're already on Ubuntu 16, you can also try using the binaries that we built.; You can just get it from this zip file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip; which has the binaries and the model files. You'll need to run `run-prereq.sh` first to set up your machine. But that will not require installing bazel.; Please let me know if that works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416272658:176,release,releases,176,,https://github.com/google/deepvariant/issues/89#issuecomment-416272658,2,"['install', 'release']","['installing', 'releases']"
Deployability,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/813#issuecomment-2091275266:372,pipeline,pipeline,372,,https://github.com/google/deepvariant/issues/813#issuecomment-2091275266,1,['pipeline'],['pipeline']
Deployability,"@sophienguyen01 thanks for the update. For now, I would recommend an alternative tumor-only caller. We may explore a tumor-only approach in the future. I will close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/728#issuecomment-1804751645:31,update,update,31,,https://github.com/google/deepvariant/issues/728#issuecomment-1804751645,1,['update'],['update']
Deployability,"@splaisan Which OS is installed on your server? Linux, Ubuntu, ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1503989450:22,install,installed,22,,https://github.com/google/deepvariant/issues/452#issuecomment-1503989450,1,['install'],['installed']
Deployability,"@tahashmi,. It looks like [nucleus](https://github.com/google/nucleus) is having issues with the newer tensorflow version you installed. I'm not exactly sure why your installing tensorflow locally fixed the issue. Can you please see if you install nucleus locally, it fixes it:; ```; pip install --user google-nucleus; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/559#issuecomment-1230609842:126,install,installed,126,,https://github.com/google/deepvariant/issues/559#issuecomment-1230609842,4,['install'],"['install', 'installed', 'installing']"
Deployability,"@tzcoolman You're very close. Just update your `singularity run` with the following two binds (`--bind ${INPUT_DIR} --bind ${OUTPUT_DIR}`), like this:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} docker://google/deepvariant:""${BIN_VERSION}"" ...; ```. This makes those directories accessible within the container. The rest can stay the same. Let me know if there is anything else. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678#issuecomment-1635203098:35,update,update,35,,https://github.com/google/deepvariant/issues/678#issuecomment-1635203098,1,['update'],['update']
Deployability,@weilu1998 you shouldn't have to install any additional software if you are using singularity. Can you try running singularity using the `--cleanenv` flag? This will prevent locally installed libraries from being used (which can cause conflicts).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580#issuecomment-1303821922:33,install,install,33,,https://github.com/google/deepvariant/issues/580#issuecomment-1303821922,2,['install'],"['install', 'installed']"
Deployability,"@williambrandler . The `bazel: command not found` line is likely ok because build-prereq.sh will plan to install bazel after that line. `curl: command not found` - this might have happened here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L75. I'm surprised that at this point there isn't `curl`, because it was installed here:. https://github.com/google/deepvariant/blob/r1.2/build-prereq.sh#L54. You can remove the ` > /dev/null` from that line and see if it was installed currently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-902142885:105,install,install,105,,https://github.com/google/deepvariant/issues/476#issuecomment-902142885,3,['install'],"['install', 'installed']"
Deployability,@williambrandler the latest version of DV requires Python 3.8 and Ubuntu 20.04. Are you able to upgrade Ubuntu on the databricks image?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896274508:96,upgrade,upgrade,96,,https://github.com/google/deepvariant/issues/476#issuecomment-896274508,1,['upgrade'],['upgrade']
Deployability,"@williamrowell yes, the flags will stay the same in the next release for DeepVariant, and we're adding this to DeepTrio too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/473#issuecomment-882719687:61,release,release,61,,https://github.com/google/deepvariant/pull/473#issuecomment-882719687,1,['release'],['release']
Deployability,"@yanyang1989 in our next release (which we have internally updated already), we'll be building clif from scratch. However, in our next release we will only support Ubuntu18. For any other versions, please directly refer to https://github.com/google/clif to build clif binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-821556321:25,release,release,25,,https://github.com/google/deepvariant/issues/441#issuecomment-821556321,3,"['release', 'update']","['release', 'updated']"
Deployability,"@zyxue I believe a proper bugfix is going in soon and will be available when we release an updated version of 0.7. Since the ETA for that isn't *right now* you can fix this in a local build yourself by:. changing:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/third_party/nucleus/io/sam_reader.cc#L448. so that you have:. ```; if (c->mtid < -1); return tf::errors::DataLoss(; ""Expected mtid >= 0 as mate is supposedly mapped: "",; read_message->ShortDebugString());; else if (c->mtid == -1) {; mate_position->set_reference_name(""*"");; } else {; mate_position->set_reference_name(h->target_name[c->mtid]);; }; mate_position->set_position(c->mpos);; mate_position->set_reverse_strand(bam_is_mrev(b));; ```. which I believe should get your running again. You'll need to build DV from sources though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-427462549:80,release,release,80,,https://github.com/google/deepvariant/issues/99#issuecomment-427462549,2,"['release', 'update']","['release', 'updated']"
Deployability,"@zyxue I did an analysis a while ago, that posted at the following link:. [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). The thing that dominated the processing at that time was the aligner, and would require more surgery to determine possibilities for optimization even though it got recently updated with the [FastPassAligner](https://github.com/google/deepvariant/blob/r0.7/deepvariant/realigner/fast_pass_aligner.cc) - which would require re-profiling. Basically a bunch of profiling tools would need to be built for you to then run, in order to determine for your scenario what the best optimization path would be. You probably noticed the following lines in [make_examples.py](https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1150-L1153) regarding the core restriction:. ```Python; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Though there are possibilities around that like @pichuan mentioned. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428686329:353,update,updated,353,,https://github.com/google/deepvariant/issues/99#issuecomment-428686329,1,['update'],['updated']
Deployability,"@zyxue Via `cloud-build-local`, below is a link to more information about it: . https://cloud.google.com/cloud-build/docs/build-debug-locally. You will need to run the following command to install it:. `gcloud components install cloud-build-local`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428375827:189,install,install,189,,https://github.com/google/deepvariant/issues/99#issuecomment-428375827,2,['install'],['install']
Deployability,"@zyxue curious whether you're able to resolve this?; I'm going to close this since the last update was a while ago. If you have more information, feel free to update and re-open.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/103#issuecomment-436139125:92,update,update,92,,https://github.com/google/deepvariant/issues/103#issuecomment-436139125,2,['update'],['update']
Deployability,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-354748344:91,install,install,91,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344,3,['install'],"['install', 'installing']"
Deployability,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/28#issuecomment-354390647:8,update,update,8,,https://github.com/google/deepvariant/issues/28#issuecomment-354390647,2,"['release', 'update']","['release', 'update']"
Deployability,"A fix is in in google, renaming math.py to genomics_math.py, which should fix the problem. The next major push of functional changes to DeepVariant will include this update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-356119694:166,update,update,166,,https://github.com/google/deepvariant/issues/32#issuecomment-356119694,1,['update'],['update']
Deployability,"ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2560,install,install,2560,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['install']
Deployability,"ARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2781,install,installed,2781,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['install'],['installed']
Deployability,"ATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4312,install,install,4312,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355441693:380,configurat,configuration,380,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693,2,"['configurat', 'deploy']","['configuration', 'deployments']"
Deployability,"Actually, when I took a closer look at the logs, it says:. ```; 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2; ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471376570:1004,release,release,1004,,https://github.com/google/deepvariant/issues/619#issuecomment-1471376570,1,['release'],['release']
Deployability,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/782#issuecomment-1990498159:242,update,update,242,,https://github.com/google/deepvariant/issues/782#issuecomment-1990498159,1,['update'],['update']
Deployability,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:; ```; ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6.; I did some search and found some old thread that could be relevant:; https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386517018:16,install,install,16,,https://github.com/google/deepvariant/issues/29#issuecomment-386517018,3,['install'],['install']
Deployability,Ah ok I figured that installing it fresh in an empty environment would by default force that environment to be python 2.7. let me try it with that extra requirement,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486066532:21,install,installing,21,,https://github.com/google/deepvariant/issues/177#issuecomment-486066532,1,['install'],['installing']
Deployability,"Ah yes, thanks @arostamianfar for providing the `--regions ""chr20:1,000-10,000""` example and I can now reproduce the issue. It seems like my previous internal ""fix"" hasn't really fixed it, so I'll send in another fix, which will come out in the upcoming release :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/334#issuecomment-679467513:254,release,release,254,,https://github.com/google/deepvariant/issues/334#issuecomment-679467513,1,['release'],['release']
Deployability,"Alex,; Thank you again for pointing out this issue with Cram v3.1. I'm happy to report that our next DV release will include htslib 1.18, which seems to handle these files without issue. I'll mark this issue as closed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/741#issuecomment-1894514001:104,release,release,104,,https://github.com/google/deepvariant/issues/741#issuecomment-1894514001,1,['release'],['release']
Deployability,"All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero."" --> The wording you have is confusing. I believe what you see here means that the `model_eval` code takes the checkpoints generated by `model_train`, and evaluated on the tuning data you've generated. And, based on the labeled tuning data, if the model thinks there are either no TNs(True Negatives) or FNs (False Negatives) --> this seems to indicate that it's likely at this point, the model might just don't call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/203#issuecomment-518462111:1464,release,release,1464,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111,1,['release'],['release']
Deployability,"Also, after that step, is there anything else I need to run or is that the last step of the pipeline?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/803#issuecomment-2038550387:92,pipeline,pipeline,92,,https://github.com/google/deepvariant/issues/803#issuecomment-2038550387,1,['pipeline'],['pipeline']
Deployability,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480642492:234,install,install,234,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492,3,['install'],"['install', 'installation']"
Deployability,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479522653:325,install,installed,325,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653,3,"['configurat', 'install', 'update']","['configuration', 'installed', 'update']"
Deployability,"An update:. I've tried increasing both memory and disk, and it has worked!; Sorry that I haven't tried increasing either, as I was under some time pressure. Given that DV-Pepper pipeline is a non-trivial part of the pipeline I'm building, I'll dig deeper with both @kishwarshafin and Paul's suggestions. For the moment, when I initially prototyped the pipeline using a normal sample's ONT data, this is what I found:. ![pepper shard-0 NVDA-P100 resources](https://user-images.githubusercontent.com/16310888/140320385-a28bedc9-1911-4381-84bb-475e9f3ae0e5.jpg)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-960911881:3,update,update,3,,https://github.com/google/deepvariant/issues/491#issuecomment-960911881,4,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,"And to clarify - it is important that during calling, you have to create examples that have the same heights as when we trained the release models. Which means you have to specify `--pileup_image_height_child=60` and `--pileup_image_height_parent=40` when applying WGS and PACBIO DeepTrio models. (And you have to have them as 100 if you're applying WES DeepTrio models.). If you run make_examples with dimensions that are different from how we train our release models, all steps might run through fine, but the accuracy will be completely off!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-929845497:132,release,release,132,,https://github.com/google/deepvariant/issues/488#issuecomment-929845497,2,['release'],['release']
Deployability,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:; ```; $ ldd --version; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead.; And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:; ```; $ /usr/local/bin/bazel version; Extracting Bazel installation...; Build label: 0.11.0- (@non-git); Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar; Build time: Wed Nov 5 12:47:48 +50302 (1525237217268); Build timestamp: 1525237217268; Build timestamp as int: 1525237217268; ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385868054:802,install,installation,802,,https://github.com/google/deepvariant/issues/29#issuecomment-385868054,1,['install'],['installation']
Deployability,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566496481:151,install,installed,151,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481,2,['install'],"['installed', 'installing']"
Deployability,Anna;; Apologies about the issues. If you also include the conda-forge channel in your install it should resolve cleanly:; ```; conda create -n deepvariant -c conda-forge -c bioconda python=2.7 deepvariant; ```; bioconda is heavily dependent on conda-forge packages so you'll want to include that whenever installing anything from bioconda to ensure all the dependencies are available. Hope this helps get it running for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584441591:87,install,install,87,,https://github.com/google/deepvariant/issues/177#issuecomment-584441591,2,['install'],"['install', 'installing']"
Deployability,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:8,update,update,8,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,3,"['release', 'update']","['release', 'released', 'update']"
Deployability,"Any update on this, please? Is there a quick way that I could do to work around this, maybe even temporarily? By the way, I am using the docker image of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-427460549:4,update,update,4,,https://github.com/google/deepvariant/issues/99#issuecomment-427460549,1,['update'],['update']
Deployability,Any updates on this? The link for the script is broken now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1691400894:4,update,updates,4,,https://github.com/google/deepvariant/issues/518#issuecomment-1691400894,1,['update'],['updates']
Deployability,Any updates regarding this?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/377#issuecomment-785387024:4,update,updates,4,,https://github.com/google/deepvariant/issues/377#issuecomment-785387024,1,['update'],['updates']
Deployability,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342#issuecomment-696223840:206,install,install,206,,https://github.com/google/deepvariant/issues/342#issuecomment-696223840,1,['install'],['install']
Deployability,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486#issuecomment-984133927:587,pipeline,pipeline,587,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927,1,['pipeline'],['pipeline']
Deployability,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well?. https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```; machine_type = 'custom-{0}-{1}'.format(; pipeline_args.make_examples_cores_per_worker,; pipeline_args.make_examples_ram_per_worker_gb * 1024); ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-460850032:22,configurat,configurations,22,,https://github.com/google/deepvariant/issues/150#issuecomment-460850032,2,"['Pipeline', 'configurat']","['Pipelines', 'configurations']"
Deployability,"BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6954,update,update-relnotes,6954,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['update'],['update-relnotes']
Deployability,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:268,install,install,268,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,1,['install'],['install']
Deployability,"By the way, I'll add a pointer from deepvariant-build-test.md to https://github.com/google/deepvariant/issues/756#issuecomment-1865388872. The change will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1865519973:176,release,release,176,,https://github.com/google/deepvariant/issues/756#issuecomment-1865519973,1,['release'],['release']
Deployability,"CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --version; singularity version 3.7.0; ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```; # Pull the image.; BIN_VERSION=1.5.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:2592,Install,Install,2592,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553,1,['Install'],['Install']
Deployability,"CP_OPTIMIZED_TF_WHL_FILENAME; +export DV_TF_NUMPY_VERSION=""1.24.1"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; ; # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}""; ; -export PYTHON_VERSION=3.8; +export PYTHON_VERSION=3.9; # shellcheck disable=SC2155; export PYTHON_BIN_PATH=""$(which python${PYTHON_VERSION})""; export PYTHON_LIB_PATH=""/usr/local/lib/python${PYTHON_VERSION}/dist-packages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:2204,a/b,a/build-prereq,2204,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['a/b'],['a/build-prereq']
Deployability,"CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; [bazel release 0.15.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:9851,configurat,configurations,9851,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['configurat'],['configurations']
Deployability,Can you confirm that you had activated the Conda environment you used to install DeepVariant when you ran that command?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593791623:73,install,install,73,,https://github.com/google/deepvariant/issues/664#issuecomment-1593791623,1,['install'],['install']
Deployability,"Can you provide the updated command you used?; The error is telling you that you have not provided a reference genome (`--ref`) argument. . It may be helpful to launch the docker container interactively, then verify that all the expected files are present. You can try:. ```bash; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; /bin/bash; ```. This will put you in a terminal where you can do `ls`. Make sure the reference is present in the expected location.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581#issuecomment-1303540940:20,update,updated,20,,https://github.com/google/deepvariant/issues/581#issuecomment-1303540940,1,['update'],['updated']
Deployability,"Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?. If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Is there any reason why you don't use cloud runner?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461288578:199,pipeline,pipeline,199,,https://github.com/google/deepvariant/issues/151#issuecomment-461288578,1,['pipeline'],['pipeline']
Deployability,"Close :) It's more indicative that `read_tfrecords` is not probably working properly, since `examples` is an empty list to iterate from. So probably an issue with the installed version of Tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/110#issuecomment-432111752:167,install,installed,167,,https://github.com/google/deepvariant/issues/110#issuecomment-432111752,1,['install'],['installed']
Deployability,Closing as there's been no update to this thread in 9 days. Please reopen if necessary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/24#issuecomment-355395157:27,update,update,27,,https://github.com/google/deepvariant/issues/24#issuecomment-355395157,1,['update'],['update']
Deployability,Closing because of inactivity. Feel free to open again with more updates.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102#issuecomment-436138829:65,update,updates,65,,https://github.com/google/deepvariant/issues/102#issuecomment-436138829,1,['update'],['updates']
Deployability,Closing this as we updated the 1.6.1 branch separately.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/787#issuecomment-2010256616:19,update,updated,19,,https://github.com/google/deepvariant/pull/787#issuecomment-2010256616,1,['update'],['updated']
Deployability,"Closing this issue as there has been no activity for a while. For anyone referencing this discussion, [here is a link](https://github.com/google/deepvariant/issues/137#issuecomment-452921108) to detailed instructions on how to install DeepVariant for CentOS7.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/95#issuecomment-470241986:227,install,install,227,,https://github.com/google/deepvariant/issues/95#issuecomment-470241986,1,['install'],['install']
Deployability,Cloud Platform CPU Information：; ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar ; Laptop CPU information：; ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png); ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-351888260:147,install,installed,147,,https://github.com/google/deepvariant/issues/16#issuecomment-351888260,1,['install'],['installed']
Deployability,"Correct - specifying the Python for INSTALL is the Python for building CLIF.; Is has _no_ connection to the user Python (they even can be Py2 and Py3 in; any combination).; When using CLIF the default will be the same _version_ (2 or 3) for; generating Python extension modules source code as the build Python was but; even that is controlled with (presence or absence of) --py3 flag for CLIF; tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>; wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of; > building, correct? If I choose with a python interpreter, will the user; > (Brad) need to also have python at the same location?; > I already built one here for CentOS6:; > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz; > But it seems like @chapmanb <https://github.com/chapmanb> is having; > trouble using it.; > Ideally we'll be able to specify the location differently at run time than; > the one at build time. Do you think that's possible?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386149042:36,INSTALL,INSTALL,36,,https://github.com/google/deepvariant/issues/29#issuecomment-386149042,1,['INSTALL'],['INSTALL']
Deployability,"Could you please run the following commands:. ```; echo $LD_LIBRARY_PATH; echo $PATH; ```. And also paste the path of where `libcublas.so.9.0` actually resides. Then we can patch the paths appropriately. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102#issuecomment-429170838:173,patch,patch,173,,https://github.com/google/deepvariant/issues/102#issuecomment-429170838,1,['patch'],['patch']
Deployability,Currently - no. But we may release a tumor only mode in the future.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/784#issuecomment-1983210187:27,release,release,27,,https://github.com/google/deepvariant/issues/784#issuecomment-1983210187,1,['release'],['release']
Deployability,"Currently some of the settings in `settings.sh` are outdated/not in-use (particularly the CUDA related ones), so I don't think you specifying the CUDA version there did anything. If you notice in `run-prereq.sh` it doesn't check for `TF_CUDA_VERSION`, it just automatically installs CUDA 9.0. You should be able to use CUDA 10, but you'll have to modify the code in the `CUDA` section of `run-prereq.sh` (which is a bit annoying since you'll have to manually figure out each URL). We'll clean this up as well - thanks for pointing out the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463726922:274,install,installs,274,,https://github.com/google/deepvariant/issues/145#issuecomment-463726922,1,['install'],['installs']
Deployability,"Currently, I have a mini motherboard with two RAM slots (32gb), so I cannot upgrade it. I am looking to buy maybe a quad channel motherboard. I was looking at AMDs Threadrippers due to their cores. Will I have a usage penalty using AMD instead of INTEL based cpus due to the MLK library?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-465088963:76,upgrade,upgrade,76,,https://github.com/google/deepvariant/issues/157#issuecomment-465088963,1,['upgrade'],['upgrade']
Deployability,"DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1318,Install,Installing,1318,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,"D_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1738,Install,Install,1738,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,2,"['Install', 'install']","['Install', 'install']"
Deployability,"Dear @gunjanbaid . thanks for your reply,. I am using the docker version, and the quick start with gpu does not work. ; I have Ubuntu 20.04.; I used the exact same command as quickstart. the issue is the TF and CUDA version which is not matched with the deep variant current ubuntu version. I am using RTX 3090 and this card needs a higher version of TF. I tried to make a Docker-based on the versions that I need but unfortunately, this failed too,. Would it be possible to have an additional docker for these gpu cards?. I followed the exact libraries mentioned in this link to make the docker; https://www.fatalerrors.org/a/rtx3090-ubuntu-20.04-tensorflow-2.4.0-installation-guide.html. update:. The issue is with the CUDA version, most recent GPU cards need CUDA 11. ; Is there any plan for an update? . Thanks in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-834942601:665,install,installation-guide,665,,https://github.com/google/deepvariant/issues/452#issuecomment-834942601,3,"['install', 'update']","['installation-guide', 'update']"
Deployability,"Dear Ji,. Glad it was helpful -- research can be fun like that during its discovery phases in opening up surprising doors :) This definitly got me thinking, which I am also thankful for. I have updated my previous post with the references to the papers on the two programs. Hope you have a wonderful day as well!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1683357406:194,update,updated,194,,https://github.com/google/deepvariant/issues/697#issuecomment-1683357406,1,['update'],['updated']
Deployability,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. ; It is really helpful!. The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: ; CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! ; Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/120#issuecomment-439734780:385,pipeline,pipeline,385,,https://github.com/google/deepvariant/issues/120#issuecomment-439734780,1,['pipeline'],['pipeline']
Deployability,"DeepVariant v1.0 includes two additional channels which align reads to the ALT sequences (with the channels being different when more than one ALT allele is present). . Even if the model shapes between versions are compatible, it is not a good idea to apply the model from one version with the machinery of another. This is because the training process for each model uses the machinery of that same version. I am not sure I understand the reason to use a model trained on only the v3.3.2 examples? The v4.2 truth set is more comprehensive and correct. It would be better to use chromsome20, which is always fully withheld from all training as a benchmark. Starting from the next release, we will fully withhold HG003 from all PacBio training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-727742145:680,release,release,680,,https://github.com/google/deepvariant/issues/381#issuecomment-727742145,1,['release'],['release']
Deployability,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/24#issuecomment-354129126:187,update,updated,187,,https://github.com/google/deepvariant/issues/24#issuecomment-354129126,1,['update'],['updated']
Deployability,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664:17,install,installed,17,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664,5,['install'],"['install', 'installation-guide-linux', 'installed']"
Deployability,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351260272:168,Install,Install,168,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272,14,"['INSTALL', 'Install', 'install', 'release']","['INSTALL', 'Install', 'install', 'releases']"
Deployability,"Does the RNA-Seq model work with BAMs created with HISAT2?. On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>; wrote:. > @husamia <https://github.com/husamia> although this was closed some time; > ago, we have just released an Illumina RNA-seq model and case study; > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>.; > If you are still interested in calling variants from RNA-seq data using; > DeepVariant, this should work for you.; >; > We have also updated the DeepVariant code base to be more memory efficient; > with RNA-seq data. This involves passing a new flag (--split_skip_reads),; > that allows for reads containing large SKIP to be processed efficiently.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484:224,release,released,224,,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484,2,"['release', 'update']","['released', 'updated']"
Deployability,Does the message still appear to indicate that it is using libraries installed on your machine rather than those present in the container?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580#issuecomment-1304596419:69,install,installed,69,,https://github.com/google/deepvariant/issues/580#issuecomment-1304596419,1,['install'],['installed']
Deployability,Estimated timeline for the next release is Q2 2020.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/258#issuecomment-573807042:32,release,release,32,,https://github.com/google/deepvariant/issues/258#issuecomment-573807042,1,['release'],['release']
Deployability,Finally fixed it - looks like fix was on LLVM side:; ![image](https://user-images.githubusercontent.com/41360525/137140712-84f012cb-b7b6-4d7a-a08b-22fd20ec93ed.png). I have no changes in our old installation and all working again (except pinning `jsonschema==3.2.0` as you've done [here](https://github.com/google/deepvariant/commit/fd02fa3ab8fa1d161e23d10c9931641d7ab1dcad) ),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489#issuecomment-942303491:195,install,installation,195,,https://github.com/google/deepvariant/issues/489#issuecomment-942303491,1,['install'],['installation']
Deployability,Fixed non-deterministic behavior by last patch.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-738803637:41,patch,patch,41,,https://github.com/google/deepvariant/pull/363#issuecomment-738803637,1,['patch'],['patch']
Deployability,"Follow up from my previous comment, @doron-st has shared his script here:; https://github.com/Ultimagen/VariantCalling/blob/c2634d8c08db4473e61b786eed06afc2bb8fccd1/ugvc/pipelines/convert_haploid_regions.py; (Thanks Doron!)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1286018716:170,pipeline,pipelines,170,,https://github.com/google/deepvariant/issues/518#issuecomment-1286018716,1,['pipeline'],['pipelines']
Deployability,"For the time being, this issue has been resolved by updating my local TF to 2.7.0 which is used in DV container.; `pip install -U tensorflow==2.7.0`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/555#issuecomment-1229404701:119,install,install,119,,https://github.com/google/deepvariant/issues/555#issuecomment-1229404701,1,['install'],['install']
Deployability,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624#issuecomment-1485499295:15,release,release,15,,https://github.com/google/deepvariant/issues/624#issuecomment-1485499295,2,['release'],['release']
Deployability,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint?; In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117#issuecomment-437086785:116,release,release,116,,https://github.com/google/deepvariant/issues/117#issuecomment-437086785,1,['release'],['release']
Deployability,From the list of files you posted it looks like variants were created for child (file call_variants_output_child.tfrecord.gz). The last stage of DeepTrio pipeline is postprocess_variants where data is converted from tfrecord to VCF. I suggest to try running postprocess_variants manually. . To generate a VCF:; ```; /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref GRCh38_full_analysis_set_plus_decoy_hla.fa; ```. To generate a g.VCF:; ```; /opt/deepvariant/bin/postprocess_variants --infile call_variants_output_child.tfrecord.gz --outfile <child VCF file name> --ref --gvcf_outfile <child G.VCF file name> GRCh38_full_analysis_set_plus_decoy_hla.fa --nonvariant_site_tfrecord_path gvcf_child.tfrecord-*.gz; ```. These commands are generated by run_deeptrio.py file. You may also look into this file to get a better insight on how each command is run.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/431#issuecomment-807106323:154,pipeline,pipeline,154,,https://github.com/google/deepvariant/issues/431#issuecomment-807106323,1,['pipeline'],['pipeline']
Deployability,"From the log, it seems like the issue is that `bazel` was not installed.; After you run `build-prereq.sh`, can you try typing in `bazel` as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415797913:62,install,installed,62,,https://github.com/google/deepvariant/issues/89#issuecomment-415797913,3,['install'],"['install', 'installation', 'installed']"
Deployability,"GPU_WHL_VERSION=1.12.0; ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.12.0; ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.12.0; ++ export DV_GPU_BUILD=1; ++ DV_GPU_BUILD=1; ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.12.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.12.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; [bazel release 0.15.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_ac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:9443,release,release,9443,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['release'],['release']
Deployability,Great! Looking forward to hearing what you find. I'll close this issue. Feel free to update here or open another issue if you encounter other problems.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-495306202:85,update,update,85,,https://github.com/google/deepvariant/issues/185#issuecomment-495306202,1,['update'],['update']
Deployability,"HI!; any update on this topic?. We called multiple genomes recently and found a first example of 4 adjacent base substitution AACT => GGTC being called 4 separate SNVs. The support information is very similar for the 4 calls and I wonder why deepvariant did not call them as one single MNP. This makes that the annotation and effect prediction downstream are wrong. Thanks for your support. ![failed_MNP-call](https://github.com/google/deepvariant/assets/858516/a02c3b5a-0362-4030-be8a-8b3c49129a8f). here is the extract of the gVCF at that location for one sample. ```; chrNN 51225801 . T <*> 0 . END=51225807 GT:GQ:MIN_DP:PL 0/0:50:27:0,81,809; chrNN 51225808 . A G,<*> 30.5 PASS . GT:GQ:DP:AD:VAF:PL 0/1:30:26:17,9,0:0.346154,0:30,0,37,990,990,990; chrNN 51225809 . A G,<*> 31.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,38,990,990,990; chrNN 51225810 . C T,<*> 31.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,39,990,990,990; chrNN 51225811 . T C,<*> 32.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:28:17,9,0:0.321429,0:32,0,37,990,990,990; chrNN 51225812 . C <*> 0 . END=51225881 GT:GQ:MIN_DP:PL 0/0:48:26:0,87,869; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/238#issuecomment-1700653315:9,update,update,9,,https://github.com/google/deepvariant/issues/238#issuecomment-1700653315,1,['update'],['update']
Deployability,Has this been patched yet?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/186#issuecomment-618598771:14,patch,patched,14,,https://github.com/google/deepvariant/issues/186#issuecomment-618598771,1,['patch'],['patched']
Deployability,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351081960:92,release,release,92,,https://github.com/google/deepvariant/issues/12#issuecomment-351081960,1,['release'],['release']
Deployability,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/258#issuecomment-572913563:33,release,release,33,,https://github.com/google/deepvariant/issues/258#issuecomment-572913563,3,"['release', 'update']","['release', 'updates']"
Deployability,"Hello @helizabeth1103,. dv_constants.py is the part of DeepVariant installation. ; You may take a look at https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md#notes-on-singularity for more details on DeepVariant with Singularity.; You may also check https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md that shows how to run make_examples from docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/771#issuecomment-1935308764:67,install,installation,67,,https://github.com/google/deepvariant/issues/771#issuecomment-1935308764,1,['install'],['installation']
Deployability,"Hello @pichuan , thanks for the swift reply. I installed bazel manually, the installations is working, however I get the same error from ./build-prereq.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-424863291:47,install,installed,47,,https://github.com/google/deepvariant/issues/98#issuecomment-424863291,2,['install'],"['installations', 'installed']"
Deployability,"Hello @pichuan, I ran the full deepvariant pipeline after deleting all output directories from the previous run. It seems call_variants outputs only 16 files to the intermediate dir, whereas make_examples outputs 19 (with --num_shards 19). Here's the full command:. `podman run -it --rm -e LD_LIBRARY_PATH=/usr/bin:/usr/lib/nvidia:/usr/local/nvidia/:/usr/local/cuda-12.3/lib64:/usr/local/cuda-12.3/bin:/usr/local/lib/python3.8/dist-packages/tensorrt_libs/ --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/run_deepvariant --model_type=WGS --regions 'chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM' --num_shards 19 --ref=/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz --reads=/data/bamfiles/sample1.E250013.L1.hg38.rg.bam --output_vcf=/data/variants/sample1.vcf.gz --output_gvcf=/data/variants/sample1.g.vcf.gz --intermediate_results_dir=/data/variants/sample1.intermediate --logging_dir=/data/variants/sample1.logs`. Adding the ld_library_path -argument gets rid of the error messages about libvinfer, however I still get the cuda error:. `2024-07-16 14:14:08.323907: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error`. Although call_variants did use gpu and ran in about half an hour. Then postprocess_variants halts with:; `ValueError: ptrue must be between zero and one: nan`. (Full error log in the first message) I'll try to play around with --num_shards next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2232682994:43,pipeline,pipeline,43,,https://github.com/google/deepvariant/issues/849#issuecomment-2232682994,1,['pipeline'],['pipeline']
Deployability,"Hello @pichuan, thank you, I will try this on Monday when I get back to my work machine and come back with an update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-425286485:110,update,update,110,,https://github.com/google/deepvariant/issues/98#issuecomment-425286485,1,['update'],['update']
Deployability,"Hello @sidharthgoel . Thank you for your help with this issue - I was able to build deepvariant! Tests failed, below, and I am happy to open a separate issue for this or take it somewhere else this is TensorFlow-specific. It seems that TensorFlow `r1.12` installed duing the deepvariant build is looking for CUDA 9:. ```; FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:255,install,installed,255,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['install'],['installed']
Deployability,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:1045,release,release,1045,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772,1,['release'],['release']
Deployability,"Hello Eugenio,. - I agree that it looks like the secondary alignments are causing problems, since neither the actual read sequence nor the base qualities are stored in the BAM records for secondary alignments. When the parser hits these rows, it seems to be having trouble parsing the `*` base quality string as individual values.; - The `minimap2` parameters `-x map-pb` and `-x asm` refer to alignment parameters, and don't make any assumptions on what types of reads are being aligned (e.g. reads-to-reference vs assembly-to-reference).; - The PacBio model for DeepVariant has been trained on reads-to-reference alignments with pbmm2.; - I would highly recommend aligning your HiFi reads for DeepVariant with [pbmm2](https://github.com/PacificBiosciences/pbmm2). In addition to the alignment presets (which I discuss in the next point), we have some post-alignment filters that are applied. It's also just easier to use with PacBio data. To align a human HiFi uBAM (`hifi_reads.bam`) to a reference for downstream variant calling, I use: `pbmm2 align --num-threads 24 --preset HIFI --sort -c 0 -y 70 --sample <sample_name> reference.fasta hifi_reads.bam aligned.bam`. This produces an aligned, sorted BAM, with all of the fields and tags necessary to be processed by DeepVariant. If you don't have a local SMRTLink installation, you can install pbmm2 with `conda install -c bioconda pbmm2`; - The `pbmm2 --preset HIFI` parameters are _roughly_ equivalent to these minimap parameters: `-x map-pb -a --eqx -L -O 5,56 -E 4,1 -B 5 --secondary=no -z 400,50 -r 2k -Y`. Notice that we don't use homopolymer compressed minimizers (`-H`). Billy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-815972148:1318,install,installation,1318,,https://github.com/google/deepvariant/issues/434#issuecomment-815972148,3,['install'],"['install', 'installation']"
Deployability,"Hello Gunjan @gunjanbaid,. Thank you for your effort attempting to reproduce this. I attempted to build deepvariant using:. ```; ./build-prereq.sh; ./build_and_test.sh; ```. With the `settings.sh` above. Noting #134, what is the Bazel version requirement of deepvariant?. Tensorflow master appears to recommend [0.20.0](https://github.com/tensorflow/tensorflow/blob/78c246bb6c4f5a444326daf8ba574a9291e0d096/tensorflow/tools/ci_build/install/install_bazel.sh#L18). Tensorflow master does not build with Bazel 0.15. Taking a step back, my goal is just to produce a working GPU build for my environment. Should I just use tensorflow `r1.12`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-462812667:433,install,install,433,,https://github.com/google/deepvariant/issues/145#issuecomment-462812667,1,['install'],['install']
Deployability,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-363230217:91,install,installed,91,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217,2,['install'],"['install', 'installed']"
Deployability,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-364172639:769,install,install,769,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,Hello! I also need to run DV on Ubuntu 18.04.; But the installation process is too complex. During it python and pip are installed and reinstall several times. I hope you'll provide one linear script for the installation...,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-897826566:55,install,installation,55,,https://github.com/google/deepvariant/issues/476#issuecomment-897826566,3,['install'],"['installation', 'installed']"
Deployability,"Hello! Yes, it is a benign error. All it means is that your BAM file has a blank line in its header section. We will be removing this technically-correct but actually-pointlessly-annoying warning from future releases of DeepVariant and Nucleus.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/190#issuecomment-504067535:208,release,releases,208,,https://github.com/google/deepvariant/issues/190#issuecomment-504067535,1,['release'],['releases']
Deployability,"Hello, . After running the command you provided, the output is as follows. I don't have the file ""/opt/models/pacbio/model.ckpt.data-00000-of-00001."" How can I install it? Thank you for your help.; ![1703122389646](https://github.com/google/deepvariant/assets/89448450/78a247aa-8a9e-40cc-9704-d14ad79b50e9)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1865353317:160,install,install,160,,https://github.com/google/deepvariant/issues/757#issuecomment-1865353317,1,['install'],['install']
Deployability,"Hello, @pgrosu , thanks for the support. `which bazel` returns `/usr/local/bin/bazel` (I installed with sudo). As for the gist, [here it is.](https://gist.github.com/vinisalazar/c68d290a68f12677211c1398ba3c6dcc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-424864841:89,install,installed,89,,https://github.com/google/deepvariant/issues/98#issuecomment-424864841,1,['install'],['installed']
Deployability,"Hello, as you can see the error message is ""samtools: command not found"". Can you please see if there's a way you can install samtools for your environment?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/579#issuecomment-1292297608:118,install,install,118,,https://github.com/google/deepvariant/issues/579#issuecomment-1292297608,1,['install'],['install']
Deployability,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-504749118:16,install,installing,16,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118,10,"['Rolling', 'install', 'update']","['Rolling', 'install', 'installing', 'update']"
Deployability,"Hello,; I created a tracking bug to add this feature to DeepVariant. We will post an update once we finalize the plan for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/571#issuecomment-1273885517:85,update,update,85,,https://github.com/google/deepvariant/issues/571#issuecomment-1273885517,2,"['release', 'update']","['release', 'update']"
Deployability,"Hello,; The way you run it looks correct. I don't understand how running it for the second time succeeded. Could you try to run it with the intermediate_results_dir set to the location inside /wd directory?; We will try to reproduce it and then update the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/422#issuecomment-775615394:245,update,update,245,,https://github.com/google/deepvariant/issues/422#issuecomment-775615394,1,['update'],['update']
Deployability,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774634611:373,Install,Install,373,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611,3,"['Install', 'install']","['Install', 'installation', 'installation-on-linux']"
Deployability,"Here is my diff for run-preset.sh edits:. ```; 274c274; < pip3 install ""${PIP_ARGS[@]}"" tensorrt==8.5.3.1; ---; > pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; 276c276; < pip3 show tensorrt; ---; > pip3 show nvidia-tensorrt; 278,285c278,282; < ## In v8.6.1, the libs got moved to tensorrt_libs:; < ## https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1; < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7""; < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7""; < ##export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs""; < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; < export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; ---; > # In v8.6.1, the libs got moved to tensorrt_libs:; > # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1; > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7""; > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7""; > export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs""; 292,295c289,290; < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; ---; > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/789#issuecomment-1994737405:63,install,install,63,,https://github.com/google/deepvariant/issues/789#issuecomment-1994737405,4,"['install', 'release']","['install', 'release-notes']"
Deployability,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:62,install,installed,62,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056,5,['install'],"['install', 'installed', 'installs']"
Deployability,"Hey @AndrewCarroll . This is a very interesting idea that I am going to try asap :) This whole study is taking much more time than planned, but I will keep you updated on the results! . Thanks a lot for your commitment",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1643589679:160,update,updated,160,,https://github.com/google/deepvariant/issues/682#issuecomment-1643589679,1,['update'],['updated']
Deployability,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap?. 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here?. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX; 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36; 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18; 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30; 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16; 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32; --- ; 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0; 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0; 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0; 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74; 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0; ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1600858931:286,pipeline,pipeline,286,,https://github.com/google/deepvariant/issues/666#issuecomment-1600858931,3,['pipeline'],['pipeline']
Deployability,"Hey @pichuan, @pgrosu ; thx for the quick reply's :) i tested your first idea paul still had the same issue sadly, but the solution from issue 559 seems to be working (i dont understand why so but thats another problem) since the workflow is still running so i cant say for sure but it makes the tfrecords what didnt happend the last few time; ![image](https://github.com/google/deepvariant/assets/138118818/14c3b254-ec78-468c-9a2f-301443bc3a5b). I will update when the Workflow is done . Best regards ; Sami",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677#issuecomment-1635666898:454,update,update,454,,https://github.com/google/deepvariant/issues/677#issuecomment-1635666898,1,['update'],['update']
Deployability,"Hi @A-Tsai , ; Thanks for reporting this. We also noticed this recently. It seems like intervaltree 3.0.0 came out on December 17. And previously our run-prereq script installs intervaltree without specifying the version. ; We will take two actions here:. 1) Internally, we have already updated run-prereq.sh to pin the version:; ```; pip install --user 'intervaltree==2.1.0'; ```; This will come out in the next release. But for now, your workaround is the correct thing to do. 2) We will also look into whether we can modify our code to be using the newer version of intervaltree. If we can get 3.0 to work, we will plan to pin to a newer version in the next release. Meanwhile, the previous docker versions of DeepVariant should not be affect by this issue. So that is another option you can consider. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131#issuecomment-449034956:168,install,installs,168,,https://github.com/google/deepvariant/issues/131#issuecomment-449034956,5,"['install', 'release', 'update']","['install', 'installs', 'release', 'updated']"
Deployability,"Hi @ASLeonard , thanks for the report.; Now I think about it, I might not have tested the combination of Singularity and our use of OpenVINO together.; I'll run this through the [PacBio example](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) and confirm that I can reproduce this issue. There are a few fixes I can think of, which you also suggested:; 1. We can provide a flag that allows users to provide a pointer to the OpenVINO model file. This will require the user to run the freeze_graph code to convert the model themselves.; 2. At this line:; https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L80; We can provide the intermediate directory to write the model.pb file in. And users can specify a directory that Singularity won't be unhappy with. I'll first try to reproduce the issue, and then I'll likely try with option 2 above. ; Thanks for reporting this issue. I'll update again when I look into it more. Also tagging @dkurt here as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761285739:993,update,update,993,,https://github.com/google/deepvariant/issues/404#issuecomment-761285739,1,['update'],['update']
Deployability,"Hi @ASLeonard ,. Is it possible to provide the input data so we can reproduce the error on our end? On our side, we didn’t update nucleus between versions so unsure why you are seeing this behavior. Would be very helpful if you can provide a small data to reproduce as all the tests involving cram files still passes on v1.6z",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/741#issuecomment-1831176447:123,update,update,123,,https://github.com/google/deepvariant/issues/741#issuecomment-1831176447,1,['update'],['update']
Deployability,"Hi @ASLeonard ,. Thanks for reporting this issue. We actually made a deliberate decision to not include OpenVINO this time, because in our test setting we were not able to get faster runtime. We did talk to @dkurt about this and tried https://github.com/google/deepvariant/pull/523. We will still plan to try OpenVINO again in the next release. But given that we didn't see faster runtime, we decided to leave it out of the default. If you would like to use it, please use our Dockerfile and build with the option on. I'm curious - were you seeing a speedup by using OpenVINO in DeepVariant v1.3.0? If so, what is the type of machine you're using?. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541#issuecomment-1152836865:336,release,release,336,,https://github.com/google/deepvariant/issues/541#issuecomment-1152836865,1,['release'],['release']
Deployability,"Hi @ASLeonard ,; A quick update: We think this is because we need to adjust our PL in our gVCF as well.; I'm working on a code change, but it might take a bit longer than I thought.; I just want to let you know that this is still in my queue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/811#issuecomment-2097115031:25,update,update,25,,https://github.com/google/deepvariant/issues/811#issuecomment-2097115031,1,['update'],['update']
Deployability,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox...; > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756; > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS; > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:; > unifier_config:; > drop_filtered: false; > min_allele_copy_number: 1; > min_AQ1: 10; > min_AQ2: 10; > min_GQ: 0; > max_alleles_per_site: 32; > monoallelic_sites_for_lost_alleles: true; > preference: common; > genotyper_config:; > revise_genotypes: true; > min_assumed_allele_frequency: 9.99999975e-05; > snv_prior_calibration: 0.600000024; > indel_prior_calibration: 0.449999988; > required_dp: 0; > allow_partial_data: true; > allele_dp_format: AD; > ref_dp_format: MIN_DP; > output_residuals: false; > more_PL: true; > squeeze: false; > trim_uncalled_alleles: true; > top_two_half_calls: false; > output_format: BCF; > liftover_fields:; > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}; > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:653,release,release,653,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,1,['release'],['release']
Deployability,Hi @AndrewCarroll . Is there an estimated date for next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/258#issuecomment-572937313:56,release,release,56,,https://github.com/google/deepvariant/issues/258#issuecomment-572937313,1,['release'],['release']
Deployability,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)?. Best! ; Peng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902:352,upgrade,upgrade,352,,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870041849:286,release,release,286,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849,1,['release'],['release']
Deployability,"Hi @Axze-rgb ,; I'll close this issue now. Please feel free to give further updates to this thread. If you have more questions that the DeepVariant team can help support, please feel free to open another issue.; I'm closing this so that it's easier for our people on rotation to track active issues for support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1662686740:76,update,updates,76,,https://github.com/google/deepvariant/issues/682#issuecomment-1662686740,1,['update'],['updates']
Deployability,"Hi @Axze-rgb . It's a reasonable question. We're in the process of training some non-human models using trio data and hopefully this experiment will both be positive and result in release-sable models. It's going to still take some time, but it remains an important area and one we think about.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/872#issuecomment-2311419625:180,release,release-sable,180,,https://github.com/google/deepvariant/issues/872#issuecomment-2311419625,1,['release'],['release-sable']
Deployability,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1591660025:282,release,release,282,,https://github.com/google/deepvariant/issues/661#issuecomment-1591660025,1,['release'],['release']
Deployability,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/291#issuecomment-607407000:1268,configurat,configuration-file-for-each,1268,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000,1,['configurat'],['configuration-file-for-each']
Deployability,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity?. I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076055167:74,Install,Installation,74,,https://github.com/google/deepvariant/issues/812#issuecomment-2076055167,1,['Install'],['Installation']
Deployability,"Hi @Carl-labhub ,. given that there is no activity on this issue for a while, I'll close it. Feel free to update if you have more comments or questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2101949617:106,update,update,106,,https://github.com/google/deepvariant/issues/812#issuecomment-2101949617,1,['update'],['update']
Deployability,"Hi @ColinR01 ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash; docker pull google/deepvariant:CL602468145; docker pull google/deepvariant:CL602468145-gpu; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/764#issuecomment-1928537464:61,patch,patch,61,,https://github.com/google/deepvariant/issues/764#issuecomment-1928537464,1,['patch'],['patch']
Deployability,"Hi @DLPerf , I have some results to update:. I followed you suggestions and swapped the `map` and `batch` in data_providers. And then, I changed these lines:; https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194; to:; ```; parsed = tf.io.parse_example(serialized=tf_example,; features=self.feature_extraction_spec); image = parsed['image/encoded']; if self.tensor_shape:; image = tf.io.decode_raw(image, tf.uint8); image = tf.reshape(image, [-1]+self.tensor_shape); ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:; (1) Accuracy is the same for 4 models - this is expected.; (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina); ```; real 114m10.723s; real 193m19.324s; real 74m39.826s; ```; ## WES (Illumina); ```; real 7m26.571s; real 1m24.083s; real 1m3.679s; ```. ## PacBio (HiFi); ```; real 126m28.198s; real 175m40.960s; real 67m49.753s; ```; ## Hybrid (Illumina + PacBio HiFi); ```; real 161m32.681s; real 200m26.225s; real 63m20.731s; ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:; (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers.; (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479#issuecomment-905684335:36,update,update,36,,https://github.com/google/deepvariant/issues/479#issuecomment-905684335,1,['update'],['update']
Deployability,"Hi @DLPerf ,; I was giving it a try this evening. And as you said, self.parse_tfexample seems to be now expecting a batch. I changed [this line](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L189) from parse_single_example to parse_example, and tried to make some changes later in that function too. Right now I'm not yet able to get the new version of code to work yet (the data_providers_test isn't working yet). I'll probably have to take another look at another time again. ; (Update: I might be making a bit more progress here.); But if you have some thoughts, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479#issuecomment-905196306:549,Update,Update,549,,https://github.com/google/deepvariant/issues/479#issuecomment-905196306,1,['Update'],['Update']
Deployability,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:818,install,install,818,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,3,['install'],['install']
Deployability,"Hi @DiableJambe ,; I noticed this issue is still open from a while ago. So I want to give a quick update and close this issue.; DeepVariant 0.8.0 is out yesterday. It might still not solve your original issue, but I just want to give you a heads up in case you want to try out the new version.; From @qili93 's last response, many versions of the dependencies will be different now. But hopefully it helped resolved your original problem.; I'll close this issue now. If you have more questions about the new version, please feel to open another issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-482634571:98,update,update,98,,https://github.com/google/deepvariant/issues/123#issuecomment-482634571,1,['update'],['update']
Deployability,"Hi @DiableJambe ,; thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-442966182:117,update,updates,117,,https://github.com/google/deepvariant/issues/123#issuecomment-442966182,2,['update'],"['updated', 'updates']"
Deployability,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc""; cmake 3.13.3; Protobuf 3.6.1 C++ (static build with --enable-static for bazel); bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc; Python 2 and Pip 19.0.2; Protobuf 3.6.1 C++ (uninstall static and build shared); Protobuf 3.6.1 Python (should build and install from source or CLIF will fail); TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f); CLIF; Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```; ================================================================================; (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s; (05:42:50) INFO: 1835 processes: 1835 local.; (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions; //deepvariant:allelecounter_test PASSED in 0.1s; //deepvariant:call_variants_test PASSED in 59.8s; //deepvariant:data_providers_test PASSED in 11.8s; //deepvariant:dv_vcf_constants_test PASSED in 0.5s; //deepvariant:exclude_contigs_test PASSED in 1.6s; //deepvariant:haplotypes_test PASSED in 1.7s. ▽; //deepvariant:modeling_test PASSED in 48.2s; //deepvariant:pileup_image_test PASSED in 1.8s; //deepvariant:postprocess_variants_lib_test PASSED in 0.1s; //deepvariant:postprocess_variants_test PASSED in 4.8s; //deepvariant:resources_test PASSED in 1.8s; //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-464686381:332,Install,Install,332,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381,3,"['Install', 'install']","['Install', 'install']"
Deployability,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-441822330:457,update,update,457,,https://github.com/google/deepvariant/issues/123#issuecomment-441822330,1,['update'],['update']
Deployability,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now?; If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776#issuecomment-1990519945:236,release,release,236,,https://github.com/google/deepvariant/issues/776#issuecomment-1990519945,1,['release'],['release']
Deployability,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491:71,release,release,71,,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491,1,['release'],['release']
Deployability,"Hi @GuillaumeHolley ,; to give you an update, I haven't had time to go search for an example. But I'll keep this open in case you want to share an example (or when I have a chance to find one). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/521#issuecomment-1074112393:38,update,update,38,,https://github.com/google/deepvariant/issues/521#issuecomment-1074112393,1,['update'],['update']
Deployability,"Hi @GuillaumeHolley, thanks for raising the issue. Could you try to run this with 1.0.0 image and let me know whether the issue persists? We haven't officially released 1.0.0 just yet, but we can use it to test this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/341#issuecomment-686052590:160,release,released,160,,https://github.com/google/deepvariant/issues/341#issuecomment-686052590,1,['release'],['released']
Deployability,"Hi @JakeHagen , . My guess is that our model isn't as confident, because 100bp reads is not the main type of data our model is trained on. Glad to hear that the number of calls are expected though. Certainly interesting to see that the VCF report here. (Side note: Maybe we should consider attaching these reports as part of our documentations like [metrics.md](https://github.com/google/deepvariant/blob/r1.4/docs/metrics.md). I'll take a note to consider for future releases!). By the way, In the past (starting v1.2), we did try augmenting the training data by creating 100bp and 125bp reads, but we did so by trimming. See this document: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-details-training-data.md#vfootnote12; But it's possible that our model still didn't feel confident enough with your datatype. I'll also ask around on my team to see if anyone else has other thoughts. Thanks for reporting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1320619115:468,release,releases,468,,https://github.com/google/deepvariant/issues/586#issuecomment-1320619115,1,['release'],['releases']
Deployability,"Hi @JakeHagen ,; this problem should be fixed in https://github.com/google/deepvariant/releases/tag/v1.5.0. And, starting in this release, we added the VCF stats plots to https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md and https://github.com/google/deepvariant/blob/r1.5/docs/metrics-deeptrio.md. We're glad to see that @MariaNattestad 's VCF stats tool was useful for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1457417030:87,release,releases,87,,https://github.com/google/deepvariant/issues/586#issuecomment-1457417030,2,['release'],"['release', 'releases']"
Deployability,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1356675582:591,release,release,591,,https://github.com/google/deepvariant/issues/586#issuecomment-1356675582,1,['release'],['release']
Deployability,"Hi @JoelDaon , were you able to run this?; What I found recently is that I actually needed to install `nvidia-docker` in addition to GPU driver.; I documented it for myself here:; https://gist.github.com/pichuan/6465d5f7ab56dd15a8f0d5f4d2763724. Once you have `nvidia-docker`, you'll run something like:. ```; ( time sudo nvidia-docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) >""${LOG_DIR}/call_variants.log"" 2>&1; ```. I'd love to hear whether you're able to get it work or not. Thank you!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/81#issuecomment-415637165:94,install,install,94,,https://github.com/google/deepvariant/issues/81#issuecomment-415637165,1,['install'],['install']
Deployability,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:678,pipeline,pipeline,678,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466,2,['pipeline'],['pipeline']
Deployability,"Hi @Lenbok . Thank you for the note, with the links from @aderzelle, I was able to pull in the file and visualize this event. . I think what is happening is that there are variants that can be represented in an internally consistent way at two different sets of positions. I think that DeepVariant reassembly is generating these two sets of candidates. The neural net always sees positions reassembled in the context of that particular position, so there looks to be evidence for support for each when inspected relative to the reference. We've had some internal discussions about how to improve candidate haplotype assignment for reads, but it will likely take some time to implement, test, and release. Thank you for highlighting this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/209#issuecomment-531590836:696,release,release,696,,https://github.com/google/deepvariant/issues/209#issuecomment-531590836,1,['release'],['release']
Deployability,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:60,pipeline,pipeline,60,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['pipeline'],['pipeline']
Deployability,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318:631,update,update,631,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318,1,['update'],['update']
Deployability,"Hi @PengJia6 . Please see ; ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1590741602:170,rolling,rolling,170,,https://github.com/google/deepvariant/issues/660#issuecomment-1590741602,3,"['release', 'rolling', 'update']","['release', 'rolling', 'update']"
Deployability,"Hi @Phillip-a-richmond ,; I want to give an update on this issue:. As part of working on v1.4.0, we did some experiments on this, which is still work in progress. If you can reach out to me and @AndrewCarroll (you can email me at pichuan@google.com), we can follow up on an experimental model for you to test, if you're still interested.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1145399411:44,update,update,44,,https://github.com/google/deepvariant/issues/518#issuecomment-1145399411,1,['update'],['update']
Deployability,"Hi @Phillip-a-richmond . I understand agree that this workflow is cumbersome and far from ideal. Here is the course of action that we'll propose to take:. Within the next 2 weeks, we anticipate that we'll likely to have GIAB labels for X and Y. The first course of action that we'll take is to incorporate those and attempt to train a new model with this. Based on whether this looks promising, we may ask if you are interested to test a Docker image with this model and provide feedback on it. I can't guarantee that this will work, but I think it has reasonable odds. If it does not, we do have other ideas for X and Y calling, but they would take a bit more time. I will plan to reach back out in 2 weeks with an update about the labels and a refined estimate for when we might have the model. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831:716,update,update,716,,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831,1,['update'],['update']
Deployability,"Hi @Phillip-a-richmond . Just as an update on this issue, we have the truth sets and have trained some experimental models. We're still in the process of refining these models, as well as how to use the T2T truth sets. So we do have progress, but it's still going to take time before we're ready with something for you to look at.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1082326802:36,update,update,36,,https://github.com/google/deepvariant/issues/518#issuecomment-1082326802,1,['update'],['update']
Deployability,"Hi @Phillip-a-richmond and everyone else who might be following this thread,; One update:. one post-processing trick that you can do now is: take the probability distribution, and ignore the 0/1 one, and just renormalize the other two. From there, you can decide whether it should be a 0/0 or 1/1.; (This idea came from our collaborator @doron-st at Ultima Genomics. They actually verified this on a dataset and showed good precision. They did this on DeepVariant, not DeepTrio. But I think the same trick can be applied). Our team is currently considering building this into an option in `postprocess_variants`, so that we can do this re-normalization ourselves. (Another more principled approach will be to build this knowledge into the modeling part. We're also considering that, but that will be an even longer-term solution.). Thanks @Phillip-a-richmond again for reporting this. We have filed an internal issue to track this work. I'll close this bug for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1284750172:82,update,update,82,,https://github.com/google/deepvariant/issues/518#issuecomment-1284750172,1,['update'],['update']
Deployability,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303:84,release,release,84,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303,1,['release'],['release']
Deployability,"Hi @Qianwangwoo . Yes, the two-pass method generally improves accuracy with PacBio small variant calling, especially for Indels. Whether it is likely to improve this call, I am not sure. Note that we anticipate a future release of DeepVariant for PacBio in the near future which will have comparable accuracy with a single pass of variant calling, so you may prefer to keep your current workflow and wait for that version if you don't mind updating.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/528#issuecomment-1068868000:220,release,release,220,,https://github.com/google/deepvariant/issues/528#issuecomment-1068868000,1,['release'],['release']
Deployability,"Hi @Redmar-van-den-Berg, thanks for reporting this! We will look into the issue and plan to release a fix in the next version of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/354#issuecomment-696243149:92,release,release,92,,https://github.com/google/deepvariant/issues/354#issuecomment-696243149,1,['release'],['release']
Deployability,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/311#issuecomment-637057495:632,install,installer,632,,https://github.com/google/deepvariant/issues/311#issuecomment-637057495,1,['install'],['installer']
Deployability,"Hi @SHuang-Broad , thanks for bringing up the issue about the logging file size. ; I've made two changes internally (which will come out in the next release):; 1. make_examples: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py#L221 to 2000 instead.; 2. call_variants: I changed this line https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py#L70 to 50000 instead. In a quick test on a WGS BAM, both will roughly be 1-2min intervals, which hopefully is more reasonable. Before release, I'll check the size of the log files as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-966527557:149,release,release,149,,https://github.com/google/deepvariant/issues/491#issuecomment-966527557,2,['release'],['release']
Deployability,Hi @SHuang-Broad . Thank you for the report. We think the two items are linked and we are working on a patch that we plan to release to cover the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769#issuecomment-1926125564:103,patch,patch,103,,https://github.com/google/deepvariant/issues/769#issuecomment-1926125564,2,"['patch', 'release']","['patch', 'release']"
Deployability,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112:100,release,releases,100,,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112,2,['release'],"['release', 'releases']"
Deployability,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:1222,release,releases,1222,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050,1,['release'],['releases']
Deployability,"Hi @Sami-St,. What @pichuan wrote is also very helpful to try -- I guess we were typing at the same time :) . So it works for me if I replace the beginning of the `shell` field of the Snakemake file with the following -- the input and output mapping would need to be updated based on your directory setup:. ```; docker run \; -v ""/input_files/input"":""/input"" \; -v ""/output_files/output"":""/output"" \; google/deepvariant:1.5.0 \; /opt/deepvariant/bin/make_examples \; ...; ```. This would be a first step to test before generalizing the file. Does this also work for you?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677#issuecomment-1634515397:267,update,updated,267,,https://github.com/google/deepvariant/issues/677#issuecomment-1634515397,1,['update'],['updated']
Deployability,"Hi @SaurabhKalikar ,; The latest release (v1.5.0) is now using TensorFlow 2.11. We'll also update Nucleus in a few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607#issuecomment-1457417936:33,release,release,33,,https://github.com/google/deepvariant/issues/607#issuecomment-1457417936,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:66,pipeline,pipeline,66,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059,1,['pipeline'],['pipeline']
Deployability,"Hi @ShrutiMarwaha ; The fix for https://github.com/google/deepvariant/issues/119 is included in the v0.7.2 release.; If you still see any issues, let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-447731692:107,release,release,107,,https://github.com/google/deepvariant/issues/118#issuecomment-447731692,1,['release'],['release']
Deployability,"Hi @Stikus , ; actually , it seems like simply removing the line; ```; #include <optional>; ```; will build. From the code, we're using ""optional"" from tensorflow::gtl::optional. So we don't really need the #include here. I have confirmed that removing this line builds on Ubuntu14.04. Please give that a try. If it doesn't work, let me know. I will make an internal fix, which will come out in the next release. For now, please make a local edit before you build.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236#issuecomment-557265475:404,release,release,404,,https://github.com/google/deepvariant/issues/236#issuecomment-557265475,1,['release'],['release']
Deployability,"Hi @Stikus , my patch is in https://github.com/google/deepvariant/commit/c0d6e242fc86509f6974b07955837509b5cc0b95 (and part of our r1.2 branch now). I'm also talking to the maintainers of https://github.com/google/clif to make sure things work there too. Closing this now. But I'd appreciate if you let me know if this works or not (especially if it still doesn't work). Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489#issuecomment-940357535:16,patch,patch,16,,https://github.com/google/deepvariant/issues/489#issuecomment-940357535,1,['patch'],['patch']
Deployability,"Hi @Suke-fudan ,; Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/494#issuecomment-1038521141:992,pipeline,pipeline,992,,https://github.com/google/deepvariant/issues/494#issuecomment-1038521141,1,['pipeline'],['pipeline']
Deployability,"Hi @TerjeNorderhaug, thanks for the suggestions! We will push out an updated version of the notebook by the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/182#issuecomment-488952977:69,update,updated,69,,https://github.com/google/deepvariant/issues/182#issuecomment-488952977,2,"['release', 'update']","['release', 'updated']"
Deployability,"Hi @Tintest,. Our team is currently working on the next release of DeepVariant/DeepTrio which will support ONT R10 data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/715#issuecomment-1747355967:56,release,release,56,,https://github.com/google/deepvariant/issues/715#issuecomment-1747355967,1,['release'],['release']
Deployability,"Hi @WenyuLiang ,; I'll close this issue now. Feel free to give us update here if you have found anything that might be useful to share with other users too. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/617#issuecomment-1470887594:66,update,update,66,,https://github.com/google/deepvariant/issues/617#issuecomment-1470887594,1,['update'],['update']
Deployability,Hi @Zero-Sun ; `dv_make_examples.py` isn't a file that our GitHub repo provided. Can you tell me a bit more about what's in that file and how did you install DeepVariant?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598#issuecomment-1354327575:150,install,install,150,,https://github.com/google/deepvariant/issues/598#issuecomment-1354327575,1,['install'],['install']
Deployability,"Hi @ZuyaoLiu ,; In the training tutorial, we used one individual as an example. Our release models are trained on more. You can see https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md for more information. The training data we're using are from NIST Genome in a Bottle (GIAB): https://www.nist.gov/programs-projects/genome-bottle; We train on data from HG001,HG002,HG004,HG005,HG006,HG007, and leave HG003 out from training. The ""silver dataset"" in the blog post you mentioned was used because there isn't high quality truth set for mosquitoes, like the ones for human from GIAB. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1790039083:84,release,release,84,,https://github.com/google/deepvariant/issues/722#issuecomment-1790039083,1,['release'],['release']
Deployability,"Hi @aardes, my understanding is that cuDNN v8, CUDA 11, TF 2.5, and Python 3.8 will be needed for RTX 3090. Our code is currently not ready to be upgraded to Python 3.8, but this is something we are looking into for future releases.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-838559679:146,upgrade,upgraded,146,,https://github.com/google/deepvariant/issues/452#issuecomment-838559679,2,"['release', 'upgrade']","['releases', 'upgraded']"
Deployability,"Hi @aderzelle , we're continuing to look into this issue. I'm leaving this open for now, and will give you an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/209#issuecomment-547735605:110,update,update,110,,https://github.com/google/deepvariant/issues/209#issuecomment-547735605,1,['update'],['update']
Deployability,"Hi @aderzelle ,; I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:; https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`; For example:. ```; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""; ```. With this extra arg, I do see that:; ```; I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10; 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {; key: ""cpu"" ; value: 1; }; intra_op_parallelism_threads: 1; inter_op_parallelism_threads: 1; ```; But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:; `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/361#issuecomment-709562825:244,configurat,configuration-,244,,https://github.com/google/deepvariant/issues/361#issuecomment-709562825,1,['configurat'],['configuration-']
Deployability,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/257#issuecomment-569131100:533,release,release,533,,https://github.com/google/deepvariant/issues/257#issuecomment-569131100,1,['release'],['release']
Deployability,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/209#issuecomment-553647872:25,release,released,25,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872,2,['release'],"['release', 'released']"
Deployability,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/326#issuecomment-659742784:794,Configurat,Configuration,794,,https://github.com/google/deepvariant/issues/326#issuecomment-659742784,1,['Configurat'],['Configuration']
Deployability,"Hi @aditya-88, I looked into this issue a bit further. Unfortunately, I don't think using a `disutils.spawn`-style solution will be possible here. There are a few different things to point out. First, we need not just the `pyclif` binary, but other files that get created when you install CLIF. Here are all the files present in my `clif` directory:. ```; $ ls clif; bin clang examples include lib local pip-selfcheck.json python share; ```. Second, even if you did obtain all the needed CLIF files, we still cannot check for them to present at other paths. In our [`WORKSPACE` file](https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L78) for Bazel, we create a `new_local_repository` for CLIF. Creating this `new_local_repository` requires specifying a full absolute path in advance (more details in [the docs](https://docs.bazel.build/versions/master/be/workspace.html#new_local_repository)). Two solutions for you to get around this would be:; 1. Create a symlink to your CLIF directory via `ln -s $SOME_PATH/clif /usr/local/clif`; OR; 2. Modify the path in the [`WORKSPACE` file](https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L81). For example, if your files are at `/home/my_account/clif/`, you can change the path to `/home/my_account`. We can definitely make this more clear in our documentation in the future.; @pichuan Feel free to add on if I missed anything.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/160#issuecomment-472209625:281,install,install,281,,https://github.com/google/deepvariant/issues/160#issuecomment-472209625,1,['install'],['install']
Deployability,"Hi @ajsa-nukovic , thanks for reporting the issue. First, I tried to reproduce your issue. I haven't been able to reproduce it yet. I wrote down my commands below:. Get a Ubuntu18.04 machine ; ```; gcloud compute instances create ""${USER}-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""e2-medium"" \; --zone ""us-west1-b""; ```. After ssh into the machine, I ran:. ```; sudo apt -y update && sudo apt -y install docker.io; ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/432#issuecomment-806341687:483,update,update,483,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687,2,"['install', 'update']","['install', 'update']"
Deployability,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458#issuecomment-844317545:1189,update,update,1189,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545,1,['update'],['update']
Deployability,"Hi @akolesnikov , thank you for your response here i attached code, terminal output and log file. code is running but neither output generating or error throwing just running.; please see below code and log file. ###### code #############; #!/usr/bin/env nextflow. nextflow.enable.dsl=2; params.outdir = '/home/deepak/integration/resu1'; params.data_dir = '/home/deepak/integration/resu1/4.markDupliM'; params.refhg38 = '/home/deepak/integration/hg381_22XYM'; params.bed = '/home/deepak/integration'. workflow {; // Define channels for input data; Channel; .fromPath(""${params.data_dir}/*_sorted_md.bam""); .map { file -> ; def sample_id = file.baseName.replace('_sorted_md', ''); return [sample_id, file]; }; .set { read_pairs }; /// Step 1. DeepVariant; DeepVariant(read_pairs, params.refhg38, params.bed); }. process DeepVariant {; tag ""deepavar on ${sample_id}""; publishDir ""${params.outdir}/5.finaleepvar"", mode: 'copy'; cpus 4; //BIN_VERSION 1.6.1. input:; tuple val(sample_id), path(read_files); val(params.refhg38); val(params.bed); ; output:; //tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), path(""${sample_id}_rawd.gvcf.gz""), emit: raw_vcfs; tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), emit: raw_vcfs. script:; """"""; docker run \; -v ""${params.data_dir}"":/opt/bam -v ""${params.refhg38}"":/opt/refhg38 -v ""${params.bed}"":/opt/bed \; google/deepvariant:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /opt/refhg38/Homo_sapiens_assembly38cleaned.fasta \; --reads /opt/bam/${read_files} \; --regions /opt/bed/hg38_exomeY.bed \; --output_vcf /opt/bam/${sample_id}_rawd.vcf.gz \; --num_shards ${task.cpus}; """"""; }. ######## code ################. terminal:; (base) deepak@ubuntu22:~/integration$ nextflow run final_deepvarian.nf . N E X T F L O W ~ version 24.04.4. Launching `final_deepvarian.nf` [hungry_stonebraker] DSL2 - revision: 4dab17f4f2. executor > local (1); [dd/64034b] DeepVariant (deepavar on SRR26512958) [ 0%] 0 of 2. log file attached",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013:318,integrat,integration,318,,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013,4,['integrat'],['integration']
Deployability,"Hi @alanlamsiu ,. If I understand correctly, your .sif file was previously built from our Docker version?. I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh; sudo bash -x install_nvidia_docker.sh ; ```; (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh; sudo bash -x install_singularity.sh ; ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash; BIN_VERSION=1.6.0; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu""; ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash; pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif ; -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif; ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I ask",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:830,install,installed,830,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,1,['install'],['installed']
Deployability,"Hi @anands-repo . Thank you for running the evaluation! The work detailed in the blog and in the publication (https://www.biorxiv.org/content/biorxiv/early/2019/01/23/519025.full.pdf) represent our initial efforts to train a model on PacBio data. Subsequent to that publication, PacBio generated more extensive training Sequel II data. We used this data to build more robust training datasets across a diverse range of coverage, insert sizes, and machine runs. This is the model which has been released in version 0.8.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/201#issuecomment-516839518:494,release,released,494,,https://github.com/google/deepvariant/issues/201#issuecomment-516839518,1,['release'],['released']
Deployability,"Hi @andremrsantos. I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend Best practices for multi-sample variant calling with DeepVariant. We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. In our benchmarks, we found this merging approach more accurate than merging with GATK GenotypeGVCFs (and we found merging DeepVariant gVCFs using GATK GenotypeGVCFs substantially less accurate than the single-sample DeepVariant VCFs themselves). Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/83#issuecomment-553660314:31,update,update,31,,https://github.com/google/deepvariant/issues/83#issuecomment-553660314,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @andrewrech ; I'll be closing this issue.; To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-467081816:147,install,install,147,,https://github.com/google/deepvariant/issues/145#issuecomment-467081816,1,['install'],['install']
Deployability,"Hi @andrewrech and @shalabhsuman. I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). Although this case study is a trio, we have optimized parameters for cohorts scaling into the 1000's, so we feel this will work well for your use cases. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-553665228:46,update,update,46,,https://github.com/google/deepvariant/issues/142#issuecomment-553665228,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @andrewrech, you should go ahead with `r1.12`. You will also need to set `pip install --user 'intervaltree==2.1.0'` in run-prereq.sh (we'll have a patch out for this shortly). Let me know if you have any issues after that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-462930823:81,install,install,81,,https://github.com/google/deepvariant/issues/145#issuecomment-462930823,2,"['install', 'patch']","['install', 'patch']"
Deployability,"Hi @anitagh , I want to give you an update that after today's release (v0.9.0), you can now use the `--sample_name` flag with run_deepvariant.py:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L89. And, we also made the behavior more robust so that even with multiple or no sample names, we'll try to assign a reasonable default, and proceed with a warning (but without crashing). If you have more questions please feel free to follow up here, or file new issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-553655187:36,update,update,36,,https://github.com/google/deepvariant/issues/222#issuecomment-553655187,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @anitagh ,; the release won't come out within a week. Sorry for the inconvenience. For now you'll have to run make_examples separately and add the `--sample_name` flag. Another way to fix this is to make sure your BAM file header has one SM tag in it. If your BAM file is not very big, using `samtools reheader` to add a proper SM tag might also be a reasonable solution for now. If you're using GCP, you can try out the Google Cloud version that @samanvp pointed to in earlier comments. (If you have any feedback on that tool, please report to https://github.com/googlegenomics/gcp-deepvariant-runner/issues.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-535654563:19,release,release,19,,https://github.com/google/deepvariant/issues/222#issuecomment-535654563,1,['release'],['release']
Deployability,"Hi @anitagh ; From my experience it's not too uncommon for BAM files in the wild to have either no sample name in the BAM file, or multiple sample names (which will crash DeepVariant right now). We can easily add `--sample_name` to the run_deepvariant.py script, so you can still run that script once. We'll do this so it'll come out in the next release. So far, we want users to explicitly add this --sample_name this flag because we want to make sure users are aware that their BAM file has more than one (or 0) sample names. But it seems like all the cases I've seen so far, none of them actually intended for them to be different sample names anyway. So I might consider just removing this constraint and just make it a warning message instead. Either way, in our next release, you should be expecting to have `--sample_name` flag in the run_deepvariant.py script which you can do in one step. Thanks for reporting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-535102260:346,release,release,346,,https://github.com/google/deepvariant/issues/222#issuecomment-535102260,2,['release'],['release']
Deployability,"Hi @anitagh and @PlatonB , ; to give you an update on this issue, we have made a change internally that:; 1) Added `--sample_name` to run_deepvariant.py; 2) When input BAM files has no sample names or more than one sample names, instead of crashing with the error message you reported, we now use a default string as the sample name (or pick one from the multiple names) and prints out a warning. This behavior should be less cumbersome to our users, and shouldn't cause any issues for most use cases. Our team is working towards a next release. Once the release is out, I can post another update to this issue to let you know. For now, please bear with us and use the solution in https://github.com/google/deepvariant/issues/222#issuecomment-534768468. We'll keep you posted!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-535311937:44,update,update,44,,https://github.com/google/deepvariant/issues/222#issuecomment-535311937,4,"['release', 'update']","['release', 'update']"
Deployability,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own.; You can find the logic here: ; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-845522247:1812,update,update,1812,,https://github.com/google/deepvariant/issues/457#issuecomment-845522247,1,['update'],['update']
Deployability,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/277#issuecomment-589689487:435,continuous,continuous,435,,https://github.com/google/deepvariant/issues/277#issuecomment-589689487,1,['continuous'],['continuous']
Deployability,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1 \; --sample_name=FOOBAR; ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```; $ zcat quickstart-output/output.vcf.gz | grep FOOBAR; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR; ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/334#issuecomment-678050738:1491,release,release,1491,,https://github.com/google/deepvariant/issues/334#issuecomment-678050738,1,['release'],['release']
Deployability,"Hi @arostamianfar, thanks for reporting this issue. We plan to push out a fix for this problem in a future release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/186#issuecomment-497158522:107,release,release,107,,https://github.com/google/deepvariant/issues/186#issuecomment-497158522,1,['release'],['release']
Deployability,"Hi @bopohdr . I cannot speak to the dataset referenced in this paper, as it is not available, and I am not sure how merging would have been performed, and the paper references an earlier DeepVariant version. However, a higher de novo rate of DeepVariant relative to other callers is not something that we observe in the investigations we have conducted. I am attaching (if they will upload) two VCF files for the Oslo University HG002-HG003-HG004 trio available in Genome in a Bottle FTP. The DeepVariant trio is called with DeepVariant exome model and merged with GLnexus. The GATK4 trio is called with HaplotypeCaller and merged with GenotypeGVCFs. In these trios, DeepVariant has 87 de novo calls where the child is not 0/0 but both parents are 0/0; The GATK4 trio has 270 de novo cases with the same criteria. We have a set of documentation for best practices in merging a trio that we are hoping to release in the near future. If you would like me to share that with you privately now, you can reach out directly at awcarroll@google.com. Thanks,; Andrew. [deepvariant.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623783/deepvariant.cohort.vcf.gz); [gatk4.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623784/gatk4.cohort.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/220#issuecomment-532442476:904,release,release,904,,https://github.com/google/deepvariant/issues/220#issuecomment-532442476,1,['release'],['release']
Deployability,"Hi @brentp . This is a reasonable request, and CRAM->BAM conversion is faster via a samtools command. I don't think it will add much to the size of the container to include this, so we'll look at doing this, and will probably plan to do so, likely in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/414#issuecomment-768582500:260,release,release,260,,https://github.com/google/deepvariant/issues/414#issuecomment-768582500,1,['release'],['release']
Deployability,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-588045756:496,release,release,496,,https://github.com/google/deepvariant/issues/272#issuecomment-588045756,1,['release'],['release']
Deployability,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-605207780:69,release,released,69,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780,4,"['Update', 'release']","['Updated', 'release', 'released', 'releases']"
Deployability,"Hi @case3526, we recommend running DeepVariant v0.8.0 using the Docker image or prebuilt binaries, instead of the `gcp_deepvariant_runner` pipeline. Here are links to case studies that show how you can run using Docker or binaries. Note: we recommend running the binaries on an Ubuntu 16.04 machine. * [DeepVariant quickstart with Docker](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md); * [Script to run WGS case study using binaries](https://github.com/google/deepvariant/blob/r0.8/scripts/run_wgs_case_study_binaries.sh)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/210#issuecomment-524688432:139,pipeline,pipeline,139,,https://github.com/google/deepvariant/issues/210#issuecomment-524688432,1,['pipeline'],['pipeline']
Deployability,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385864674:23,update,update,23,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674,5,"['Install', 'install', 'release', 'update']","['Install', 'install', 'release-SCL', 'update']"
Deployability,"Hi @chris-sf ; Thanks again for reporting this issue. We made an internal change to address this issue, which should come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112#issuecomment-442521534:138,release,release,138,,https://github.com/google/deepvariant/issues/112#issuecomment-442521534,1,['release'],['release']
Deployability,"Hi @chrisfleisch ; I want to follow up on this issue. The path change earlier was actually due to this commit back in Oct 2018:; https://github.com/google/deepvariant/commit/f3207bb2e4db50f858343b97fa8bdec0fb908ab7. Because we added `--user` to pip install (even when the user is root), that's what caused the change you observed. In our upcoming release, we will plan to change this so that we won't pip install with `--user` when the user is root, I have tested internally that our new docker image will be convertible to a singularity image. I will also plan to document the steps of the conversion, which is basically what you mentioned in in earlier comment in this thread. Thank you for pointing out this issue. I will close this issue now. When v0.8.0 comes out, if you're still encountering any issues, please feel free to file another issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-480945659:249,install,install,249,,https://github.com/google/deepvariant/issues/132#issuecomment-480945659,3,"['install', 'release']","['install', 'release']"
Deployability,Hi @coneheadusa ; Can you try installing intervaltree==3.0.2?; This is what we required in the run-prereq.sh script:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/run-prereq.sh#L89,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255#issuecomment-568962849:30,install,installing,30,,https://github.com/google/deepvariant/issues/255#issuecomment-568962849,1,['install'],['installing']
Deployability,"Hi @crazysummerW , I'm curious whether you're able to resolve this. Given that there hasn't been updates for 2 months now, I'll close this for now. But please feel free to reopen if you still have issues, or feel free to post updates if you have new findings. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1882088395:97,update,updates,97,,https://github.com/google/deepvariant/issues/725#issuecomment-1882088395,2,['update'],['updates']
Deployability,"Hi @crazysummerW , from the error log, it seems like your BAM file doesn't have base quality scores. Is that expected from your pipeline?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1613645470:128,pipeline,pipeline,128,,https://github.com/google/deepvariant/issues/672#issuecomment-1613645470,1,['pipeline'],['pipeline']
Deployability,"Hi @crazysummerW ,. DeepTrio will ignore the existing HP value from the BAM, and will use our internal read phasing.; Specifically, this block of logic deals with that:; https://github.com/google/deepvariant/blob/r1.6/deepvariant/make_examples_core.py#L2133-L2135. I agree that the way our case study is written is confusing. We should at least mentioned that the HP information from the BAM would be discarded, or use an unphased BAM as an example. I will file an internal issue to track this, so we can update it in the future. Thanks for noticing this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1782229530:505,update,update,505,,https://github.com/google/deepvariant/issues/720#issuecomment-1782229530,1,['update'],['update']
Deployability,"Hi @crazysummerW ,. One more thought:; You mentioned ""I also tested the Revio Hifi data. It succeeded."" --> If that's the case, you don't need to re-run DeepConsensus before running DeepVariant, because DeepConsensus is already run on Revio, and in fact, the model deployed on Revio machine is the appropriate one to use. So, I am not sure why you need to re-run DeepConsensus. Maybe I miss some context here.; That's why I encourage you to post directly on https://github.com/google/deepconsensus/issues instead of here. So that we can understand your use case of DeepConsensus. One more thing:; When running DeepConsensus, the code snippet you posted is only an intermediate step. You'll need to finish running through the steps to get to the FASTQ file here: https://github.com/google/deepconsensus/blob/r1.2/docs/quick_start.md#run-deepconsensus . Specifically this step:; ```; deepconsensus run \; --subreads_to_ccs=${shard_id}.subreads_to_ccs.bam \; --ccs_bam=${shard_id}.ccs.bam \; --checkpoint=model/checkpoint \; --output=${shard_id}.output.fastq; ```. is the one that generate the FASTQ you'll use for mapping and then variant calling. Hope this helps. For more DeepConsensus questions, please post on https://github.com/google/deepconsensus/issues and we can take it from there later next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1615974070:265,deploy,deployed,265,,https://github.com/google/deepvariant/issues/672#issuecomment-1615974070,1,['deploy'],['deployed']
Deployability,"Hi @crazysummerW ,; I noticed in your title you mentioned ""Revio data"". If you're using Revio, DeepConsensus is already run. You don't need to run it again. So, please proceed with your usual mapping + DeepVariant pipeline, and you should be able to get your variant calls that way. If I'm not interpreting your use case correctly, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1615281965:214,pipeline,pipeline,214,,https://github.com/google/deepvariant/issues/672#issuecomment-1615281965,1,['pipeline'],['pipeline']
Deployability,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/623#issuecomment-1482167397:64,release,release,64,,https://github.com/google/deepvariant/issues/623#issuecomment-1482167397,2,['release'],"['release', 'releases']"
Deployability,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631#issuecomment-1507742336:303,continuous,continuous,303,,https://github.com/google/deepvariant/issues/631#issuecomment-1507742336,1,['continuous'],['continuous']
Deployability,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483448362:280,configurat,configuration,280,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362,2,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team.; If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests!. Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress.; If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749#issuecomment-1848067102:1343,pipeline,pipeline,1343,,https://github.com/google/deepvariant/issues/749#issuecomment-1848067102,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/755#issuecomment-1865191737:495,update,updated,495,,https://github.com/google/deepvariant/issues/755#issuecomment-1865191737,3,['update'],"['updated', 'updates']"
Deployability,"Hi @desmodus1984 ,. Fundamentally, training a DeepVariant requires truth data (truth variants and confident regions).; The core question here is: **Would you be able to get truth data for the bats you're studying**?. I quickly looked through your recent discussion with @kishwarshafin .; I believe @kishwarshafin has been trying to give you some tips on ways to construct truth. Note that this is an advanced topic. We don't expect most of our users to train DeepVariant models, or to construct truth data. However, if you do have truth data (truth variants and confident regions), you should be able to follow the documentation https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md to try to train a model. From your description above, I still think the best way to proceed is to directly use DeepVariant release models. Once you have the callsets, try to evaluate the calls first. Even if you plan to train a model, it'll be good to have those baseline metrics available, so you know whether your trained model is working or not. Does this help? If I'm misunderstanding your question, let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878#issuecomment-2364726373:842,release,release,842,,https://github.com/google/deepvariant/issues/878#issuecomment-2364726373,1,['release'],['release']
Deployability,"Hi @desmodus1984 ,. I just checked our singularity solution and it works on a n2-standard-64 GCP machine. Without knowing much about your system, my guess is it's probably something related to how singularity is installed in your system? Are you on an HPC?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/886#issuecomment-2375010750:212,install,installed,212,,https://github.com/google/deepvariant/issues/886#issuecomment-2375010750,1,['install'],['installed']
Deployability,"Hi @desmodus1984 ,; For many of the non-human applications, we see many use cases where people just apply our release models. ; I think directly using our model would be a good enough start for your downstream application. If you have some way to evaluate (e.g., Mendelian violation), then you can do some comparison to assess how good the calls are. If you have trio data, you can try to run DeepTrio (also using our release model) , which I think will be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878#issuecomment-2333164220:110,release,release,110,,https://github.com/google/deepvariant/issues/878#issuecomment-2333164220,2,['release'],['release']
Deployability,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```; ...; W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-712402658:15,update,update,15,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658,1,['update'],['update']
Deployability,"Hi @dkurt , sorry it took me a while to get to this. Update: I've run internally once with your updated code. So far I've only run on one WES example so I don't have full timing stats yet. I want to confirm that it's expected that I'm seeing:. ```; Please install required versions of components or run pip installation; pip install openvino-dev[tensorflow]; ```. I'll continue to look into the runtime and let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/523#issuecomment-1069241340:53,Update,Update,53,,https://github.com/google/deepvariant/pull/523#issuecomment-1069241340,5,"['Update', 'install', 'update']","['Update', 'install', 'installation', 'updated']"
Deployability,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release.; 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion.; 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False.; 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736148441:27,update,update,27,,https://github.com/google/deepvariant/pull/363#issuecomment-736148441,3,"['release', 'update']","['release', 'update']"
Deployability,"Hi @dkurt ,; thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement.; 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-710709303:557,update,update,557,,https://github.com/google/deepvariant/pull/363#issuecomment-710709303,1,['update'],['update']
Deployability,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709920659:674,release,release,674,,https://github.com/google/deepvariant/pull/363#issuecomment-709920659,2,"['release', 'update']","['release', 'updates']"
Deployability,"Hi @dridk, thank you for the pull request! Unfortunately, we cannot merge pull requests through GitHub. I can fix this internally, and the change should go out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/424#issuecomment-780944002:172,release,release,172,,https://github.com/google/deepvariant/pull/424#issuecomment-780944002,1,['release'],['release']
Deployability,"Hi @drtamermansour ; I have not tried building Singularity before this latest release, so I can't guarantee how easy it would be. Regarding your issue with `--regions`, I'm pretty sure it's because the intervaltree version issue. Please see this comment for a solution if you have to build from DeepVariant before 0.8.0:; https://github.com/google/deepvariant/issues/131#issuecomment-449034956. We highly recommend building from the latest one (0.8.0) if possible, though. . In terms of sharing file, we'll see if @williamrowell can share his file. I can also see if I can find a reasonable place to share mine (which is also built on a CentOS machine on GCP)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-487657800:78,release,release,78,,https://github.com/google/deepvariant/issues/178#issuecomment-487657800,1,['release'],['release']
Deployability,"Hi @drtamermansour,; In my first attempt to reproduce this, I wasn't able to reproduce the issue with the following setting:. 1. I got a CentOS 7 machine with:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-487218238:439,install,installed,439,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238,2,"['INSTALL', 'install']","['INSTALL', 'installed']"
Deployability,"Hi @eaooms ,. Unfortunately, our released DeepVariant Illumina WES or WGS models will not work with phasing information. This use case could be possible if you collect training data on your own, and train your own model accordingly. But that's an advanced use case that our team won't be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/828#issuecomment-2159309932:33,release,released,33,,https://github.com/google/deepvariant/issues/828#issuecomment-2159309932,1,['release'],['released']
Deployability,"Hi @ed5152 - adding an additional class may require a substantial amount of work. You will likely need to update the make_examples step, the training/inference step, and the postprocessing step. Can you provide additional context for what you are trying to do?. [make_examples_core.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_core.py#L1504-L1519) contains the logic for generating examples. You would modify the code to allow for an additional class here. Then you would need to subclass the DeepVariant model to modify the final layer and add an extra output. https://github.com/google/deepvariant/blob/b6a91d5f8514cc09a4f059b9980ac80043a899c3/deepvariant/modeling.py#L420-L426. Finally, you would need to update [postprocess_variants.py](https://github.com/google/deepvariant/blob/r1.2/deepvariant/postprocess_variants.py) to handle an additional class output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/495#issuecomment-981766539:106,update,update,106,,https://github.com/google/deepvariant/issues/495#issuecomment-981766539,2,['update'],['update']
Deployability,"Hi @edg1983 , thanks for bringing up this issue!; We have already been looking into this, and have already made a few internal fixes (done by @akolesnikov) that will be out in the next release.; I'm closing this issue for now. Feel free to comment or reopen if you have more questions or suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/487#issuecomment-927384413:185,release,release,185,,https://github.com/google/deepvariant/issues/487#issuecomment-927384413,1,['release'],['release']
Deployability,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-815421572:1584,release,releases,1584,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572,1,['release'],['releases']
Deployability,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:; 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now); 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2168729043:445,update,update,445,,https://github.com/google/deepvariant/issues/830#issuecomment-2168729043,4,"['PATCH', 'release', 'update']","['PATCH', 'releases', 'update']"
Deployability,"Hi @fo40225, thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from GitHub back into our codebase within Google. We will look into these changes and are happy to submit a patch after internal testing and benchmarking. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-462244723:345,patch,patch,345,,https://github.com/google/deepvariant/pull/152#issuecomment-462244723,1,['patch'],['patch']
Deployability,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/301#issuecomment-617907163:536,install,installing,536,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163,1,['install'],['installing']
Deployability,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash; rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \; --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \; --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \; --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \; --lenient --output-inconsistent trio-non-mendelian.vcf.gz; ```. You can generate the SDF by running:; ```bash; rtg format -o /path/to/ref.sdf /path/to/ref.fasta; ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/779#issuecomment-1974860720:87,pipeline,pipeline-based,87,,https://github.com/google/deepvariant/issues/779#issuecomment-1974860720,1,['pipeline'],['pipeline-based']
Deployability,"Hi @frapaport , ; I would like to understand better how we can help the users that don't have root permission.; One question for you; Given that you don't have root permission on your machine, I assume that using our pre-built binaries is also not possible. (Because it requires running run-prereq.sh, which currently uses `sudo` to install a bunch of stuff.). Other than bioconda, are there other common ways to run/install softwares that you think works well?; For example, several of our users mentioned Singularity. Have you used that, and would you consider that?. (Currently I'm personally not very familiar with either bioconda or singularity. I'm trying to get a better understanding of what will be more generally useful for users who don't have root permission.) If you have any suggestions, please let me know!. I'll come back to your questions later as well. Might take me a while to try this again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-480342454:333,install,install,333,,https://github.com/google/deepvariant/issues/137#issuecomment-480342454,2,['install'],['install']
Deployability,"Hi @frapaport , I added some notes about Singularity in https://github.com/google/deepvariant/issues/132#issuecomment-482430728 . I'm still figuring out what's a best way to distribute images. I do have the image files that I built. If it's useful to share those files, let me know. ; @kokyriakidis I have an example for a GPU run in that comment as well. And, addressing the original topic about bioconda, I'll get in touch with @chapmanb to see how to update the version to 0.8.0 and I'll also see if I can try it out more myself as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-482432189:454,update,update,454,,https://github.com/google/deepvariant/issues/137#issuecomment-482432189,1,['update'],['update']
Deployability,"Hi @gambalab . The data for our hybrid model is described here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-hybrid-case-study.md . It's merged from PacBio and Illumina data. We have a page that describes the amount of training data we used for the models for each release, you can find them in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md. If you need some BAM files to train on, you can also see our [An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022) manuscript. (Data can be found in `gs://brain-genomics-public/research/sequencing` ) For Hybrid, you'll need to make the hybrid BAM files yourself following the instructions in the first documentation. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/721#issuecomment-1777228301:285,release,release,285,,https://github.com/google/deepvariant/issues/721#issuecomment-1777228301,1,['release'],['release']
Deployability,"Hi @gambalab . We'll discuss this in the team today. We don't make or directly control the conda install for DeepVariant, that is done by external people. We'll try to assess our ability to debug or offer suggestions for conda issues like this today, but it might be a bit before we have a suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1601287727:97,install,install,97,,https://github.com/google/deepvariant/issues/669#issuecomment-1601287727,1,['install'],['install']
Deployability,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:90,install,installed,90,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351,4,"['install', 'release']","['install', 'installed', 'releases']"
Deployability,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:; ```; # Bazel's --build_python_zip replaces our carefully engineered symbolic links; # with copies. This function puts the symbolic links back.; function fix_zip_file {; orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place.; TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX); # The .zip version of the binary doesn't have the header that makes it; # self-executable. We use that version because otherwise unzip would; # complain and raise an error code.; cp ""${orig_zip_file}.zip"" ""${TMPDIR}""; ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356#issuecomment-699007209:93,install,installation,93,,https://github.com/google/deepvariant/issues/356#issuecomment-699007209,1,['install'],['installation']
Deployability,"Hi @gunjanbaid, thank you! I'll try the v0.8.0 release and other recommendations. I’m working on a cannabis variants project with a Googler @allenday.; We consulted @AndrewCarroll previously, from his preliminary analysis - seems that default DeepVariant model should work fine with our data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207#issuecomment-527322608:47,release,release,47,,https://github.com/google/deepvariant/issues/207#issuecomment-527322608,1,['release'],['release']
Deployability,"Hi @gunjanbaid,; I'm starting pipeline with a GCP runner like this:; ```; MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard; IMAGE_VERSION=0.6.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". ./opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ""${PROJECT_ID}"" \; --zones ""${ZONES}"" \; --docker_image ""${DOCKER_IMAGE}"" \; --docker_image_gpu ""${DOCKER_IMAGE_GPU}"" \; --gpu \; --outfile ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" \; --staging ""${OUTPUT_BUCKET}""/""${STAGING_FOLDER_NAME}"" \; --model ""${MODEL}"" \; --ref ""${INPUT_REF}"" \; --bam ""${INPUT_BAM}"" \; --shards 512 \; --make_examples_workers 16 \; --make_examples_cores_per_worker 10 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 16 \; --call_variants_cores_per_worker 8 \; --call_variants_ram_per_worker_gb 30 \; --call_variants_disk_per_worker_gb 50; ```; I've checked logs - a bunch of tasks failed (8, 32, 67, 105, 192, 231, 261, 293, 358).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207#issuecomment-523380440:30,pipeline,pipeline,30,,https://github.com/google/deepvariant/issues/207#issuecomment-523380440,1,['pipeline'],['pipeline']
Deployability,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/687#issuecomment-1654874111:525,configurat,configuration,525,,https://github.com/google/deepvariant/issues/687#issuecomment-1654874111,2,['configurat'],['configuration']
Deployability,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```; docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true; ```. The output I get:. ```; paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994:194,update,updated,194,,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994,1,['update'],['updated']
Deployability,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624:32,update,update,32,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624,1,['update'],['update']
Deployability,"Hi @internalsensor , please see my answer below.; (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===; Hi @internalsensor , ; I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found.; In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh; bash -x run_wes_case_study_prebuilt_binaries.sh; ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error.; You can also use `pip show intervaltree` to double check what pip package you have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155#issuecomment-464534790:280,install,installed,280,,https://github.com/google/deepvariant/issues/155#issuecomment-464534790,2,['install'],"['install', 'installed']"
Deployability,"Hi @internalsensor ,; can you tell us how you're running DeepVariant?; Is this binaries you built on your own, or pre-built binaries you copied from us, or Docker image?; If it's the first or second, you need to change run-prereq.sh to; ```; pip install --user 'intervaltree==2.1.0'; ```. This is because intervaltree got upgraded during this time, and we didn't pin it in the last version. We'll plan to fix it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155#issuecomment-464508728:246,install,install,246,,https://github.com/google/deepvariant/issues/155#issuecomment-464508728,3,"['install', 'release', 'upgrade']","['install', 'release', 'upgraded']"
Deployability,"Hi @japhill , to give you an update, I plan to add this section to our FAQ in the next release:. ---. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid this issue. See:; https://github.com/google/deepvariant/issues/530#issuecomment-1076923302 for more details. ---. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1079509951:29,update,update,29,,https://github.com/google/deepvariant/issues/530#issuecomment-1079509951,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @japhill ,; Just to make sure I understand this - are you saying that the Docker image has /mnt in it, and as result was causing problem with Singularity?. I do see a /mnt directory:; ```; $ sudo docker run google/deepvariant:1.3.0 ls -lh /; total 48K; lrwxrwxrwx 1 root root 7 Oct 6 16:47 bin -> usr/bin; drwxr-xr-x 2 root root 4.0K Apr 15 2020 boot; drwxr-xr-x 5 root root 340 Mar 23 23:34 dev; drwxr-xr-x 1 root root 4.0K Mar 23 23:34 etc; drwxr-xr-x 2 root root 4.0K Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 6 16:47 lib -> usr/lib; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib32 -> usr/lib32; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib64 -> usr/lib64; lrwxrwxrwx 1 root root 10 Oct 6 16:47 libx32 -> usr/libx32; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 media; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 mnt; drwxr-xr-x 1 root root 4.0K Dec 6 23:17 opt; dr-xr-xr-x 525 root root 0 Mar 23 23:34 proc; drwx------ 1 root root 4.0K Dec 6 23:15 root; drwxr-xr-x 5 root root 4.0K Oct 6 16:58 run; lrwxrwxrwx 1 root root 8 Oct 6 16:47 sbin -> usr/sbin; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 srv; dr-xr-xr-x 13 root root 0 Mar 23 23:34 sys; drwxrwxrwt 1 root root 4.0K Dec 6 23:19 tmp; drwxr-xr-x 1 root root 4.0K Oct 6 16:47 usr; drwxr-xr-x 1 root root 4.0K Oct 6 16:58 var; ```; which is empty, so I think your suggestion of something like ""RUN rm -rf /mnt/"" makes sense. I'll also do a quick search to see if there are better approaches here. Thanks for the feedback. I'll track internally and make sure this is updated in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454:1511,update,updated,1511,,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454,2,"['release', 'update']","['release', 'updated']"
Deployability,"Hi @jaqueytw . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/170#issuecomment-553659345:27,update,update,27,,https://github.com/google/deepvariant/issues/170#issuecomment-553659345,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/45#issuecomment-482355184:680,update,update,680,,https://github.com/google/deepvariant/issues/45#issuecomment-482355184,1,['update'],['update']
Deployability,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written?. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/282#issuecomment-954067904:913,configurat,configuration-and-analysis,913,,https://github.com/google/deepvariant/issues/282#issuecomment-954067904,1,['configurat'],['configuration-and-analysis']
Deployability,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim.; ```; ! pip install tf-slim. import tensorflow.compat.v1 as tf; import os; import tf_slim as slim; from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/; ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():; images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):; _, end_points = inception_v3.inception_v3(images, is_training=False, ; num_classes=3,; create_aux_logits=False); ; print(""end_points:""); print(end_points.keys()); # Restore the checkpoint; sess = tf.Session(graph=graph); saver = tf.train.Saver(); saver.restore(sess, ckpt_file); ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339#issuecomment-682069726:333,install,install,333,,https://github.com/google/deepvariant/issues/339#issuecomment-682069726,1,['install'],['install']
Deployability,"Hi @jguhlin ,; thanks for the feedback, and for letting us know that our users are still interested in Singularity images.; I have been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-561996442:198,release,release,198,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442,1,['release'],['release']
Deployability,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/37#issuecomment-531149691:694,release,release,694,,https://github.com/google/deepvariant/issues/37#issuecomment-531149691,1,['release'],['release']
Deployability,"Hi @jkalleberg ,; Thank you for reaching out. The model ckpt you're using was older than v1.4. And you're right: in v1.4 we added an extra channel, and we haven't trained a new allele frequency model with v1.4. So we actually don't yet have a model that has both insert_size as well allele_frequency!. Two things:. 1. If you want to run v1.4.0 code with the older model (which didn't have the insert_size channel), you can add `,channels=''` to the end of your make_examples_extra_args. I added a section to my public gist here:; https://gist.github.com/pichuan/64d73bc965300645470eb29a66116593#update-if-youre-using-our-v140-docker-codebase; 2. I'm currently training a new WGS AF model that will have both the insert_size channel, as well as the allele_frequency channel. So, stay tuned! I can give you an update when I have it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568#issuecomment-1251633143:595,update,update-if-youre-using-our-,595,,https://github.com/google/deepvariant/issues/568#issuecomment-1251633143,2,['update'],"['update', 'update-if-youre-using-our-']"
Deployability,"Hi @jkalleberg thanks raising this issue. You're right that this actually should be optional, this seems to be bug - I've noted it in internally and we'll have a fix in the next release. For now, I would suggest just specifying some path for the output_gvcf flags, even if you don't actually need them. Also, would you be able to share why you don't want to use the gVCF file? Normally people will want to merge these callsets, which would require using the gVCFs, so I'd be curious to know if there's a specific use case where you are not needing gVCF output.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429#issuecomment-796937010:178,release,release,178,,https://github.com/google/deepvariant/issues/429#issuecomment-796937010,1,['release'],['release']
Deployability,"Hi @jrvanalstine! Unfortunately, the TensorFlow Estimator API does not support running prediction on multiple GPUs, so `call_variants.py` currently uses only 1 GPU at prediction time. That said, the API does support running training and evaluation (both steps require labels) with multiple GPUs through distribution strategies (i.e. `MirroredStrategy`). We hope to look into this and will post an update if we are able to run these steps using multiple GPUs. I'll close this issue for now but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/163#issuecomment-486883166:397,update,update,397,,https://github.com/google/deepvariant/issues/163#issuecomment-486883166,1,['update'],['update']
Deployability,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/256#issuecomment-568660419:937,release,release,937,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419,4,['release'],"['release', 'released']"
Deployability,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/306#issuecomment-620378545:493,release,release,493,,https://github.com/google/deepvariant/issues/306#issuecomment-620378545,1,['release'],['release']
Deployability,"Hi @karoliinas , for stability and reproducibility, using CPU version is likely the better way to go. In terms of GPU updates, in our developmental branch (https://github.com/google/deepvariant/tree/dev) we actually update the GPU version (and Ubuntu version). For example you can see: https://github.com/google/deepvariant/blob/dev/Dockerfile; But, we won't be building a new Docker version until next release. I'll close this now. Please let us know if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2246057554:118,update,updates,118,,https://github.com/google/deepvariant/issues/849#issuecomment-2246057554,3,"['release', 'update']","['release', 'update', 'updates']"
Deployability,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. ; DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). ; In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models.; If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703:610,release,released,610,,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703,1,['release'],['released']
Deployability,"Hi @karoliinas,. Given that you're having weird numerical prediction values from call_variants output, and that you mentioned your GPU version is newer that what we used in DeepVariant 1.6, I strongly suspect your GPU+DeepVariant setting is producing unexpected output. Would it be possible for you to:; 1. Use the compatible GPU driver version? (I understand this is annoying. We've made the CUDA update internally already, and it'll be out in the next version. But if it's possible to test with a compatible one, that might be easier for you); 2. Just to confirm whether it's the hardware issue: Can you run with CPU and see if it still crashes with the same error? That will help us identify whether it's the hardware, or actually something unexpected with your input file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2233667709:398,update,update,398,,https://github.com/google/deepvariant/issues/849#issuecomment-2233667709,1,['update'],['update']
Deployability,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/343#issuecomment-688064227:823,pipeline,pipeline,823,,https://github.com/google/deepvariant/issues/343#issuecomment-688064227,1,['pipeline'],['pipeline']
Deployability,"Hi @kishwarshafin . I did install Singularity in a conda environment and then I used the singularity installed in the HPC, and in both systems I couldn't download the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/886#issuecomment-2378167772:26,install,install,26,,https://github.com/google/deepvariant/issues/886#issuecomment-2378167772,2,['install'],"['install', 'installed']"
Deployability,"Hi @koido ; thanks for your report. Currently our internal setup as well as [our training tutorial on GitHub](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) are both done with TPU training. Getting training to work on CPU isn't really a priority of the DeepVariant team right now. Using the default for the TPU tutorial will likely not work for CPU. (Not just the batch size, but also other hyper parameters as well). It will also likely be too slow to be practical. Another suggestion:; You might know that the Google Cloud team has a GCP runner here: https://cloud.google.com/genomics/docs/tutorials/deepvariant; Currently this is only for calling variants with pre-trained models. But the Cloud team will be interested in understand more external training use cases, so they can understand whether they can build an analogous training pipeline on GCP. If you're interested in getting in touch with the Cloud team, please reach out to @nmousavi. I'll close this bug for now. If you have question about training, this might also be a question for the TensorFlow team.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/147#issuecomment-461439899:884,pipeline,pipeline,884,,https://github.com/google/deepvariant/issues/147#issuecomment-461439899,1,['pipeline'],['pipeline']
Deployability,"Hi @kokyriakidis ,; in this thread (when it was still r0.8), running `././build-prereq.sh && ./build_release_binaries.sh` fixed the original user's question. In the latest release (r0.9), it should work even if you run `build_and_test.sh` instead of `build_release_binaries.sh`. The main trick was this `fix_zip_rule` that makes sure the symbolic links are correct:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/build_release_binaries.sh#L41. In the issue that you reported in bcbio/bcbio-nextgen#3048, there is one more layer because you're using bioconda. I'm surprised that your issue was actually with v0.9.0. Given that we actually fixed `build_and_test.sh` in the latest version. @kokyriakidis Two questions for you:; 1. With bioconda, were you able to run with v0.8.0 like @chapmanb suggested?; 1. Did you actually try building DeepVariant binaries on your own? If you tried, did that work for you or did you get the same error?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-570115804:172,release,release,172,,https://github.com/google/deepvariant/issues/199#issuecomment-570115804,1,['release'],['release']
Deployability,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-465225098:110,release,release,110,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098,3,"['configurat', 'release']","['configuration', 'release']"
Deployability,"Hi @ksw9 ,; It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:; https://numpy.org/devdocs/release/1.17.5-notes.html; Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/265#issuecomment-581773805:111,release,release,111,,https://github.com/google/deepvariant/issues/265#issuecomment-581773805,3,['release'],['release']
Deployability,"Hi @li1ba , this problem should be fixed in the latest release: https://github.com/google/deepvariant/releases/tag/v1.5.0. Let us know if it works. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592#issuecomment-1457415830:55,release,release,55,,https://github.com/google/deepvariant/issues/592#issuecomment-1457415830,2,['release'],"['release', 'releases']"
Deployability,"Hi @linlin-coder , ; Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1661405006:63,update,update,63,,https://github.com/google/deepvariant/issues/689#issuecomment-1661405006,1,['update'],['update']
Deployability,"Hi @linlin-coder ,; Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:; Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817:1206,release,release,1206,,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817,2,['release'],['release']
Deployability,"Hi @llllaaaa , to give you an update:; We're now aiming for an earlier release in March.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/258#issuecomment-585527577:30,update,update,30,,https://github.com/google/deepvariant/issues/258#issuecomment-585527577,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/45#issuecomment-553659703:27,update,update,27,,https://github.com/google/deepvariant/issues/45#issuecomment-553659703,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @loipf ; I've made changes in internal code. It'll come out in the next release.; After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/362#issuecomment-737653058:75,release,release,75,,https://github.com/google/deepvariant/issues/362#issuecomment-737653058,2,['release'],['release']
Deployability,"Hi @lucasbrambrink ,. Thanks for your prompt response. I started with an empty docker container and ran each script manually. And after I built, I committed the container to create a new image: deepvbuild:latest. Because I made quite a few changes, I have lost track of all the changes I have made. . For example, I made quite some changes to the version numbers because pip has version conflicts. The main ones are that I had to use python3.9 and pandas 1.4.4. Also, I kind of have to switch between numpy 2.0.2 (to build) and 1.24.1 (to run). I also had to install tensorflow-addons from git repo because pip does not have that. In build-prereq.sh, I downloaded bazel 7.3.1 linux arm64 binary. The main difficulty lies in building pyclif. There were a lot of errors related to protobuf cmake modules or linking abseil but it worked in the end. I don't think they should cause problems if they build successfully. For `build_release_binaries.sh`, I had to change the folder name from `k8-opt` to `aarch64-opt`. I am not sure whether this would cause problems but I also commented out line 59: `find ""runfiles/com_google_deepvariant"" -name '*.so' -exec ln --force -s --relative ""runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so"" {} \;` because there were no such folders or .so files. Because all bazel tests are passed and the binaries are successfully built, I would think it is not an error related to the dependencies. I am not sure how to get more information on what may go wrong from the error trace. Please feel free to let me know if there is anything else I can run to generate more useful information. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334737190:559,install,install,559,,https://github.com/google/deepvariant/issues/879#issuecomment-2334737190,1,['install'],['install']
Deployability,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2033267320:208,update,updated,208,,https://github.com/google/deepvariant/issues/802#issuecomment-2033267320,1,['update'],['updated']
Deployability,"Hi @lykstudio,. Ultima has a separate fork of deepvariant.; It's not completely public yet.; Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,; Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. ; We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file.; Official gatk and picard installations are required. . ### Files required for the analysis (download locally); The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict; gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files; DeepVariant model files ; * WGS calling (model 1.2); ```; gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001; gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index; gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta; ```. DeepVariant-ug docker container:; ```; gcr.io/ganymede-331016/deepvariant:ug-1.4.13; ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/711#issuecomment-1734891580:496,install,installations,496,,https://github.com/google/deepvariant/issues/711#issuecomment-1734891580,1,['install'],['installations']
Deployability,"Hi @maca8e . To update this issue, we have now made the change to DeepVariant_unfiltered GLnexus preset in the DeepTrio documentation. Thank you again for pointing this out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475#issuecomment-894387782:16,update,update,16,,https://github.com/google/deepvariant/issues/475#issuecomment-894387782,1,['update'],['update']
Deployability,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475#issuecomment-892390206:118,update,update,118,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @marchoeppner ; Thank you for reporting this. Currently, MT is excluded in this code:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/exclude_contigs.py. We actually got many requests that they want to be able to call MT. So, we have already made the internal change to remove MT and chrM from that `exclude_contigs.py` file already. In our next release, you'll be able to call MT by default!. For the current codebase, if you want to call it, you'll need to remove it from code and build your own binaries. Sorry that we don't have a flag for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/333#issuecomment-671441195:398,release,release,398,,https://github.com/google/deepvariant/issues/333#issuecomment-671441195,1,['release'],['release']
Deployability,"Hi @marchoeppner ; To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/333#issuecomment-695166040:34,update,update,34,,https://github.com/google/deepvariant/issues/333#issuecomment-695166040,1,['update'],['update']
Deployability,"Hi @maryawood , thanks for the question.; As you probably noticed in the code, `very_sensitive_caller` is the default that we use for candidate generation. In all our released models, this is the default so there is no need to change it. `vcf_candidate_importer` is a more advanced / experimental implementation , where we allow the users to provide a list of candidates that they want DeepVariant to call. You can see an example use case here: https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38. Another important use case for `vcf_candidate_importer` is [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper) which works on Oxford Nanopore long-read data. We haven't written an official documentation for this, because there are many subtle details when providing the VCF file for `--proposed_variants`, otherwise it might confuse DeepVariant. You're welcome to give it a try, and feel free to ask questions here. . Our team might be a bit slower to answer specific questions on this, but we can try our best to support. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/433#issuecomment-807046936:167,release,released,167,,https://github.com/google/deepvariant/issues/433#issuecomment-807046936,1,['release'],['released']
Deployability,"Hi @matthdsm , we're actively working on a new release that will be using Python3.; We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/285#issuecomment-602758544:47,release,release,47,,https://github.com/google/deepvariant/issues/285#issuecomment-602758544,3,"['release', 'update']","['release', 'updated']"
Deployability,"Hi @meghanasp21 . I suspect that what is occurring is that previous, no longer used Docker containers from DeepVariant runs are taking up space on your filesystem. Can you run . **docker system prune** (https://docs.docker.com/config/pruning/). This should clean up those images. I will also make a note for us to remove intermediate files from within the Docker image after runs (this would only come out in future releases) which should decrease the impact of previous Docker images on storage space. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-609147540:416,release,releases,416,,https://github.com/google/deepvariant/issues/296#issuecomment-609147540,1,['release'],['releases']
Deployability,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290#issuecomment-606102679:80,release,released,80,,https://github.com/google/deepvariant/issues/290#issuecomment-606102679,2,"['release', 'update']","['released', 'update']"
Deployability,"Hi @melop,. Great suggestion! As luck would have it, this will be a feature in our next release (`1.7.0`). We have parallelized/sharded `postprocess_variants` across multiple CPUs, which helps to reduce its maximum RAM footprint. It also takes in a `--regions` flag directly so you can easily split up the process further if that's necessary (although it shouldn't be). . Until `1.7.0` is released, your only option is to follow @pichuan's suggestion. You can use `bcftools concat` to join the region-specific VCFs back together. I hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/868#issuecomment-2293775698:88,release,release,88,,https://github.com/google/deepvariant/issues/868#issuecomment-2293775698,2,['release'],"['release', 'released']"
Deployability,"Hi @mkazanov ,. Update:; First, I want to remind you that:; * Our next release (r1.2) will be on Ubuntu 18.04. ; * And, even right now (with r1.1), you should be able to run our `google/deepvariant:1.1.0` on Ubuntu18.04. Just in case you really need to build your own on a Ubuntu18.04 now, I put together some changes that we already made internally to update to Ubuntu 18.04 here:; https://github.com/pichuan/deepvariant/tree/r1.1-ubuntu18; (**Note**: This is my own fork. In the future I might delete it. But at least before r1.2 comes out, I'll keep it there.). If you want to understand what changes have been made, you can see this:; https://github.com/google/deepvariant/compare/r1.1...pichuan:r1.1-ubuntu18",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/443#issuecomment-821755400:16,Update,Update,16,,https://github.com/google/deepvariant/issues/443#issuecomment-821755400,3,"['Update', 'release', 'update']","['Update', 'release', 'update']"
Deployability,"Hi @mkazanov ; Internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. Given that there's another similar question in https://github.com/google/deepvariant/issues/441 , I can try to see if it's easy for me to share some of the updated files (I'll need to test it with the v1.1 repo first).; Otherwise, I can try to push a bit more on the timeline for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/443#issuecomment-821522061:32,update,updated,32,,https://github.com/google/deepvariant/issues/443#issuecomment-821522061,5,"['release', 'update']","['release', 'updated', 'updates']"
Deployability,"Hi @moldach ; Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:; https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-604129307:61,release,release,61,,https://github.com/google/deepvariant/issues/287#issuecomment-604129307,2,"['release', 'update']","['release', 'updated']"
Deployability,"Hi @mpinese ,; I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638521636:254,release,release,254,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636,1,['release'],['release']
Deployability,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/352#issuecomment-696511944:639,release,released,639,,https://github.com/google/deepvariant/issues/352#issuecomment-696511944,1,['release'],['released']
Deployability,"Hi @nurmians ,; The link the @pgrosu link to seems like the right one from Ultima!. Our team is working on incorporating this as part of postprocess_variants. The official version will be out in the next release (before end of 2023). ; You can get a preview of these 2 flags that we'll be adding:; https://github.com/google/deepvariant/blob/dev/deepvariant/postprocess_variants.py#L170-L188. (Note that we don't officially support the code in ""dev"" branch. If you want an official documentation and usage of this, please wait for the release later. But please feel free to take a look at the code if you like)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1692451997:204,release,release,204,,https://github.com/google/deepvariant/issues/518#issuecomment-1692451997,2,['release'],['release']
Deployability,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1680213963:1294,release,release,1294,,https://github.com/google/deepvariant/issues/697#issuecomment-1680213963,1,['release'],['release']
Deployability,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207#issuecomment-524686835:97,release,release,97,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835,3,"['release', 'update']","['release', 'updated']"
Deployability,"Hi @one-matrix ,; In the past few months, our team member @akolesnikov has been experimenting with something conceptually similar to what you described. The work is not finalized, but hopefully in our next release in 2024 , some improved implementation will come out.; Thanks for the suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1865450778:206,release,release,206,,https://github.com/google/deepvariant/issues/756#issuecomment-1865450778,1,['release'],['release']
Deployability,"Hi @paupaiz , I'll close this issue now. Hopefully Maria's response answered your question.; One thing I'll add is that when you use make_examples, you can use the `pileup_image_height` to change the height of images you create. ; But, be careful that our default release models won't work directly if you change the pileup_image_height, so if you create images of different heights, you'll likely need to retrain your own model.; I'll close this issue now. Feel free to reopen or ask another question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/536#issuecomment-1110491714:264,release,release,264,,https://github.com/google/deepvariant/issues/536#issuecomment-1110491714,1,['release'],['release']
Deployability,"Hi @pgrosu , thanks for your feedback!; Thanks to @nmousavi 's work, the Cloud runner page is now updated:; https://cloud.google.com/genomics/docs/tutorials/deepvariant with 0.7.0, and 0.7.0 deepvariant_runner image is now tagged as latest. In terms of our GitHub page --; I fixed a few small thing such as the typo (and ran spell checking and fixed a few more!) Also fixed the `0.4.1` issue. I'll also address the contributing document at some point soon. These changes are right now still just in our internal codebase. I'll get it out when I have a chance to push out some documentation fixes. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-415643215:98,update,updated,98,,https://github.com/google/deepvariant/issues/87#issuecomment-415643215,1,['update'],['updated']
Deployability,"Hi @pgrosu ,. Thanks, that solved it. I had no idea there was a conda build for this package, otherwise it'd be the first way I'd try to install it. However, I'd recommend looking into these building problems. Thank you very much, ; V",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-425108869:137,install,install,137,,https://github.com/google/deepvariant/issues/98#issuecomment-425108869,1,['install'],['install']
Deployability,"Hi @pgrosu . In the meantime I was running DRAGEN 4.2.4 RNA pipeline to check the variants, and I have some interesting findings. . | GT (DV) | GT(D) | AD (DV) | AD (D) |; | ------------- | ------------- | ------------- | ------------- |; | 1/1 | 0/1 | 67,67 | 715,697. `samtools coverage` at this position reports the following:. | numreads | covbases | coverage | meandepth | meanbaseq | meanmapq; | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |; 3009 |1 | 100 | 2997 | 36.7 | 30.3. I will run DeepVariant again to inspect the realigned BAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1697238562:60,pipeline,pipeline,60,,https://github.com/google/deepvariant/issues/701#issuecomment-1697238562,1,['pipeline'],['pipeline']
Deployability,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338#issuecomment-681126260:260,release,released,260,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260,2,['release'],"['release', 'released']"
Deployability,"Hi @pgrosu,. 1. I've updated the table in my original post with QUAL and GQ values (the QUAL/GQ of the second variant are low); 2. How can I check this? I mainly work with WGS data; 3. Should be skin stanza; 4. No, these two sites are not in this portal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1695851251:21,update,updated,21,,https://github.com/google/deepvariant/issues/701#issuecomment-1695851251,1,['update'],['updated']
Deployability,"Hi @pichuan , . I am actually not familiar with singularity. I checked the docs and my understanding is that the installation requires root privileges but, according to a quick forum searches, there are some ways to get around it. Most of the software I have installed is either available through R (distributed in bioconductor) or distributed as a JAR archive. I have had to run a few makefiles but I think deepvariant is the most complex installation I took care of myself (without the help of a sysadmin) in a long time. Thanks for your help !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-480358330:113,install,installation,113,,https://github.com/google/deepvariant/issues/137#issuecomment-480358330,3,['install'],"['installation', 'installed']"
Deployability,"Hi @pichuan , I am planning to integrate deepvariant into a whole germline variant calling pipeline which is using python3.8 and built everything into a Docker. So I need to build every prereq relating deepvariant inside it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-821249453:31,integrat,integrate,31,,https://github.com/google/deepvariant/issues/441#issuecomment-821249453,2,"['integrat', 'pipeline']","['integrate', 'pipeline']"
Deployability,"Hi @pichuan . Unfortunately, I do not have the log file. None of the make_examples is running. The last update occurred on March,1 at 18:44. Can we say whether the make_example / gvcf process has finished? If yes, can I launch deevariant again and go directly to call_variants?. -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00000-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00001-of-00016.gz; -rw-r--r-- 1 root root 353632256 Mar 1 18:44 gvcf.tfrecord-00002-of-00016.gz; -rw-r--r-- 1 root root 353107968 Mar 1 18:44 gvcf.tfrecord-00003-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00004-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00005-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00006-of-00016.gz; -rw-r--r-- 1 root root 353370112 Mar 1 18:44 gvcf.tfrecord-00007-of-00016.gz; -rw-r--r-- 1 root root 353894400 Mar 1 18:44 gvcf.tfrecord-00008-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00009-of-00016.gz; -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00010-of-00016.gz; -rw-r--r-- 1 root root 354942976 Mar 1 18:44 gvcf.tfrecord-00011-of-00016.gz; -rw-r--r-- 1 root root 354418688 Mar 1 18:44 gvcf.tfrecord-00012-of-00016.gz; -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00013-of-00016.gz; -rw-r--r-- 1 root root 354680832 Mar 1 18:44 gvcf.tfrecord-00014-of-00016.gz; -rw-r--r-- 1 root root 354156544 Mar 1 18:44 gvcf.tfrecord-00015-of-00016.gz; -rw-r--r-- 1 root root 10704650240 Mar 1 18:44 make_examples.tfrecord-00000-of-00016.gz; -rw-r--r-- 1 root root 10710417408 Mar 1 18:44 make_examples.tfrecord-00001-of-00016.gz; -rw-r--r-- 1 root root 10699931648 Mar 1 18:44 make_examples.tfrecord-00002-of-00016.gz; -rw-r--r-- 1 root root 10677649408 Mar 1 18:44 make_examples.tfrecord-00003-of-00016.gz; -rw-r--r-- 1 root root 10692329472 Mar 1 18:44 make_examples.tfrecord-00004-of-00016.gz; -rw-r--r-- 1 root root 107253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/427#issuecomment-789089400:104,update,update,104,,https://github.com/google/deepvariant/issues/427#issuecomment-789089400,1,['update'],['update']
Deployability,"Hi @pichuan ; thanks a lot for the update. No worries ... in the meanwhile I am working on improving the genome assembly, it's a very complex one (in short, I haven't moved on forgetting about deepvariants ;) ).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/209#issuecomment-549049881:35,update,update,35,,https://github.com/google/deepvariant/issues/209#issuecomment-549049881,1,['update'],['update']
Deployability,"Hi @pichuan, are you sure that doc uses GPU?. >For this case study, we used a 64-core non-preemptible instance with 128GiB and no GPU. and I didn't see GPU configuration in the link to the command you posted?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428637497:156,configurat,configuration,156,,https://github.com/google/deepvariant/issues/99#issuecomment-428637497,1,['configurat'],['configuration']
Deployability,"Hi @pichuan,. Thanks for the update! I will try your suggestions next time I come up with a ""heavy"" bam file. Kostas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/427#issuecomment-793492563:29,update,update,29,,https://github.com/google/deepvariant/issues/427#issuecomment-793492563,1,['update'],['update']
Deployability,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences!. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321#issuecomment-675317201:131,configurat,configurations,131,,https://github.com/google/deepvariant/issues/321#issuecomment-675317201,2,"['configurat', 'install']","['configurations', 'installations']"
Deployability,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321#issuecomment-674744222:213,install,install,213,,https://github.com/google/deepvariant/issues/321#issuecomment-674744222,2,['install'],"['install', 'installed']"
Deployability,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-864505178:353,configurat,configuration,353,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178,2,['configurat'],['configuration']
Deployability,"Hi @pioneer-pi ,; Got it. Unfortunately I don't know how to build if you're already inside docker. The issue you encounter there might be more relevant to the interaction between docker and cmake, which will be outside our support scope. Right now, when I build new source code, I've always used a machine with root permissions. Then I either build from scratch, or I docker build from Dockerfile. But both will require certain permissions. Sorry I can't be of more help here. This is outside what our team can support now, so I'll close this issue.; If you do find more workaround, please feel free to share and update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823788047:613,update,update,613,,https://github.com/google/deepvariant/issues/739#issuecomment-1823788047,1,['update'],['update']
Deployability,"Hi @ptrebert ; Thanks for the suggestion. I have fixed this in our internal version, and the fix will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/242#issuecomment-596040916:123,release,release,123,,https://github.com/google/deepvariant/issues/242#issuecomment-596040916,1,['release'],['release']
Deployability,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:; ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42; from third_party.nucleus.protos import reference_pb2. ### Insert at line 346; labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-465775663:173,release,release,173,,https://github.com/google/deepvariant/issues/154#issuecomment-465775663,1,['release'],['release']
Deployability,"Hi @qili93 ; I wasn't able to reproduce your error. Here is what I did (on a Ubuntu 16 machine); ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh && pip install --user 'intervaltree==2.1.0'; bazel test -c opt //deepvariant/labeler:haplotype_labeler_test; ```. And this passed for me. Currently your setting might be outside our actively supported platform. If you can suggest a patch (that wouldn't break other settings), we're happy to make a fix internally as well. But we don't currently have extra bandwidth to look into this issue. One more question for you (it'll really help us understand your need and why you need to build) -- Were you able to run with our prebuilt docker container image? If so, what was the reason that you decided to build on your own? . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464843333:196,install,install,196,,https://github.com/google/deepvariant/issues/154#issuecomment-464843333,2,"['install', 'patch']","['install', 'patch']"
Deployability,"Hi @qili93 ; Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. ; Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464963634:90,upgrade,upgrade,90,,https://github.com/google/deepvariant/issues/154#issuecomment-464963634,1,['upgrade'],['upgrade']
Deployability,"Hi @qili93 ; can you paste your error messages here?. Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:; https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86; to:; ```; pip install --user 'intervaltree==2.1.0'; ```; and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464599230:183,release,release,183,,https://github.com/google/deepvariant/issues/154#issuecomment-464599230,2,"['install', 'release']","['install', 'release']"
Deployability,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-899967692:158,update,updated,158,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692,1,['update'],['updated']
Deployability,"Hi @ramcn,. Just go into the WORKSPACE file at the following location:. https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L87. And update your path to the full path of `opt` - (below is an example):. ```; new_local_repository(; name = ""clif"",; build_file = ""third_party/clif.BUILD"",; path = ""/home/ramcn/opt"",; ); ```. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/93#issuecomment-421252090:139,update,update,139,,https://github.com/google/deepvariant/issues/93#issuecomment-421252090,1,['update'],['update']
Deployability,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:32,update,update,32,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948,4,"['install', 'update']","['install', 'installation', 'installing', 'update']"
Deployability,"Hi @s-batalov ,; internally, we've updated to CUDA 11.8 already. Like @kishwarshafin mentioned, this change will show up in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844#issuecomment-2207998495:35,update,updated,35,,https://github.com/google/deepvariant/issues/844#issuecomment-2207998495,2,"['release', 'update']","['release', 'updated']"
Deployability,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/232#issuecomment-596040813:27,update,update,27,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @se2cheeese ; I will close this issue for now because there's no reason update from you. But please feel free to follow up with more questions, either reopening this issue, or another one. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249#issuecomment-586409546:75,update,update,75,,https://github.com/google/deepvariant/issues/249#issuecomment-586409546,1,['update'],['update']
Deployability,"Hi @segoerge . Thank you for your question. It is a good observation that better support for MNP would be helpful. We have discussed this internally to a limited extent, but have not yet mapped out what would be involved to make the change. I couldn't give a timeframe for this yet (or give an indication as to whether this could be something in the next release). Your feedback is appreciated, as it helps us understand the needs of the user community. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/238#issuecomment-557229093:355,release,release,355,,https://github.com/google/deepvariant/issues/238#issuecomment-557229093,1,['release'],['release']
Deployability,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321#issuecomment-671016803:606,install,install,606,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803,2,['install'],['install']
Deployability,"Hi @serverchief , ; a quick update for you:; In the next release, we'll make sure that if you accidentally run with `--call_variants_extra_args=use_openvino=true`, the program will exit earlier (before it starts make_examples) and warn you that openvino is no longer built in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597#issuecomment-1423319704:28,update,update,28,,https://github.com/google/deepvariant/issues/597#issuecomment-1423319704,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690820723:252,release,release,252,,https://github.com/google/deepvariant/issues/345#issuecomment-690820723,2,['release'],"['release', 'released']"
Deployability,"Hi @shadrinams . My apologies for the delay in reply. It took me awhile to get to this issue. I have some thoughts about how to improve consistency in the trio calling that may help some of these cases. But it may take a bit of time to explore those. For the chr7_54624683 case, I suspect there is some interaction with the variant call and the normalization done by GLnexus. When I look at the VCF for the Proband and Father, I see:. Proband; ```; chr7 54624686 . A ATC 61.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:61:39:17,21:0.538462:61,0,7; ```; Father; ```; chr7 54624686 . A ATC 54.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:55:33:18,15:0.454545:54,0,68; ```. In my GLnexus VCF, I also see the following line, which has a call in both the child and the father (the variant position is a bit different, but this does seem to reflect the reads I see in the sample). ; ```; chr7 54624686 chr7_54624686_A_ATC A ATC 61 . AF=0.333333;AQ=61 GT:DP:AD:GQ:PL:RNC 0/1:33:18,15:55:54,0,68:.. 0/0:32:32,0:50:0,180,1799:.. 0/1:39:17,21:61:61,0,76:..; ```. Would you mind pasting the command that you used to merge the gVCFs from this sample? When you jointly genotyped, were there also other samples at the time of joint genotyping, or was it only this sample?. For reference, this is the command that I used on the gVCFs of the files to generate this gVCF:. ```; sudo apt-get -y install bcftools; sudo apt-get -y install tabix; sudo docker pull quay.io/mlin/glnexus:v1.2.7. sudo docker run \; -v ""${PWD}/outputs"":""/outputs"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWGS \; /outputs/Child_chr7_54624683.g.vcf.gz \; /outputs/Father_chr7_54624683.g.vcf.gz \; /outputs/Mother_chr7_54624683.g.vcf.gz \; | bcftools view - | bgzip -c > outputs/GitHub440.cohort.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-836280023:1351,install,install,1351,,https://github.com/google/deepvariant/issues/440#issuecomment-836280023,2,['install'],['install']
Deployability,"Hi @sophienguyen01 , let me know if you have a chance to try and provide some updates here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2057620089:78,update,updates,78,,https://github.com/google/deepvariant/issues/802#issuecomment-2057620089,1,['update'],['updates']
Deployability,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants?. If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/690#issuecomment-1662683748:594,release,released,594,,https://github.com/google/deepvariant/issues/690#issuecomment-1662683748,1,['release'],['released']
Deployability,"Hi @splaisan . We are currently working on a more complete approach for MNPs, but we don't have any update at this time and it won't be in the very next release. Hopefully sometime after that we will have results for MNPs, but I can't give a specific timeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/238#issuecomment-1702210635:100,update,update,100,,https://github.com/google/deepvariant/issues/238#issuecomment-1702210635,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi @spz1st ,; Thanks for checking. If you want to run 1.6.0, then the one with `1.6.0` is the version to use.; There are previous versions (like 1.5.0 or earlier), which corresponds to previous releases https://github.com/google/deepvariant/releases. I'm glad to hear that you're able to run the Docker image. I'll close this bug. Feel free to let us know if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/740#issuecomment-1828772975:194,release,releases,194,,https://github.com/google/deepvariant/issues/740#issuecomment-1828772975,2,['release'],['releases']
Deployability,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/626#issuecomment-1504023688:217,release,release,217,,https://github.com/google/deepvariant/issues/626#issuecomment-1504023688,1,['release'],['release']
Deployability,"Hi @tahashmi ,. It does look like a TensorFlow issue as described here: https://github.com/tensorflow/text/issues/385. You can check the tensorflow version of DeepVariant this way:; ```bash; singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu python3 -c 'import tensorflow as tf; print(tf.__version__)'; ```; And then please try to install the tensorflow version locally to see if the error gets fixed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/555#issuecomment-1223153383:368,install,install,368,,https://github.com/google/deepvariant/issues/555#issuecomment-1223153383,1,['install'],['install']
Deployability,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. ; * call_variants will be run with the same number of shards.; * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```; bin/make_examples \; --examples /tmn/your_examples.tfrecord@200.gz \; --mode calling \; --reads /tmp/your_input_bam.bam \; --realign_reads \; --ref=/tmp/your_reference.fna \; --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:; bin/call_variants.par \; --batch_size=32 \; --checkpoint <Path to the model checkpoint or saved model>.ckpt \; --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:; /tmp/your_call_variants_output.cvo.tfrecord@200.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/744#issuecomment-1836586525:143,pipeline,pipeline,143,,https://github.com/google/deepvariant/issues/744#issuecomment-1836586525,1,['pipeline'],['pipeline']
Deployability,"Hi @vinisalazar , I just tried to build DeepVariant on a Ubuntu 18.04 machine. I did encounter some issue, but it didn't seem to be the same as the one you have. I suspect when you run `build-prereq.sh`, it actually failed already at this line:. ```; sudo -H apt-get -qq -y update; ```. Can you let me know if that works for you?; Maybe you can run it with out the `-qq` so it will actually report why it fails. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-425266931:274,update,update,274,,https://github.com/google/deepvariant/issues/98#issuecomment-425266931,1,['update'],['update']
Deployability,"Hi @vinisalazar , note that the current bioconda version seems to be 0.6.1. It's also not directly maintained by our team, but it's contribution from @chapmanb. I'll check with him to see if we can update it. And yes, we'll look into the building issues. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-425162985:198,update,update,198,,https://github.com/google/deepvariant/issues/98#issuecomment-425162985,1,['update'],['update']
Deployability,"Hi @vinisalazar,. Out of curiosity, what happens if you type the following:. `conda install -c bioconda deepvariant`. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-424941786:84,install,install,84,,https://github.com/google/deepvariant/issues/98#issuecomment-424941786,1,['install'],['install']
Deployability,"Hi @wangtz , thanks again for reporting. ; I've released v0.6.1 which should addressed the issue you reported.; https://github.com/google/deepvariant/releases/tag/v0.6.1. Please let me know if you see other problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/68#issuecomment-385594090:48,release,released,48,,https://github.com/google/deepvariant/pull/68#issuecomment-385594090,2,['release'],"['released', 'releases']"
Deployability,"Hi @yangyxt ,; I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646#issuecomment-1549015401:397,release,release,397,,https://github.com/google/deepvariant/issues/646#issuecomment-1549015401,1,['release'],['release']
Deployability,"Hi @yanyang1989 ,; internally we've updated to Ubuntu18.04 and made a bunch of updates accordingly too. It will come out in the next release. We're still using Python 3.6 though. Can you tell us why you're considering using 3.8?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-820768080:36,update,updated,36,,https://github.com/google/deepvariant/issues/441#issuecomment-820768080,3,"['release', 'update']","['release', 'updated', 'updates']"
Deployability,"Hi @ydLiu-HIT . Thank you for your question. The answer to this is complicated. It looks like the region that these elements is in is a LINE element - long regions with multiple copies through the genome that have high sequence similarity to each other. Because of the high sequence similarity, reads to line elements can map to other parts of the genome, and they are generally very difficult regions to call correctly. We've seen the behavior in DeepVariant not calling variants that are near other variants and in regions with two (or more) variant-rich haplotypes. We think that one of the reasons for this is that DeepVariant has learned that these regions represent uncaptured segmental duplication and LINE elements, which are often labelled as not variant in the more comprehensive genome in a bottle truth set. . Whether these positions represent true variants at that position, or sequences from a similar LINE element elsewhere is difficult to say. Since this is HG002, if this is within the confident regions, you can see whether Genome in a Bottle indicates them to be true variants. However, Genome in a Bottle has some more recent corrections to variants in/near LINE elements, so it may be better to check the updated (though still beta) [truth set](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4beta_SmallVariantDraftBenchmark_07192019/). DeepVariant will output every candidate considered, so if you want to find positions that are called in this way, looking for 0/0 or ./. calls with more than 35% ALT support and within 100bp of 2 or more candidate variants may be able to pull out many of these examples. . The other option to pull out examples like this would be to intersect with a LINE element annotation track from UCSC. Please let us know if there is something unclear about this answer. This is a rather complicated concept and explanation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/233#issuecomment-550436768:1226,update,updated,1226,,https://github.com/google/deepvariant/issues/233#issuecomment-550436768,1,['update'],['updated']
Deployability,"Hi @yingchen69 ; from your questions, it seems like you're asking whether you can change the height of the pileup image.; I believe this previous GitHub issue is very similar, and it has a bunch of in-depth discussion: https://github.com/google/deepvariant/issues/62. A direct answer to your question:; DeepVariant is currently a germline variant caller. Our released models are trained for that purpose. The models are trained on data that are 221x100x6 in dimension (221 bases wide, 100 reads deep, 6 channels). You can see an example for visualization [here](https://github.com/google/deepvariant/blob/master/docs/visualizing_examples.ipynb). From [my answer in the earlier issue](https://github.com/google/deepvariant/issues/62#issuecomment-379107194), it is possible to change the pileup image height. However, even if you do that, it won't just work out of the box for you. I'm closing this issue, but feel free to ask if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/176#issuecomment-484947045:359,release,released,359,,https://github.com/google/deepvariant/issues/176#issuecomment-484947045,1,['release'],['released']
Deployability,"Hi @zihhuafang ,; The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/816#issuecomment-2121705660:53,release,released,53,,https://github.com/google/deepvariant/issues/816#issuecomment-2121705660,2,['release'],"['release', 'released']"
Deployability,"Hi @znikasz thanks for bringing this up. CUDA 11 support was added in the latest version of TensorFlow (2.4.0), so when we upgrade our version of TF, this support will be included. However, I don't have a specific timeframe for when this will happen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/430#issuecomment-796847786:123,upgrade,upgrade,123,,https://github.com/google/deepvariant/issues/430#issuecomment-796847786,1,['upgrade'],['upgrade']
Deployability,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/712#issuecomment-1741642049:39,release,release,39,,https://github.com/google/deepvariant/issues/712#issuecomment-1741642049,3,"['release', 'update']","['release', 'update']"
Deployability,"Hi @zyxue , we'll look into this a bit more. Might be something we can improve on the Nucleus codebase. Feel free to ping again if we don't give another updates in a few days.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-427117714:153,update,updates,153,,https://github.com/google/deepvariant/issues/99#issuecomment-427117714,1,['update'],['updates']
Deployability,"Hi @zyxue ; since your questions seem to be no longer relevant to the original question, I'll close this issue. Internally we've made the code updates to address the original issue, and will come out in a next release. It also seems like the local changes that @depristo suggested works for you now. Regarding your latest questions:; (1) There are areas in the genome that do take longer to run. This is usually areas where more reads piled up. We've made a lot of speed optimization over time, but it's still certainly true that will be regions that seem to run for much longer.; (2) Case study says ""no GPU"" in the sentence you quoted. Currently the case studies (WGS and WES) were both CPU only.; (3) For how to run with GPU, see this previous issue: https://github.com/google/deepvariant/issues/81",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428646375:143,update,updates,143,,https://github.com/google/deepvariant/issues/99#issuecomment-428646375,2,"['release', 'update']","['release', 'updates']"
Deployability,"Hi Alex,. Thank you for your patience. We were able to reproduce your issue and have determined that it is likely caused by `htslib`. In particular, DV uses `htslib` version `1.13` which only experimentally supports CRAM v3.1. Updating the `htslib` version to the latest (`1.18`) fixed this issue for me. You can update `htslib` yourself for the time being by updating [this section](https://github.com/google/deepvariant/blob/r1.6/WORKSPACE#L24-L32).; ```; http_archive(; name = ""htslib"",; build_file = ""//:third_party/htslib.BUILD"",; sha256 = ""f1ab53a593a2320a1bfadf4ef915dae784006c5b5c922c8a8174d7530a9af18f"",; strip_prefix = ""htslib-1.18"",; urls = [; ""https://github.com/samtools/htslib/releases/download/1.18/htslib-1.18.tar.bz2"",; ],; ); ```. Please note that I have not yet tried `htslib 1.18` in other workflows, and those might be affected by the change: there is a chance this change has negative consequences in other circumstances. That said, if you are currently blocked in your workflow you can give that a try! Meanwhile, we are working on updating `htslib` on our end to be included it in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/741#issuecomment-1850869554:313,update,update,313,,https://github.com/google/deepvariant/issues/741#issuecomment-1850869554,3,"['release', 'update']","['release', 'releases', 'update']"
Deployability,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321#issuecomment-656000987:897,install,installed,897,,https://github.com/google/deepvariant/issues/321#issuecomment-656000987,1,['install'],['installed']
Deployability,"Hi Amy,. Basically, this all happens when the candidates are selected during the `make_examples` stage:. $`1)`$ DP (read depth) is set from the total allele counts [in this line](https://github.com/google/deepvariant/blob/r1.5/deepvariant/variant_calling.cc#L307):. ```Python; nucleus::SetInfoField(kDPFormatField, TotalAlleleCounts(allele_count), call);; ```. $`2)`$ In the `make_examples` stage, a tensor image (set of matrices) representing information about your reads gets created. That tensor is usually preinitialized with zeros (0), and then the values get updated with the read information for the rows. The first 5 rows are the reference representation anyway, before it gets populated with your read representation starting with row 6. Then this tensor gets processed through the model $`-`$ during the `call_variants` stage $`-`$ generating the genotype probabilities for: homozygous ref, het, and homozygous alt. $`3)`$ Then in `postprocess_variants` $`-`$ based on the most likely genotype (having the maximum genotype probability) from step $`2`$ $`-`$ the [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function assigns the `PASS` or `RefCall` for the `FILTER` column of the VCF file. $`4)`$ If you want to filter out these low depth candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference (notice the the default count is already 2, which you are obvserving):. * vsc_min_count_snps (the default is 2); * vsc_min_count_indels (the default is 2); * vsc_min_fraction_snps (the default is 0.12); * vsc_min_fraction_indels (the default is 0.06); * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```; --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'; ```. You can read more details about these parameters at the following lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/684#issuecomment-1645637328:565,update,updated,565,,https://github.com/google/deepvariant/issues/684#issuecomment-1645637328,1,['update'],['updated']
Deployability,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/; ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667663801:720,release,release,720,,https://github.com/google/deepvariant/issues/691#issuecomment-1667663801,3,['release'],"['release', 'releases']"
Deployability,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-567477006:103,install,installation,103,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006,3,['install'],"['installation', 'installed']"
Deployability,"Hi Andrew,. Thank you very much for your kind and prompt response. For _point 1_ (and part of _point 2_), thank you very much for that additional information. I think my question was a simpler one: you can see multiple submissions for some groups, and I was trying to see if I understood all the submissions that used DeepVariant. Since there is only one ""rpoplin"" label (and no other entries from Verily Life Sciences), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that pap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476428247:536,pipeline,pipelines,536,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247,1,['pipeline'],['pipelines']
Deployability,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,; Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/606#issuecomment-1408682802:82,pipeline,pipeline,82,,https://github.com/google/deepvariant/issues/606#issuecomment-1408682802,1,['pipeline'],['pipeline']
Deployability,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-435084063:101,pipeline,pipeline,101,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063,2,['pipeline'],['pipeline']
Deployability,"Hi Arkanosis,; thanks for your question!; It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well.; However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues.; Thanks!; -pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/2#issuecomment-349070609:220,install,install,220,,https://github.com/google/deepvariant/issues/2#issuecomment-349070609,2,['install'],['install']
Deployability,"Hi Brad,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/18#issuecomment-353673281:300,patch,patch,300,,https://github.com/google/deepvariant/pull/18#issuecomment-353673281,2,"['patch', 'release']","['patch', 'release']"
Deployability,"Hi Brent I will check in on this and get back to you. Just to be clear, are using `FROM google/deepvariant` and then installing bioconda and using that to install bcftools / samtools or are you using bioconda to install deepvariant, samtools, and bcftools?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445#issuecomment-822571186:117,install,installing,117,,https://github.com/google/deepvariant/issues/445#issuecomment-822571186,3,['install'],"['install', 'installing']"
Deployability,"Hi Charles,; there are a few 3rd party Cloud orchestrations for DeepVariant that you can consider. I'll provide the links below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479685222:358,pipeline,pipelines,358,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222,4,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-364627307:43,release,release,43,,https://github.com/google/deepvariant/issues/27#issuecomment-364627307,2,['release'],['release']
Deployability,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash; $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/; gs://deepvariant/binaries/DeepVariant/0.4.0/; gs://deepvariant/binaries/DeepVariant/0.4.1/; gs://deepvariant/binaries/DeepVariant/0.5.0/; gs://deepvariant/binaries/DeepVariant/0.5.1/; $; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/57#issuecomment-371614660:39,update,updated,39,,https://github.com/google/deepvariant/pull/57#issuecomment-371614660,2,['update'],"['update', 'updated']"
Deployability,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:1139,configurat,configuration,1139,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896,1,['configurat'],['configuration']
Deployability,"Hi Daniele,. Thanks for your response. I updated the post as you suggested. . How can I set ```CUDA_VISIBLE_DEVICES=0``` within the container and ```SINGULARITYENV_CUDA_VISIBLE_DEVICES=0``` outside of the container? I just typed ```export CUDA_VISIBLE_DEVICES=0``` in the terminal and then typed the apptainer command to run. . I do have ```nvidia-container-cli```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774#issuecomment-1958269613:41,update,updated,41,,https://github.com/google/deepvariant/issues/774#issuecomment-1958269613,1,['update'],['updated']
Deployability,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-692271819:798,update,update,798,,https://github.com/google/deepvariant/issues/346#issuecomment-692271819,3,['update'],['update']
Deployability,"Hi Fra,. As far as I can infer, there is something in the BAM and reference files that the variant caller is unable to call variants for. Can you please take a look in the chromosome 8 region with the [Integrative Genomics Viewer (IGV)](https://software.broadinstitute.org/software/igv/) using your BAM aligned to the same exact reference, to confirm that you see some variation and proper alignment. I am looking at the call variant code, and the allele counts are dependent on the reference and read support by position. You should also limit your analysis with the `--regions` flag so it is quicker as well. If your data is WES then you should use the WES model, in order for the results to be scientifically valid. Maybe it might help if you realign your FASTQ files to the reference used by DeepVariant to regenerate the BAM file and the subsequent BAI file, just to be sure nothing happened along the way. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1652409458:202,Integrat,Integrative,202,,https://github.com/google/deepvariant/issues/675#issuecomment-1652409458,1,['Integrat'],['Integrative']
Deployability,"Hi I'm the GLnexus maintainer. From that side we're currently lacking a motivating, medium/large scale study with DeepVariant gVCF files which would provide the context to chase down the gnarly corner cases that arise in gVCF merging / joint genotyping, completing the current ""experimental"" integrated configuration. There are a few candidates in the pipeline but more would be really welcome. Ping me if I can help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-465059695:292,integrat,integrated,292,,https://github.com/google/deepvariant/issues/142#issuecomment-465059695,3,"['configurat', 'integrat', 'pipeline']","['configuration', 'integrated', 'pipeline']"
Deployability,"Hi KBT59,; because the released models are trained with the default height of the pileup images, by just changing `--pileup_image_height` at inference time won't really give you better results. Currently DeepVariant is a germline variant caller, so it's not designed to call variants with 1% frequency.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-379107194:23,release,released,23,,https://github.com/google/deepvariant/issues/62#issuecomment-379107194,1,['release'],['released']
Deployability,"Hi Kiran,. We don't have a general release for this. If you are interested, you can email awcarroll@google.com and we can follow up on options.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/692#issuecomment-1670070938:35,release,release,35,,https://github.com/google/deepvariant/issues/692#issuecomment-1670070938,1,['release'],['release']
Deployability,"Hi LiYi,. No worries, and this is very helpful! So this is a good sign, but here's the issue. The files in `/data2/share/home/liyi/TEs/dv/input` actually point to other directories that are not connected to Docker. It is best to just refer to them directly. For that I've updated your script to the following:. ```; docker run \; -v ""/data2/share/home/liyi/TEs/Pan-genome/Giraffe/C111"":""/input"" \; -v ""/data2/share/home/liyi/TEs/dv/output"":""/output"" \; -v ""/data2/share/home/liyi/TEs/Genome/fasta"":""/fasta"" \; google/deepvariant:""1.5.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/fasta/C162-2_final.fasta \; --reads=/input/C111_mapped.bam \; --output_vcf=/output/C111.vcf.gz \; --output_gvcf=/output/C111.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=5. ```. Let me know if this works for you. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/577#issuecomment-1702219580:272,update,updated,272,,https://github.com/google/deepvariant/issues/577#issuecomment-1702219580,1,['update'],['updated']
Deployability,"Hi Lucas,. yes, the only version I can install is DeepVariant 0.7.0. I tried to install tensorflow-estimator first (v2.13.1) but still have same error as mentioned. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/835#issuecomment-2187865343:39,install,install,39,,https://github.com/google/deepvariant/issues/835#issuecomment-2187865343,2,['install'],['install']
Deployability,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339#issuecomment-681204545:338,configurat,configurations,338,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545,1,['configurat'],['configurations']
Deployability,"Hi Maria,. The updated Docker image made the module work perfectly. I appreciate the quick fix and response. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/235#issuecomment-557652463:15,update,updated,15,,https://github.com/google/deepvariant/issues/235#issuecomment-557652463,1,['update'],['updated']
Deployability,"Hi Maria,; thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. ; And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:; Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design?. thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338#issuecomment-680771609:443,update,updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint,443,,https://github.com/google/deepvariant/issues/338#issuecomment-680771609,1,['update'],['updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint']
Deployability,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355440557:1251,pipeline,pipeline,1251,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557,2,['pipeline'],['pipeline']
Deployability,"Hi Masaru,; It looks like you're using DirectRunner, which is fine for smaller datasets, but when we have larger datasets we instead use DataflowRunner (see where we shuffle training set [here](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each)). In the Case Study, the DirectRunner is used for the small validation set and DataflowRunner is used for the large training set. The fact that it works when you split up the data into smaller pieces suggests this may be the issue. Please try running with DataflowRunner and let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133#issuecomment-450955687:335,configurat,configuration-file-for-each,335,,https://github.com/google/deepvariant/issues/133#issuecomment-450955687,1,['configurat'],['configuration-file-for-each']
Deployability,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:624,pipeline,pipeline,624,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287,3,"['configurat', 'pipeline']","['configuration', 'pipeline', 'pipelines']"
Deployability,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-414549806:67,update,updated,67,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806,5,"['configurat', 'pipeline', 'release', 'update']","['configuration-file-for-each', 'pipeline', 'release', 'update', 'updated']"
Deployability,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366745899:158,configurat,configuration,158,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899,1,['configurat'],['configuration']
Deployability,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894:398,install,installed,398,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894,2,['install'],['installed']
Deployability,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353701703:94,update,update,94,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703,1,['update'],['update']
Deployability,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-361015565:455,install,installation,455,,https://github.com/google/deepvariant/issues/41#issuecomment-361015565,1,['install'],['installation']
Deployability,"Hi Paul,; there is actually some explanation in the file (below the Copyright lines):; https://github.com/google/deepvariant/blob/r0.7/cloudbuild.yaml#L28-L34. We use these yaml files to build and push docker images. You probably could re-use them to build and push DeepVariant docker images to projects where you have write access to. We didn't document these more, because we didn't think it's a very common use case.; If it's confusing to have these files on the top level, I can check with my colleagues and see if we can move them to a sub-directory in the next release. Would that help?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-413716778:567,release,release,567,,https://github.com/google/deepvariant/issues/87#issuecomment-413716778,1,['release'],['release']
Deployability,"Hi Paul; Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1705891486:119,pipeline,pipeline,119,,https://github.com/google/deepvariant/issues/704#issuecomment-1705891486,1,['pipeline'],['pipeline']
Deployability,"Hi Phil,; an update:; @cmclean pointed out that it comes from this line of our code:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/resources.py#L158. We're getting this information for debugging purpose only (DeepVariant outputs some information about the run in case developers need to remember how the run was done). ; I suspect your run was done on a system where the method wasn't implemented. One possible fix is to make our code more robust is to:; ```; try:; freq = psutil.cpu_freq(); return freq.current if freq is not None else 0.0; except NotImplementedError:; return 0.0; ```. We'll fix this internally soon, and it should come out next time we make a release. Thanks for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191#issuecomment-504492772:13,update,update,13,,https://github.com/google/deepvariant/issues/191#issuecomment-504492772,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi Pi-Chuan and @genieusbio,. Try it with `--regions chr1:783006-783007` - you might see magic happen :). ```Bash; $ zcat output/HG003.output.vcf.gz | grep -P 'chr1\t783006\t'; chr1 783006 . A G 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0:0:0,33,33. $ zcat output/HG003.output.g.vcf.gz | grep -P 'chr1\t783006\t'; chr1 783006 . A G,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:30:0:0,0,0:0,0:0,33,33,990,990,990; ```. My preference would be to update the BED file with that region. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/708#issuecomment-1719093859:438,update,update,438,,https://github.com/google/deepvariant/issues/708#issuecomment-1719093859,1,['update'],['update']
Deployability,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,; `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353712933:463,release,release-notes,463,,https://github.com/google/deepvariant/issues/21#issuecomment-353712933,1,['release'],['release-notes']
Deployability,"Hi Ram,. Could you update the script on the bazel lines with `--verbose_failures`, rerun and show us the log. I'm assuming you have plenty of memory and CPU resources, which means that either you are not on Ubuntu or have an older GCC. What's your Linux distribution and GCC version?. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-422241830:19,update,update,19,,https://github.com/google/deepvariant/issues/94#issuecomment-422241830,1,['update'],['update']
Deployability,"Hi Ram,. You really do not want to have any errors show up in any of your tests, especially when performing variant calling. It's an easily fixable issue where the Google folks would just need to update their TensorFlow Python package distributed with DeepVariant, which was probably compiled with an earlier version of protobuf, while currently the Tensorflow requires `3.6` of protobuf and the Protobuf version being compiled with DeepVariant is `3.6`. Looking at your log file, you are using the CPU version. The current version on PyPI of TensorFlow is 1.10.1, and DeepVariant uses version 1.9. Here's the process by which I went about to determine what is happening:. 1. If you download both `whl` files of Tensorflow like this:. ```; wget https://storage.googleapis.com/deepvariant/packages/tensorflow/tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. wget https://files.pythonhosted.org/packages/1a/c4/8cb95df0bf06089014259b25997c3921a87aa08e2cd981417d91ca92f7e9/tensorflow-1.10.1-cp27-cp27mu-manylinux1_x86_64.whl; ```. 2. Next rename each of the `whl` files to `zip`, and then uncompress them in separate directories. If you then look at one of the version `1.10.1` files that has the `serialized_options` such as `purelib/tensorflow/core/framework/resource_handle_pb2.py`, you will see the following difference between the versions, where the `serialized_options` keyword is present in the `1.10.1` version, but not in the `1.9.0` version:. #### _*For version 1.9*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-423037408:196,update,update,196,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408,1,['update'],['update']
Deployability,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:766,configurat,configurations,766,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771,1,['configurat'],['configurations']
Deployability,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/259#issuecomment-573804485:31,update,update,31,,https://github.com/google/deepvariant/issues/259#issuecomment-573804485,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi Shruti,. I notice in your .yaml that both the --outfile and --gvcf_outfile flags are set to ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" . These flags should have different names set, as they are two distinct output files. Please reopen this issue if rerunning the pipeline with the distinct names does not resolve your problem. regards,; Cory",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/86#issuecomment-411048876:264,pipeline,pipeline,264,,https://github.com/google/deepvariant/issues/86#issuecomment-411048876,1,['pipeline'],['pipeline']
Deployability,"Hi Shruti,. The issue is that computations have locality, and would prefer to stay within the same data center. In your 0.6.1 scripts you specified zones for you pipeline runs (`--zones us-west1-b`) and for your gcp_deepvariant_runner (`--zones 'us-*'`), and not a region. In your 0.7.0 script you included both a zone `--zones us-west1-*` for your gcp_deepvariant_runner, and a region in your pipelines run `--regions us-west1`. Try to stick to one or the other, where a zone is more precise and a region contains a collection of zones. For more information here a couple of useful links:. https://cloud.google.com/compute/docs/regions-zones/. https://cloud.google.com/compute/docs/regions-zones/global-regional-zonal-resources. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96#issuecomment-423515203:162,pipeline,pipeline,162,,https://github.com/google/deepvariant/issues/96#issuecomment-423515203,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi Simon,. You'll need the bz2 libs installed. Either yum bzip2-libs and/or libbz2, as they are required. @ramcn Which do you have installed on your system?. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/95#issuecomment-423039615:36,install,installed,36,,https://github.com/google/deepvariant/issues/95#issuecomment-423039615,2,['install'],['installed']
Deployability,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">; ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">; ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">; ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2; Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:402,update,updated,402,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,2,['update'],['updated']
Deployability,"Hi Tamer, I tried to replicate this issue, and here is what I found. I created an instance from [this CentOS7 VM](https://console.cloud.google.com/marketplace/details/centos-cloud/centos-7). I chose the default location for all installations. When asked if I wanted to update my PATH during installations, I chose to do so. I was able to install DeepVariant through Bioconda using the below steps. . I ran into a particular error with `gsutil`. After running `source ~/.bashrc`, I saw an error when I ran `gsutil`. `gsutil` is used by the DeepVariant installation, so that failed as well. To address this, I referenced [this post](https://stackoverflow.com/questions/38783140/importerror-no-module-named-google-compute-engine) and ran `export BOTO_CONFIG=/dev/null` before installing DeepVariant again. Running these commands in order allows me to successfully install on the VM. ```; # install gsutil; curl https://sdk.cloud.google.com | bash; exec -l $SHELL; # verify that gsutil is working; gsutil. # install wget and bzip2, which are both needed to download miniconda; sudo yum install bzip2 wget; wget https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh; bash Miniconda2-latest-Linux-x86_64.sh ; source ~/.bashrc. # gsutil is failing now; gsutil; export BOTO_CONFIG=/dev/null; # gsutil should be working again; gsutil. # create new conda env, add channels, install deepvaraint; conda create -n dv python=2.7; conda activate dv; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda install -n dv deepvariant -v; ```. In the output from running `conda install -n dv deepvariant -v`, I see the first error you posted even with a successful installation. I was not able to replicate the second error. Some sanity checks for you:. * Are you able to successfully run `gsutil`?; * Did you add all conda channels in the correct order?; * Could you post the entire output from running `conda install -v deepvariant`?.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-452921108:228,install,installations,228,,https://github.com/google/deepvariant/issues/137#issuecomment-452921108,8,"['install', 'update']","['install', 'installation', 'installations', 'installing', 'update']"
Deployability,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/42#issuecomment-360510853:183,configurat,configuration,183,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853,1,['configurat'],['configuration']
Deployability,"Hi Tomasz,; We currently do not support running make_examples on GPUs, just the parallelized threads that you are currently using. However, you could consider [this external solution](https://docs.parabricks.com/standalone-tools/deepvariant) from Parabricks, which provides multi-GPU support for running our pipelines. Please note that we are not involved with creating or maintaining this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/168#issuecomment-481424791:308,pipeline,pipelines,308,,https://github.com/google/deepvariant/issues/168#issuecomment-481424791,1,['pipeline'],['pipelines']
Deployability,"Hi Tomasz,; this issue might be the same as an earlier one. See this comment for a temporary solution:; https://github.com/google/deepvariant/issues/7#issuecomment-350528251. We are currently working on a fix, and will post an update to this github issue when it's fully fixed. Meanwhile, please let us know if you're having problems with the temporary fix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/11#issuecomment-350780716:227,update,update,227,,https://github.com/google/deepvariant/issues/11#issuecomment-350780716,1,['update'],['update']
Deployability,"Hi again, indeed the cpu -version works. Boy am I glad there's no problem with the data! We'll have to wait for the deepvariant update to get the gpu going. Will it be using the same model, so that the samples processed the new version will be compatible with the samples processed with the current one?. I'm getting new samples in every week, and we will have ~150 in a couple of months, so I'm interested as to not end up running them twice. The other option would be to upgrade the vm to one without gpu and more cpu:s to continue with the current cpu -version.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2244483153:128,update,update,128,,https://github.com/google/deepvariant/issues/849#issuecomment-2244483153,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,"Hi all,; @akolesnikov has pushed a fix internally on Nov 3rd based on @cmclean 's suggestion in the previous comment. The change also includes a documentation change in deepvariant-gvcf-support.md. For now I'll close this issue. The change should come out officially in the next release. (@mlin if you'd like to see the change earlier, we can also push it to the `dev` branch. Let me know)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/714#issuecomment-1839939161:279,release,release,279,,https://github.com/google/deepvariant/issues/714#issuecomment-1839939161,1,['release'],['release']
Deployability,"Hi all,; I'm posting an update to this PR after talking to @fo40225: ; Our team has done changes similar to the latest code in this PR (using Close() on writer). Our current findings are:; 1. With async writer, if `call_variants` doesn't write to a .gz file, the run finishes fine and results now seem complete (meaning, the VCF and GVCF after `postprocess_variants` are the same as the results when async writer is turned off); 1. However, when outputting as a .tfrecord.gz file, currently async writer in `call_variants` here will output a .gz file that will cause an issue in the following `postprocess_variants` step. This issue is currently blocking us adding in the suggested change in this PR. The current plan is our team will try to fix the issue in the second item. We want to confirm that the outputs from the async writer proposed in this PR are completely correct before we proceed. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-487140094:24,update,update,24,,https://github.com/google/deepvariant/pull/152#issuecomment-487140094,1,['update'],['update']
Deployability,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-385593624:142,release,released,142,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624,3,['release'],"['release', 'released', 'releases']"
Deployability,"Hi cclaus,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/4#issuecomment-349494695:302,patch,patch,302,,https://github.com/google/deepvariant/pull/4#issuecomment-349494695,2,"['patch', 'release']","['patch', 'release']"
Deployability,"Hi everyone, . I am trying to install the DeepVariant bioconda on RedHat Entreprise Server 7.2; I am really not familiar with conda but this looks like the most straightforward way to run deepvariant on a machine for which I do not (and will never get) sudo privileges. The above discussion helped to pass a lot of kinks but I am still struggling. . I am having the two following problems : ; - conda now installs libcrypto.so.1.1 instead of libcrypto.so.1.0.0 . I solved it by adding a hard link from libcrypto.so.1.1 to a libcrypto.so.1.0.0 which is dirty and may break things down the line. - I am really struggling with compiling GLIBC-2.23 . I configured it with ""-O2 -g -Wall"" which are the gcc flags recommended in the RHES doc.; The make command ran well (no error of what I can see) but when I do make check it crashes with ; test-math-isinff.cc: Command not found; If I add the GLIBC path to my LD_LIBRARY_PATH as suggested by @pgrosu , it then corrupts the environment (i.e. every command goes to segmentation fault core dump) so I assume the GLIBC is not built properly . Do you have any idea about how to solve this ?; Any help would be greatly appreciated. Thanks !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-480128599:30,install,install,30,,https://github.com/google/deepvariant/issues/137#issuecomment-480128599,2,['install'],"['install', 'installs']"
Deployability,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:; 1. Did you get this Nextflow pipeline from somewhere online or write it yourself?; 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this.; 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run.; 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581314748:252,pipeline,pipeline,252,,https://github.com/google/deepvariant/issues/659#issuecomment-1581314748,1,['pipeline'],['pipeline']
Deployability,"Hi! We would like to propose optional OpenVINO backend for `call_variants` step. Do you accept external PRs?. Also, this PR has Actions pipeline. Feel free to enable by https://github.com/google/deepvariant/actions to run it. /cc @pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709411831:136,pipeline,pipeline,136,,https://github.com/google/deepvariant/pull/363#issuecomment-709411831,1,['pipeline'],['pipeline']
Deployability,"Hi, . This is duplicate of https://github.com/google/deepsomatic/issues/22. I will give an update there and close this issue as this is unrelated to DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/884#issuecomment-2350252743:91,update,update,91,,https://github.com/google/deepvariant/issues/884#issuecomment-2350252743,1,['update'],['update']
Deployability,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-549130970:300,install,installing,300,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970,2,"['Rolling', 'install']","['Rolling', 'installing']"
Deployability,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916:1982,release,release,1982,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916,1,['release'],['release']
Deployability,"Hi, @pichuan . I upgraded numpy from 1.22.1 to 1.24.4, and it worked!!; But I'm a bit confused as I upgraded the numpy in one of the containers using ""pip install numpy --upgrade"", and it worked in the other container. Does the container image use numpy from outside of the container?. Thanks a million anyway!. ```; Singularity deepvariant_1.6.0.sif:> pip freeze | grep numpy; numpy==1.24.4; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746#issuecomment-1851412236:17,upgrade,upgraded,17,,https://github.com/google/deepvariant/issues/746#issuecomment-1851412236,4,"['install', 'upgrade']","['install', 'upgrade', 'upgraded']"
Deployability,"Hi, @pichuan! OpenVINO 2021.3 is the latest version. 2021.4 will be released only at least in next month so it's OK to use 2021.3. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/442#issuecomment-851332367:68,release,released,68,,https://github.com/google/deepvariant/pull/442#issuecomment-851332367,1,['release'],['released']
Deployability,"Hi, @pichuan.; I am sure the CUDA is installed on the host. The CUDA version is V11.8.89.; ![image](https://user-images.githubusercontent.com/43125963/225511464-62c75283-b925-4b19-aea4-a75913ffd224.png)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471272942:37,install,installed,37,,https://github.com/google/deepvariant/issues/619#issuecomment-1471272942,1,['install'],['installed']
Deployability,"Hi, I just wanted to check in to see if there are any updates on this thread? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1147942215:54,update,updates,54,,https://github.com/google/deepvariant/issues/537#issuecomment-1147942215,1,['update'],['updates']
Deployability,"Hi, I wondered if there are any updates on this issue. Were you able to separate the true positives from false positives?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-899266001:32,update,updates,32,,https://github.com/google/deepvariant/issues/440#issuecomment-899266001,1,['update'],['updates']
Deployability,"Hi, is this using WGS model?; You mentioned your reads are shorter. Can you try running DeepVariant v1.2.0 and see if it helps?; In this new release, we added some training data with shorter reads. I can also ask Alexey to give you an update on his investigation of this problem. But please let us know which version you're using, and whether the new release helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470#issuecomment-891873668:141,release,release,141,,https://github.com/google/deepvariant/issues/470#issuecomment-891873668,3,"['release', 'update']","['release', 'update']"
Deployability,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7?. Is it possible for you to check (on both machines) what's the different between the protobuff version installed?; (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1012497475:330,install,installed,330,,https://github.com/google/deepvariant/issues/499#issuecomment-1012497475,1,['install'],['installed']
Deployability,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1012212010:195,install,installed,195,,https://github.com/google/deepvariant/issues/499#issuecomment-1012212010,2,['install'],['installed']
Deployability,"Hi, thanks for reporting this issue. If you used a GCE instance, can you share the command you used to start a cloud machine? . And, if you look at your `""${LOG_DIR}/call_variants.log""`, you should be able to see lines like these:; ```; I0405 16:03:16.308625 140490269800192 call_variants.py:353] Processed 4680192 examples in 146256 batches [0.67 sec per 100]; I0405 16:03:16.524780 140490269800192 call_variants.py:353] Processed 4680224 examples in 146257 batches [0.67 sec per 100]; ```. Can you tell me what your ""sec per 100"" is? This will also confirm your speed for call_variants. I'm guessing it's much slower than 0.67 sec per 100. You should also watch your systems resources -- is there enough RAM, etc. And, I would also suggest that you check out the [cost- and speed-optimized, Docker-based pipelines](https://cloud.google.com/genomics/deepvariant) created for Google Cloud Platform. Case studies are created so that the users can understand the key components of DeepVariant, but if you want to look for production-grade performance, you should consider the Cloud pipelines instead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391092118:806,pipeline,pipelines,806,,https://github.com/google/deepvariant/issues/74#issuecomment-391092118,2,['pipeline'],['pipelines']
Deployability,"Hi, we recently updated the quick start and case studies to use docker.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md. It seems like I should rearrange and simplify our documentation to make it more clear.; Can you tell me where is the place you first read? Is it the main github page, or did you clone the codebase and directly start from there?; I will try to make some improvement next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415984031:16,update,updated,16,,https://github.com/google/deepvariant/issues/89#issuecomment-415984031,1,['update'],['updated']
Deployability,"Hi,. I am re-opening in hope to get some update on this issue. I think adding support for MNP calling has been a community request dating back to at least 2019. And as this particular issue highlights, the lack of MNP support really does have implications for how the data can be used. . I found in several discussions that people are entirely unaware of this particular limitation in Deepvariant. . So please, at the very least make this a very visible disclaimer, or preferably implement support for MNP calling in your otherwise excellent tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520#issuecomment-1541965936:41,update,update,41,,https://github.com/google/deepvariant/issues/520#issuecomment-1541965936,1,['update'],['update']
Deployability,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383347052:1228,Pipeline,Pipeline,1228,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,; it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,; and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1683209021:998,continuous,continuous,998,,https://github.com/google/deepvariant/issues/697#issuecomment-1683209021,1,['continuous'],['continuous']
Deployability,"Hi,. It is actually possible to train DeepVariant on multiple GPUs, using the [MirroredStrategy](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train.py#L112). You can find the tensorflow documentation here: [link](https://www.tensorflow.org/guide/distributed_training). . It looks like we need to update the [FAQ](https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#can-model_train-be-run-on-multiple-gpus) to reflect that—thanks for brining it to our attention! The [training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#start-train) is up-to-date, so feel to continue to reference that. It looks like it already applies the `mirrored ` strategy.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/885#issuecomment-2362016240:313,update,update,313,,https://github.com/google/deepvariant/issues/885#issuecomment-2362016240,1,['update'],['update']
Deployability,"Hi,. Just to make sure I understand the issue: you are able to successfully install DeepVariant 0.7.0 with Python 2.7, but you are unable to install 1.15 or later; the goal is for you to install the latest version of DeepVariant, 1.6.0. . The relevant error that I see is:; ```; deepvariant [1.0.0|1.1.0|...|1.5.0] would require; │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;; ```; Is it possible to resolve this conflict? I will try myself to see if I can get it to work on GCP, and report back.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/835#issuecomment-2187229102:76,install,install,76,,https://github.com/google/deepvariant/issues/835#issuecomment-2187229102,4,['install'],"['install', 'installable']"
Deployability,"Hi,. Thanks for pointing this out! We are going to investigate this issue more closely and provide updates as we make progress. Thank you for providing links to the files, those are extremely helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/763#issuecomment-1894532485:99,update,updates,99,,https://github.com/google/deepvariant/issues/763#issuecomment-1894532485,1,['update'],['updates']
Deployability,"Hi,. Thanks for providing all of that information, it is very helpful. The issue does actually lie in pyclif and protobuf. We use an older version of protobuf that is stable with pyclif; updating it will break the interoperability between C++ and Python (the source of that seg fault). Our tests in `build_and_test.sh` do not appear to catch this, of which I have made a note to update in the future. . Is there a reason you cannot use our Docker builds from `1.6.1`? Also, if docker does not work for you, are you able to use singularity? I am unsure that brute forcing a docker build will lead to a container that runs everything as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334875005:379,update,update,379,,https://github.com/google/deepvariant/issues/879#issuecomment-2334875005,1,['update'],['update']
Deployability,"Hi,. Thanks for your answer. It is good to know that pyclif and protobuf is the problem. I think I did not change the version of protobuf and pyclif because I did not change the commit numbers of the repositories, i.e. `ABSL_PIN`, `PROTOBUF_VERSION` or `CLIF_PYTHON_VERSION` in `tools/build_clif.sh`. I did change the python version number though because otherwise some packages are not installable through pip, e.g. jaxlib from clu requires python >= 3.9. I am not sure whether this is normal or it is the root cause of the problem. I did pull the Docker builds before I tried to build my own image but it seems it is built only for amd64 platform. The machine I use is in arm architecture, i.e. aarch64, so the docker build did not work for me. Please feel free to let me know if I am wrong. But I don't think singularity would solve this problem because it still needs the docker to be built for the target architecture or using multi-platform builds. Do you think it is feasible to have such a build in the future?. P.S. I solved the linking error for clif by building abseil-cpp again right before building clif. I am not sure whether it is because something in between overwrites the path.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2335169866:387,install,installable,387,,https://github.com/google/deepvariant/issues/879#issuecomment-2335169866,1,['install'],['installable']
Deployability,"Hi,; Currently, only the call_variants step can utilize GPU. And it currently only uses one GPU. Our team is actively looking into getting it to work with multi-gpu. Hopefully we'll have an update soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/163#issuecomment-475252649:190,update,update,190,,https://github.com/google/deepvariant/issues/163#issuecomment-475252649,1,['update'],['update']
Deployability,"Hi,; I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```; def channels_to_rgb(channels):; # Reconstruct the original channels; base = channels[0]; qual = np.minimum(channels[1], channels[2]); strand = channels[3]; alpha = np.multiply(; channels[4] / 254.0,; channels[5] / 254.0); return np.multiply(; np.stack([base, qual, strand]),; alpha).astype(np.uint8).transpose([1, 2, 0]); ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/110#issuecomment-432092110:106,release,release,106,,https://github.com/google/deepvariant/issues/110#issuecomment-432092110,2,"['release', 'update']","['release', 'update']"
Deployability,"Hi,; I want to give an update on this - ; in the next minor release of DeepVariant, we'll plan to update the building script to allow you build on Ubuntu 18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-433210620:23,update,update,23,,https://github.com/google/deepvariant/issues/98#issuecomment-433210620,3,"['release', 'update']","['release', 'update']"
Deployability,"Hi,; In the latest release we added this page:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md; As well as a training tutorial:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md. If you decide to train a model, we would love to hear your feedback as detailed as you are willing to provide us. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/80#issuecomment-415562625:19,release,release,19,,https://github.com/google/deepvariant/issues/80#issuecomment-415562625,1,['release'],['release']
Deployability,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/204#issuecomment-518518311:759,release,release,759,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311,2,['release'],"['release', 'released']"
Deployability,"Hi,; [Quick Start](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md) provides a quick example of how you identify SNPs and Indels in a small region. [Case Study](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md) gives you a full example of how to identify SNPs and Indels in a whole genome on a single machine. We don't currently have detailed documentation on how to train your own model. I recommend you start with running DeepVariant using the models we released.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/63#issuecomment-379300704:518,release,released,518,,https://github.com/google/deepvariant/issues/63#issuecomment-379300704,1,['release'],['released']
Deployability,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-437647446:587,release,release,587,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446,3,"['install', 'release']","['install', 'release']"
Deployability,"Hi,; for your question in: https://github.com/google/deepvariant/issues/70#issuecomment-386838731; you'll need to set `--make_examples_disk_per_worker_gb` to a large enough value based on the size of their BAMs. We set it to 200gb for the WGS pipeline (it's set to 50gb by default; for quickstart). You'll need to change that if you have a really large BAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70#issuecomment-387174391:243,pipeline,pipeline,243,,https://github.com/google/deepvariant/issues/70#issuecomment-387174391,1,['pipeline'],['pipeline']
Deployability,"Hi,; if you run `build-prereq.sh`, it should have installed it for you. This section is relevant:; https://github.com/google/deepvariant/blob/r0.7/build-prereq.sh#L88. But if that somehow didn't work for you, you can directly use the instructions on bazel's page:; https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu. I would still recommend using our docker image instead of building your own binaries!! If you do that, you don't even need to install bazel!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415937760:50,install,installed,50,,https://github.com/google/deepvariant/issues/89#issuecomment-415937760,4,['install'],"['install', 'install-ubuntu', 'install-with-installer-ubuntu', 'installed']"
Deployability,"Hi. What OS are you running on? I actually don't see this crash on the Ubuntu 16 setup.; If you can let me know how to reproduce the error, we'll make sure to fix it and will come out with the next release. Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/78#issuecomment-398508100:198,release,release,198,,https://github.com/google/deepvariant/issues/78#issuecomment-398508100,1,['release'],['release']
Deployability,"Hmm, empirically we've been able to handle larger input BAM than 20GB with less RAM you had. . Another known issue (which we will fix in the next release) is that if auxiliary fields are being read in, and if the BAM has many of them, it could unnecessarily increase the RAM usage. However, in this case given that you're using WGS model (which by default isn't parsing aux fields), that shouldn't be an issue... @kirti141 Another question for you - is there a BAM file that you can make public (with no sensitive information, of course) that we can attempt to reproduce this issue? Thanks again for reporting this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925383292:146,release,release,146,,https://github.com/google/deepvariant/issues/482#issuecomment-925383292,1,['release'],['release']
Deployability,I agree this is a problem. We have an internal bug tracking this. Probably we will just rename deepvariant/core/math.py to core_math.py or equivalent. I'll update this bug when the change is in internally and it'll show up in the next push of deepvariant to github. Note you can workaround this issue just like https://github.com/notoraptor/deepvariant/commit/15c2deb211672a8ba32c1cbe609d81e1a2b0fb74,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-355654713:156,update,update,156,,https://github.com/google/deepvariant/issues/32#issuecomment-355654713,1,['update'],['update']
Deployability,"I agree this issue is probably system specific. ; This creates a problem when using the container in nextflow since nextflow automatically configures few folder bindings when it prepares the run, namely the working directory, the directories of files staged into the process as inputs and the temp dir indicated by $TMPDIR.; Since it prepares all the scripts in advance, the $TMPDIR points to the standard /tmp location if I start nextflow from a login node, while in my system this is set to a node specific scratch space (/local scratch) when the job is submitted to a computing node by SLURM. Thus, I end up having the tmp dir not correctly mounted in the container. I'm not sure how common such a configuration is, so maybe it's a problem affecting just me and few others. What I've done is to add a line like this before the actual `run_deepvariant` command in my `script` section in the Nextflow process:; `export TMPDIR=""$PWD/tmp_dir""`. This overwrites the original variable and set the TMPDIR to a subfolder in the working directory. It works fine in this context since deepvariant is the only operation running in the process and thus changing TMPDIR does not interfere with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987:701,configurat,configuration,701,,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987,1,['configurat'],['configuration']
Deployability,I am fine with you making this change internally and releasing in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-475479349:75,release,release,75,,https://github.com/google/deepvariant/pull/152#issuecomment-475479349,1,['release'],['release']
Deployability,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)""; >NAME=""Debian GNU/Linux""; >VERSION_ID=""11""; >VERSION=""11 (bullseye)""; >VERSION_CODENAME=bullseye; >ID=debian; >HOME_URL=""https://www.debian.org/""; >SUPPORT_URL=""https://www.debian.org/support""; >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1602330355:43,install,install,43,,https://github.com/google/deepvariant/issues/669#issuecomment-1602330355,2,"['install', 'release']","['install', 'release']"
Deployability,"I assign this to my self, but --; since we're not using Keras in DeepVariant, it will take a uncertain amount time for me to prioritize this on my list of things. I will also open an internal bug to track, but this is something that we likely won't be able to help with in the short term. If you want to take a look at how we load the checkpoint, you can find examples in the inference code here:; https://github.com/google/deepvariant/blob/r0.7/deepvariant/call_variants.py#L346; And for training, in order to start from a checkpoint, you can see code in this:; https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L408. We use the Estimator API. So, another possibility to find more information for things that might help with converting any TensorFlow models to a Keras model, such as:; https://github.com/keras-team/keras/issues/5273; (I don't know if this one helps, but worth a look). I will keep this issue open for a while. Please feel free to share back if you find anything. I'll also update if I find anything useful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127#issuecomment-446036697:1014,update,update,1014,,https://github.com/google/deepvariant/issues/127#issuecomment-446036697,1,['update'],['update']
Deployability,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me.; Here's what I did to install an older version of bazel:; ```; BAZEL_VERSION=0.8.1; wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user; ```; Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353388370:112,install,install,112,,https://github.com/google/deepvariant/issues/19#issuecomment-353388370,4,"['install', 'release']","['install', 'installer-linux-', 'releases']"
Deployability,I can confirm that placing the exomes.bed file in `gs://public-debug` results in successful reading of the BED file (though the pipeline fails at a latter step due to inconsistent contig names within the BED vs. the reference genome).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-438063018:128,pipeline,pipeline,128,,https://github.com/google/deepvariant/issues/116#issuecomment-438063018,1,['pipeline'],['pipeline']
Deployability,"I converted the new 0.8.0 release docker to a singularity image on a Digital Ocean Centos 7 droplet and took notes, in case it's useful to anyone else. The instructions were based on [the notes](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) from @pichuan above, but includes instructions for installing docker, nvidia-docker, and singularity, as well as parameters specific to singularity v3.1.1. ```; # install docker; sudo yum check-update; curl -fsSL https://get.docker.com/ | sh. # install nvidia-docker; distribution=$(. /etc/os-release;echo $ID$VERSION_ID); curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \; sudo tee /etc/yum.repos.d/nvidia-docker.repo; sudo yum install -y nvidia-docker2; semanage fcontext -a -f f -t container_runtime_exec_t -s system_u /usr/bin/nvidia-docker; sudo restorecon -v /usr/bin/nvidia-docker. # start docker; sudo systemctl start docker; sudo systemctl status docker; sudo systemctl enable docker. # install deps; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y epel-release && \; sudo yum install -y golang openssl-devel libuuid-devel libseccomp-devel squashfs-tools; echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \; echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \; source ~/.bashrc. # install singularity; mkdir -p ${GOPATH}/src/github.com/sylabs && \; cd ${GOPATH}/src/github.com/sylabs && \; git clone https://github.com/sylabs/singularity.git && \; cd singularity; git checkout v3.1.1; cd ${GOPATH}/src/github.com/sylabs/singularity && \; ./mconfig && \; cd ./builddir && \; make && \; sudo make install; ; DVVER=0.8.0; # make deepvariant CPU image; sudo docker pull gcr.io/deepvariant-docker/deepvariant:${DVVER}; sudo docker tag gcr.io/deepvariant-docker/deepvariant:${DVVER} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482761898:26,release,release,26,,https://github.com/google/deepvariant/issues/132#issuecomment-482761898,7,"['install', 'release', 'update']","['install', 'installing', 'release', 'update']"
Deployability,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:; https://github.com/ink1/deepvariant/releases/tag/v0.5.2a; The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372644789:139,release,releases,139,,https://github.com/google/deepvariant/issues/6#issuecomment-372644789,2,['release'],"['release', 'releases']"
Deployability,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304#issuecomment-620715339:136,update,update,136,,https://github.com/google/deepvariant/issues/304#issuecomment-620715339,1,['update'],['update']
Deployability,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351084133:183,install,installed,183,,https://github.com/google/deepvariant/issues/12#issuecomment-351084133,4,"['install', 'upgrade']","['installation', 'installed', 'upgrades']"
Deployability,"I downloaded and ran the pre-built binaries *.zip (DeepVariant v 7.0.2) from GitHub. In fact, I have installed 'intervaltree==2.1.0', but it does not works? Strange.....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155#issuecomment-464509971:101,install,installed,101,,https://github.com/google/deepvariant/issues/155#issuecomment-464509971,1,['install'],['installed']
Deployability,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/332#issuecomment-669331139:50,release,release,50,,https://github.com/google/deepvariant/issues/332#issuecomment-669331139,1,['release'],['release']
Deployability,"I followed the directions on that page, and for the most part everything seemed to work, but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-749313156:104,install,install,104,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156,8,"['Release', 'install', 'update']","['Release', 'install', 'installation', 'update']"
Deployability,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```; gcloud compute instances create ""${USER}-test-speed"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-highmem-8"" \; --zone ""us-west2-b"" \; --boot-disk-size 100G \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I installed Singularity:; ```; curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \; sed -e s'|github.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:469,install,installed,469,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['install'],['installed']
Deployability,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED; 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though).; Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761#issuecomment-1990592785:92,install,installed,92,,https://github.com/google/deepvariant/issues/761#issuecomment-1990592785,1,['install'],['installed']
Deployability,"I got a machine to test:. ```; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I install nvidia driver first:. ```; sudo yum update -y && sudo yum install -y python3; curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py; sudo python3 install_gpu_driver.py; ```. After that, I can confirm that nvidia-smi exists:; ```; [pichuan@pichuan-gpu2 ~]$ nvidia-smi; Thu Mar 16 04:47:54 2023 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:423,install,install,423,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553,4,"['install', 'update']","['install', 'installation', 'update']"
Deployability,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437006790:147,configurat,configuration,147,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790,3,"['configurat', 'pipeline']","['configuration', 'pipeline', 'pipelines']"
Deployability,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. ; But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/634#issuecomment-1518574416:7,install,installed,7,,https://github.com/google/deepvariant/issues/634#issuecomment-1518574416,2,['install'],"['install', 'installed']"
Deployability,"I have no problem in making my .bam file public, but please help me where; to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM.; >; > Coverage is 46x; >; > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> Hmm, empirically we've been able to handle larger input BAM than 20GB; >> with less RAM you had.; >>; >> Another known issue (which we will fix in the next release) is that if; >> auxiliary fields are being read in, and if the BAM has many of them, it; >> could unnecessarily increase the RAM usage. However, in this case given; >> that you're using WGS model (which by default isn't parsing aux fields),; >> that shouldn't be an issue...; >>; >> @kirti141 <https://github.com/kirti141> Another question for you - is; >> there a BAM file that you can make public (with no sensitive information,; >> of course) that we can attempt to reproduce this issue? Thanks again for; >> reporting this.; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>; >> .; >> Triage notifications on the go with GitHub Mobile for iOS; >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; >> or Android; >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925387861:512,release,release,512,,https://github.com/google/deepvariant/issues/482#issuecomment-925387861,1,['release'],['release']
Deployability,"I have now solved this error.; Firstly, input within shell:; > singularity shell deepvariant_1.5.0.sif. Then,; > pip install --upgrade pip; > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/634#issuecomment-1518577444:117,install,install,117,,https://github.com/google/deepvariant/issues/634#issuecomment-1518577444,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"I hope to make such an installation script soon. How do you think, is it possible for you to change the installation script for mine one if it works? Or there is some kind of building system on your side which is the reason of such complicated and not straight-forward installation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-901424155:23,install,installation,23,,https://github.com/google/deepvariant/issues/476#issuecomment-901424155,3,['install'],['installation']
Deployability,"I installed CUDA-9.0 based on instruction here, https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. and nvidia-docker based on instruction here, https://github.com/NVIDIA/nvidia-docker#ubuntu-140416041804-debian-jessiestretch. I am on a Ubuntu-16.04 GCE box. Sorry, there was a typo in my previous comment, corrected it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102#issuecomment-429155535:2,install,installed,2,,https://github.com/google/deepvariant/issues/102#issuecomment-429155535,2,['install'],"['install-driver-script', 'installed']"
Deployability,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0; [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>; wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try.; > Before I built I did something like:; >; > # Install Python 2.7; > sudo yum install -y centos-release-SCL; > sudo yum install -y python27; > source /opt/rh/python27/enable; >; > I think starting from there it just assumes python is in; > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can; > make it recognize python at any path.; > Is there a convention that people use to build something so that they can; > point to other Python locations?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386079018:44,INSTALL,INSTALL,44,,https://github.com/google/deepvariant/issues/29#issuecomment-386079018,5,"['INSTALL', 'Install', 'install', 'release']","['INSTALL', 'Install', 'install', 'release-SCL']"
Deployability,"I ran make_examples and generated a realigned BAM for the region overlapping chr1:109161996. As a result some of the reads aligned differently. For example read ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is originally aligned at chr1:109,161,935 but after the realignment it is aligned at chr1:109,161,895. I also tried to align this read with BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat) and the only alignment there is the one we have after the realignment.; Next, I generated a debug pileup image (using show_examples utility which is part of DeepVariant). The image allows to visualize what CNN would ""see"" when inference is done for this variant. In the image there are reads that do not support the variant which by design means that these reads support REF. ; It is obviously a bug which we will try to fix in the future releases. The problem occurs because we should only include reads that overlap our candidate. In this case we include reads that do not overlap a candidate. And it happens because in the original mapping these reads do overlap the candidate but after the local realignment these reads do not overlap the candidate.; This situation should be rare unless there is something wrong with the mapper which results in misaligned reads. Pileup visualization. Take a look at ""read supports variant"" column. Reads marked white support variant, and reads marked gray support REF:; ![show_examples__channels_chr1:109161995_A- G](https://user-images.githubusercontent.com/1168691/125844383-90091827-0bc1-41fd-9b04-dd315718cbd4.png). IGV, original alignment is top, realigned reads are at the bottom. ""A00685:52:H2VF3DSXY:4:1352:12789:18818"" is marked red:; ![Screenshot from 2021-07-15 12-15-03](https://user-images.githubusercontent.com/1168691/125844668-69037a66-e3f5-4750-88b5-e7c1ee488708.png)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470#issuecomment-880951168:827,release,releases,827,,https://github.com/google/deepvariant/issues/470#issuecomment-880951168,1,['release'],['releases']
Deployability,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:; ```; 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader; I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs; 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437596560:320,patch,patch,320,,https://github.com/google/deepvariant/issues/116#issuecomment-437596560,1,['patch'],['patch']
Deployability,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`; does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. ; A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-587538179:515,release,released,515,,https://github.com/google/deepvariant/issues/272#issuecomment-587538179,1,['release'],['released']
Deployability,"I see!; @MorganHow , unfortunately DeepVariant isn't designed to run on MacOS. ; We can update our [prerequisites](https://github.com/google/deepvariant#prerequisites) to be a bit more specific. We have not tried it on MacOS because even if you can get a small example to run, it won't be feasible to run on a real sample. @MorganHow Do you have another Linux machines you can try this? Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304#issuecomment-620713923:88,update,update,88,,https://github.com/google/deepvariant/issues/304#issuecomment-620713923,1,['update'],['update']
Deployability,I should also mention that DeepVariant has had 6 (or more) channels since the first open source release (v0.4). The type of image mentioned in the 2018 paper was only used in the very first version of DeepVariant before the software was rewritten for open-source.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/331#issuecomment-667352024:96,release,release,96,,https://github.com/google/deepvariant/issues/331#issuecomment-667352024,1,['release'],['release']
Deployability,"I should also note that we ran the same pipeline on data we produced in-house, and that worked perfectly fine (including the locus I picked out here). So I am wondering if there is something about the reads our collaborator provided that somehow conflicts with assumptions made by DeepVariant (?).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470#issuecomment-878807029:40,pipeline,pipeline,40,,https://github.com/google/deepvariant/issues/470#issuecomment-878807029,1,['pipeline'],['pipeline']
Deployability,"I think RAM is not an issue, my machine has 2TB RAM. Coverage is 46x. On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>; wrote:. > Hmm, empirically we've been able to handle larger input BAM than 20GB with; > less RAM you had.; >; > Another known issue (which we will fix in the next release) is that if; > auxiliary fields are being read in, and if the BAM has many of them, it; > could unnecessarily increase the RAM usage. However, in this case given; > that you're using WGS model (which by default isn't parsing aux fields),; > that shouldn't be an issue...; >; > @kirti141 <https://github.com/kirti141> Another question for you - is; > there a BAM file that you can make public (with no sensitive information,; > of course) that we can attempt to reproduce this issue? Thanks again for; > reporting this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925386459:294,release,release,294,,https://github.com/google/deepvariant/issues/482#issuecomment-925386459,1,['release'],['release']
Deployability,"I think it's because it's a Mac M1 which is `aarch64` (`arm64`), so let's try the following one:; ```; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-linux-arm64; ```; Let me know if it runs for you when you execute it via the following:. ```; chmod +x bazel-5.3.0-linux-arm64; ./bazel-5.3.0-linux-arm64; ```. Let me know if that fixes it. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577767675:150,release,releases,150,,https://github.com/google/deepvariant/issues/657#issuecomment-1577767675,1,['release'],['releases']
Deployability,"I think the easiest way to do it is as follows:; 1. Install VirtualBox on your laptop, then Ubuntu, then Docker.; 2. Pull the DeepVariant image, and then follow the instructions at the following link on how to modify the image and committing it:; https://www.techrepublic.com/article/how-to-commit-changes-to-a-docker-image/; 3. Perform `docker save` to be able to transfer it so you can convert it to a Singularity image - with a link provided below on how to use `docker save`:; https://docs.docker.com/engine/reference/commandline/save/. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-458642985:52,Install,Install,52,,https://github.com/google/deepvariant/issues/132#issuecomment-458642985,1,['Install'],['Install']
Deployability,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you!. To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2011022724:2,update,updated,2,,https://github.com/google/deepvariant/issues/793#issuecomment-2011022724,1,['update'],['updated']
Deployability,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-704106130:22,update,update,22,,https://github.com/google/deepvariant/issues/346#issuecomment-704106130,1,['update'],['update']
Deployability,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:; ```; WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/; export SINGULARITY_CACHEDIR=$WORKING_DIR; export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/; mkdir -p $WORKING_DIR/tmp/. singularity exec \; 	-e \; 	-c \; 	-H $WORKING_DIR \; 	-B $WORKING_DIR/tmp:/tmp \; 	-B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/output"" \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/genomedir/$FASTA_FILE"" \; --reads=""/bamdir/$PROBAND_BAM"" \; --output_vcf=""/output/$PROBAND_VCF"" \; --output_gvcf=""/output/$PROBAND_GVCF"" \; --intermediate_results_dir=""/output/intermediate"" \; --num_shards=$NSLOTS ; ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1156819727:1244,deploy,deployment,1244,,https://github.com/google/deepvariant/issues/514#issuecomment-1156819727,1,['deploy'],['deployment']
Deployability,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you?. Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/305#issuecomment-620759329:509,update,update,509,,https://github.com/google/deepvariant/issues/305#issuecomment-620759329,1,['update'],['update']
Deployability,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```; I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader; I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437974705:25,pipeline,pipeline,25,,https://github.com/google/deepvariant/issues/116#issuecomment-437974705,2,['pipeline'],['pipeline']
Deployability,"I wonder if the installation for bazel is different on Ubuntu 18.04.; Can you try installing bazel separately, see https://docs.bazel.build/versions/master/install-ubuntu.html and see if you can install bazel?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-424847921:16,install,installation,16,,https://github.com/google/deepvariant/issues/98#issuecomment-424847921,4,['install'],"['install', 'install-ubuntu', 'installation', 'installing']"
Deployability,"I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372439840:273,install,install,273,,https://github.com/google/deepvariant/issues/6#issuecomment-372439840,1,['install'],['install']
Deployability,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-464903381:78,configurat,configuration,78,,https://github.com/google/deepvariant/issues/157#issuecomment-464903381,1,['configurat'],['configuration']
Deployability,I'll close this bug for now. Later on I can plan to post a summarized update.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/824#issuecomment-2235552132:70,update,update,70,,https://github.com/google/deepvariant/issues/824#issuecomment-2235552132,1,['update'],['update']
Deployability,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646#issuecomment-1552499124:99,update,update,99,,https://github.com/google/deepvariant/issues/646#issuecomment-1552499124,1,['update'],['update']
Deployability,I'll close this one for now. Please feel free to update and let me know if it worked or not.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597#issuecomment-1356038369:49,update,update,49,,https://github.com/google/deepvariant/issues/597#issuecomment-1356038369,1,['update'],['update']
Deployability,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355389148:341,release,releases,341,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148,1,['release'],['releases']
Deployability,"I'm going to close this issue, but please let us know whether the updated Docker image works for you. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/235#issuecomment-557203919:66,update,updated,66,,https://github.com/google/deepvariant/issues/235#issuecomment-557203919,1,['update'],['updated']
Deployability,"I'm going to close this issue, since it seems to boil down to either a network issue or a problem installing gsutil.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/17#issuecomment-352605040:98,install,installing,98,,https://github.com/google/deepvariant/issues/17#issuecomment-352605040,1,['install'],['installing']
Deployability,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```; wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run; sudo sh cuda_11.3.0_465.19.01_linux.run; ```. ```; export PATH=/usr/local/cuda-11.3/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Now, with this, I tried:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648:17,install,installing,17,,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648,2,"['install', 'release']","['installing', 'release']"
Deployability,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```; sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:179,install,installed,179,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355,4,"['install', 'release']","['install', 'installed', 'release']"
Deployability,"I'm happy to see that this forum has sparked potential research collaborations!; I'm going to close this issue for now so it's easier for me to track what issues still need our attention. Feel free to post further updates on this topic if you like. If you have more questions for the DeepVariant team, also feel free to open more issues. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-437181543:214,update,updates,214,,https://github.com/google/deepvariant/issues/114#issuecomment-437181543,1,['update'],['updates']
Deployability,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/66#issuecomment-403636246:48,install,installation,48,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246,4,['install'],"['install', 'installation', 'installing']"
Deployability,"I'm using Singularity 2.6.0. Below is what I've been using for my Singularity.spec file for version 0.7.0 which still works for me. Changing the version number of DeepVariant to 0.7.2 gives me the ImportError: No module named numpy when trying to run make_examples. ```; Bootstrap: localimage; From: deepvariant-0.7.0.simg. # Modified from: https://gist.github.com/pansapiens/717efcdefb51fa0ce1a6abf092bcb2f4; # Install Google Cloud SDK for the gcloud tool, then:; # We grab the official Docker image, push it to a locally running container repo, then get; # Singularity to pull it back.; # docker pull gcr.io/deepvariant-docker/deepvariant:0.7.0; # docker tag gcr.io/deepvariant-docker/deepvariant:0.7.0 localhost:5000/deepvariant:0.7.0; # docker run -d -p 5000:5000 --restart=always --name registry registry:2; # docker push localhost:5000/deepvariant:0.7.0; # sudo SINGULARITY_NOHTTPS=1 singularity pull docker://localhost:5000/deepvariant:0.7.0; #; # Then use that container image to make this customized one; # sudo singularity build deepvariant-custom.simg Singularity.spec. %environment; PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; DV_GPU_BUILD=0; export PATH DV_GPU_BUILD. %apprun download_testdata; BUCKET=""gs://deepvariant""; DATA_BUCKET=""${BUCKET}/quickstart-testdata/*""; mkdir -p input; gsutil cp -R ""${DATA_BUCKET}"" input/. %apprun make_examples; exec /opt/deepvariant/bin/make_examples \; --mode calling \; --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; --reads /dv2/input/NA12878_S1.chr20.10_10p1mb.bam \; --examples output.examples.tfrecord \; --regions ""chr20:10,000,000-10,010,000"". %apprun call_variants; exec /opt/deepvariant/bin/call_variants \; --outfile call_variants_output.tfrecord \; --examples output.examples.tfrecord \; --checkpoint /models/wgs/model.ckpt. %apprun postprocess_variants; exec /opt/deepvariant/bin/postprocess_variants \; --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; --infile call_variants_output.tfrecord \; --outfile ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-458208323:412,Install,Install,412,,https://github.com/google/deepvariant/issues/132#issuecomment-458208323,1,['Install'],['Install']
Deployability,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out.; Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/414#issuecomment-771075195:43,update,update,43,,https://github.com/google/deepvariant/issues/414#issuecomment-771075195,3,"['release', 'update']","['release', 'update']"
Deployability,"I've updated tensorflow to 2.7.0 with `sed -i ""s|2.5.0|2.7.0|"" ""deepvariant/settings.sh""` and everything working fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511#issuecomment-1021003424:5,update,updated,5,,https://github.com/google/deepvariant/issues/511#issuecomment-1021003424,1,['update'],['updated']
Deployability,"ILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages""; export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif; export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip; $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees; mkdir -p $LLVM_DIR; cd $LLVM_DIR; svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm; cd llvm/tools; svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang; ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree.; mkdir -p $BUILD_DIR; cd $BUILD_DIR; # Note to remove -DLLVM_TARGETS_TO_BUILD=X86; # ""rm CMakeCache.txt"" to remove cmake cache; cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \; -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \; -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \; -DCMAKE_BUILD_TYPE=Release \; -DLLVM_BUILD_DOCS=false \; -DLLVM_TARGETS_TO_BUILD=PowerPC \; -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \; -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:13657,Release,Release,13657,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['Release'],['Release']
Deployability,"IX_PATH"" \; -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \; -DCMAKE_BUILD_TYPE=Release \; -DLLVM_BUILD_DOCS=false \; -DLLVM_TARGETS_TO_BUILD=PowerPC \; -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \; -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:14535,install,install,14535,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"If there's no update on this, I'm going to close this for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/77#issuecomment-403634683:14,update,update,14,,https://github.com/google/deepvariant/issues/77#issuecomment-403634683,1,['update'],['update']
Deployability,"If you type `sudo docker run 45f6c7767ff0`, does that also not launch it? Also you can add your username to the Docker group so you don't need sudo, as shown here -- but do that after we solve your issue:. [https://docs.docker.com/engine/install/linux-postinstall/](https://docs.docker.com/engine/install/linux-postinstall/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/700#issuecomment-1687392992:238,install,install,238,,https://github.com/google/deepvariant/issues/700#issuecomment-1687392992,2,['install'],['install']
Deployability,"If you want to directly take from the output of make_examples and create vcf from there, you can see this implementation here:; https://github.com/google/deepvariant/blob/r1.1/deepvariant/labeler/labeled_examples_to_vcf.py. Note that this currently requires the examples to have labels (i.e., generated with `training` mode). But you can relax the constraint here:; https://github.com/google/deepvariant/blob/r1.1/deepvariant/labeler/labeled_examples_to_vcf.py#L123-L127. But, also note that I'm not sure if this code properly deals with multi-allelics. So, waiting for the next release to use the new option that @danielecook mentioned might be a better solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/467#issuecomment-870965925:579,release,release,579,,https://github.com/google/deepvariant/issues/467#issuecomment-870965925,1,['release'],['release']
Deployability,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md; The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290#issuecomment-606166642:350,pipeline,pipeline,350,,https://github.com/google/deepvariant/issues/290#issuecomment-606166642,1,['pipeline'],['pipeline']
Deployability,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-728683346:15,release,release,15,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346,1,['release'],['release']
Deployability,"In the next release, we'll have a code update that only saves the tags we use in memory. Which will resolve this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/490#issuecomment-948986764:12,release,release,12,,https://github.com/google/deepvariant/issues/490#issuecomment-948986764,2,"['release', 'update']","['release', 'update']"
Deployability,"In your first comment you said: ""I am running this on an AWS graviton4 machine (aarch64 architecture)."" so I thought switching instance type was an option for you. I am sorry for your trouble. We can give it a shot but the next release is pretty close and we may not have enough bandwidth to support aarch64. However, I will file a bug internally and see if we can support aarch64 in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2339418958:228,release,release,228,,https://github.com/google/deepvariant/issues/879#issuecomment-2339418958,1,['release'],['release']
Deployability,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353466661:0,Install,Installing,0,,https://github.com/google/deepvariant/issues/19#issuecomment-353466661,2,"['Install', 'install']","['Installing', 'install']"
Deployability,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355#issuecomment-697537665:0,Install,Installing,0,,https://github.com/google/deepvariant/issues/355#issuecomment-697537665,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:; ```; ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting ; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel ; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; OSError: Operation not permitted. ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-360833579:148,Install,Install,148,,https://github.com/google/deepvariant/issues/41#issuecomment-360833579,2,['Install'],"['Install', 'Installing']"
Deployability,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:; ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge; _tflow_select: 2.1.0-gpu ; absl-py: 0.8.1-py27_0 conda-forge; astor: 0.7.1-py_0 conda-forge; backports: 1.0-py_2 conda-forge; backports.weakref: 1.0.post1-py27_1000 conda-forge; boost: 1.70.0-py27h9de70de_1 conda-forge; boost-cpp: 1.70.0-h8e57a91_2 conda-forge; bzip2: 1.0.8-h516909a_2 conda-forge; c-ares: 1.15.0-h516909a_1001 conda-forge; ca-certificates: 2019.11.28-hecc5488_0 conda-forge; certifi: 2019.11.28-py27_0 conda-forge; cffi: 1.13.2-py27h8022711_0 conda-forge; chardet: 3.0.4-py27_1003 conda-forge; cliff: 2.15.0-py27_0 conda-forge; cmd2: 0.8.6-py27_0 conda-forge; contextlib2: 0.6.0-py_0 conda-forge; crcmod: 1.7-py27_1002 conda-forge; cryptography: 2.8-py27h72c5cf5_1 conda-forge; cudatoolkit: 9.2-0 ; cudnn: 7.6.4-cuda9.2_0 ; cupti: 9.2.148-0 ; curl: 7.65.3-hf8cf82a_0 conda-forge; enum34: 1.1.6-py27_1002 conda-forge; funcsigs: 1.0.2-py_3 conda-forge; futures: 3.3.0-py27_0 conda-forge; gast: 0.3.2-py_0 conda-forge; google-cloud-sdk: 166.0.0-py27_0 bioconda ; grpcio: 1.23.0-py27he9ae1f9_0 conda-forge; h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge; hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge; htslib: 1.9-h244ad75_9 bioconda ; httplib2: 0.14.0-py27_0 conda-forge; icu: 64.2-he1b5a44_1 conda-forge; idna: 2.8-py27_1000 conda-forge; intervaltree: 3.0.2-py_0 conda-forge; ipaddress: 1.0.23-py_0 conda-forge; keras-applications: 1.0.8-py_1 conda-forge; keras-preprocessing: 1.1.0-py_0 conda-forge; krb5: 1.16.4-h2fd8d38_0 conda-forge; libblas: 3.8.0-14_openblas conda-forge; libcblas: 3.8.0-14_openblas conda-forge; libcurl: 7.65.3-hda55be3_0 conda-forge; libdeflate: 1.3-h516909a_0 conda-forge; libedit: 3.1.20170329-hf8c457e_1001 conda-forge; libffi: 3.2.1-he1b5a44_1006 conda-forge; libgcc-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566427577:79,install,installed,79,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577,1,['install'],['installed']
Deployability,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366748047:294,configurat,configuration,294,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047,1,['configurat'],['configuration']
Deployability,"Is the one you install on conda this one?; https://bioconda.github.io/recipes/deepvariant/README.html; If it's that, I believe it's recently updated to 0.7. ; This package actually isn't manage by the team at Google. But the owner ( @chapmanb ) has updated to the latest version (0.7). A quick search of `dv_make_examples.py` seems to be pointed to this file; https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/dv_make_examples.py; which it does call python make_examples.zip.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101#issuecomment-430074459:15,install,install,15,,https://github.com/google/deepvariant/issues/101#issuecomment-430074459,3,"['install', 'update']","['install', 'updated']"
Deployability,Is there any update regarding this issue? I am also trying to build a singularity container for deepvariant 0.7.2 and I am producing the same error. Will this be fixed on later versions?. I assume a lot of teams don't have a root access when using HPC. It will be awesome if you add support for building singularity containers in your documentations as this will things a lot easier when using deepvariant on HPC either for calling or training.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-457977330:13,update,update,13,,https://github.com/google/deepvariant/issues/132#issuecomment-457977330,1,['update'],['update']
Deployability,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/13#issuecomment-351172185:725,release,release,725,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185,1,['release'],['release']
Deployability,It is customised. And Python 2.7 was installed using Anaconda.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-355386945:37,install,installed,37,,https://github.com/google/deepvariant/issues/32#issuecomment-355386945,1,['install'],['installed']
Deployability,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud.; One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/17#issuecomment-352085708:118,install,install,118,,https://github.com/google/deepvariant/issues/17#issuecomment-352085708,1,['install'],['install']
Deployability,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353393763:21,install,install,21,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763,3,"['continuous', 'install', 'integrat']","['continuous', 'install', 'integration']"
Deployability,"It should also be available in the v0.10.0 release. If not, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290#issuecomment-606155318:43,release,release,43,,https://github.com/google/deepvariant/issues/290#issuecomment-606155318,1,['release'],['release']
Deployability,"It took about 8 hours, but I could run the **postprocess_variants** step on my local computer (using the commands [specified above](https://github.com/google/deepvariant/issues/167#issuecomment-480640009)). If Official Amazon Support doesn't have a solution for running this program on AWS, I might cross-post this on StackExchange (to see if I can figure out if there is some sort of configuration issue on AWS, and/or if I am not using ECS efficiently/correctly). However, I realize you have a lot of support to provide, so I will close this ticket and provide the successful output from my local computer:. ```; 2019-04-07 21:06:30.591035: I deepvariant/postprocess_variants.cc:88] Read from: Genos_Provided/call_variants_output.tfrecord.gz; 2019-04-07 21:06:33.504711: I deepvariant/postprocess_variants.cc:97] Done reading: Genos_Provided/call_variants_output.tfrecord.gz. #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505181: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505270: I deepvariant/postprocess_variants.cc:103] Start SortSingleSiteCalls; 2019-04-07 21:06:34.914308: I deepvariant/postprocess_variants.cc:105] Done SortSingleSiteCalls; I0407 21:06:36.217032 139687245461248 postprocess_variants.py:596] Writing output to VCF file: Genos_Provided/output.vcf.gz; I0407 21:06:36.221911 139687245461248 genomics_writer.py:163] Writing Genos_Provided/output.vcf.gz with NativeVcfWriter; I0407 21:06:36.231071 139687245461248 postprocess_variants.py:601] 1 variants written.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480702286:385,configurat,configuration,385,,https://github.com/google/deepvariant/issues/167#issuecomment-480702286,1,['configurat'],['configuration']
Deployability,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-364535704:46,release,released,46,,https://github.com/google/deepvariant/issues/46#issuecomment-364535704,2,['release'],['released']
Deployability,Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipel,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4031,Pipeline,Pipeline,4031,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['Pipeline'],['Pipeline']
Deployability,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56; Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470#issuecomment-891869362:9,update,update,9,,https://github.com/google/deepvariant/issues/470#issuecomment-891869362,1,['update'],['update']
Deployability,"LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; #######################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:14791,install,installed,14791,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],"['install', 'installed']"
Deployability,Looks good to me. Will the flags will stay the same in the next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/473#issuecomment-882717016:64,release,release,64,,https://github.com/google/deepvariant/pull/473#issuecomment-882717016,1,['release'],['release']
Deployability,"My spidey sense is telling me that you have two versions installed. If you run the following Python commands you can see the search-order of paths to determine if you have another `intervaltree` module installed system-wide, and it prioritizes that one before yours:. ```Python; import sys; print(sys.path); ```. The easiest thing is to set the `PYTHONPATH`, or just download the package and untar it in the local deepvariant directory. Here's more info on the Python's search-order of modules:. https://leemendelowitz.github.io/blog/how-does-python-find-packages.html. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155#issuecomment-464532038:57,install,installed,57,,https://github.com/google/deepvariant/issues/155#issuecomment-464532038,2,['install'],['installed']
Deployability,"New model checkpoints associated with new releases will be under gs://deepvariant/models/DeepVariant as you noticed. I mentioned that starting from v1.4.0, you can see this file:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. The ""channels"" values are enums. You can look them up in this proto:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/protos/deepvariant.proto#L1048. From the example above, it's saying that DeepVariant v1.4.0 WGS model has 7 channels, and they are:. ```; CH_READ_BASE = 1;; CH_BASE_QUALITY = 2;; CH_MAPPING_QUALITY = 3;; CH_STRAND = 4;; CH_READ_SUPPORTS_VARIANT = 5;; CH_BASE_DIFFERS_FROM_REF = 6;; CH_INSERT_SIZE = 19;; ```. Note that the allele frequency model isn't part of our regular release process yet. It's made public as part of our preprint https://doi.org/10.1101/2021.01.06.425550. Right now, we're retraining it when users request it. We're certainly hoping to see more uses cases (thank you for letting us know!). If it's become more mature, we can consider building it into part of our regular release process. (Adding more regular supports also means more overhead for each release, so we need to balance this carefully.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568#issuecomment-1253235153:42,release,releases,42,,https://github.com/google/deepvariant/issues/568#issuecomment-1253235153,4,['release'],"['release', 'releases']"
Deployability,"Nice catch! Thank you! It does seem that there are different number of outputs, though I've (to my knowledge) only used the commands from the full run with 19 cpus and --dry_run=true. I've cleared the output directories and am running the full command from the beginning. Which nvidia-driver - cuda combination do you run deepvariant with? I'm looking into the gpu -problem, I'm thinking I need to install an older nvidia-driver and cuda. Currently the driver is 555.42.02, but looking at this https://docs.nvidia.com/deploy/cuda-compatibility/#id1 , it's not compatible with cuda 11.3.1 that the deepvariant:1.6.1-gpu is using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2230253485:398,install,install,398,,https://github.com/google/deepvariant/issues/849#issuecomment-2230253485,2,"['deploy', 'install']","['deploy', 'install']"
Deployability,Nice! This is fixed now and will go out with the next release of DeepVariant. Thanks for the great suggestion.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/37#issuecomment-360202139:54,release,release,54,,https://github.com/google/deepvariant/issues/37#issuecomment-360202139,1,['release'],['release']
Deployability,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437114999:80,Pipeline,Pipeline,80,,https://github.com/google/deepvariant/issues/118#issuecomment-437114999,3,"['Pipeline', 'release', 'update']","['Pipeline', 'release', 'updated']"
Deployability,"No. The model we released is diploid. . Long answer: in terms of the implementation, many of the utility functions try to take into account different ploidy. But in the actual case of DeepVariant we haven't done anything other than the diploid scenario. so if you're looking for an already pre-packaged caller, the answer is no. But if you're looking for Python functions that might help with your development, please feel free to look into the code in Nucleus and deepvariant, and let us know if you have any questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/79#issuecomment-398526499:17,release,released,17,,https://github.com/google/deepvariant/issues/79#issuecomment-398526499,1,['release'],['released']
Deployability,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896286964:301,install,installs,301,,https://github.com/google/deepvariant/issues/476#issuecomment-896286964,1,['install'],['installs']
Deployability,"Note you will also have to revert a couple of lines referred to in run-preset.sh to match the tensorrt version. This is the text in run-preset.sh, you should probably revert to the way it was in 1.5.0 for this section:; . ```; # In v8.6.1, the libs got moved to tensorrt_libs:; # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1; sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7""; sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/789#issuecomment-1992748017:326,release,release-notes,326,,https://github.com/google/deepvariant/issues/789#issuecomment-1992748017,1,['release'],['release-notes']
Deployability,"OK, to give this a try myself, here is what I did:. Get a CentOS7 machine to try:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b""; ```. ssh into the machine. ```; gcloud compute ssh ${USER}-centos7 --zone us-west1-b; ```. On the machine, install singularity:; I used the instructions on https://github.com/sylabs/singularity/blob/master/INSTALL.md. After installation, I checked the version:. ```; $ singularity --version; singularity-ce version 3.9.2; ```. Then, I followed the steps on https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get data. And then:. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. This seems to work for me. @marchoeppner Sorry that I don't think we have the bandwidth to support your specific case now. But if do find out how to make it work, please share what you find. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1014178969:420,install,install,420,,https://github.com/google/deepvariant/issues/499#issuecomment-1014178969,3,"['INSTALL', 'install']","['INSTALL', 'install', 'installation']"
Deployability,"ON); details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 1; exitStatus: 1; stderr: |+; /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'; - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 1; ipAddress:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:5170,pipeline,pipeline,5170,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['pipeline'],['pipeline']
Deployability,"OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:6081,install,install,6081,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['install'],['install']
Deployability,"Oh I thought I was using a consistent model and codebase — thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461399048:484,pipeline,pipeline,484,,https://github.com/google/deepvariant/issues/151#issuecomment-461399048,1,['pipeline'],['pipeline']
Deployability,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params; ```; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \; --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-438034821:90,pipeline,pipeline,90,,https://github.com/google/deepvariant/issues/116#issuecomment-438034821,1,['pipeline'],['pipeline']
Deployability,"Okay, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/270#issuecomment-594228762:21,update,update,21,,https://github.com/google/deepvariant/issues/270#issuecomment-594228762,1,['update'],['update']
Deployability,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python?. I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593865376:35,upgrade,upgrade,35,,https://github.com/google/deepvariant/issues/664#issuecomment-1593865376,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374#issuecomment-723753324:46,update,update,46,,https://github.com/google/deepvariant/issues/374#issuecomment-723753324,1,['update'],['update']
Deployability,"Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1192,Pipeline,Pipeline,1192,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222,1,['Pipeline'],['Pipeline']
Deployability,"Overall, I feel like this thread has mentioned a few different things that in my mind are pretty different. Realigner, ( @pgrosu 's pointer 2. 4. above) , which does a local realignment, which was proven helpful especially for our Indel accuracy. Note that this was turned off by default for PacBio model. This is something that @akolesnikov will be able to answer more questions on. How candidates are proposed (which seems to be the main question at the initial question) is based on set of thresholds, which can be adjusted with these flags:; https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py#L189-L202; If you change these thresholds in your make_examples during calling, you will be able to get different number of candidates proposed in the `make_examples` stage. But note that our default release models are trained with examples based on the default thresholds. @pgrosu 's pointer 6 above is the pointer of how these initial set of candidates are proposed before they're passed to the classifier (used in `call_variants` step). And in fact, if you look at the final output VCF at the end, it should include all the candidates. If the classifier decided a candidate is not actually a variant, it will get a `0/0` genotype and have ""RefCall"" in its FILTER field. @anands-repo , I'm not sure if my answer and Paul's answer cover everything you want to ask now. If not, feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512120721:823,release,release,823,,https://github.com/google/deepvariant/issues/197#issuecomment-512120721,1,['release'],['release']
Deployability,"PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:15608,install,install,15608,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,Part of this PR is included in the latest release: https://github.com/google/deepvariant/releases/tag/v0.8.0; Thank you for your contribution!. @fo40225 I'll try to schedule a separate discussion about the async writer with you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-482391198:42,release,release,42,,https://github.com/google/deepvariant/pull/152#issuecomment-482391198,2,['release'],"['release', 'releases']"
Deployability,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/18#issuecomment-353720479:0,Patch,Patching,0,,https://github.com/google/deepvariant/pull/18#issuecomment-353720479,1,['Patch'],['Patching']
Deployability,"Peter;; Thanks for following up and glad to hear that this got it installed for you. I've been trying to replicate to fix the issue and avoid the manual pinning but can't seem to do on my system. Perhaps this was a temporary download issue with the google-cloud-sdk package, but it seems okay now. If anyone else stumbles across this same issue we can dig more but hopefully it was just something transient and we're good going forward. Thanks again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-487313448:66,install,installed,66,,https://github.com/google/deepvariant/issues/177#issuecomment-487313448,1,['install'],['installed']
Deployability,"Peter;; Thanks for testing, it sounds like there is a problem with the recent google-cloud-sdk packages. I'll take a look to see if I can figure out what is going wrong but an immediate thing you could try is to restrict that dependency version to try and avoid the issue:; ```; conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; ```; Hope this helps get it installed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486163437:386,install,installed,386,,https://github.com/google/deepvariant/issues/177#issuecomment-486163437,1,['install'],['installed']
Deployability,"Peter;; Thanks for the report and apologies about the install issues. It looks like you're installing deepvariant in a python 3 environment, and it only works with python 2.7, which might be the source of the problems. Hopefully if you try installing with `'python=2.7'` as an additional requirement that will help avoid it. If you still hit issues, it might be a legit problem with the latest `google-cloud-sdk` and you could also add `'google-cloud-sdk<243.0.0'` as another requirement. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486033904:54,install,install,54,,https://github.com/google/deepvariant/issues/177#issuecomment-486033904,3,['install'],"['install', 'installing']"
Deployability,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101#issuecomment-430171385:287,install,installs,287,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385,2,['install'],"['install', 'installs']"
Deployability,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385500259:198,install,installable,198,,https://github.com/google/deepvariant/issues/29#issuecomment-385500259,1,['install'],['installable']
Deployability,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385485505:189,install,install,189,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505,1,['install'],['install']
Deployability,"Pi-Chuan -- thanks so much for looking at this.; Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385125512:142,update,update,142,,https://github.com/google/deepvariant/issues/29#issuecomment-385125512,2,"['install', 'update']","['installing', 'update']"
Deployability,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1009,install,installed,1009,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002,3,['install'],"['install', 'installed', 'installing']"
Deployability,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386869866:1543,install,installing,1543,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866,1,['install'],['installing']
Deployability,"Pi-Chuan;; Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know?. For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386644191:833,install,installing,833,,https://github.com/google/deepvariant/issues/29#issuecomment-386644191,1,['install'],['installing']
Deployability,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386074270:27,update,update,27,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270,4,"['install', 'update']","['install', 'update', 'updated']"
Deployability,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385874525:217,configurat,configurations,217,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525,2,['configurat'],['configurations']
Deployability,"Please provide more details on the issue (commands run, the error you saw, etc.) You can start with the [quickstart doc](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) or one of the case studies (linked on the top-level page) to better understand how to install and run DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/169#issuecomment-481854459:288,install,install,288,,https://github.com/google/deepvariant/issues/169#issuecomment-481854459,1,['install'],['install']
Deployability,"Please refer to [this](https://github.com/google/deepvariant/issues/736) issue that contains some information on Conda.; Unfortunately, we don't support Conda and there is no documentation on how to install through Conda.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/865#issuecomment-2278620019:199,install,install,199,,https://github.com/google/deepvariant/issues/865#issuecomment-2278620019,1,['install'],['install']
Deployability,"Please see https://github.com/google/deepvariant/issues/443#issuecomment-821755400 for how to update to Ubuntu 18.04.; I haven't tried updating to Python 3.8 yet. But I'll create an internal issue to track it.; Closing this issue now, but feel free to add more questions/comments if related to this one, or feel free to open another issue for any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-821755610:94,update,update,94,,https://github.com/google/deepvariant/issues/441#issuecomment-821755610,1,['update'],['update']
Deployability,"Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; ...; ```; After this, the call_variants steps started running with no issues:; ```; ...; I0116 03:14:04.107081 140161826506496 openvino_estimator.py:118] Using OpenVINO in call_variants.; I0116 03:14:04.367440 140161826506496 openvino_estimator.py:155] Processed 1 examples in call_variants (using OpenVINO); I0116 03:14:23.278933 140161826506496 openvino_estimator.py:155] Processed 15001 examples in call_variants (using OpenVINO); I0116 03:14:42.118756 140161826506496 openvino_estimator.py:155] Processed 30001 examples in call_variants (using OpenVINO); I0116 03:15:00.622359 140161826506496 openvino_estimator.py:155] Processed 45001 examples in call_variants (using OpenVINO); ...; ```; My run complete",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:4323,upgrade,upgrade,4323,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['upgrade'],['upgrade']
Deployability,"Protobuf isn't installed on either host machine. yum list installed | grep proto -> only returns ""xorg-x11-proto-devel"". (I guess it should be included in the Docker container, no?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1012803228:15,install,installed,15,,https://github.com/google/deepvariant/issues/499#issuecomment-1012803228,2,['install'],['installed']
Deployability,Provider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor ',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:1774,Pipeline,Pipeline,1774,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['Pipeline'],['Pipeline']
Deployability,"Quick answer: Can you try `./build_release_binaries.sh` instead of `./build_and_test.sh` in your steps above?; Then it should work.; The issue you're encountering has nothing to do with Ubuntu 18. ----. More details:. Earlier today, @akolesnikov and I were just wondering why our internal tests didn't capture this.; We have daily tests that run scripts like this:; https://github.com/google/deepvariant/blob/r0.8/scripts/run_wes_case_study_binaries.sh. After checking what that script was doing, I found out that if you run `scripts/run_wes_case_study_binaries.sh`, it'll work on both Ubuntu 16 and 18. We'll fix build_and_test.sh in the next release. Thanks for reporting! ; If using `build_release_binaries.sh` still doesn't work for you, feel free to reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514472365:644,release,release,644,,https://github.com/google/deepvariant/issues/199#issuecomment-514472365,1,['release'],['release']
Deployability,"Quick update and FYI for you @dkurt ; I ran with the internal latest code (which we switched all metrics to be on HG003 BAMs). Here are the improvements with `--use_openvino=true`.; * wgs: 233m14.191s --> 204m35.065s; * wes: 1m41.381s --> 1m31.513s; * pacbio: 193m20.407s --> 169m45.878s; * hybrid_pacbio_illumina: 241m7.426s --> 189m40.148s. This was after your ""Process OpenVINO in thread (#8)"" change yesterday.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735940070:6,update,update,6,,https://github.com/google/deepvariant/pull/363#issuecomment-735940070,1,['update'],['update']
Deployability,"RIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m0.146s`. ```; $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json""; {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:2387,update,update,2387,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,2,"['install', 'update']","['install', 'update']"
Deployability,"RRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6924,Install,Installing,6924,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:347,release,released,347,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,2,"['pipeline', 'release']","['pipelines-worker-', 'released']"
Deployability,"Right, but which version of the Cuda Toolkit do you have? You can easily install the 9.0 from the link below, but I would do it as a local (non-sudo) user so you sandbox the changes to your environment in order to easily remove them later, if necessary:. https://developer.nvidia.com/cuda-90-download-archive. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102#issuecomment-429154858:73,install,install,73,,https://github.com/google/deepvariant/issues/102#issuecomment-429154858,1,['install'],['install']
Deployability,"Right, so now update your LD_LIBRARY_PATH with this version closer to the beginning of it. Make sure you echo it first to see what it's set to via `echo $LD_LIBRARY_PATH` as you might want to include those things as well. Study the following two links for more information:. https://www.tecmint.com/understanding-shared-libraries-in-linux/; https://docs.oracle.com/cd/E19455-01/816-0559/chapter2-48927/index.html. LD_LIBRARY_PATH is a string of colon-separated paths that a program will search (from left-to-right) through for the libraries it needs. You don't have to export it if you want to test things like this:. LD_LIBRARY_PATH=....(your paths)... python $HOME/miniconda3/envs/deepVar...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453800484:14,update,update,14,,https://github.com/google/deepvariant/issues/137#issuecomment-453800484,1,['update'],['update']
Deployability,"SE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; [bazel release 0.15.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages load",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:10155,install,install,10155,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Install', 'install']","['Installs', 'install']"
Deployability,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:; https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/107#issuecomment-430072409:549,pipeline,pipelines-on-noisy-wgs-data,549,,https://github.com/google/deepvariant/issues/107#issuecomment-430072409,1,['pipeline'],['pipelines-on-noisy-wgs-data']
Deployability,"Singularity recommends to install packages, programs, data, and files into operating system locations (e.g. not /home, /tmp , or any other directories that might get commonly binded on (like /root)). It would be great if the DeepVariant container would work nicely with Singularity in this way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-458240918:26,install,install,26,,https://github.com/google/deepvariant/issues/132#issuecomment-458240918,1,['install'],['install']
Deployability,"So from what I see you've completed the following steps:. 1) Built and installed CLIF; 2) Installed Bazel; 3) Installed the Tensorflow Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:71,install,installed,71,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104,3,"['Install', 'install']","['Installed', 'installed']"
Deployability,"So in `tools/build_build.clif` script, add the following on the [line right above](https://github.com/google/deepvariant/blob/r1.5/tools/build_clif.sh#L146) `./INSTALL.sh`:. ```; sed -i -e 's/LLVM 11.1.0/LLVM 11.0.0/g' clif/cmake/modules/CLIFUtils.cmake; ```. It should look like this:. ```; if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; sed -i -e 's/LLVM 11.1.0/LLVM 11.0.0/g' clif/cmake/modules/CLIFUtils.cmake; ./INSTALL.sh; ```; Then try it running `tools/build_build.clif` again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1576305703:160,INSTALL,INSTALL,160,,https://github.com/google/deepvariant/issues/657#issuecomment-1576305703,2,['INSTALL'],['INSTALL']
Deployability,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804#issuecomment-2113514063:151,release,release,151,,https://github.com/google/deepvariant/issues/804#issuecomment-2113514063,1,['release'],['release']
Deployability,"So, as @pichuan said, this is non-actionable for now and will have to wait until next release. So I'll close this bug.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844#issuecomment-2211196204:86,release,release,86,,https://github.com/google/deepvariant/issues/844#issuecomment-2211196204,1,['release'],['release']
Deployability,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632#issuecomment-1512478424:26,pipeline,pipeline,26,,https://github.com/google/deepvariant/issues/632#issuecomment-1512478424,2,"['Install', 'pipeline']","['Installing', 'pipeline']"
Deployability,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-504940567:16,install,install,16,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567,3,"['configurat', 'install']","['configuration', 'install', 'installed']"
Deployability,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-844185505:614,install,installed,614,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505,1,['install'],['installed']
Deployability,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:305,install,install,305,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725,1,['install'],['install']
Deployability,"Sorry for the delay.; I use DV both from the hos OS and from docker. My host OS is Ubuntu 18.04. And I have many other bioinformatic tools installed, so I can't use the current installation scripts. That's why I'm trying to create another build script, suitable for 18.04 and not conflicting with other python packages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-901422710:139,install,installed,139,,https://github.com/google/deepvariant/issues/476#issuecomment-901422710,2,['install'],"['installation', 'installed']"
Deployability,"Sorry to answer that late, I am extremely happy to read this and about the complexity of the problem. How many times have I heard ""just lowered the gap penalty"" or ""just increase the score for a mismatch"". While well-meaning, those didn't help me. I have arrived to a point when I am even doubting the validity of the whole field outside of human. Let's make an MA of Daphnia. Then let's ask another team to repeat exactly the experiment. Are you confident you will get the same kind of value? I realise I am absolutely not sure... Sorry I digress. But I am happy because you are a caller developer, which I am not, and you write exactly all the issues I have with the field. I think the complexity of variant calling is vastly underestimated. And many forget that it's not because a program keeps giving you the same answer, that it's correct.; Anyway on my side I also have a totally different approach in the pipeline and I would be very excited to compare with yours. Cheers",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/872#issuecomment-2364668453:912,pipeline,pipeline,912,,https://github.com/google/deepvariant/issues/872#issuecomment-2364668453,1,['pipeline'],['pipeline']
Deployability,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`.; ```; $ mamba search deepvariant; Name Version Build Channel; deepvariant 1.3.0 py36hf3e76ba_0 bioconda; deepvariant 1.4.0 py36hf3e76ba_0 bioconda; $ mamba install deepvariant; ```; When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site.; [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url); ```; $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/; $ls; call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh; call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip; deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598#issuecomment-1354787986:23,install,install,23,,https://github.com/google/deepvariant/issues/598#issuecomment-1354787986,2,['install'],['install']
Deployability,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372590216:38,install,installed,38,,https://github.com/google/deepvariant/issues/6#issuecomment-372590216,2,['install'],"['install', 'installed']"
Deployability,"Sounds good, thanks a lot @pichuan . Just an additional query. If I want to joint call SNVs/indels in a set of multiple trios, which pipeline is recommended? Deepvariant or DeepTrio. My understanding is that if we joint call multiple samples together, the cohort allele frequency could be used to filter rare variants. If we use DeepTrio, AF calculation will be restricted to only the trio or family. What is the best practice here in terms of Deepvariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/816#issuecomment-2097802994:133,pipeline,pipeline,133,,https://github.com/google/deepvariant/issues/816#issuecomment-2097802994,1,['pipeline'],['pipeline']
Deployability,"T_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances reset --zone us-west1-b pichuan-gpu2; ```. and then ssh back to the machine. ```; BIN_VERSION=1.5.0; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:2049,install,installing,2049,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355,1,['install'],['installing']
Deployability,"Tamer;; Thanks for the report and apologies about the install issue. This looks like you might be trying to install inside an miniconda environment running python 3. Is that a possibility? Right now, gsutil and deepvariant are only compatible with python 2.7 so using miniconda 2 (https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh) or a python=2 environment inside your existing conda install will hopefully resolve the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-452831730:54,install,install,54,,https://github.com/google/deepvariant/issues/137#issuecomment-452831730,3,['install'],['install']
Deployability,"Thank you - yes, I just tried in 0.7.1 and now am running into a new error. I have python and numpy installed and I am wondering how to make this accessible or if this is a Docker issue?; Thank you in advance,; Best,; ```; File ""/tmp/Bazel.runfiles_uDUZWS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 37, in <module>; import numpy as np; ImportError: No module named numpy; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-438484727:100,install,installed,100,,https://github.com/google/deepvariant/issues/104#issuecomment-438484727,1,['install'],['installed']
Deployability,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-763821009:173,update,updated,173,,https://github.com/google/deepvariant/issues/404#issuecomment-763821009,2,['update'],"['update', 'updated']"
Deployability,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828:600,update,updated,600,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828,1,['update'],['updated']
Deployability,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. Version:. ```; $ uname -a; Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. Install conda:. ```bash; curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda; eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. It completed without any error messages. I see:. ```; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/; bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0; call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh; deeptr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405:86,install,install,86,,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405,2,"['Install', 'install']","['Install', 'install']"
Deployability,"Thank you @edg1983 . I will plan to add this section to our FAQ.md:. ---. ## Singularity related questions:. ### `TMPDIR`. If your run with Singularity is having issues with `TMPDIR`, try adding this to your command:; ```bash; export TMPDIR=""$PWD/tmp_dir""; ```. See https://github.com/google/deepvariant/issues/524#issuecomment-1067597987. ---. This should show up in our next release. Thanks for providing this information!; If you have more suggestions, let me know. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/524#issuecomment-1079508369:377,release,release,377,,https://github.com/google/deepvariant/issues/524#issuecomment-1079508369,1,['release'],['release']
Deployability,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release.; After our next release, your VCF output will have a line that looks like this:; ```; ##DeepVariant_version=1.0.0; ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:; ```; docker run google/deepvariant:""${BIN_VERSION}"" --version; ```; to output the current version?. I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/332#issuecomment-669330605:135,release,release,135,,https://github.com/google/deepvariant/issues/332#issuecomment-669330605,2,['release'],['release']
Deployability,"Thank you Andrew, I'm looking forward to reading. > 19, at 19:17, Andrew Carroll <notifications@github.com> wrote:; > ; > ﻿; > Hi @andrewrech and @shalabhsuman; > ; > I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend Best practices for multi-sample variant calling with DeepVariant; > ; > Although this case study is a trio, we have optimized parameters for cohorts scaling into the 1000's, so we feel this will work well for your use cases.; > ; > Thank you.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-553890845:179,update,update,179,,https://github.com/google/deepvariant/issues/142#issuecomment-553890845,2,"['release', 'update']","['release', 'update']"
Deployability,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480616982:645,install,installed,645,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982,1,['install'],['installed']
Deployability,Thank you also to @pgrosu for taking a look. I think the next steps for me are:; 1 -- disassemble the BAM file and re-align using BWA-MEM to the Verily Genome; 2 -- Re run the pipeline with the GRCh38 aligned file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437359605:176,pipeline,pipeline,176,,https://github.com/google/deepvariant/issues/116#issuecomment-437359605,1,['pipeline'],['pipeline']
Deployability,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:312,pipeline,pipeline,312,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017,5,"['pipeline', 'release']","['pipeline', 'pipelines', 'release']"
Deployability,"Thank you for answer.; We are using Ubuntu 14.04 for our images (yes, still).; ```; # uname -a; Linux 417d805a5037 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; ```; ```; root@417d805a5037:/outputs# lsb_release -a; No LSB modules are available. ; Distributor ID: Ubuntu ; Description: Ubuntu 14.04.6 LTS ; Release: 14.04 ; Codename: trusty ; ```; ```; root@417d805a5037:/outputs# gcc --version ; gcc (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4 ; Copyright (C) 2013 Free Software Foundation, Inc. ; This is free software; see the source for copying conditions. There is NO ; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```. Maybe we need newer `gcc`?. And we don't clone repos - we download release archives.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236#issuecomment-557117908:355,Release,Release,355,,https://github.com/google/deepvariant/issues/236#issuecomment-557117908,2,"['Release', 'release']","['Release', 'release']"
Deployability,"Thank you for reporting the issue.; Can you tell us more about what environment you're building it in? (OS version, gcc version); I tried build_and_test.sh just now, and didn't see any issue. I'd like to reproduce the error so we can help fix it for your setting. For example, here is my version:; ```; pichuan@pichuan-build:~$ uname -a; Linux pichuan-build 4.15.0-1049-gcp #52-Ubuntu SMP Fri Nov 8 10:30:54 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; pichuan@pichuan-build:~/deepvariant$ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.6 LTS; Release: 16.04; Codename: xenial; ```. Then I clone our repo:; ```; pichuan@pichuan-build:~$ git clone https://github.com/google/deepvariant.git; ```; Confirmed it's on r0.9:; ```; pichuan@pichuan-build:~$ cd deepvariant/; pichuan@pichuan-build:~/deepvariant$ git branch; * r0.9; ```; And then I build:; ```; pichuan@pichuan-build:~/deepvariant$ ./build-prereq.sh && ./build_and_test.sh ; ```; This completed without an error. I checked my gcc version:; ```; pichuan@pichuan-build:~/deepvariant$ gcc --version; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609; Copyright (C) 2015 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236#issuecomment-557101397:590,Release,Release,590,,https://github.com/google/deepvariant/issues/236#issuecomment-557101397,1,['Release'],['Release']
Deployability,"Thank you for saying these words, @pichuan. I believe we are at the very last steps of getting the DeepVariant running on Apple silicon, but since the singularity on linux system works for us, I think I will not pursue this installation anymore.; I am grateful to @pgrosu who has been helping me with these 2 options. ; Perhaps this thread could help others who want to pursue the same pathway. Regards,. Hez",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1622187258:224,install,installation,224,,https://github.com/google/deepvariant/issues/657#issuecomment-1622187258,1,['install'],['installation']
Deployability,"Thank you for the reply. I am running it on a linux server instead of using GCE. . The ""sec per 100"" is ""2.68 sec per 100"". My machine has 16Gb RAM and DeepVariant added 1-2 Gb RAM usage and total RAM usage peaks at 5Gb with 8 cores fully utilized. The data file, make_examples and call_variants result are all written to local storage instead of network storage. . I was trying to get a timing profile and check if the program runs as expected. One thing I am curious is that the time taken to run make_examples matches the report time closely, however, the call_variants is much slower. I wonder if there is any problem in setting or configuration as the reported time of call_variants is much faster.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391321427:636,configurat,configuration,636,,https://github.com/google/deepvariant/issues/74#issuecomment-391321427,1,['configurat'],['configuration']
Deployability,"Thank you for the response ; I reordered the conda channels to match yours and installed gsutil but using conda and thus I did not need to edit the path. This is the set of commands I used:. ```; conda create -n deepvariant python=2.7; source activate deepvariant; conda install -c conda-forge google-cloud-sdk; conda install -v -y deepvariant &> deepvariant_insatll.log; ```. I got a successful installation inspite of the first error message just like you; However, running the code is producing another error:. ```; python $HOME/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/binaries/DeepVariant/0.7.2/DeepVariant-0.7.2+cl-225213413/make_examples.zip \; --mode training --reads ""${BAM}"" --ref ""${REF}"" --examples ""$training.tfrecord.gz"" \; --truth_variants ""${TRUTH_VCF}"" --confident_regions ""${TRUTH_BED}"" \; --exclude_regions ""chr20:14000000-15000000"" --sample_name ""train"" ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4i44qy/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; import tensorflow as tf; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_intern",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453685106:79,install,installed,79,,https://github.com/google/deepvariant/issues/137#issuecomment-453685106,4,['install'],"['install', 'installation', 'installed']"
Deployability,"Thank you for this response dear @kishwarshafin,; Will be looking forward for the coming release.; Cheers",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/875#issuecomment-2322280632:89,release,release,89,,https://github.com/google/deepvariant/issues/875#issuecomment-2322280632,1,['release'],['release']
Deployability,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-568717631:118,update,updates,118,,https://github.com/google/deepvariant/issues/252#issuecomment-568717631,1,['update'],['updates']
Deployability,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. However, when I try to run it I get the following error:; ```; python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>; import tensorflow as tf; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>; from google.protobuf import descriptor as _descriptor; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>; from google.protobuf.pyext import _message; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so); ```; I've tried to install an updated version of the library using ; ```conda install librosa```; but it didn't work. Any suggestion?. Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566564983:259,install,installed,259,,https://github.com/google/deepvariant/issues/252#issuecomment-566564983,4,"['install', 'update']","['install', 'installed', 'updated']"
Deployability,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; I'm not sure I've been able to find it. . Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566566093:259,install,installed,259,,https://github.com/google/deepvariant/issues/252#issuecomment-566566093,2,['install'],['installed']
Deployability,"Thank you for your reply!; 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:199,install,install,199,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260,2,['install'],['install']
Deployability,"Thank you very much for your prompt response, particularly on the weekend!. I can see that I overlooked the line `MODEL=""${HOME}/${MODEL_NAME}/model.ckpt""` in this [quick-start](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) guide. I apologize about that. So, if I use `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model.ckpt`, then that works!. If I only put the prefix with ""model,"" I get another error mentioning a checkpoint, but I am assuming that is what the .ckpt extension stands for. Thank you again. **Update (4/1)**: I just remembered that I am supposed to close the issues on GitHub, which is what I did :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166#issuecomment-478414504:582,Update,Update,582,,https://github.com/google/deepvariant/issues/166#issuecomment-478414504,1,['Update'],['Update']
Deployability,"Thank you very much. That will help out the folks new to docker/singularity. After using the -B option, we are good to go now!. From: Pi-Chuan Chang ***@***.***>; Sent: Friday, March 25, 2022 7:19 PM; To: google/deepvariant ***@***.***>; Cc: Jason Phillips ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] Image /mnt overriding my machine's /mnt causing errors (Issue #530). Hi @japhill<https://github.com/japhill> , to give you an update, I plan to add this section to our FAQ in the next release:. ________________________________; Issues with /mnt/. User reported that sometimes their setup uses /mnt/, which exists in our Docker image, and it has caused an issue in Singularity. You can use -B in Singularity to avoid this issue. See:; #530 (comment)<https://github.com/google/deepvariant/issues/530#issuecomment-1076923302> for more details. ________________________________. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/530#issuecomment-1079509951>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAS6GHJOPUE7DQPYAT264TVBZCWLANCNFSM5Q7A6FXQ>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356:453,update,update,453,,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356,2,"['release', 'update']","['release', 'update']"
Deployability,Thank you! Yes - the newest release 0.6.0 has fixed this issue and my truth VCF now works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/64#issuecomment-380632313:28,release,release,28,,https://github.com/google/deepvariant/issues/64#issuecomment-380632313,1,['release'],['release']
Deployability,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-728645413:398,release,release,398,,https://github.com/google/deepvariant/issues/381#issuecomment-728645413,1,['release'],['release']
Deployability,"Thank you, this will be updated in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/897#issuecomment-2427339601:24,update,updated,24,,https://github.com/google/deepvariant/issues/897#issuecomment-2427339601,2,"['release', 'update']","['release', 'updated']"
Deployability,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-727285166:227,release,released,227,,https://github.com/google/deepvariant/issues/381#issuecomment-727285166,1,['release'],['released']
Deployability,"Thank's for the answer, I saw it, but all servers in the genomic research center I work are on CentOS-7 and Ubuntu is not an option. So I can't use Ubuntu, and of course, no way to send human genomic data on the cloud. Maybe a more generic installation procedure, not stick on ubuntu, could be a good idea, at least for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372381243:240,install,installation,240,,https://github.com/google/deepvariant/issues/6#issuecomment-372381243,1,['install'],['installation']
Deployability,"Thanks @ASLeonard , a few follow ups:; 1. I updated https://github.com/google/deepvariant/releases/tag/v1.4.0 to mention that this time we didn't build in OpenVINO by default.; 2. Thanks for the `call_variants` observation. Now I wonder how much of the RAM difference is OpenVINO and how much of it is TensorFlow version updates...; 3. Great to hear that you're also seeing good postprocess_variants improvements! Our 20%er @MosheWagner worked really hard on this, so I'm sure he'll be happy to hear that too!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541#issuecomment-1156752742:44,update,updated,44,,https://github.com/google/deepvariant/issues/541#issuecomment-1156752742,3,"['release', 'update']","['releases', 'updated', 'updates']"
Deployability,"Thanks @ASLeonard . Can you tell me:; 1. Which Singularity version do you have. (and OS version too); 2. What command did you run?. I'd still like to reproduce the issue if possible on my side. If I can get a setting as close as yours, I might be able to reproduce the issue. And @dkurt thanks for taking a look.; I also think it'll be better to write to the intermediate output dir rather than the current working directory. I'll work on an internal fix. I'll update here when I get it to work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761609874:461,update,update,461,,https://github.com/google/deepvariant/issues/404#issuecomment-761609874,1,['update'],['update']
Deployability,"Thanks @ASLeonard for the pointer on the timeline for Python 3.6 and 3.8. I'll take a look to update the Python version.; @yanyang1989 thanks for sharing your use case! We'll look into updating to Python 3.8 this but it will take a while. If you can try pulling our image directly like @aliceseaborn , that might work for you now?. @aliceseaborn My experience is that `google/deepvariant:1.1.0` and `google/deepvariant:1.1.0-gpu` work with no issues directly on Ubuntu 18.04 machines. Are you having any issues when you pull our images?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-821524931:94,update,update,94,,https://github.com/google/deepvariant/issues/441#issuecomment-821524931,1,['update'],['update']
Deployability,"Thanks @DLPerf .; Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479#issuecomment-903990739:151,update,update,151,,https://github.com/google/deepvariant/issues/479#issuecomment-903990739,1,['update'],['update']
Deployability,"Thanks @Stikus , I noticed this and was just looking at it!. I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:; ```; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; ```. and then , right in front of the line of `./INSTALL.sh` , add this line:; ```; sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake; ```; Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489#issuecomment-940166600:438,INSTALL,INSTALL,438,,https://github.com/google/deepvariant/issues/489#issuecomment-940166600,2,"['INSTALL', 'install']","['INSTALL', 'installation']"
Deployability,Thanks @Stikus . Glad it's working. Internally I've made another change similar to https://github.com/google/clif/commit/83c7941aae9f01a575adc719b1baed3c6607b8f4 which should work for 18.04. I haven't pushed that to GitHub but will hopefully be updated to our r1.2 as well.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489#issuecomment-943865449:245,update,updated,245,,https://github.com/google/deepvariant/issues/489#issuecomment-943865449,1,['update'],['updated']
Deployability,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-964833815:28,update,update,28,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815,3,"['release', 'update']","['release', 'update']"
Deployability,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:108,upgrade,upgraded,108,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705,1,['upgrade'],['upgraded']
Deployability,"Thanks @akolesnikov for responding! . I'm running Python `Python 2.7.15+`. This was on a fresh Ubuntu 14.4 install, using your default settings in DeepVariant v0.8/master and running `./build-prereq.sh` followed by `./build_and_test.sh`. What I'm confused by is that `bazel-bin/deepvariant/make_examples_test` runs and everything passes... ```; $ bazel-bin/deepvariant/make_examples_test; Running tests under Python 2.7.15: /usr/bin/python; ...; .; ----------------------------------------------------------------------; Ran 101 tests in 6.501s. OK; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514034399:107,install,install,107,,https://github.com/google/deepvariant/issues/199#issuecomment-514034399,1,['install'],['install']
Deployability,"Thanks @carsonhh .; I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works!. I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/789#issuecomment-2007754121:58,release,release,58,,https://github.com/google/deepvariant/issues/789#issuecomment-2007754121,3,"['release', 'update']","['release', 'updated']"
Deployability,Thanks @chapmanb so much for you help. And Thanks @pjedge for your update. ; I'm closing this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486826886:67,update,update,67,,https://github.com/google/deepvariant/issues/177#issuecomment-486826886,1,['update'],['update']
Deployability,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc.; But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad.; Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-461253040:24,update,update,24,,https://github.com/google/deepvariant/issues/150#issuecomment-461253040,1,['update'],['update']
Deployability,Thanks @jguhlin . In the next release we'll plan to include the converted version in the same GCS bucket.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-565122777:30,release,release,30,,https://github.com/google/deepvariant/issues/243#issuecomment-565122777,1,['release'],['release']
Deployability,Thanks @jjfarrell. We've added a bug and will fix for the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/37#issuecomment-357087121:63,release,release,63,,https://github.com/google/deepvariant/issues/37#issuecomment-357087121,1,['release'],['release']
Deployability,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413#issuecomment-1480069130:130,pipeline,pipeline,130,,https://github.com/google/deepvariant/issues/413#issuecomment-1480069130,1,['pipeline'],['pipeline']
Deployability,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30#issuecomment-355031587:146,install,installing,146,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587,2,['install'],"['install', 'installing']"
Deployability,"Thanks @marchoeppner . Good to hear that 1.2 worked.; I believe @akolesnikov 's recent investigation showed that the accounting was actually correct. So, what he mentioned in https://github.com/google/deepvariant/issues/470#issuecomment-880951168 wasn't really a problem like he thought before. And he thinks the improvement needs to come from the classifier. Given that this was WES, I suspect this improvement we mentioned in [v1.2.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.2.0) might be relevant to why 1.2 WES model works better on this:. > * In the ""training"" model for make_examples, we committed (https://github.com/google/deepvariant/commit/4a11046de0ad86e36d2514af9f035c9cb34414bf) that fixed an issue introduced in an earlier commit (https://github.com/google/deepvariant/commit/a4a654769f1454ea487ebf0a32d45a9f8779617b) where make_examples might generate fewer REF (class0) examples than expected. I'll close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470#issuecomment-892811998:436,release,release,436,,https://github.com/google/deepvariant/issues/470#issuecomment-892811998,2,['release'],"['release', 'releases']"
Deployability,Thanks @mattwood-codifiedgenomics . I'll take a look at this before the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-870966227:77,release,release,77,,https://github.com/google/deepvariant/issues/469#issuecomment-870966227,1,['release'],['release']
Deployability,Thanks @nikostr for reporting this.; The zip file for 1.1.0 should be avaiable at https://github.com/google/deepvariant/releases/download/v1.1.0/deepvariant.zip now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/436#issuecomment-814505837:120,release,releases,120,,https://github.com/google/deepvariant/issues/436#issuecomment-814505837,1,['release'],['releases']
Deployability,"Thanks @pgrosu for the suggestion, and @gambalab for confirming it works. I don't think our team has tried `udocker` before. So it's good to know about this. I'll add an internal task to try this out and consider adding a pointer to it in the future release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1604566408:250,release,release,250,,https://github.com/google/deepvariant/issues/669#issuecomment-1604566408,1,['release'],['release']
Deployability,"Thanks @pichuan , I will wait for the Singularity image then.; If Singularity works then I will most likely not need the bioconda install.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-480875241:130,install,install,130,,https://github.com/google/deepvariant/issues/137#issuecomment-480875241,1,['install'],['install']
Deployability,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:; ```; >>> import mock; >>>; >>> expected_start=9; >>> expected_end=21; >>> bufsize=0; >>> expected_bases = 'A' * (expected_end - expected_start); >>>; >>> start=10; >>> end=21; >>> contig='20'; >>> ref_reader = mock.MagicMock(); >>> ref_reader.query.return_value = expected_bases; >>> contig_nbp = ref_reader.contig(contig).n_bases; >>> res = min(end + bufsize, contig_nbp); >>> res; 21; ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```; >>> import mock; >>>; >>> expected_start=9; >>> expected_end=21; >>> bufsize=0; >>> expected_bases = 'A' * (expected_end - expected_start); >>>; >>> start=10; >>> end=21; >>> contig='20'; >>> ref_reader = mock.MagicMock(); >>> ref_reader.query.return_value = expected_bases; >>> contig_nbp = ref_reader.contig(contig).n_bases; >>> res = min(end + bufsize, contig_nbp); >>> res; <MagicMock name='mock.contig().n_bases' id='140373647899088'>; >>> int(res); 1; ```. And I accidentally run the above command with Python3, and it throw the following error:. ```; >>> res = min(end + bufsize, contig_nbp); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; TypeError: '<' not supported between instances of 'MagicMock' and 'int'; ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464961152:1542,upgrade,upgrade,1542,,https://github.com/google/deepvariant/issues/154#issuecomment-464961152,1,['upgrade'],['upgrade']
Deployability,Thanks @ptrebert .; I added this as a future improvement that we'll work on and include in the next release. I'll leave this open.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/242#issuecomment-561296792:100,release,release,100,,https://github.com/google/deepvariant/issues/242#issuecomment-561296792,1,['release'],['release']
Deployability,Thanks Andrew! We've put this on our internal buganizer component for DeepVariant and we'll update the case study to use this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/15#issuecomment-351203489:92,update,update,92,,https://github.com/google/deepvariant/issues/15#issuecomment-351203489,1,['update'],['update']
Deployability,"Thanks Cory. I will fix that and re-run. I shall update you if that resolves the issue. Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/86#issuecomment-411146581:49,update,update,49,,https://github.com/google/deepvariant/issues/86#issuecomment-411146581,1,['update'],['update']
Deployability,"Thanks Michael, I'd love to know what conclusions about how the accuracy; and background noise detection you have on the TB genome. If it is not too; much trouble, can you keep me posted? Or you have some blog/post or paper I; can follow up on?. You guys are totally rocking it!! Very exciting study!!. On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant; > to call SNPs in bacterial genomes. For our current pipeline we are using; > BWA to map reads to the reference and then PILON from the Broad to call; > variants. Specifically we are working on the Tuberculosis genome.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-437155211:506,pipeline,pipeline,506,,https://github.com/google/deepvariant/issues/114#issuecomment-437155211,1,['pipeline'],['pipeline']
Deployability,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/57#issuecomment-371628818:139,release,release,139,,https://github.com/google/deepvariant/pull/57#issuecomment-371628818,1,['release'],['release']
Deployability,"Thanks a lot @pichuan, I am working on this - apparently my OpenVino installation wasn't good. Your answer was very helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/432#issuecomment-810913512:69,install,installation,69,,https://github.com/google/deepvariant/issues/432#issuecomment-810913512,1,['install'],['installation']
Deployability,"Thanks a lot, both of you, for your reply. I will for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/467#issuecomment-871094311:63,release,release,63,,https://github.com/google/deepvariant/issues/467#issuecomment-871094311,1,['release'],['release']
Deployability,Thanks a lot. We want to support both to run many patients as well as single ones within a pipeline.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/510#issuecomment-1029076026:91,pipeline,pipeline,91,,https://github.com/google/deepvariant/issues/510#issuecomment-1029076026,1,['pipeline'],['pipeline']
Deployability,Thanks for bringing this to our attention!; I'm running our tests with `nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04` now and will update here when I have confirmed whether it's working.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/676#issuecomment-1631277506:130,update,update,130,,https://github.com/google/deepvariant/issues/676#issuecomment-1631277506,1,['update'],['update']
Deployability,"Thanks for bringing this up Brendan! I have added the white background to the diagram internally, so it will be fixed on GitHub in the next release. If this is annoying to more people, I could cherry-pick it to fix it sooner. Leave the pair-of-eyes emoji here to vote for us to prioritize this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/423#issuecomment-777731334:140,release,release,140,,https://github.com/google/deepvariant/issues/423#issuecomment-777731334,1,['release'],['release']
Deployability,"Thanks for clarifying! Hopefully this can be addressed soon. The particular example I have shown here actually has fairly severe consequences for interpretation (MNP = stop gained). So this is not a minor issue. Closing this for now, but will keep an eye on the update notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520#issuecomment-1048481061:262,update,update,262,,https://github.com/google/deepvariant/issues/520#issuecomment-1048481061,1,['update'],['update']
Deployability,"Thanks for clarifying. I was just curious, because when I used GATK to call germline variants, it gave me 10x less, so I just was curious about the large discrepancy. But, perhaps this is more of a GATK pipeline issue. So, the ""PASS"" variants should be the final filtered germline variants?. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/300#issuecomment-617891031:203,pipeline,pipeline,203,,https://github.com/google/deepvariant/issues/300#issuecomment-617891031,1,['pipeline'],['pipeline']
Deployability,"Thanks for digging into this - yea I guess that makes sense. Will be waiting for a fix in future releases then , thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470#issuecomment-881195025:97,release,releases,97,,https://github.com/google/deepvariant/issues/470#issuecomment-881195025,1,['release'],['releases']
Deployability,Thanks for letting us know. I've made a note of this to look into it for the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511#issuecomment-1021613047:82,release,release,82,,https://github.com/google/deepvariant/issues/511#issuecomment-1021613047,1,['release'],['release']
Deployability,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314#issuecomment-638091561:478,install,install,478,,https://github.com/google/deepvariant/issues/314#issuecomment-638091561,1,['install'],['install']
Deployability,"Thanks for the fix!; Because the way our repo is set up, I have to make this fix in our internal codebase. I have made the fix internally now and it'll come out next time we make another release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/187#issuecomment-503755200:187,release,release,187,,https://github.com/google/deepvariant/pull/187#issuecomment-503755200,1,['release'],['release']
Deployability,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461398822:1354,pipeline,pipeline,1354,,https://github.com/google/deepvariant/issues/151#issuecomment-461398822,1,['pipeline'],['pipeline']
Deployability,Thanks for the pull request. I've updated the code internally and it'll go out with the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/97#issuecomment-424431766:34,update,updated,34,,https://github.com/google/deepvariant/pull/97#issuecomment-424431766,2,"['release', 'update']","['release', 'updated']"
Deployability,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/472#issuecomment-879985843:34,update,updated,34,,https://github.com/google/deepvariant/pull/472#issuecomment-879985843,1,['update'],['updated']
Deployability,Thanks for the question!; I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1533748996:315,install,install,315,,https://github.com/google/deepvariant/issues/640#issuecomment-1533748996,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Thanks for the question!; We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data.; See https://github.com/google/deepvariant/releases/tag/v1.5.0 ; ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641#issuecomment-1535452815:149,release,releases,149,,https://github.com/google/deepvariant/issues/641#issuecomment-1535452815,1,['release'],['releases']
Deployability,"Thanks for the question. I'm agreed with Pi-Chuan, trying in a python 2.7 conda environment should hopefully resolve these issues. Hope that gets the install figured out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566199242:150,install,install,150,,https://github.com/google/deepvariant/issues/252#issuecomment-566199242,1,['install'],['install']
Deployability,"Thanks for the quick response. ; I checked my bam file and see only one sample name in the header as shown below. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 ./references/bwa_index/RHA.fa ./SNP_Control_Fastq/SNP-Cnt-5p_S6_L001_R1_001.fastq.gz ./SNP_Control_Fastq/SNP-Cnt-5p_S6_L001_R2_001.fastq.gz. I also check the content of the bam file and all records contain the same sample name. > samtools view SNP-Cnt-5p_S6_L001_001_paired.bam | cut -f 3 | sort | uniq -c; 418776 RHA. Do you know when the next release is out? I have to complete my analysis within a week. I guess for now I should figure out how to call the three steps separately. . Thanks again, ; Azita",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-535373169:564,release,release,564,,https://github.com/google/deepvariant/issues/222#issuecomment-535373169,1,['release'],['release']
Deployability,"Thanks for the report. Unfortunately we can't currently take pull request from GitHub. ; This include is fixed in an internal version, but the 0.6 release was cut before that change. I will plan to create a 0.6.1 to include this fix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/68#issuecomment-384434821:147,release,release,147,,https://github.com/google/deepvariant/pull/68#issuecomment-384434821,1,['release'],['release']
Deployability,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. ; Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; Here is the command to execute deepvariant:; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); and here is the new errors I'm seeing:; ValueError: Failed precondition: Cannot query without an index; parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples; --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/307#issuecomment-628230170:175,update,updated,175,,https://github.com/google/deepvariant/issues/307#issuecomment-628230170,2,['update'],['updated']
Deployability,"Thanks for the suggestion @arostamianfar . Sounds like a good suggestion and should be easy to do : adding a flag to postprocess_variants. If `sample_name` is specified for run_deepvariant.py, we'll use it for both. . I'll file an internal issue to track this, and we should be able to have this in our next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/334#issuecomment-673773009:308,release,release,308,,https://github.com/google/deepvariant/issues/334#issuecomment-673773009,1,['release'],['release']
Deployability,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory; ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717#issuecomment-1771016203:15,update,update,15,,https://github.com/google/deepvariant/issues/717#issuecomment-1771016203,2,['update'],['update']
Deployability,Thanks for the update @ivanwilliammd !,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632#issuecomment-1518194290:15,update,update,15,,https://github.com/google/deepvariant/issues/632#issuecomment-1518194290,1,['update'],['update']
Deployability,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394#issuecomment-742700347:15,update,update,15,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347,5,"['Install', 'install', 'release', 'update']","['Installing', 'install', 'release', 'update']"
Deployability,"Thanks for the update, @sgoe1. Just wanted to make sure I wasn't interpreting the help page incorrectly. I went ahead and did as you recommended. As for why it's mostly for convenience. I'm wanting to contrast the per-sample output differences across GATK, DeepVariant, and DeepTrio. The fewer method-specific flags required, the cleaner my pipeline for running either (or both) across trios.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429#issuecomment-797059169:15,update,update,15,,https://github.com/google/deepvariant/issues/429#issuecomment-797059169,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,Thanks for the update. Good to know that it works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/847#issuecomment-2250711517:15,update,update,15,,https://github.com/google/deepvariant/issues/847#issuecomment-2250711517,1,['update'],['update']
Deployability,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624#issuecomment-1499324669:15,update,update,15,,https://github.com/google/deepvariant/issues/624#issuecomment-1499324669,1,['update'],['update']
Deployability,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase.; Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-477842467:15,update,updates,15,,https://github.com/google/deepvariant/pull/152#issuecomment-477842467,2,['update'],"['update', 'updates']"
Deployability,"Thanks for the updates. It looks like you are building a customized docker image. Before we can debug that, could you please confirm whether you can:. - Run quickstart on your machine (without using docker)? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md; - Run DeepVariant using our provided docker image from gcr.io/deepvariant-docker/deepvariant:0.4.1? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-351757223:15,update,updates,15,,https://github.com/google/deepvariant/issues/16#issuecomment-351757223,1,['update'],['updates']
Deployability,"Thanks for your interest in trying out training!. Currently, DeepVariant is trained using a combination of public and non-public data, as described in the release notes for each version. For public data sources, like Genome in a Bottle or BaseSpace, we encourage you to download the data directly from the original source. . I'm currently working on a list of the public data sources that we're using. We'll plan to release that information in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/80#issuecomment-401673684:155,release,release,155,,https://github.com/google/deepvariant/issues/80#issuecomment-401673684,3,['release'],['release']
Deployability,"Thanks for your response. I have a sample with ultra long ONT data and PacBio hifi data. I am trying to figure out the best way to get a final phased vcf and phased bam from the deepvariant/margin pipeline. I can get a phased bam and vcf for the ONT data and a phased bam and vcf for the hifi data, but then there is no easy way to combine these into a single phased vcf and bam file. I would be very grateful if you were to consider a hybrid model in the future. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/729#issuecomment-1808749941:197,pipeline,pipeline,197,,https://github.com/google/deepvariant/issues/729#issuecomment-1808749941,1,['pipeline'],['pipeline']
Deployability,"Thanks paul for the detailed analysis and explanation of how you went about it. To confirm this analysis I tried out by changing TF_WHL_VERSION and other related symbols to 1.10.1 in settings.sh so that build-prereq.sh installs the matching tensorflow. Now, the build and test passes and I am able to run variant calling. I presume this is a good workaround to address this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-423268455:219,install,installs,219,,https://github.com/google/deepvariant/issues/94#issuecomment-423268455,1,['install'],['installs']
Deployability,"Thanks you!. An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-960053107:16,update,update,16,,https://github.com/google/deepvariant/issues/491#issuecomment-960053107,1,['update'],['update']
Deployability,"Thanks! I don't know if I agree that a lifted over variant set would not provide value - after all, even if the variants themselves are the same, we'd expect there to be fewer pileup regions due to differences in how off-target reads align. Even if you're right, there's only one way to know for sure - try it out and see!. That being said, your reasoning makes sense. I'll keep an eye out for any updates. PS - I wonder if someone has begun working with the HG002 Q100 assembly? That can be aligned to GRCh38 or T2T reference and, in theory, used as a ground truth variant set for HG002 reads. (Although, I believe that Zook is still evaluating the viability of that option.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534#issuecomment-2263721491:398,update,updates,398,,https://github.com/google/deepvariant/issues/534#issuecomment-2263721491,1,['update'],['updates']
Deployability,"Thanks!. בתאריך יום ב׳, 21 בנוב׳ 2022, 23:50, מאת Pi-Chuan Chang ‏<; ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I; > recommend that you try Singularity:; > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity; >; > If you don't have root permission, you won't be able to install necessary; > things before running the binaries either.; > ------------------------------; >; > Here is what I did:; >; > Get a machine. (Not required to run on GCP. I just use this to get a; > machine to test); >; > gcloud compute instances create ""${USER}-cpu"" --scopes; > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts""; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:365,install,install,365,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566,1,['install'],['install']
Deployability,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:; https://github.com/ink1/deepvariant/releases/tag/v0.5.2a; The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372647046:335,release,releases,335,,https://github.com/google/deepvariant/issues/6#issuecomment-372647046,2,['release'],"['release', 'releases']"
Deployability,"That paper is an early version of the code - so some things changed and some didn't. So let's parse this out:. 1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the genera",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512112524:866,update,update,866,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524,1,['update'],['update']
Deployability,That works for me (Ubuntu 18.04.1-based OS with Docker 19.03.6): https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/335#issuecomment-674188410:96,install,install,96,,https://github.com/google/deepvariant/issues/335#issuecomment-674188410,1,['install'],['install']
Deployability,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/324#issuecomment-658908897:308,release,release,308,,https://github.com/google/deepvariant/issues/324#issuecomment-658908897,1,['release'],['release']
Deployability,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:632,install,install,632,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377,8,"['configurat', 'install', 'release']","['configurations', 'install', 'installation', 'installer-linux-', 'release', 'releases']"
Deployability,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```; paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/; CommandException: No URLs matched: /not/there; CommandException: 1 file/object could not be transferred.; paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/; CommandException: No URLs matched: /not/there; CommandException: 1 file/object could not be transferred. ```. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437180541:36,pipeline,pipeline,36,,https://github.com/google/deepvariant/issues/116#issuecomment-437180541,1,['pipeline'],['pipeline']
Deployability,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355032456:324,pipeline,pipeline,324,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456,1,['pipeline'],['pipeline']
Deployability,The change is in internally and will appear in the next OSS release. Thanks again!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/4#issuecomment-349500262:60,release,release,60,,https://github.com/google/deepvariant/pull/4#issuecomment-349500262,1,['release'],['release']
Deployability,"The code for the current, released version of DeepVariant does not use PairHMM to score haplotypes. Since the submission of the DeepVariant manuscript, there have been 4 releases which have improved various aspects of the code, training regime, and training data for models. The DeepVariant paper does validly describe the methods used in a working version, both the original PrecisionFDA submission and the improvements made for the first open source release (v0.4). However, there are further improvements which are not captured in that publication, and which are instead represented either in other joint publications (e.g. https://www.biorxiv.org/content/10.1101/519025v2) or in blogs produced by our team or close partners (e.g. https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html), (e.g. https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488762439:26,release,released,26,,https://github.com/google/deepvariant/issues/180#issuecomment-488762439,3,['release'],"['release', 'released', 'releases']"
Deployability,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```; for bam in $READS; do; 	echo ""running deepvariant on $bam""; 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES; 	echo ""finished with $bam""; done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam; # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****; # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946:16,install,installing,16,,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946,5,"['install', 'update']","['installing', 'update']"
Deployability,The documentation has been updated and will be included in our next release. Thank you very much for the suggestion!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/795#issuecomment-2033045923:27,update,updated,27,,https://github.com/google/deepvariant/issues/795#issuecomment-2033045923,2,"['release', 'update']","['release', 'updated']"
Deployability,The fix for the issue with the haplotype_labeler is ready in the internal codebase and will appear in the next DeepVariant release. Thanks for working through these issues with us.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71#issuecomment-388507714:123,release,release,123,,https://github.com/google/deepvariant/issues/71#issuecomment-388507714,1,['release'],['release']
Deployability,"The fix is done, and will be included in the next release. . For the ref: Htslib is not able to read from private gcs bucket and there is a TODO for it. . https://github.com/samtools/htslib/blob/9cd85ab512a74fe42af31fc6321c70203d161ed6/hfile_gcs.c#L75",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/119#issuecomment-441712357:50,release,release,50,,https://github.com/google/deepvariant/issues/119#issuecomment-441712357,1,['release'],['release']
Deployability,The functionality has been implemented in the internal codebase and will be available in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/334#issuecomment-675909928:98,release,release,98,,https://github.com/google/deepvariant/issues/334#issuecomment-675909928,1,['release'],['release']
Deployability,"The images all have to be the same size for the model, so they are standardized to a height of 100 (5 rows for reference + 95 reads). Standardization like this is common in machine learning. Coverage does actually get that high for many of our training datasets when we use the full coverage without downsampling.; By the way, where did you find that link? It's a pretty old version of the notebook. We have since updated it here: https://github.com/google/deepvariant/blob/r1.3/docs/visualizing_examples.ipynb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811:414,update,updated,414,,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811,1,['update'],['updated']
Deployability,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,; Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-751141693:319,install,install,319,,https://github.com/google/deepvariant/issues/399#issuecomment-751141693,3,"['deploy', 'install', 'release']","['deploy', 'install', 'release']"
Deployability,The issue is fixed with 0.6.1 release of docker images. Please update image version to. `IMAGE_VERSION=0.6.1`. See https://cloud.google.com/genomics/deepvariant for more info.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70#issuecomment-387177566:30,release,release,30,,https://github.com/google/deepvariant/issues/70#issuecomment-387177566,2,"['release', 'update']","['release', 'update']"
Deployability,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575672535:249,install,installed,249,,https://github.com/google/deepvariant/issues/657#issuecomment-1575672535,2,['install'],"['install', 'installed']"
Deployability,"The solution in this case was to use the original GRCh38 reference genome, not GRCh38 patch 12.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/411#issuecomment-765805463:86,patch,patch,86,,https://github.com/google/deepvariant/issues/411#issuecomment-765805463,1,['patch'],['patch']
Deployability,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1602257279:72,install,install,72,,https://github.com/google/deepvariant/issues/669#issuecomment-1602257279,1,['install'],['install']
Deployability,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/33#issuecomment-355423493:197,pipeline,pipelines,197,,https://github.com/google/deepvariant/issues/33#issuecomment-355423493,1,['pipeline'],['pipelines']
Deployability,This fixed is included in v0.7.2 release:; https://github.com/google/deepvariant/releases/tag/v0.7.2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/119#issuecomment-447731582:33,release,release,33,,https://github.com/google/deepvariant/issues/119#issuecomment-447731582,2,['release'],"['release', 'releases']"
Deployability,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:504,update,update,504,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104,2,['update'],['update']
Deployability,This has been fixed by the DeepVariant 0.5.1 release that just came out a few minutes ago. Thank you for raising attention to this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-364553446:45,release,release,45,,https://github.com/google/deepvariant/issues/27#issuecomment-364553446,1,['release'],['release']
Deployability,This is in and will go out in the next release. Thank you so much Brad!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/18#issuecomment-355332563:39,release,release,39,,https://github.com/google/deepvariant/pull/18#issuecomment-355332563,1,['release'],['release']
Deployability,This is included in our latest release:; https://github.com/google/deepvariant/releases/tag/v0.8.0. Thank you for your contribution to DeepVariant!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-482391000:31,release,release,31,,https://github.com/google/deepvariant/pull/159#issuecomment-482391000,2,['release'],"['release', 'releases']"
Deployability,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:348,install,install,348,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236,5,"['Install', 'install']","['Install', 'install', 'installing']"
Deployability,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371238076:238,upgrade,upgrade,238,,https://github.com/google/deepvariant/issues/52#issuecomment-371238076,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```; if self.options.max_reads_per_partition > 0:; reads = utils.reservoir_sample(; reads, self.options.max_reads_per_partition, self.random); ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up.; Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112#issuecomment-433598823:168,update,update,168,,https://github.com/google/deepvariant/issues/112#issuecomment-433598823,1,['update'],['update']
Deployability,"Turns out I was on some nodes of the HPC that hadn't been updated, issue solved itself when on current nodes. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/179#issuecomment-488287371:58,update,updated,58,,https://github.com/google/deepvariant/issues/179#issuecomment-488287371,1,['update'],['updated']
Deployability,"U_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2256,install,install,2256,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with ; sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-363252951:101,install,install,101,,https://github.com/google/deepvariant/issues/47#issuecomment-363252951,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,Unfortunately it's not clear from your post what might be going wrong here. Is this on a clean install of Ubuntu 16? We'd recommend starting there first to make sure everything is working and then moving to whatever environment you are running on.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/5#issuecomment-349829021:95,install,install,95,,https://github.com/google/deepvariant/issues/5#issuecomment-349829021,1,['install'],['install']
Deployability,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:67,release,release,67,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |; | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \; --call_variants_extra_args=""use_openvino=True""; ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-725869282:52,configurat,configuration,52,,https://github.com/google/deepvariant/pull/363#issuecomment-725869282,1,['configurat'],['configuration']
Deployability,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:; ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same.; However, it seems like bad practice to ignore that warning message that shows up in red.; Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385118050:0,Update,Update,0,,https://github.com/google/deepvariant/issues/29#issuecomment-385118050,1,['Update'],['Update']
Deployability,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:0,Update,Update,0,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759,2,"['Update', 'install']","['Update', 'installing']"
Deployability,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/811#issuecomment-2297681816:0,Update,Update,0,,https://github.com/google/deepvariant/issues/811#issuecomment-2297681816,1,['Update'],['Update']
Deployability,"Update: I've emailed @drtamermansour but haven't back from him. I'll close this issue for now. If anyone had a chance to try @williamrowell 's image above, let us know how it wokr.s; If anyone else is encountering the same issue, please feel free to comment on this issue again. I will close it now since there's no information right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-494913584:0,Update,Update,0,,https://github.com/google/deepvariant/issues/178#issuecomment-494913584,1,['Update'],['Update']
Deployability,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/811#issuecomment-2235560333:0,Update,Update,0,,https://github.com/google/deepvariant/issues/811#issuecomment-2235560333,1,['Update'],['Update']
Deployability,Update: we discussed offline. Using `--regions` helped.; I'll close this bug.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-581727444:0,Update,Update,0,,https://github.com/google/deepvariant/issues/260#issuecomment-581727444,1,['Update'],['Update']
Deployability,Update:; I can confirm that I'm able to reproduce your error. We're working on a fix. Stay tuned!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-388450895:0,Update,Update,0,,https://github.com/google/deepvariant/issues/62#issuecomment-388450895,1,['Update'],['Update']
Deployability,"Update:; if you look at https://github.com/google/deepvariant/blob/r0.5/docs/visualizing_examples.ipynb; the code in read_tfrecords is an example of how you can read a tfrecord file, and count examples if you like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10#issuecomment-361769858:0,Update,Update,0,,https://github.com/google/deepvariant/issues/10#issuecomment-361769858,1,['Update'],['Update']
Deployability,Update:; the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:; https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh; https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385593875:0,Update,Update,0,,https://github.com/google/deepvariant/issues/29#issuecomment-385593875,3,"['Update', 'release', 'update']","['Update', 'releases', 'update']"
Deployability,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607#issuecomment-1414165945:68,update,update,68,,https://github.com/google/deepvariant/issues/607#issuecomment-1414165945,2,"['release', 'update']","['release', 'update']"
Deployability,"VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:4503,install,install,4503,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['install'],['install']
Deployability,"ValueError: NOT_FOUND: Could not open /input/SAMPLE01.align.sort.marked.bam; parallel: This job failed:; /opt/deepvariant/bin/make_examples. I have a right bam file. ; Can be that ""make examples"" require a python version incompatible with my python 3.10.x version installed on Unix-virtual machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1632242067:264,install,installed,264,,https://github.com/google/deepvariant/issues/675#issuecomment-1632242067,1,['install'],['installed']
Deployability,"Versions; - singularity version 3.6.4-1.el7; - CentOS Linux release 7.9.2009; - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue.; ; ```; singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \; deepvariant_1.1.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /input/asm.fasta \; --reads /input/hifi.bam \; --output_vcf /output/asm.output.vcf.gz \; --output_gvcf /output/asm.output.g.vcf.gz \; --num_shards ""${THREADS}"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-762181198:60,release,release,60,,https://github.com/google/deepvariant/issues/404#issuecomment-762181198,1,['release'],['release']
Deployability,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,; `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/57#issuecomment-371635934:485,install,install-prerequisites,485,,https://github.com/google/deepvariant/pull/57#issuecomment-371635934,1,['install'],['install-prerequisites']
Deployability,"Vini, Paul and Pi-Chuan;; Thanks much for the ping about updating the conda recipe. This is now synced to the latest release (0.7.0) so you can use that as a backup option in addition to your work on getting it built on 18.04. Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98#issuecomment-425452266:117,release,release,117,,https://github.com/google/deepvariant/issues/98#issuecomment-425452266,1,['release'],['release']
Deployability,We are also interested in exploring the possibility of using DeepVariant to call SNPs in bacterial genomes. For our current pipeline we are using BWA to map reads to the reference and then PILON from the Broad to call variants. Specifically we are working on the Tuberculosis genome.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-437149890:124,pipeline,pipeline,124,,https://github.com/google/deepvariant/issues/114#issuecomment-437149890,1,['pipeline'],['pipeline']
Deployability,"We are working on this, please check related bug #116 for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-438037122:62,update,update,62,,https://github.com/google/deepvariant/issues/118#issuecomment-438037122,1,['update'],['update']
Deployability,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. We currently recommend that the BAM be; duplicate marked, but it's unclear if this is even necessary. Finally, it's not; necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confide",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/35#issuecomment-356807166:122,release,release,122,,https://github.com/google/deepvariant/issues/35#issuecomment-356807166,1,['release'],['release']
Deployability,"We have updated the documentation to mention that AVX instructions are needed. These changes will come out with the next release. Thank you for the feedback!. Edit: we specifically mention this requirement again in the quickstart documentation, in addition to the page linked below by @pgrosu. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248#issuecomment-566700041:8,update,updated,8,,https://github.com/google/deepvariant/issues/248#issuecomment-566700041,2,"['release', 'update']","['release', 'updated']"
Deployability,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/25#issuecomment-354157667:400,install,install,400,,https://github.com/google/deepvariant/issues/25#issuecomment-354157667,1,['install'],['install']
Deployability,We just released a new docker image located at `gcr.io/cloud-lifesciences/gcp-deepvariant-runner`. Please let us know if you still observe gcsfuse issue using the latest release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214#issuecomment-530561319:8,release,released,8,,https://github.com/google/deepvariant/issues/214#issuecomment-530561319,2,['release'],"['release', 'released']"
Deployability,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:; ```; WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl; curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}""; pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/263#issuecomment-577940658:85,install,installation,85,,https://github.com/google/deepvariant/issues/263#issuecomment-577940658,3,"['install', 'upgrade']","['install', 'installation', 'upgrade']"
Deployability,"We're using 18.04 to build your releases from r0.7 - everything works fine. This problem appears first time and is not related to Ubuntu version. I suppose that `pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""` won't work on Ubuntu 16.04 too if you try. But you are correct: we're trying to build on Ubuntu 18.04, but were having issue on numpy. For now, I've fixed this issue by widening check (now it doesn't apply ` --no-binary=:all: ` to Ubuntu 18.04 too). I'll wait until you switch to 18.04 for official support, thanks. Closing this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394#issuecomment-742326989:32,release,releases,32,,https://github.com/google/deepvariant/issues/394#issuecomment-742326989,2,"['install', 'release']","['install', 'releases']"
Deployability,We've added a more verbose message. It will come out in later releases of Nucleus and DeepVariant. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/73#issuecomment-389700500:62,release,releases,62,,https://github.com/google/deepvariant/issues/73#issuecomment-389700500,1,['release'],['releases']
Deployability,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file.; [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url); [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,; Anil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/606#issuecomment-1406588815:191,release,releases,191,,https://github.com/google/deepvariant/issues/606#issuecomment-1406588815,1,['release'],['releases']
Deployability,What Python version do you have installed?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-513999168:32,install,installed,32,,https://github.com/google/deepvariant/issues/199#issuecomment-513999168,1,['install'],['installed']
Deployability,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:636,release,release,636,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538,1,['release'],['release']
Deployability,"While AWS was better than my local computer (with 8 GB of RAM and 4 cores) for running the 1st step (_make_examples_), I can likewise try running the Docker container in interactive mode on my own computer:. `docker run -it -v /c/Users/Charles/Documents/WGS_Exome_Analysis/My_Veritas_WGS:/mnt/wgs gcr.io/deepvariant-docker/deepvariant`. followed by moving to the appropriate directory and running the following script:. ```; OUTPUT_DIR=Genos_Provided; REF=../hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. This is different than running interactive mode for **postprocess_variants** on AWS (which almost immediately ends, without error message or results file), but it has been running for more than one hour. So, I will provide an update if this works, but this sounds different than your experience with the t1.micro AWS instance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480640009:960,update,update,960,,https://github.com/google/deepvariant/issues/167#issuecomment-480640009,1,['update'],['update']
Deployability,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437414607:21,pipeline,pipeline,21,,https://github.com/google/deepvariant/issues/116#issuecomment-437414607,2,['pipeline'],['pipeline']
Deployability,"Yeah I think that would be fine. This case was more unfortunate as the missing OpenVINO leads to the missing `from tensorflow.python.tools import optimize_for_inference_lib` dependency, and so I assumed I had really messed something up as I hadn't changed anything with tensorflow. On a slightly different note, the memory usage for call_variants (open_vino=False) in v1.4 is much better than v1.3 (open_vino=True) at ~ 7gb compared to ~20gb RAM for a lot of medium coverage samples. Also postprocessing seems to be about 70% faster (although the previous runs had IO issues so probably inflated). Great improvements this release!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541#issuecomment-1154742774:622,release,release,622,,https://github.com/google/deepvariant/issues/541#issuecomment-1154742774,1,['release'],['release']
Deployability,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112#issuecomment-433250645:358,configurat,configuration,358,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645,1,['configurat'],['configuration']
Deployability,"Yeah, we could remove `use_openvino` completely from call_variants.py. But given that I do still want to try it again in the future, it would be nice to keep it as a flag. (And like you said, users who build their own binaries or own Docker images, they can still enable it). @ASLeonard Will it be helpful if we add a bullet point in https://github.com/google/deepvariant/releases/tag/v1.4.0 to specifically call out that we're not building OpenVINO into out default Docker image?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541#issuecomment-1154454592:372,release,releases,372,,https://github.com/google/deepvariant/issues/541#issuecomment-1154454592,1,['release'],['releases']
Deployability,"Yes, I agree that --help is important! Thanks for reporting. This is now fixed internally and will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/78#issuecomment-398534109:120,release,release,120,,https://github.com/google/deepvariant/issues/78#issuecomment-398534109,1,['release'],['release']
Deployability,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. ; In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`.; We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:; https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-586546504:1117,release,released,1117,,https://github.com/google/deepvariant/issues/272#issuecomment-586546504,2,['release'],"['released', 'releases']"
Deployability,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166#issuecomment-478851260:139,release,release,139,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260,2,['release'],['release']
Deployability,"[log1.txt](https://github.com/google/deepvariant/files/2393958/log1.txt); Hi Paul,. Attached is the log with --verbose_failures. Yes I am using a node on our HPC cluster. Some more details about my environment:; 1. gcc version 4.8.5; 2. centos 7; 3. protobuf version 3.5.1 installed as module with PATH and LD_LIB_PATH set accordingly.; 4. clif installed under HOME directory and WORKSPACE updated accordingly. Thank you; Ram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-422487671:273,install,installed,273,,https://github.com/google/deepvariant/issues/94#issuecomment-422487671,3,"['install', 'update']","['installed', 'updated']"
Deployability,"_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:2964,install,installer-linux-,2964,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['install'],['installer-linux-']
Deployability,"_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took `5m25.905s`. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. # This parts starts shuffling... ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash; $ pip3 freeze; absl-py==2.1.0; apache-beam==2.50.0; astunparse==1.6.3; cachetools==5.3.3; certifi==2024.2.2; charset-normalizer==3.3.2; cloudpickle==2.2.1; crcmod==1.7; Deprec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:4771,install,install,4771,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,1,['install'],['install']
Deployability,"_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:2881,install,installer-linux-,2881,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['install'],['installer-linux-']
Deployability,"`; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-ge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:1918,install,installation,1918,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,1,['install'],['installation']
Deployability,"`[bazel release 0.21.0]` -- should I try a more recent version?. I believe this was prescribed by default `settings.sh`.; https://github.com/google/deepvariant/blob/r0.8/settings.sh. Is there a good review stable release from which to try? Sorry. Really at a loss here, as tests pass.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514397821:8,release,release,8,,https://github.com/google/deepvariant/issues/199#issuecomment-514397821,2,['release'],['release']
Deployability,"``. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:2705,install,install,2705,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['install'],['install']
Deployability,"``; diff --git a/settings.sh b/settings.sh; index 9d5f58c0..649b1bb8 100755; --- a/settings.sh; +++ b/settings.sh; @@ -89,18 +89,18 @@ export DV_GPU_BUILD=""${DV_GPU_BUILD:-0}""; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; -export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; +export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-0}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow""; -export DV_TF_NUMPY_VERSION=""1.19.2"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; +export DV_TF_NUMPY_VERSION=""1.24.1"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; ; # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}""; ; -export PYTHON_VERSION=3.8; +export PYTHON_VERSION=3.9; # shellcheck disable=SC2155; export PYTHON_BIN_PATH=""$(which python${PYTHON_VERSION})""; export PYTHON_LIB_PATH=""/usr/local/lib/python${PYTHON_VERSION}/dist-packages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function not",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:1146,install,install,1146,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['install'],['install']
Deployability,"```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4355,install,install,4355,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,"```; I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found; []; ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2291334900:880,install,installed,880,,https://github.com/google/deepvariant/issues/820#issuecomment-2291334900,1,['install'],['installed']
Deployability,"`bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:9559,install,install,9559,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,a 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8955,install,installation,8955,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installation']
Deployability,"a conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:1607,Install,Install,1607,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Install'],['Install']
Deployability,"a"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed.; # perl: warning: Please check that your locale settings:; # 	LANGUAGE = (unset),; # 	LC_ALL = (unset),; # 	LC_CTYPE = ""C.UTF-8"",; # 	LANG = ""en_US.UTF-8""; # are supported and installed on your system.; # perl: warning: Falling back to the standard locale (""C"").; # perl: warning: Setting locale failed.; # perl: warning: Please check that your locale settings:; # 	LANGUAGE = (unset),; # 	LC_ALL = (unset),; # 	LC_CTYPE = ""C.UTF-8"",; # 	LANG = ""en_US.UTF-8""; # are supported and installed on your system.; # perl: warning: Falling back to the standard locale (""C"").; # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader; # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs; # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader; # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'unplac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946:2887,install,installed,2887,,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946,1,['install'],['installed']
Deployability,"ad Chapman <notifications@github.com>; wrote:. > Pi-Chuan and Mike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1629,install,installed,1629,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937,1,['install'],['installed']
Deployability,"ages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:13800,upgrade,upgrade,13800,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['upgrade'],['upgrade']
Deployability,"ain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:2861,install,installing,2861,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,1,['install'],['installing']
Deployability,"andard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash; gcloud compute ssh pichuan-test --zone ""us-west1-b""; ```. Check the Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1335,Install,Install,1335,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['Install'],['Install']
Deployability,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1216,Install,Installing,1216,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,9,"['Install', 'install', 'upgrade']","['Installing', 'install', 'upgrade']"
Deployability,any update?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-837436553:4,update,update,4,,https://github.com/google/deepvariant/issues/452#issuecomment-837436553,1,['update'],['update']
Deployability,"ase ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11873,install,installed,11873,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['installed']
Deployability,"atible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you lik",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6447,Install,Install,6447,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Install'],['Install']
Deployability,"ation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8665,Configurat,Configuration,8665,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Configurat'],['Configuration']
Deployability,"ation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8188,Configurat,Configuration,8188,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Configurat'],['Configuration']
Deployability,"atures; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") ; -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") ; -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") ; -- Configuring done; -- Generating done; -- Build files have been written to: /root/clif/build; ```; which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash; root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build; root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:3980,install,install,3980,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,1,['install'],['install']
Deployability,"ault is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8800,install,installation,8800,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['install'],['installation']
Deployability,"ault is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8323,install,installation,8323,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installation']
Deployability,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3217,install,installed,3217,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['install'],['installed']
Deployability,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4243,install,installed,4243,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installed']
Deployability,"ay from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```; chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:..; ```; Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:3717,Configurat,Configuration,3717,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,1,['Configurat'],['Configuration']
Deployability,"b.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase.; 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most.; 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF?; For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh); ```; $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l; 7753721; $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l; 91714977; ```; And here is the breakdown of P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-695126772:1419,release,release,1419,,https://github.com/google/deepvariant/issues/346#issuecomment-695126772,1,['release'],['release']
Deployability,"b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14736,upgrade,upgrade,14736,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['upgrade'],['upgrade']
Deployability,"below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pichuan@pichuan-centos8 ~]$ l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1654,install,installation-on-linux,1654,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612,1,['install'],['installation-on-linux']
Deployability,"bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be written to /tmp/6178902.1.st_supermem.q/tmpezd79ese in docker. ****; ***** Running the command:*****; time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/Reference/PacBio/minimap2/Homo_sapiens_assembly38.fasta"" --reads ""/hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v4/T202302180201/T202302180201.pbmm2.CCS.alignment.sort.bam"" --examples ""/tmp/6178902.1.st_supermem.q/tmpezd79ese/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/6178902.1.st_supermem.q/tmpezd79ese/gvcf.tfrecord@20.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Error in tempfile() using template /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/temporary_internet_files/parXXXXX.par: Parent directory (/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/temporary_internet_files/) does not exist at /usr/bin/parallel line 3889. real 0m2.813s; user 0m0.258s; sys 0m0.406s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:5985,install,installed,5985,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300,2,['install'],['installed']
Deployability,"buf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6568,install,installed,6568,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,3,"['Install', 'install']","['Install', 'installed']"
Deployability,"buntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11362,install,install,11362,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['install']
Deployability,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4115,install,installed,4115,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['install'],['installed']
Deployability,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5141,install,installed,5141,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installed']
Deployability,"c)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7900,release,release,7900,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['release'],['release']
Deployability,"cal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11996,upgrade,upgraded,11996,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"cal/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:13955,upgrade,upgrade,13955,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['upgrade'],['upgrade']
Deployability,"cal/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8653,install,installed,8653,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['installed']
Deployability,"ce code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # env",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1943,install,install,1943,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['install']
Deployability,"ckages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3644,Install,Install,3644,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,4,"['Install', 'install']","['Install', 'installed']"
Deployability,"clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}"" --no-install-recommends \; wget \; unzip; ; +apt-get install ""${APT_ARGS[@]}"" python3-apt; +cd /usr/lib/python3/dist-packages; +if [ -e apt_pkg.so ]; then; + rm apt_pkg.so; +fi; +ln -s apt_pkg.cpython-38-aarch64-linux-gnu.so apt_pkg.so; +cd -; +; +export PATH=/root/.local/bin/:$PATH; +apt-get install ""${APT_ARGS[@]}"" libcairo2-dev; +pip install pygobject; +apt-get install ""${APT_ARGS[@]}"" libgirepository1.0-dev; +pip install --upgrade pygobject; +sed -i 's/isAlive/is_alive/g' /usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py ; +; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_ARGS[@]}"" \; libllvm11 \; llvm-11 \; llvm-11-dev \; - llvm-11-linker-tools \; python3-dev \; zlib1g-dev; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/cmake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <27>(char const (&) [27])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:4750,install,install,4750,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['install'],['install']
Deployability,"cloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1107,update,update,1107,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,2,"['install', 'update']","['install', 'update']"
Deployability,"command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1307,install,install,1307,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['install'],['install']
Deployability,"common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=mon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6856,patch,patch-,6856,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['patch'],['patch-']
Deployability,"constraining google-cloud-sdk version worked! thank you. On Wed, Apr 24, 2019 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Peter;; > Thanks for testing, it sounds like there is a problem with the recent; > google-cloud-sdk packages. I'll take a look to see if I can figure out what; > is going wrong but an immediate thing you could try is to restrict that; > dependency version to try and avoid the issue:; >; > conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; >; > Hope this helps get it installed.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/177#issuecomment-486163437>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABUBV2DKBMPCNW6H6H2OGKLPSAXVJANCNFSM4HH7EBWQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486821218:537,install,installed,537,,https://github.com/google/deepvariant/issues/177#issuecomment-486821218,1,['install'],['installed']
Deployability,"core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build comman",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6521,install,installed,6521,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,3,"['Install', 'install']","['Install', 'installed']"
Deployability,"cs/tutorials/deepvariant):. ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=cdw-deepvariant-wgs-exome; OUTPUT_BUCKET=gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/DeepVariant_Output; STAGING_FOLDER_NAME=wgs_staging; OUTPUT_FILE_NAME=WGS_Provided.vcf. ## Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1261,pipeline,pipeline,1261,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['pipeline'],['pipeline']
Deployability,"cting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10629,install,installed,10629,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installed']
Deployability,"ctory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3742,upgrade,upgraded,3742,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"d filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: num",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10018,Install,Install,10018,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['Install'],['Install']
Deployability,"d for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7458,install,install,7458,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"d just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. ## GPU image; ```; VERSION=0.9.0-gpu; sudo n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-561996442:1322,update,update,1322,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442,2,"['install', 'update']","['install', 'update']"
Deployability,"d much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```; genotype_probabilities: 0.9999428988; genotype_probabilities: 1.8287e-05; genotype_probabilities: 3.88142e-05; ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:1309,pipeline,pipelines,1309,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,1,['pipeline'],['pipelines']
Deployability,"d of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls. Is there a way to filter my variants from DeepTrio further?. Thank you!. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-820564569:1135,pipeline,pipeline,1135,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569,1,['pipeline'],['pipeline']
Deployability,"d to replicate this issue, and here is what I found. I created an instance from [this CentOS7 VM](https://console.cloud.google.com/marketplace/details/centos-cloud/centos-7). I chose the default location for all installations. When asked if I wanted to update my PATH during installations, I chose to do so. I was able to install DeepVariant through Bioconda using the below steps. . I ran into a particular error with `gsutil`. After running `source ~/.bashrc`, I saw an error when I ran `gsutil`. `gsutil` is used by the DeepVariant installation, so that failed as well. To address this, I referenced [this post](https://stackoverflow.com/questions/38783140/importerror-no-module-named-google-compute-engine) and ran `export BOTO_CONFIG=/dev/null` before installing DeepVariant again. Running these commands in order allows me to successfully install on the VM. ```; # install gsutil; curl https://sdk.cloud.google.com | bash; exec -l $SHELL; # verify that gsutil is working; gsutil. # install wget and bzip2, which are both needed to download miniconda; sudo yum install bzip2 wget; wget https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh; bash Miniconda2-latest-Linux-x86_64.sh ; source ~/.bashrc. # gsutil is failing now; gsutil; export BOTO_CONFIG=/dev/null; # gsutil should be working again; gsutil. # create new conda env, add channels, install deepvaraint; conda create -n dv python=2.7; conda activate dv; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda install -n dv deepvariant -v; ```. In the output from running `conda install -n dv deepvariant -v`, I see the first error you posted even with a successful installation. I was not able to replicate the second error. Some sanity checks for you:. * Are you able to successfully run `gsutil`?; * Did you add all conda channels in the correct order?; * Could you post the entire output from running `conda install -v deepvariant`?. CC @melkerdawy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-452921108:1004,install,install,1004,,https://github.com/google/deepvariant/issues/137#issuecomment-452921108,7,['install'],"['install', 'installation']"
Deployability,"d-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2494,install,install,2494,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,1,['install'],['install']
Deployability,"de -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc '-f-I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif); # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46; # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789; 1:# Copyright 2018 Google LLC.\n; 2:#\n; 3:# Redistribution and use in source and binary forms, with or without\n; 4:# modification, are permitted provided that the following conditions\n; 5:# are met:\n; 6:#\n; 7:# 1. Redistributions of source code must retain the above copyright notice,\n; 8:# this list of conditions and the following disclaimer.\n; 9:#\n; 10:# 2. Redistributions in binary form must reproduce the above copyright\n; 11:# notice, this list of conditions and the following disclaimer in the\n; 12:# documentation and/or other materials provided with the distribution.\n; 13:#\n; 14:# 3. Neither the name of the copyright holder nor the names of its\n; 15:# contributors may be used to endorse or promote products derived from this\n; 16:# software without specific prior written permission.\n; 17:#\n; 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLD",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:12543,Configurat,Configuration,12543,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['Configurat'],['Configuration']
Deployability,"deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1749,pipeline,pipeline,1749,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222,1,['pipeline'],['pipeline']
Deployability,"dency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android build",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6400,Install,Install,6400,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Install'],['Install']
Deployability,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6138,configurat,configuration,6138,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,2,['configurat'],['configuration']
Deployability,"directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help; CPU Builds a DeepVariant Docker image for CPU usage.; GPU Builds a DeepVariant Docker image for GPU usage.; Runner Builds a DeepVariant Docker image for large-scale analysis run; using the Genomics Pipelines API. $; ```. Even `Runner` is a bit too general, so maybe calling it `LargeScaleAnalysis`, or something which should be instantly recognizable as to its intended use. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-413745335:2429,Pipeline,Pipelines,2429,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335,1,['Pipeline'],['Pipelines']
Deployability,"dnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config set",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:5230,install,install,5230,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['install']
Deployability,"do docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. ## GPU image; ```; VERSION=0.9.0-gpu; sudo nvidia-docker pull google/deepvariant:${VERSION}; sudo nvidia-docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant_gpu:latest; sudo nvidia-docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo nvidia-docker push localhost:5000/deepvariant_gpu:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant_gpu:latest; ```. Running through Quick Start just to make sure nothing wrong:; ```; singularity -s exec --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. Both CPU and GPU versions finished on Quick Start data without errors. ## Misc; For installing nvidia-docker and singularity, you can also refer to https://github.com/google/deepvariant/blob/r0.9/scripts/install_nvidia_docker.sh; and; https://github.com/google/deepvariant/blob/r0.9/scripts/install_singularity.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-561996442:3269,install,installing,3269,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442,1,['install'],['installing']
Deployability,"docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase.; 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most.; 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF?; For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh); ```; $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l; 7753721; $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l; 91714977; ```; And here is the breakdown of PASS/RefCall:; ```; $ zcat HG002.outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-695126772:1472,release,releases,1472,,https://github.com/google/deepvariant/issues/346#issuecomment-695126772,1,['release'],['releases']
Deployability,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by; ```; sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb; ```; seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-466661657:60,update,updates,60,,https://github.com/google/deepvariant/issues/41#issuecomment-466661657,2,"['install', 'update']","['install', 'updates']"
Deployability,"drew,. Thank you for your quick reply!. > 1. Are you taking the values from the QUAL field of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls. Is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-820564569:1024,pipeline,pipeline,1024,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569,1,['pipeline'],['pipeline']
Deployability,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9296,Install,Installing,9296,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8819,Install,Installing,8819,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2498,pipeline,pipelines,2498,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,4,"['integrat', 'pipeline']","['integrated', 'integration', 'pipelines']"
Deployability,"e our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:1711,update,update,1711,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,1,['update'],['update']
Deployability,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:1690,install,install,1690,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107,6,"['install', 'release']","['install', 'installation', 'release']"
Deployability,"e update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16467,install,installed,16467,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['install'],['installed']
Deployability,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2729,install,install,2729,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,10,"['INSTALL', 'install', 'release']","['INSTALL', 'install', 'releases']"
Deployability,"e/cloud-sdk:alpine""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent; imageUri: google/cloud-sdk:alpine; timestamp: '2018-11-08T14:28:12.427407Z'; - description: Started pulling ""google/cloud-sdk:alpine""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: google/cloud-sdk:alpine; timestamp: '2018-11-08T14:28:05.897359Z'; - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:28:05.747135Z'; - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:27:34.961215Z'; - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7""; assigned in ""us-west1-b""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:7229,pipeline,pipelines-worker-,7229,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['pipeline'],['pipelines-worker-']
Deployability,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13080,install,installation,13080,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,3,"['configurat', 'install']","['configuration', 'installation']"
Deployability,"eads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data.; > ; > `--emit_realigned_reads` - enables writing out of realigned reads; > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```; docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics; ```. but runs into. ```; Traceback (most recent call last): ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> ; app.run(main) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run ; _run_main(main, args) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main ; sys.exit(main(argv)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main ; commands = create_all_commands() ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands ; sample_name=FLAGS.sample_name)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command ; kwargs.update(_extra_args_to_dict(extra_args)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict ; (flag_name, flag_value) = extra_arg.split('=') ; ValueError: need more than 1 value to unpack ; ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/280#issuecomment-598767292:2028,update,update,2028,,https://github.com/google/deepvariant/issues/280#issuecomment-598767292,1,['update'],['update']
Deployability,"ease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Loo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12256,upgrade,upgraded,12256,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"eate failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1303,install,installed,1303,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709,2,['install'],"['installation', 'installed']"
Deployability,"ed; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9682,install,installed,9682,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['install'],['installed']
Deployability,"ed; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:9205,install,installed,9205,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installed']
Deployability,"ef_reads --vsc_min_fraction_indels ""0.12"" --task {}. /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME""; ```. And here are the two last commands with std out ... ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started.; I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]; I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True; I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0510 12:13:52.1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2104434632:1615,release,release,1615,,https://github.com/google/deepvariant/issues/818#issuecomment-2104434632,1,['release'],['release']
Deployability,"efault float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package; bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install; pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification; python -c ""import tensorflow as tf; print(tf.__version__)""; ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash; # Prerequisites; cmake --version #3.5+; protoc --version # 3.2.0+ build from source code for both C++ and Python; pip install virtualenv; pip install pyparsing; yum install subversion; yum install ocaml; pip install 'pyparsing>=2.2.0'; pkg-config --libs python # workable. # download source code; cd $HOMEPATH; git clone https://github.com/google/clif.git; cd clif. # set environment; export INSTALL_DIR=""$HOMEPATH/inst""; export CLIFSRC_DIR=""$HOMEPATH/clif""; export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend""; export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages""; export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif; export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip; $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees; mkdir -p $LLVM_DIR; cd $LLVM_DIR; svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm; cd llvm/tools; svn co https://llvm.org/svn/llvm-pro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:12228,install,install,12228,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,5,['install'],['install']
Deployability,"eloper.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,; Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one).; 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:3900,install,installed,3900,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,2,"['install', 'update']","['installed', 'update']"
Deployability,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391801518:2057,install,install,2057,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518,1,['install'],['install']
Deployability,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --call_variants_extra_args=""use_openvino=True"" \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-723242914:2188,Install,Install,2188,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914,3,"['Install', 'install', 'upgrade']","['Install', 'install', 'upgrade']"
Deployability,"etch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; #################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16305,Install,Install,16305,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,5,"['Install', 'install']","['Install', 'install']"
Deployability,"evel : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --intermediate_results_dir /out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1762,release,release,1762,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,2,['release'],['release']
Deployability,"f --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}"" --no-install-recommends \; wget \; unzip; ; +apt-get install ""${APT_ARGS[@]}"" python3-apt; +cd /usr/lib/python3/dist-packages; +if [ -e apt_pkg.so ]; then; + rm apt_pkg.so; +fi; +ln -s apt_pkg.cpython-38-aarch64-linux-gnu.so apt_pkg.so; +cd -; +; +export PATH=/root/.local/bin/:$PATH; +apt-get install ""${APT_ARGS[@]}"" libcairo2-dev; +pip install pygobject; +apt-get install ""${APT_ARGS[@]}"" libgirepository1.0-dev; +pip install --upgrade pygobject; +sed -i 's/isAlive/is_alive/g' /usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py ; +; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_ARGS[@]}"" \; libllvm11 \; llvm-11 \; llvm-11-dev \; - llvm-11-linker-tools \; python3-dev \; zlib1g-dev; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/cmake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:4241,install,install,4241,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,3,['install'],['install']
Deployability,"flow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8882,Install,Install,8882,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,"['Install', 'install']","['Install', 'installed']"
Deployability,"g dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12470,install,installed,12470,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['installed']
Deployability,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14906,upgrade,upgrade,14906,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,8,"['Install', 'install', 'upgrade']","['Install', 'Installing', 'installed', 'upgrade']"
Deployability,ge manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:11780,install,installation,11780,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installation']
Deployability,"h:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machine where you create it to make sure it worked there? ; (b) Do you think it'll help if either I or @williamrowell share our *simg file with you to try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-487218238:1561,release,release,1561,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238,3,"['install', 'release']","['installation', 'release']"
Deployability,"he Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singula",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1474,install,installation-steps,1474,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['install'],['installation-steps']
Deployability,"hey yes, this is building off the databricks runtime, Ubuntu 18.04. So replacing,; ARG FROM_IMAGE=ubuntu:20.04; with,; ARG FROM_IMAGE=databricksruntime/minimal:9.x. I also added, ; `sudo -H apt-get -qq -y install python3-apt python3-distutils`. what do you recommend doing to troubleshoot?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-895630749:205,install,install,205,,https://github.com/google/deepvariant/issues/476#issuecomment-895630749,1,['install'],['install']
Deployability,"hg19.chr20.unittest.fasta.gz \; --infile call_variants_output.tfrecord \; --outfile output.vcf. %runscript; if [ $# -eq 0 ]; then; echo '''Example Usage:. # download data to input and models; singularity run --app download_testdata deepvariant-custom.simg. # make the examples, mapping inputs; singularity run --bind input:/dv2/input/ --app make_examples deepvariant-custom.simg. # call variants, mapping models; singularity run --app call_variants deepvariant-custom.simg. # postprocess variants; singularity run --bind input:/dv2/input/ --app postprocess_variants deepvariant-custom.simg. # https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md; '''; else; exec ""$@""; fi. %post; export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)""; echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -; apt-get -y update && apt-get install -y google-cloud-sdk parallel wget; rm -rf /var/lib/apt/lists/*. # https://github.com/google/deepvariant/blob/r0.5/deepvariant/docker/Dockerfile; BASH_HEADER='#!/bin/bash' && \; printf ""%s\n%s\n"" \; ""${BASH_HEADER}"" \; 'python /opt/deepvariant/bin/make_examples.zip ""$@""' > \; /opt/deepvariant/bin/make_examples && \; printf ""%s\n%s\n"" \; ""${BASH_HEADER}"" \; 'python /opt/deepvariant/bin/call_variants.zip ""$@""' > \; /opt/deepvariant/bin/call_variants && \; printf ""%s\n%s\n"" \; ""${BASH_HEADER}"" \; 'python /opt/deepvariant/bin/postprocess_variants.zip ""$@""' > \; /opt/deepvariant/bin/postprocess_variants && \; printf ""%s\n%s\n"" \; ""${BASH_HEADER}"" \; 'python /opt/deepvariant/bin/model_train.zip ""$@""' > \; /opt/deepvariant/bin/model_train && \; printf ""%s\n%s\n"" \; ""${BASH_HEADER}"" \; 'python /opt/deepvariant/bin/model_eval.zip ""$@""' > \; /opt/deepvariant/bin/model_eval && \; chmod +x /opt/deepvariant/bin/make_examples \; /opt/deepvariant/bin/call_variants \; /opt/deepvariant/bin/postprocess_vari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-458208323:2889,update,update,2889,,https://github.com/google/deepvariant/issues/132#issuecomment-458208323,2,"['install', 'update']","['install', 'update']"
Deployability,"hi @SHuang-Broad ,. Please try the following docker that has the patch incorporated that should fix your issue:. ```bash; docker pull google/deepvariant:CL602468145; docker pull google/deepvariant:CL602468145-gpu; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769#issuecomment-1928535877:65,patch,patch,65,,https://github.com/google/deepvariant/issues/769#issuecomment-1928535877,1,['patch'],['patch']
Deployability,"hi @dennishendriksen,. Currently DeepTrio ONT is not supported officially. The code and a model exists but no documentation because we were not satisfied to make a official release. One of the reasons for the segfault could be because of low memory? Can you let us know the command you ran and the memory you have? Also, if easier then if you can share the bams then I can try to run it on our end and see what's happening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724#issuecomment-1789458414:173,release,release,173,,https://github.com/google/deepvariant/issues/724#issuecomment-1789458414,1,['release'],['release']
Deployability,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/792#issuecomment-2010558958:121,install,install,121,,https://github.com/google/deepvariant/issues/792#issuecomment-2010558958,1,['install'],['install']
Deployability,"hi @sounkou-bioinfo,. Thank you for bringing up this issue. We have not worked on this specifically but I will try to give this a shot in our next release. It's a reasonable request and we may have the right logics to fix this in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/875#issuecomment-2322170478:147,release,release,147,,https://github.com/google/deepvariant/issues/875#issuecomment-2322170478,2,['release'],['release']
Deployability,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run ; 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. ; ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/7#issuecomment-350528251:151,install,install,151,,https://github.com/google/deepvariant/issues/7#issuecomment-350528251,1,['install'],['install']
Deployability,"hi, @akolesnikov ; In the detailed information of releases v1.6, I noticed that DV1.6 has added new models trained with Complete Genomics data, and added case studies.; I followed your doc:` https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md`; The model file was downloaded from here:; ![image](https://github.com/google/deepvariant/assets/70870741/3dee9f96-bdb6-4ece-891f-802eb4297878)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1798035805:50,release,releases,50,,https://github.com/google/deepvariant/issues/725#issuecomment-1798035805,1,['release'],['releases']
Deployability,"ials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1321,Pipeline,Pipeline,1321,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222,1,['Pipeline'],['Pipeline']
Deployability,"iant-wgs-exome; OUTPUT_BUCKET=gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/DeepVariant_Output; STAGING_FOLDER_NAME=wgs_staging; OUTPUT_FILE_NAME=WGS_Provided.vcf. ## Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1345,pipeline,pipelines,1345,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['pipeline'],['pipelines']
Deployability,"iant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I sshed into the machine. ```bash; gcloud compute ssh pichuan-cpu --zone us-west1-b; ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant/; git checkout r1.5; ```. And I confirmed the version:. ```; pichuan@pichuan-cpu:~/deepvariant$ git log | head; commit ab068c4588a02e2167051bd9e74c0c9579462b51; Author: pichuan <pichuan@google.com>; Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md; ; PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md; So I ran:. ```bash; sudo su; ./build-prereq.sh; ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX comp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:1353,Update,Update,1353,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,1,['Update'],['Update']
Deployability,"id start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:1538,release,released,1538,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,1,['release'],['released']
Deployability,"ike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1668,install,installing,1668,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937,2,['install'],"['install', 'installing']"
Deployability,"in. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4187,Install,Install,4187,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,"['Install', 'install']","['Install', 'install']"
Deployability,"in/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git Open",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6765,install,install,6765,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"in:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4165,configurat,configuration,4165,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['configurat'],['configuration']
Deployability,"inst `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:; ```stdout; 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; ```. Here's where I found `libnvinfer.so.7`:; ```stdout; [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib; libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8; ```. Here's my `ldd` call to see what it's linked to:; ```stdout; [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7; 	linux-vdso.so.1 (0x0000155555524000); 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000); 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000); 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000); 	libcublas.so.12 => not found; 	libcublasLt.so.12 => not found; 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000); 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000); 	libstdc++.so.6 => /lib/x86_64-linux-g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060:1253,install,installed,1253,,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060,1,['install'],['installed']
Deployability,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8113,install,installed,8113,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['install'],['installed']
Deployability,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2852,install,installed,2852,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['install'],['installed']
Deployability,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3878,install,installed,3878,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installed']
Deployability,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2507,install,installed,2507,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['install'],['installed']
Deployability,"inux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1975,install,install,1975,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,2,['install'],['install']
Deployability,ir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:3861,configurat,configuration,3861,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['configurat'],['configuration']
Deployability,"is it possible to make this available https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-20.latest.tgz. Since I have another problem about ""Install CLIF binary"" using ubuntu 20.04. Quote from build-prereq.sh script:. # Figure out which linux installation we are on to fetch an appropriate # version of the pre-built CLIF binary. Note that we only support now Ubuntu # 14, 16, and 18.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-821553096:170,Install,Install,170,,https://github.com/google/deepvariant/issues/441#issuecomment-821553096,2,"['Install', 'install']","['Install', 'installation']"
Deployability,"ist.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Buildi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8201,Install,Install,8201,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['Install'],['Install']
Deployability,"kages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:2468,Update,Update,2468,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,2,"['Update', 'install']","['Update', 'installed']"
Deployability,"kages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3082,install,installed,3082,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installed']
Deployability,"ke -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2205,install,install,2205,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"ke into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/ke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5847,Update,Update,5847,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Update'],['Update']
Deployability,"ke sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out.; ```; sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2; ```. 2. Inside the interactive mode, run the following:; ```; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard""; DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; N_SHARDS=""64"". ## Download extra packages; sudo apt-get -y update; sudo apt-get -y install parallel; sudo apt-get -y install aria2; ## Download models, and test data; # Copy the model files to your local disk.; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi; ```. Then, I ran `make_examples` similar to the way you did in your original post:; ```; ## Run `make_examples`; ( time seq",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:1975,update,update,1975,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,3,"['install', 'update']","['install', 'update']"
Deployability,"l-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check nump",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1645,release,releases,1645,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,1,['release'],['releases']
Deployability,"lafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:; It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8?. ---. Here is what I tried. On a GCE instance, I ran:. ```bash; wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda; eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. Then, I ran:. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```; (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/; call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip; call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh; deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip; freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip; ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/736#issuecomment-1829204521:1414,release,release,1414,,https://github.com/google/deepvariant/issues/736#issuecomment-1829204521,5,"['install', 'release', 'update']","['install', 'release', 'releases', 'update']"
Deployability,"larity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check numpy version:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import numpy as np; print(np.version.version)'; ```; Which shows:; ```; 1.19.2; ```. It seems like my machine doesn't already have numpy, though:. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy; WARNING: Package(s) not found: numpy; ```; doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:; ```; pip3.8 install numpy==1.23.0; ```; (Because you mentioned your cluster has 1.23.0). Now this shows:. ```; [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy; Name: numpy; Version: 1.23.0; Summary: NumPy is the fundamental package for array computing with Python.; Home-page: https://www.numpy.org; Author: Travis E. Oliphant et al.; Author-email: None; License: BSD; Location: /home/pichuan/.local/lib/python3.8/site-packages; Requires: ; Required-by: ; ```. Then I re-ran:. ```; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/interm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:3063,Install,Install,3063,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,1,['Install'],['Install']
