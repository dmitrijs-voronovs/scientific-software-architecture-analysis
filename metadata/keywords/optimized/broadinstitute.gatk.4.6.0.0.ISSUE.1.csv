quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"O MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 679.0 B, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on d01.capitalbiotech.local:41352 (size: 679.0 B, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 17 from broadcast at ReadsSparkSink.java:146; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 7.3 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 679.0 B, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on d01.capitalbiotech.local:41352 (size: 679.0 B, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 18 from broadcast at BamSink.java:76; 23/05/23 13:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 23/05/23 13:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 23/05/23 13:20:18 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78; 23/05/23 13:20:18 INFO DAGScheduler: Registering RDD 68 (mapToPair at SparkUtils.java:161) as input to shuffle 7; 23/05/23 13:20:18 INFO DAGScheduler: Got job 6 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions; 23/05/23 13:20:18 INFO DAGScheduler: Final stage: ResultStage 30 (runJob at SparkHadoopWriter.scala:78); 23/05/23 13:20:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29); 23/05/23 13:20:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 29); 23/05/23 13:20:18 INFO DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[68] at mapToPair at SparkUtils.java:161), which has no missing parents; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.8 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8339:48305,failure,failures,48305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339,1,['failure'],['failures']
Availability,"O ReblockGVCF - Inflater: IntelInflater; 11:25:55.711 INFO ReblockGVCF - GCS max retries/reopens: 20; 11:25:55.711 INFO ReblockGVCF - Requester pays: disabled; 11:25:55.711 WARN ReblockGVCF - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: ReblockGVCF is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:25:55.711 INFO ReblockGVCF - Initializing engine; 11:25:56.290 INFO FeatureManager - Using codec VCFCodec to read file file:///rprojectnb2/kageproj/gatk/gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz; 11:25:56.569 INFO ReblockGVCF - Done initializing engine; 11:25:56.690 INFO ProgressMeter - Starting traversal; 11:25:56.690 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:26:06.694 INFO ProgressMeter - chr1:5066659 0.2 771000 4624612.6; 11:26:16.711 INFO ProgressMeter - chr1:12628456 0.3 1886000 5652065.3; 11:26:26.103 INFO ReblockGVCF - Shutting down engine; [June 30, 2021 11:26:26 AM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ReblockGVCF done. Elapsed time: 0.51 minutes.; Runtime.totalMemory()=3303538688; java.lang.IllegalArgumentException: cannot add a genotype with GQ=-1 because it's not within bounds [0,20); 	at org.broadinstitute.hellbender.utils.variant.writers.HomRefBlock.add(HomRefBlock.java:99); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.createNewBlock(GVCFBlockCombiner.java:168); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.addHomRefSite(GVCFBlockCombiner.java:137); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.submit(GVCFBlockCombiner.java:200); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFWriter.add(GVCFWriter.java:91); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ReblockGVCF.apply(ReblockGVCF.java:229); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334:4168,down,down,4168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334,1,['down'],['down']
Availability,"OR_SAMTOOLS : false; 09:54:28.494 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:54:28.495 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:54:28.495 INFO FilterMutectCalls - Deflater: IntelDeflater; 09:54:28.495 INFO FilterMutectCalls - Inflater: IntelInflater; 09:54:28.495 INFO FilterMutectCalls - GCS max retries/reopens: 20; 09:54:28.495 INFO FilterMutectCalls - Requester pays: disabled; 09:54:28.495 INFO FilterMutectCalls - Initializing engine; 09:54:28.840 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/md0/DataProcess/Ranshi/Mutect2/Try.vcf.gz; 09:54:28.923 INFO FilterMutectCalls - Done initializing engine; 09:54:28.975 INFO ProgressMeter - Starting traversal; 09:54:28.975 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:54:28.976 INFO FilterMutectCalls - Starting first pass through the variants; 09:54:29.139 INFO FilterMutectCalls - Shutting down engine; [August 20, 2019 9:54:29 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1809317888; java.lang.ArrayIndexOutOfBoundsException: 2; 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:64); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); 	at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemain",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6102:2866,down,down,2866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6102,1,['down'],['down']
Availability,O] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Pa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:1050,FAILURE,FAILURE,1050,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['FAILURE'],['FAILURE']
Availability,"On OS X El Capitan 10.11.6, when creating the conda environment I get the following error:; ```; gcc -undefined dynamic_lookup -L/Users/markw/anaconda/envs/gatk/lib -L/Users/markw/anaconda/envs/gatk/lib -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/pysam/libchtslib.o build/temp.macosx-10.7-x86_64-3.6/pysam/htslib_util.o build/temp.macosx-10.7-x86_64-3.6/htslib/kfunc.o build/temp.macosx-10.7-x86_64-3.6/htslib/knetfile.o build/temp.macosx-10.7-x86_64-3.6/htslib/kstring.o build/temp.macosx-10.7-x86_64-3.6/htslib/bcf_sr_sort.o build/temp.macosx-10.7-x86_64-3.6/htslib/bgzf.o build/temp.macosx-10.7-x86_64-3.6/htslib/errmod.o build/temp.macosx-10.7-x86_64-3.6/htslib/faidx.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_net.o build/temp.macosx-10.7-x86_64-3.6/htslib/hts.o build/temp.macosx-10.7-x86_64-3.6/htslib/hts_os.o build/temp.macosx-10.7-x86_64-3.6/htslib/md5.o build/temp.macosx-10.7-x86_64-3.6/htslib/multipart.o build/temp.macosx-10.7-x86_64-3.6/htslib/probaln.o build/temp.macosx-10.7-x86_64-3.6/htslib/realn.o build/temp.macosx-10.7-x86_64-3.6/htslib/regidx.o build/temp.macosx-10.7-x86_64-3.6/htslib/sam.o build/temp.macosx-10.7-x86_64-3.6/htslib/synced_bcf_reader.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcf_sweep.o build/temp.macosx-10.7-x86_64-3.6/htslib/tbx.o build/temp.macosx-10.7-x86_64-3.6/htslib/textutils.o build/temp.macosx-10.7-x86_64-3.6/htslib/thread_pool.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcf.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcfutils.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_codecs.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_decode.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_encode.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_external.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_index.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_io.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742:84,error,error,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742,1,['error'],['error']
Availability,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490:138,failure,failure,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490,1,['failure'],['failure']
Availability,"Once https://github.com/broadinstitute/gatk/pull/3620/ is in, we should be able to remove the download of picard.jar from .travis.yml, and change the M2 WDL to no longer depend having access to it. Workflow calls to picard tools can be replaced with calls to the same tools in GATK, although the argument syntax will have to change from picard style to Barclay style (""I=..."" to ""-I ..."").",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3625:94,down,download,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3625,1,['down'],['download']
Availability,"Once we choose the library to use for GATK configuration, let's have a design meeting to make sure we come up with something that works for Spark, downstream projects, our users, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079:147,down,downstream,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079,1,['down'],['downstream']
Availability,"Once we have built junction trees for linked de Bruijn graphs we can use them to find phased haplotypes that handle repeats / cycles. This essentially amounts to running the current Dijkstra's algorithm on junction trees (which are, after all, DAGs) instead of `SeqGraph`s. That is, the edge weights can remain log branching ratios. The complication is that while using the the ""oldest"" junction tree for edge weights we must also traverse younger junction trees in order to rely on them downstream. We may or may not want to work out a rule for when to use a younger junction tree with much higher coverage than an older one, or to combine multiple trees into a single vote.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5925:488,down,downstream,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5925,1,['down'],['downstream']
Availability,"One more step towards using this new tool. Does:; * output a single VCF containing `<INS>`, `<DEL>`, `<DUP>`, `<INV>` calls (there will be more `<INV>` calls, but that cannot happen until someone takes a look at PR #4789 and check if the proposed algorithm makes sense); * since this new tool applies more permissive filters on MQ and alignment length of the assembly contigs' mappings, I've introduced some downstream filtering parameters allowing to filter VCF records based on annotations `MAPPING_QUALITIES` and `MAX_ALIGN_LENGTH`; the default value is chosen after some experimentation using the CHM PacBio as truth and the branch ; sh-sv-interlvatree-eval.; * cleans up VCF headers and related tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4996:408,down,downstream,408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4996,1,['down'],['downstream']
Availability,"Ongoing conversation from <https://github.com/broadinstitute/gatk-protected/pull/1130> can now continue in this merged repo. So far, @vdauwera @droazen and @samuelklee have agreed to delete the code that was requested to be archived in favor of using git versioning as the archive method with these stipulations from Geraldine:. - the PR and commit message specify whether there is a replacement for each of the tools. ; - should be a deprecation message so that if I try to run one of these tools in a newer version, I get a helpful error message that tells me the tool was removed and by what it was replaced if applicable. See GATK3 for how we implemented this previously. This should be done for all tools that we remove, regardless of whether they were purely internal or experimental. It's only a one line addition per tool and it can potentially save us a lot of headaches later down the road (even just internally). Currently, this PR is the original PR where I placed the to-be-archived code in an archive folder. . ### I have yet to make additional changes so as to follow the above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2809:534,error,error,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809,2,"['down', 'error']","['down', 'error']"
Availability,"Only 2bit references load the reference data into memory and can be effectively broadcast -- need to add a check that we have a 2bit reference when using BROADCAST in `BaseRecalibratorSpark`, and throw an error with instructions on how to create one when we don't.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1130:205,error,error,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1130,1,['error'],['error']
Availability,"Only performance optimizations were made to the copy-ratio denoising method in the ModelSegments pipeline, which is otherwise identical to that used in GATK CNV. No special care is taken to preserve the normalization of the overall copy-ratio profile during the process. This may become important in downstream tumor-heterogeneity inference; estimates of the ploidy may be otherwise biased. We can investigate using simulated data. This issue could be obviated by #4121 in the near future, but a quick fix might nevertheless be in order.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4150:300,down,downstream,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4150,1,['down'],['downstream']
Availability,Only the first letter of unknown arguments is shown in the error.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1751:59,error,error,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1751,1,['error'],['error']
Availability,"Ops reported several instances in which the allele-specific filtering failed. In the case I examined, the MQ distribution is much tighter around the mode at 60, which causes lin alg failures because that variable is effectively constant. Added more jitter, which has served well in the past.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6262:182,failure,failures,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6262,1,['failure'],['failures']
Availability,"Optimized(NioEventLoop.java:580); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QName",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:46886,down,down,46886,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['down'],['down']
Availability,Option to recover all dangling branches -- default in M2 mito mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5693:10,recover,recover,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693,1,['recover'],['recover']
Availability,Originally by @vruano . Currently the dangling head and tail recovery algorithm only handle simple paths without furcations from the dangling source/sink vertex and the reference path. . However some variation that fail in complex dangling subgraphs can be lost. For example. https://www.pivotaltracker.com/story/show/80381400 ; So this story is about implementing an improved algorithm to handle these cases.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/266:61,recover,recovery,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/266,1,['recover'],['recovery']
Availability,"Originally by @vruano in Classic GATK Pivotal. Improve the read threading process in order to minimize loss of information without affecting the accuracy of calls. . Here I list a few details and ideas to take in consideration:. A. Currently (unless recover of dangling heads is active) we start threading at the first unique kmer of the read (sequence). There are at least two unsound aspect to this approach:. A.1 Since we are generating those vertices as we thread the resulting graph and edge weights may be different depending of the sequence (read) threading order. . A.2 We are throwing away information located at the beginning of the read before the first unique (and existing) k-mer in each sequence is found. This is partly fixed by the approach taken when we recover dangling heads yet it seems to have other problems downstream when selecting or pruning haplotypes:. ```; https://www.pivotaltracker.com/story/show/67601310; ```. B. Low support chain pruning might not be longer needed. Now we have a newer approach to select best haplotypes that can handle complex graph we might well not need to prune low supported hap early as they seemly they won't be selected if the are not amongst the best haplotypes. . B.1 Now that still would produce a considerable number of unlikely haplotypes that would cause a CPU burden. That can be changed by imposing another kinds of limit, For example we include all haplotypes with scores (likelihoods) that are Q0 - Q40 or we include haplotypes until the sum of their likelihoods is larger than the 99.99% probability mass. . B.2 This could provide a downstream solution to the problem caused by ranging heads recovery (explained above in A.2). B.3 If pruning is to be maintained, it makes more sense to do it at the very end after all dangling ends hav been recovered and the edges supports are finalized. Of course I assuming here that dangling end recovery does the sensible think of updating those supports are the graphs is modified. C. The use ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/264:250,recover,recover,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/264,3,"['down', 'recover']","['downstream', 'recover']"
Availability,"Originally from @vruano . Depending of what ploidy we use AR may return different active region boundaries. This differences cause the haploid assembly to fail with the larger region hightlight the lack of robustness of the current approach. More concretely the problem seem to be the presence of cycle in the larger region. Files are located in . ```; /humgen/gsa-hpprojects/dev/valentin/bug-reports/non-rubsassembly-with-ploidy4. cd $THAT_DIR; sh ./run.sh; ```. in CEUTrio*ploidy4.vcf the variant 20:22064431 is missing (as some other in the same region) which is a TP in knowledge base. . If you look into the debug output ploidy2.err and ploidy4.err, the latter attempts to assemble a larger region failing due to a cycle. . AR traversal comes out with different active region boundaries because the engine used takes as a parameter the ploidy. That is not by itself a bug and a bad think is just that the assembly fails for the extended region. . The task here is to improve the assembly algorithm to cope with this situations better (perhaps handle cycles appropriately).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/267:206,robust,robustness,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/267,1,['robust'],['robustness']
Availability,"Our R dependency is primarily for producing plots. It could be possible to create plots using javascript instead. Javascript plots have several potential advantages but also several major downsides. The biggest and most obvious drawback is that we don't have any code to produce them yet, and they are likely harder to generate and experiment with than R scripts. . The advantage would be that we could avoid requiring an R installation to run hellbender scripts, we could potentially also include interactive plotting or other neat tricks to make the plots more useful. I see 2 possible routes to replacing Rscripts with javascript. The first would be for tools that require graphs to perform some html generation and produce html reports with embedded javascript. The user could then open these in their browser and view the plots ( much like how our test suite report and jacoco is done). . A different option would be to use javascript plotting libraries directly within the jvm to generate SVG. Java 8 has a new javascript engine which is supposed to be reasonably fast and offers access to java objects from within it. Unfortunately it doesn't offer a full DOM like a browser does, so most existing javascript libraries will fall over. It seems like it would take a lot of hacking to get something like d3 to run directly on the jvm. (someone has done something of the kind here: http://jazdw.net/content/server-side-svg-generation-using-d3js) . Other options would be to use the javafx web panes to display a browser directly, or to plot directly on a canvas. Either of these options seem like they would be painful and awful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/248:188,down,downsides,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/248,1,['down'],['downsides']
Availability,Our [spark tests on jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) are failing with:; ```; Runtime.totalMemory()=554696704; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:189); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:93); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:238); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2449:225,ERROR,ERROR,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449,1,['ERROR'],['ERROR']
Availability,Our current logging level argument (VERBOSITY) is only hooked up to the legacy Picard logger. We need to hook this up to log4j as well. Related to:; https://github.com/broadinstitute/hellbender/issues/146 (standardize on log4j across GATK + Picard); https://github.com/broadinstitute/hellbender/issues/216 (fix log4j error that happens on every run),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/243:317,error,error,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243,1,['error'],['error']
Availability,"Our default downsampling settings in HaplotypeCaller / Mutect2 (cap the maximum number of reads that can start at the same position) is uniquely unsuited to amplicon data. We should detect amplicon data on startup, and warn the user to adjust the downsampling settings (as discussed with @davidbenjamin). @ldgauthier Thoughts on this idea?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7567:12,down,downsampling,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7567,2,['down'],['downsampling']
Availability,"Our goal is to have the combination step for allele-specific annotations handled by TileDB, but we should still port this code to GATK4 for the following reasons:. -We can likely simplify the code greatly, reducing it down to the three cases of List concatenation, sum, and contingency table combination, making it easier for Intel to replicate in TileDB. -It will be good to have a non-TileDB way to combine gvcfs as a model implementation and fallback option.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1893:218,down,down,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1893,1,['down'],['down']
Availability,"Our jenkins nightly tests are failing, but they're reporting success. This shouldn't be happening. . Some of the failures are due to #3067, but the spark failures look like something else is causing them. Notice the very short runtimes because nothing is actually happening.; <img width=""958"" alt=""screen shot 2017-06-09 at 2 04 31 pm"" src=""https://user-images.githubusercontent.com/4700332/26988271-a61ab3fc-4d1c-11e7-9110-9941888b66ce.png"">. This ticket is to fix the fact that the tests report success even when they fail, not to fix the tests themselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3077:113,failure,failures,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3077,2,['failure'],['failures']
Availability,Our jenkins worker nodes are dead with out of disk space errors. We need to either clean stuff off of them or give them more space. . We should determine what's eating their disk space as well so we can prevent this in the future. We may need to do some clean up after some of our tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2180:57,error,errors,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2180,1,['error'],['errors']
Availability,"Our patches to `google-cloud-java` in https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281 and https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2283 to fix the transient NIO errors have now been merged into master, and will be part of their next release (which will be the release after `0.22.0`). We should update to the next release as soon as it's out, to remove our existing dependency on a SNAPSHOT build of `google-cloud-java`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3500:201,error,errors,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3500,1,['error'],['errors']
Availability,Our travis builds are getting killed intermittently with out-of-memory errors -- it's unclear whether it's the test suite JVM or the JVM with gradle that is getting killed. This is happening more and more often...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1113:71,error,errors,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1113,1,['error'],['errors']
Availability,"Out of 11 runs on exactly the same data, FilterByOrientationBias fails 6 times and succeeds 5 times. Assigning @LeeTL1220, given prior interaction with user. - User reports this error in: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40412#Comment_40412; - My recapitulation is in: https://github.com/broadinstitute/dsde-docs/issues/2294. Data is at `/humgen/gsa-scr1/pub/incoming/byoo_FilterByOrientationBias.zip`. Command is:; ```; gatk-launch FilterByOrientationBias \; -A 'G/T' -A 'C/T' \; -V test2.vcf \; -P test2.pre_adapter_detail_metrics \; --output ob_filtered2.vcf; ```. Error message changes between:; ```; java.lang.IllegalStateException: Allele in genotype C* not in the variant context [G*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3291:178,error,error,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291,2,"['Error', 'error']","['Error', 'error']"
Availability,Output a more infromative out of memory error for GermlineCNVCaller,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6362:40,error,error,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6362,1,['error'],['error']
Availability,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7498:1200,down,downstream,1200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498,1,['down'],['downstream']
Availability,PCR Error read indel quaility correction issues.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2915:4,Error,Error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915,1,['Error'],['Error']
Availability,PGEN max alt alleles sharp edge fix EchoCallset edition [VS-1279],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8815:36,Echo,EchoCallset,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8815,1,['Echo'],['EchoCallset']
Availability,"PGEN not compressed, Echo edition [VS-1412]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8874:21,Echo,Echo,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8874,1,['Echo'],['Echo']
Availability,"PM CST; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Version: 2.21.2; > 21:14:29.496 INFO GenotypeGVCFs - Picard Version: 2.21.9; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:14:29.496 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:14:29.496 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:14:29.496 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:14:29.496 INFO GenotypeGVCFs - Requester pays: disabled; > 21:14:29.496 INFO GenotypeGVCFs - Initializing engine; > **[TileDB::StorageManager] Error: Cannot lock consolidation filelock; Cannot lock.**; > 21:14:30.336 INFO GenotypeGVCFs - Shutting down engine; > [May 25, 2020 9:14:30 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; > Runtime.totalMemory()=1199570944; > ***********************************************************************; > ; > A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader; > ; > ***********************************************************************; > Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. and last GATK commands used:. ```; gatk GenomicsDBImport \; -V SRR630496.erc.g.vcf \; -V SRR630877.erc.g.vcf \; --genomicsdb-workspace-path mydatabase \; --intervals chr22; ```. > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627:2829,Error,Error,2829,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627,1,['Error'],['Error']
Availability,"PPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:2268,FAILURE,FAILURE,2268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,2,"['ERROR', 'FAILURE']","['ERROR', 'FAILURE']"
Availability,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2297:167,down,downstream,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297,1,['down'],['downstream']
Availability,PSUtils getMatchesLessDeletions() should not throw error if number of…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3639:51,error,error,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3639,1,['error'],['error']
Availability,"P_AF=1.000e-03;P_GERMLINE=-2.169e-04;TLOD=14.94	GT:AD:AF:ALT_F1R2:ALT_F2R1:FOXOG:MBQ:MCL:MFRL:MMQ:MPOS:REF_F1R2:REF_F2R1:SA_MAP_AF:SA_POST_PROB	0/1:6,5:0.455:3:2:0.400:30,33:0,0:191,278:60,60:11,20:1:5:0.404,0.444,0.455:0.025,0.025,0.950; WMCF9-CB5:working shlee$ gzcat 2_normalforpon.vcf.gz | grep 'chrX\t153909841'; chrX	153909841	.	C	A	.	.	DP=11;ECNT=1;POP_AF=1.000e-03;P_GERMLINE=-2.169e-04;TLOD=14.94	GT:AD:AF:ALT_F1R2:ALT_F2R1:FOXOG:MBQ:MCL:MFRL:MMQ:MPOS:REF_F1R2:REF_F2R1:SA_MAP_AF:SA_POST_PROB	0/1:6,5:0.455:3:2:0.400:30,33:0,0:191,278:60,60:11,20:1:5:0.404,0.444,0.455:0.025,0.025,0.950; WMCF9-CB5:working shlee$ gzcat 3_discard_practice_pon.vcf.gz | grep 'chrX'; ##contig=<ID=chrX,length=156040895>; ##contig=<ID=chrX_KI270880v1_alt,length=284869>; ##contig=<ID=chrX_KI270881v1_alt,length=144206>; ##contig=<ID=chrX_KI270913v1_alt,length=274009>; chrX	132097402	.	TACAC	T,TAC	.	.	.; ```; This site should have been called in the PoN. Finally, for the `-vcfs` parameter, if I provide a list of files, one per line, the tool errors with; ```; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file, for input source: /Users/shlee/Desktop/August2017_tutorial_dev/working/list_of_normals_for_pon.txt; 	at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:253); 	at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:101); 	at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:126); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:110); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:74); 	at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:58); 	at org.broadinstitute.hellbender.tools.walkers.mutect.CreateSomaticPanelOfNormals.doWork(CreateSom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3510:2623,error,errors,2623,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3510,1,['error'],['errors']
Availability,"P_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 14:24:34.617 INFO DataSourceUtils - Resolved data source file path: file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/acmg_lof.tsv -> file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 14:24:35.311 INFO Funcotator - Shutting down engine; [October 29, 2020 2:24:35 PM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2055733248; code: 400; message: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Bad Request""; }; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Bad Request""; }; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:439); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:244); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:241); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at shaded.cloud_nio.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at shaded.cloud_nio.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:240); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:7469,error,error,7469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['error'],['error']
Availability,"Package precompiled HDF5 as hdf5-java-bindings, and pull it down via maven",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1990:60,down,down,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1990,1,['down'],['down']
Availability,PackagesNotFoundError: The following packages are not available from current channels:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8838:54,avail,available,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8838,1,['avail'],['available']
Availability,Part 1 of HC memory fixes: add downsampling to AssemblyRegion traversal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1972:31,down,downsampling,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1972,1,['down'],['downsampling']
Availability,"Passing a .ped file to the VariantAnnotator tool does not work:. > gatk4 VariantAnnotator **-ped** my.ped --annotation PossibleDeNovo --variant my.vcf --output out.vcf. Result: A USER ERROR has occurred: **p** is not a recognized option. > gatk4 VariantAnnotator **--ped** my.ped --annotation PossibleDeNovo --variant my.vcf --output out.vcf. Result: A USER ERROR has occurred: **ped** is not a recognized option. Above, ""gatk4"" is an alias of ""/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/bin/java -jar ~/lib/gatk/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4604:184,ERROR,ERROR,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604,2,['ERROR'],['ERROR']
Availability,Passing workflow here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/38d22351-33cd-4c2c-abf9-feccda71d40a. Mostly passing integration test here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/4a7e6628-6c19-442d-90b8-202da267d8bb; (the failure was a bq time out.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8374:306,failure,failure,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8374,1,['failure'],['failure']
Availability,PathSeq Illumina adapter trimming and simple repeat masking,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354:52,mask,masking,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354,2,['mask'],['masking']
Availability,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:90,error,error,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,6,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,PathSeqBuildKmers - Shutting down engine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8204:29,down,down,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204,1,['down'],['down']
Availability,PathSeqPipelineSpark aborted due to stage failure,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:42,failure,failure,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['failure'],['failure']
Availability,PathSeqPipelineSpark error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8339:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339,1,['error'],['error']
Availability,PathSeqPipelineSpark: ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293:22,ERROR,ERROR,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293,1,['ERROR'],['ERROR']
Availability,PathseqPipelineSpark stops with error message regarding com.esotericsoftware.kryo.KryoException: Buffer underflow.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:32,error,error,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['error'],['error']
Availability,Perform downsampling in AssemblyRegionWalkerSpark's strict mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5508:8,down,downsampling,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5508,1,['down'],['downsampling']
Availability,"Picard Version: 3.0.0; 03:15:18.983 INFO PostprocessGermlineCNVCalls - Built for Spark Version: 3.3.1; 03:15:18.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 03:15:18.985 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 03:15:18.985 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 03:15:18.986 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:15:18.987 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 03:15:18.989 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 03:15:18.990 INFO PostprocessGermlineCNVCalls - Initializing engine; 03:15:43.480 INFO PostprocessGermlineCNVCalls - Done initializing engine; 03:15:47.833 INFO PostprocessGermlineCNVCalls - Shutting down engine; [April 15, 2024, 3:15:47 AM CST] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=1207959552; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:74); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:388); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8776:20956,down,down,20956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776,1,['down'],['down']
Availability,Picard added a test that runs all the data providers and makes sure they don't error out and cause skipped tests. Maybe we should add a similar test. see https://github.com/broadinstitute/picard/pull/931,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3619:79,error,error,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3619,1,['error'],['error']
Availability,Picard tools don't perform validation of the sequence dictionary which will occasionally lead to errors. They should implement the same checking as the rest of our tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1272:97,error,errors,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1272,1,['error'],['errors']
Availability,Pileup-based read error corrector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6470:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6470,1,['error'],['error']
Availability,"Please find below the user report of this issue:. Here is the error I got:; [February 25, 2019 8:18:10 PM PST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 11.08 minutes.; Runtime.totalMemory()=5379719168; java.lang.StringIndexOutOfBoundsException: String index out of range: 545; at java.lang.String.substring(String.java:1951); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForDeletion(ProteinChangeInfo.java:192); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:96); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:371); [...]. and here is offending record:; `chr12 70747693 . TAAAAAAA T,TAAAA,TAAAAA,TAAAAAA,TAAAAAAAA . artifact_in_normal;germline_risk;multiallelic CONTQ=93;DP=537;ECNT=1;GERMQ=253,113,0,0,18;MBQ=36,24,36,28,36,33;MFRL=293,529,291,288,325,299;MMQ=60,29,60,60,60,60;MPOS=43,43,41,44,26;NALOD=0.912,0.217,-2.040e+00,-1.342e+01,-4.057e+00;NLOD=20.49,13.33,2.58,-1.476e+01,2.13;POPAF=2.27,1.08,2.19,2.53,5.40;REF_BASES=GCAAGCCTTCTAAAAAAAAAA;RPA=25,18,22,23,24,26;RU=A;SAAF=0.394,0.404,0.420;SAPP=0.019,0.015,0.965;STR;TLOD=3.88,7.29,6.62,36.73,3.85 GT:AD:AF:DP:F1R2:F2R1 0/0:21,0,1,7,14,6:0.011,0.025,0.099,0.262,0.124:49:14,0,1,6,7,4:7,0,0,1,7,2 0/1/2/3/4/5:65,3,11,20,47,18:0.016,0.057,0.076,0.245,0.092:164:32,3,6,14,30,8:33,0,5,6,17,10`. As a result the Funcotator output is truncated. Bug?. Thanks!; Ivan. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23631/funcotator-reproducibly-crushes-on-specific-wes-vcf-record-produced-by-gatk4-1-0-java-1-8-0-45/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5783:62,error,error,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5783,1,['error'],['error']
Availability,"Please see <https://github.com/broadinstitute/gatk/issues/3030> for the errors I encountered when running this tool. I have not been able to run the tool successfully and so ask what are we missing in the documentation that will help users get this tool running. Otherwise, I recommend labeling this tool experimental as well as in BETA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3031:72,error,errors,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3031,1,['error'],['errors']
Availability,Plotting throws the wrong error when given reference fasta instead of dict,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2941:26,error,error,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941,1,['error'],['error']
Availability,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ​; ​@ArgumentCollection; ​public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. ​. .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; ​@ArgumentCollection; ​public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; ​@Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); ​public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213:1238,error,error,1238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213,1,['error'],['error']
Availability,Poisson regression is not robust to outliers and leads to wrong inferences in TargetCoverageSexGenotyper,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3015:26,robust,robust,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015,1,['robust'],['robust']
Availability,"Port VQSR tests, slimming down as necessary to achieve reasonable runtime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2063:26,down,down,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2063,1,['down'],['down']
Availability,"Port https://github.com/broadgsa/gatk-protected/pull/24; ```; Without this patch the stream is only closed (thus, flushed) when the; object is garbage collected. This is problematic when subsequent jobs; proceed and expect the output to be available, for example; AnalyzeCovariates. We see failures in approximately 50% of runs due to; this issue and they are confirmed as fixed when applying the patch (on; a busy machine using NFS storage).; ```. The tool `BaseRecalibratorSparkSharded` is affected. The fix will be in `BaseRecalibratorEngineSparkWrapper.saveTextualReport`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3161:240,avail,available,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3161,2,"['avail', 'failure']","['available', 'failures']"
Availability,"Port the remainder of tools in picard.sam, excluding ViewSam (redundant with PrintReads) and SplitSamByLibrary (see https://github.com/broadinstitute/hellbender/issues/140). . Note that new unit tests will have to be written for some of these tools (see https://github.com/broadinstitute/hellbender/issues/144).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/156:62,redundant,redundant,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/156,1,['redundant'],['redundant']
Availability,Ported from Classic-GATK: Assembly not robust to changes in active region boundaries,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/267:39,robust,robust,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/267,1,['robust'],['robust']
Availability,"PostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:5995,down,down,5995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['down'],['down']
Availability,PostprocessGermlineCNVCalls error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8776:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776,1,['error'],['error']
Availability,PostprocessGermlineCNVCalls errors on chunk consisting of random and unplaced contigs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:28,error,errors,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,1,['error'],['errors']
Availability,PreprocessIntervals contig start position error?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7009:42,error,error,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7009,1,['error'],['error']
Availability,"Prevent a git lfs error that was caused by accidentally checking storing a .gitattributes file; in gitlfs. When checkout out the repository for the first time or moving from an old commit to a newish one, there's been an error report from git lfs. This was caused by accidentally checking a .gitattributes file into git-lfs which then is read as part of the git lfs checkout process, but since the file is tracked by lfs at the point of checkout it is an lfs stub and throws an error. The problem was introduced here: #6694. See below to reproduce:; ```; git checkout 9951f77c6; git checkout f548ccd708009ddcdfead6525edd23a68d73027b; https://git-lfs.github.com/spec/v1 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:1; sha256:cb156adb10b491dd3ba88c2b491bfb021b3c94fc956d36310c67492504fcdc58 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:2; Updating files: 100% (363/363), done.; Note: switching to 'f548ccd708009ddcdfead6525edd23a68d73027b'.; ```. This fixes the problem going forward by removing the file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7594:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594,3,['error'],['error']
Availability,"PrimaryAlignmentReadFilter filters out secondary alignments, but not supplementary alignments (this is what GATK3 does as well). This doesn't match the SAM spec (though it does match the language used by htsjdk); is redundant given that in GATK4 we have a NotSecondaryAlignmentReadFilter for use with InsertSizeMetricsCollector; and is likely confusing. Proposed change is to visit the usages of the PrimaryAlignmentReadFilter (BaseRecalibrator, Pileup, CalculateTargetBaseCallCoverage and HaplotypCaller) and for any that don't want supplementary alignments filtered, replace the filter with NotSecondary, and for those that do, change Primary to also reject supplementary reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2165:216,redundant,redundant,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2165,1,['redundant'],['redundant']
Availability,"PrintReadsSpark gives ""htsjdk.samtools.SAMFormatException: Invalid GZIP header"" error with WGS bam and interval file",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:80,error,error,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['error'],['error']
Availability,PrintReadsSpark log4j error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:22,error,error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['error'],['error']
Availability,"PrintReadsSpark throws error ""Invalid splitting BAM index"" on writing BAM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:23,error,error,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,1,['error'],['error']
Availability,"Prior to assembly (in `AssemblyBasedCallerUtils.assembleReads`, we transform reads in several ways that are meant to be permanent (that is, we want to use them in both assembly and genotyping) within `finalizeRegion`. (Additionally, we error reads within `ReadThreadingAssembler.runLocalAssembly`, but this is done on temporary copies of reads that are used for kmers and discarded). These transformations include hard clipping low-quality ends, adaptor sequences, and, optionally, soft-clipped bases, as well as correcting the base qualities of overlapping mates. According to the git history, these transformations have been accidentally temporary for quite a while. Let's look at the relevant code. First, in `Mutect2Engine.callRegion` we have (comments added and code simplified for clarity). ```; final AssemblyRegion assemblyActiveRegion = AssemblyBasedCallerUtils.assemblyRegionWithWellMappedReads(originalAssemblyRegion . . .);. // assembleReads finalizes region, modifying reads as a side effect; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(assemblyActiveRegion. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(MTAC.maxMnpDistance);. // when we trim on the originalAssemblyRegion, the trimmingResult takes its un-modified reads!; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(originalAssemblyRegion, allVariationEvents, referenceContext);. // now the assemblyResult gets the unmodified reads of the trimmingResult!; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. If we want things like `-dont-use-soft-clipped-bases` to work, we should call `trimmer.trim` on `untrimmedAssemblyResult`. I think that change alone may be all we need. Let's look at the corresponding code in `HaplotypeCallerEngine`:. ```; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(region. . .);.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6686:236,error,error,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6686,1,['error'],['error']
Availability,"Processing an exome takes ~1 minute, which means most of the time is spent on spinning up a VM, pulling docker images, etc. This is not very cost efficient. This PR allows for a `batch_size` to be set and then each task processes that many samples as a unit. The default is `1` which yields the current behavior, but in exomes I have set it to 20 and seen the cost to ingest drop dramatically. The GitHub PR makes it look like a lot has changed but really the changes are:; - a new parameter; - a new task to turn the Array[File] for the VCFs into set of FOFNs (file-of-file-names) similar to how we split up intervals; - a loop in the actual Create TSV task to loop over the files in the FOFNs. For SA mode we copy down each file, and for non-SA mode we rely on the fact that localization is optional and we read them directly anywy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7382:716,down,down,716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7382,1,['down'],['down']
Availability,Promote gradle build change that helps with certain spark config errors during build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1447:65,error,errors,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447,1,['error'],['errors']
Availability,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2740:18,redundant,redundantly,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740,1,['redundant'],['redundantly']
Availability,Provide an easy mechanism for downstream toolkits to customize CommandLineProgram output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101:30,down,downstream,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101,1,['down'],['downstream']
Availability,"Providing a bad value for an argument is the definition of a ""user exception"" (an error that is the user's fault). If we can't make it a `UserException` directly because of the gatk/barclay split, then we should at least wrap these exceptions in `UserException` at the top level catch, and ensure that a full error message gets printed for the user for these exceptions. . Currently I think `CommandLineException` can be silently caught and ignored with no output due to a bug in `Main.mainEntry()`:. ```; protected final void mainEntry(final String[] args) {; try {; final Object result = instanceMain(args);; handleResult(result);; System.exit(0);; } catch (final CommandLineException e){; //the usage has already been printed so don't print it here.; if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; System.exit(COMMANDLINE_EXCEPTION_EXIT_VALUE);; } catch (final UserException e){; CommandLineProgram.printDecoratedUserExceptionMessage(System.err, e);. if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; ```. We should fix this as part of this ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324:82,error,error,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324,3,"['error', 'fault']","['error', 'fault']"
Availability,"Pulls down a temp table of genotype counts, calculates excess het and call rate and writes them to a tsv for future upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6812:6,down,down,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6812,1,['down'],['down']
Availability,Pushing MLLib down to public to make it easier for people to use with GATK4. Took SVD along for the ride (to test that it works).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1846:14,down,down,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1846,1,['down'],['down']
Availability,"Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MBQ,Number=A,Type=Integer,Description=""median base quality"">; ##FORMAT=<ID=MFRL,Number=R,Type=Integer,Description=""median fragment length"">; ##FORMAT=<ID=MMQ,Number=A,Type=Integer,Description=""median mapping quality"">; ##FORMAT=<ID=MPOS,Number=A,Type=Integer,Description=""median distance from end of read"">; ##FORMAT=<ID=OBAM,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact modes."">; ##FORMAT=<ID=OBAMRC,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact mode complements."">; ##FORMAT=<ID=OBF,Number=A,Type=Float,Description=""Fraction of alt reads indicating orientation bias error (taking into account artifact mode complement)."">; ##FORMAT=<ID=OBP,Number=A,Type=Float,Description=""Orientation bias p value for the given REF/ALT artifact or its complement."">; ##FORMAT=<ID=OBQ,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for a given REF/ALT error."">; ##FORMAT=<ID=OBQRC,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for the complement of a given REF/ALT error."">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SA_MAP_AF,Number=3,Type=Float,Description=""MAP estimates of allele fraction given z"">; ##FORMAT=<ID=SA_POST_PROB,Number=3,Type=Float,Description=""posterior probabilities of the presence of strand artifact"">; etc..; etc..; etc..; 1 237752 . A G . artifact_in_n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5158:3258,error,error,3258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5158,1,['error'],['error']
Availability,"R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""L",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7315:1425,error,error,1425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315,1,['error'],['error']
Availability,"R addresses two phasing bugs, https://github.com/broadinstitute/gatk/issues/6463 and https://github.com/broadinstitute/gatk/issues/6845. https://github.com/broadinstitute/gatk/issues/6463 identified a bug in the phasing algorithm which caused the wrong phase information to be output for scenarios where the first variant in a phase set is homozygous variant and it is followed by het variants in opposite phase. Without this change the het variants were incorrectly placed on the same phase strand because the phase set was tied to the hom var variant, and the algorithm assumed that each het variant could be put in the same phase strand as it because it was on all haplotypes. I've modified the algorithm to keep track, for variants that occur on all haplotypes, of which of the haplotypes have already been used for phasing an upstream ""comp"" variant so that further downstream variants can be checked against the remaining set. https://github.com/broadinstitute/gatk/issues/6845 showed an example of phase sets being disrupted by the presence of an alternate haplotype that supported an additional, uncalled, variant in the region. In this case there was an alternate haplotype supported by two reads that supported a SNP downstream of two pairs of SNPs in alternate phase. The presence of an additional haplotype causes the phasing algorithm to break the phase sets in the region. I've modified the algorithm to only use haplotypes that support the alternate alleles present in called variants in phasing by modifying the number that we pass as `AssemblyBasedCallerUtils.constructPhaseSetMapping()`'s `totalAvailableHaplotypes` parameter. In my opinion this ; fix produces output that is still correct and is much easier to understand (since it only depends on sites that are visible in the output VCF), but if anyone objects to this change please let me know. . Non-test code changes for this PR are in two different commits to try to make it easier to understand the scope of the two changes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7019:1233,down,downstream,1233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7019,1,['down'],['downstream']
Availability,RAW_MQ/sumSquaredMQs parsing error when running GenotypeGVCFs for JointGenotyping,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5433:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5433,1,['error'],['error']
Availability,"RK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port. But when i run other commands with gatk like gatk --list, i got this error :; gatk --list; Using GATK jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; 	java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8280:2071,error,error,2071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280,2,"['Error', 'error']","['Error', 'error']"
Availability,"ROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in stage 25.0 (TID 43224, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:5973,down,down,5973,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['down'],['down']
Availability,R] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:14447,ERROR,ERROR,14447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"R_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); 	at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.java:197); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:236); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:3266,down,down,3266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['down'],['down']
Availability,"Ran into this when trying to create a PoN with 100 samples x 100 bp bins = 1.27 * max int elements. This currently causes issues when truncating outliers, at which point all elements are loaded into an array so that Percentiles can be naively computed, resulting in a `java.lang.NegativeArraySizeException`. Solutions include: 1) simply throwing a message and failing early if the counts matrix is too large (perhaps recommend scattering by contig, see #4728), 2) changing the outlier truncation procedure to be more robust. I'm not sure how important outlier truncation is to the SVD, as it remains to be evaluated, but for now we should be able to get around this with no code changes by simply disabling it (i.e., setting the relevant truncation percentile to 0). Note that file I/O takes about an hour for this case. Also note that this is probably on the extreme end of what we should expect to support on a single machine with all counts in memory, as the SVD is probably sufficiently good with 100 samples and 100 bp is on the order of the read length. #4728 will get around this and also make downstream tasks complete faster in parallel, at the very small expense of reducing a few global parameters to per-contig parameters in the modeling step.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4734:517,robust,robust,517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4734,2,"['down', 'robust']","['downstream', 'robust']"
Availability,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:78,error,errors,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,6,"['Error', 'error', 'reliab']","['Error', 'error', 'errors', 'reliable']"
Availability,"Read error correction tends to follow the basic strategy of 1) collect kmer counts 2) replace rare kmers with their closest non-rare match. For germline calling where there is a huge gap between error rates and diploid het allele fractions this is sufficient. Mutect, however, must contend with cases where counts alone do not discriminate perfectly between errors and real mutations. Without committing to an approach, it seems like phasing might help. That is, we could construct haplotypes of rare kmers and error correct those. This should work because sequencing errors are unphased and real variants are. There are phased artifacts, of course, but we handle those in downstream filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4868:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868,6,"['down', 'error']","['downstream', 'error', 'errors']"
Availability,"ReadFilter system needs to be ported from GATK. It should be available to tools by ""request"". The specifics are to be figured out as part of addressing this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5:61,avail,available,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5,1,['avail'],['available']
Availability,ReadTransformer system needs to be available to hellbender tools. The mechanism of how it gets enabled needs to be coordinated with ReadFilters (issue #5 ),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6:35,avail,available,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6,1,['avail'],['available']
Availability,Reader; at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:463); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:365); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291); at org.broadinstitute.hellbender.engine.VariantLocusWalker.initialize at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFava:726); at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: GenomicsDB JNI Error: Broad combine GVCFs exception : No sample/CallSet name specified in JSON file/Protobuf object for TileDB row 72; at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:460). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8527:7436,Error,Error,7436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527,1,['Error'],['Error']
Availability,"Reading from GenomicsDB fails when a some records containing spanning deletion alleles are imported into a workspace. Not all records seem to cause this to fail; I haven't been able to figure out what specific properties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4716:246,error,error,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716,2,['error'],['error']
Availability,Readme: Git LFS download size not accurate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6932:16,down,download,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6932,1,['down'],['download']
Availability,"Reads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:28.284 INFO PrintReads - Deflater: IntelDeflater; 15:00:28.284 INFO PrintReads - Inflater: IntelInflater; 15:00:28.285 INFO PrintReads - GCS max retries/reopens: 20; 15:00:28.285 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:00:28.285 INFO PrintReads - Initializing engine; 15:00:33.117 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 15:00:33.134 INFO PrintReads - Shutting down engine; [October 5, 2017 3:00:34 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=2255486976; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; -bash-4.1$ ; ```. ## Confirm all files present in bucket; ```; WMCF9-CB5:newCNV shlee$ gsutil ls gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN*; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.bam.bas; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:23908,ERROR,ERROR,23908,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['ERROR'],['ERROR']
Availability,Realignment in Mutect2 reports error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6123:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6123,1,['error'],['error']
Availability,"RecalibratorEngine.java:43); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:625); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I believe this is derived from an error earlier in the log, since the `stderr` gives the same Java heap space error: ; ```; [2019-09-16 19:05:59,50] [error] WorkflowManagerActor Workflow 9f7a01a4-0632-4817-8622-aa51e520abf1 failed (during ExecutingWorkflowState): Job JointGenotyping.SNPsVariantRecalibratorClassic:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /path/to/stderr.; ```. I have read past issues (https://gatkforums.broadinstitute.org/gatk/discussion/23880/java-heap-space) regarding this that may suggest it is a bug. It has pointed me to increasing the available heap memory through the primary command of -Xmx. Is this the way to do it? ; ```; java -Xmx600G -Dconfig.file=' + re.sub('input.json', 'overrides.conf', input_json) + ' -jar ' + args.cromwell_path + ' run ' + re.sub('input.json', 'joint-discovery-gatk4.wdl', input_json) + ' -i ' + input_json; ```; where I substitute in the corresponding config, json, and wdl files. . Is 600G enough? Each vcf is around 6G large and since I have 150, does that mean I should be allocating more than 900G (6G x 150)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165:2460,avail,available,2460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165,1,['avail'],['available']
Availability,"Recently I was setting up GATK to run in a VM and I had forgotten to install Java8 onto the machine. When I tried to run GATK from the launch script I ran into the following error: ; ```; Using GATK jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar -help; Traceback (most recent call last):; File ""./gatk"", line 479, in <module>; main(sys.argv[1:]); File ""./gatk"", line 152, in main; runGATK(sparkRunner, sparkSubmitCommand, dryRun, gatkArgs, sparkArgs, javaOptions); File ""./gatk"", line 328, in runGATK; runCommand(cmd, dryrun); File ""./gatk"", line 384, in runCommand; check_call(cmd, env=gatk_env); File ""/usr/lib/python2.7/subprocess.py"", line 181, in check_call; retcode = call(*popenargs, **kwargs); File ""/usr/lib/python2.7/subprocess.py"", line 168, in call; return Popen(*popenargs, **kwargs).wait(); File ""/usr/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ```; This should perhaps be made a little bit clearer for users as this isn't particularly helpful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5992:174,error,error,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5992,1,['error'],['error']
Availability,"Recently ran into an issue where spark.yarn.executor.memoryOverhead is not being set. . Running Cromwell v29 on DataProc (image version 1.1) with the following launch command in my WDL:; ````; set -eu; export GATK_GCS_STAGING=${jarCacheBucket}; ${gatk} \; PathSeqPipelineSpark \; ...; -- \; --spark-runner GCS \; --cluster ${clusterName} \; --driver-memory 8G \; --executor-memory 32G \; --num-executors 1 \; --executor-cores 30 \; --conf spark.yarn.executor.memoryOverhead=132000; ````; I get the following error:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 37.0 failed 4 times, most recent failure: Lost task 25.3 in stage 37.0 (TID 19238, mw-pathseq-w-3.c.broad-dsde-methods.internal): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 48.9 GB of 34 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; In my Cromwell log I see the parameter is there:; ```; ...; Replacing spark-submit style args with dataproc style args. --cluster mw-pathseq --driver-memory 8G --executor-memory 32G --num-executors 1 --executor-cores 30 --; conf spark.yarn.executor.memoryOverhead=132000 -> --cluster mw-pathseq --properties spark.driver.userC; lassPathFirst=true,spark.io.compression.codec=lzf,spark.driver.maxResultSize=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.driver.memory=8G,spark.executor.memory=32G,spark.executor.instances=1,spark.execut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4273:508,error,error,508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4273,3,"['error', 'failure']","['error', 'failure']"
Availability,Reduce the logging a bit.; Probably should make a PR directly into gatk master so that when we next merge gatk master changes we'll get this goodness?. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8c38f97-7945-414f-9432-13b2f12138bb) (note failed one of the subtests for a random docker pull error); Example CreateFilterSet run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b2e7eb86-e494-4891-885b-5a96cb1056b3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8650:343,error,error,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8650,1,['error'],['error']
Availability,Redundant implementations of dotProduct(),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3311:0,Redundant,Redundant,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3311,1,['Redundant'],['Redundant']
Availability,Redundant representations of genotypes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1907:0,Redundant,Redundant,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1907,1,['Redundant'],['Redundant']
Availability,Reenable CRAM tests in GatherBamFilesIntegrationTest and SortSamIntegrationTests when htsjdk issue #365 fix is available,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1141:111,avail,available,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1141,1,['avail'],['available']
Availability,Refactor AlleleListUtilsUnitTest to have no skips and be robustly deterministic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/607:57,robust,robustly,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/607,1,['robust'],['robustly']
Availability,Reg error in DetermineGermlineContigPloidy,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['error'],['error']
Availability,"Related to VS-1395. On the PR to EchoCallset, Bec had a couple of minor suggestions. I folded them into that PR and am now adding them here to `ah_var_store`. Passing test run without a backslash on output_path [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/f02829f5-d546-4f54-85e6-e7ea6be5829e).; Passing test run with a backslash on output_path [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/3003841c-0e62-4ef6-bb37-6eeb038da9af).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8881:33,Echo,EchoCallset,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8881,1,['Echo'],['EchoCallset']
Availability,"Related to https://github.com/broadgsa/gatk/pull/22.; Port of https://github.com/broadinstitute/gsa-unstable/pull/1606. Also replaced --allowMissingData with --errorIfMissingData since we'd rather not fail by default for missing data.; The root cause of the problem is due the implementation of `Genotype.hasAnyAttribute`. It always returns true for FT, even if it's missing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3252:160,error,errorIfMissingData,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3252,1,['error'],['errorIfMissingData']
Availability,Relax CRAN remote failure mode.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5602:18,failure,failure,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5602,1,['failure'],['failure']
Availability,Remove GATK classes that are redundant with Picard classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6678:29,redundant,redundant,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6678,1,['redundant'],['redundant']
Availability,Remove download of picard.jar from .travis.yml and update Mutect2 WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3625:7,down,download,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3625,1,['down'],['download']
Availability,Remove file causing git-lfs error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7594:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594,1,['error'],['error']
Availability,"Remove guava-jdk5 dependency to fix ""stopWatch"" error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/867:48,error,error,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/867,1,['error'],['error']
Availability,Remove redundant references from test data now that we have full-sized references,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5313:7,redundant,redundant,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5313,1,['redundant'],['redundant']
Availability,"Remove the code in `org.broadinstitute.hellbender.utils.gene`, because it is redundant with the code in Picard `picard.annotation`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3695:77,redundant,redundant,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3695,1,['redundant'],['redundant']
Availability,Remove/repair bogus CRAM test files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1368:7,repair,repair,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1368,1,['repair'],['repair']
Availability,Removed mapping error rate from estimate of denoised copy ratios output by gCNV and updated sklearn.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7261:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261,1,['error'],['error']
Availability,Removing the error check in RMSMappingQuality.getNumReads which was causing a crash when depth was being reported as 0 or less.; It will now return -1 which should match gatk3 behavior.; Fixes #2658,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2669:13,error,error,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2669,1,['error'],['error']
Availability,Repair the CRAM detector/diagnostics test output to reflect the CRAM file name change that was introduced by updating the large CRAM files to v3.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8971:0,Repair,Repair,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8971,1,['Repair'],['Repair']
Availability,"Repeated runs of the mentioned tool results in different number of kmers and templates discovered, shown as below, which sometimes result in the number of variants discovered downstream to be slightly varying as well (so fart only the number of `DEL`s are observed to be affected). ```; 01:30:55.991 INFO FindBreakpointEvidenceSpark - Discovered 32911079 kmers.; 01:35:37.667 INFO FindBreakpointEvidenceSpark - Discovered 30639344 unique template names for assembly.; 02:03:37.992 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4910 variants.; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2777; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DUP: 954; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INS: 975. 02:09:24.685 INFO FindBreakpointEvidenceSpark - Discovered 32911147 kmers.; 02:14:21.462 INFO FindBreakpointEvidenceSpark - Discovered 30637728 unique template names for assembly.; 02:44:21.009 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4906 variants.; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2773; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DUP: 954; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INS: 975. 03:54:34.062 INFO FindBreakpointEvidenceSpark - Discovered 32911130 kmers.; 03:59:31.807 INFO FindBreakpointEvidenceSpark - Discovered 30637959 unique template names for assembly.; 04:31:05.403 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4907 variants.; 04:31:05.416 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 04:31:05.416 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2774; 04:31:05.416 INFO DiscoverStructuralVa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2600:175,down,downstream,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2600,1,['down'],['downstream']
Availability,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6088:577,down,downsampling,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088,6,"['down', 'reliab']","['downsampling', 'reliable']"
Availability,Reporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:6258,ERROR,ERROR,6258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4642:189,error,errors,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642,5,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"Request created from: ""Did not inflate expected amount"" Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:56,Error,Error,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['Error'],['Error']
Availability,Request: fine-grained configuration for codec packages for downstream projects,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:59,down,downstream,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,1,['down'],['downstream']
Availability,"Requested on the forum from a researcher.; ___; Hi GATK team,. I just wanted to let you know, that there is a minor mistake in the documentation here:; https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_haplotypecaller_HaplotypeCaller.php#--pcr-indel-model. The file in which you want to replace --genotyping_mode with --genotyping-mode is this one:; gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/StandardCallerArgumentCollection.java. I don't seem to be able to create a pull request, so I'll leave it to you. Thanks,; Tommy. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23442/minor-error-in-the-documentation-regarding-genotyping-mode/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5657:741,error,error-in-the-documentation-regarding-genotyping-mode,741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5657,1,['error'],['error-in-the-documentation-regarding-genotyping-mode']
Availability,"Researcher reports error in GenotypeGVCFs that uses a GenomicsDB database using v4.0.5.0. Removing `new qual` param OR using v4.0.4.0 allows the command to run without error. I saw a similar error with v4.0.5.1 when I tried to add `new qual` to a workshop hands-on tutorial GenotypeGVCFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975,4,['error'],['error']
Availability,"Researcher reports that GATK4 VariantAnnotator errors with a PED file that works fine with CalculateGenotypePosteriors. The same VariantAnnotator command and PED file work fine in GATK3. ---; Hi @shlee ,. We have ped files. And it worked fine for CalculateGenotypePosteriors too. But not with VariantAnnotator. So we downraded to GATK3 with same files on same parameters in order to finish that part - we hope calculations are same. Thank You!; Sergey. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/50327#Comment_50327",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4987:47,error,errors,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987,2,"['down', 'error']","['downraded', 'errors']"
Availability,"Resolves https://github.com/broadinstitute/dsp-spec-ops/issues/239. See README.md in this PR for full details. ----; To make this easier to review, the changes break down into a few sections. 1. Docs -- the README.md. Does it make sense? Could you follow it?. 2. Comparison Script (compare_data.py)-- is it clear? Obvs any bugs would be great. The Github Issue for this PR describes _what_ is compared. 3. WDL changes -- should be straightforward to review, just minor changes; ; 4. Code changes (java) -- we can walk through this together if that's more effective",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7106:166,down,down,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7106,1,['down'],['down']
Availability,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5895:457,Failure,Failures,457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895,1,['Failure'],['Failures']
Availability,"Return NA if VariantsToTable is printing a null object, replace --allowMissingData with --errorIfMissingData",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3252:90,error,errorIfMissingData,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3252,1,['error'],['errorIfMissingData']
Availability,Revert exposure of optional parameters in CNV WDLs when Cromwell support is available.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4287:76,avail,available,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4287,1,['avail'],['available']
Availability,"Reverting the update to gkl 0.3.1 for now because we've encountered some downstream errors that are making it impossible to update gatk-protected. This reverts commit 9e3c6e3d7370c503d2a57be0c662fb1016d8b764, reversing; changes made to 767974906e91c90079cefa4512b463138ca09f68.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2319:73,down,downstream,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2319,2,"['down', 'error']","['downstream', 'errors']"
Availability,"Reverts VS-569 now that Temurin downloads are working again, leaves in the Corretto breadcrumbs and minor improvements like `-o xtrace`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7972:32,down,downloads,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7972,1,['down'],['downloads']
Availability,"Reverts the reversion in #5225, this time addressing the lexicographical ordering issue in #5217 at the WDL level by simply renaming gCNV output at the command line. If desired, we can eventually change gCNV itself to output filenames that are robust against lexicographic ordering, but this is low priority in my opinion. @vruano this is what we discussed last week. Tests pass on Travis, and I'm pretty sure this fix should work OK, but I have not done an actual run with enough samples to see the fix in action. Can I assign you to review once I get a chance to do this?. EDIT: Also went ahead and rolled an older PR #5304 into this one so I can test both at the same time. Closes #4724.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5490:244,robust,robust,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490,1,['robust'],['robust']
Availability,"Right now it is not available (even after #3457).; Though the VCF spec doesn't seem to specify if it is mandated, it makes it easier for parsing and making sense of the event. Note that this is different from `PARID`, which, unlike BND that symbols novel adjacency, symbols ; ""novel"" disruptions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3508:20,avail,available,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3508,1,['avail'],['available']
Availability,"Right now the alignments are filtered in various places in SV discovery stage. ; Having a single logic unit for doing this makes; * debugging and modular development, ; * complex SV discovery/interpretation, and ; * future improvements (e.g. not filtering but downgrading certain alignments and use an optimization approach). easier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3221:260,down,downgrading,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3221,1,['down'],['downgrading']
Availability,Right now we get the following error message when we don't supply sufficient memory:. ```; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 137; ```. Ideally we would report out of memory message explicitly. Perhaps we can just catch that exception and output message that this error is likely due to insufficient memory. @cmnbroad Do you have any thoughts on this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6362:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6362,2,['error'],['error']
Availability,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1481:291,down,downstream,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481,1,['down'],['downstream']
Availability,"Running GATK4 BwaSpark encounter the following fatal error message:; `[M::mem_sam_pe] Paired reads have different names: ""206B4ABXX100825:7:66:2632:21260"", ""206B4ABXX100825:7:66:2632:31752""`. Script: ; `$GATK_LAUNCH BwaSpark -I $unsorted_bam_hdfs -O $sorted_bam_hdfs -t 10 --disableSequenceDictionaryValidation true -R $ref_hdfs -K 10000000 -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-cores 10 --executor-memory 40g`. $unsorted_bam_hdfs is a file generated by FastqToBam, and copied to HDFS. ; spark 2.0 is used. . The original Fastq files are perfectly fine, and we have been using it for all our tests using previous versions, including 3.6. I also manually checked the generated name-sorted BAM file generated by FastToBam, and the neighboring lines are perfectly paired as well. . What I suspect is that chunk is cut inside a pair, and thus not just this one, all subsequent lines are all error'ed out. To confirm this, I ran the job with different -K and -bps options, and the error will occur at different locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2296:53,error,error,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2296,3,['error'],['error']
Availability,Running `gatk --version` produces a dump of all the available tools and then a user exception.; ```. ... UpdateVcfSequenceDictionary (Picard) Takes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.; VariantAnnotator (BETA Tool) Tool for adding annotations to VCF files; VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. ***********************************************************************. A USER ERROR has occurred: '--version' is not a valid command. ***********************************************************************; ```; It should print the version instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5533:52,avail,available,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5533,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability,"Running a spark tool with a read input that doesn't exist on hdfs results in a confusing error message. ex: `hdfs://user/local/print_reads.sorted.bam` doesn't exist on the file system ; produces . ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; ```. Full command line to reproduce:. ```; spark-submit --master yarn-client --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 build/libs/gatk-all-4.pre-alpha-196-g94b53ee-SNAPSHOT-spark.jar PrintReadsSpark --sparkMaster yarn-client -I hdfs://user/local/print_reads.sorted.bam -O output.bam; ```. ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1257:89,error,error,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257,1,['error'],['error']
Availability,Running in local mode on a large GCE instance when using external evidence (it looks like any attempt to use external evidence will trigger this) results in the following error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Partition boundaries are not coordinate sorted.; 	at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.readExternalEvidence(FindBreakpointEvidenceSpark.java:309); 	at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getMappedQNamesSet(FindBreakpointEvidenceSpark.java:208); 	at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.gatherEvidenceAndWriteContigSamFile(FindBreakpointEvidenceSpark.java:111); 	at org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark.runTool(StructuralVariationDiscoveryPipelineSpark.java:79); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3717:171,error,error,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3717,1,['error'],['error']
Availability,"Running on the hg19/b37 NA12878 bam file, I'm getting the following exception in stage 0:. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 589 in stage 0.0 failed 4 times, most recent failure: Lost task 589.3 in stage 0.0 (TID 757, cwhelan-na12878-pcr--30x-bam-w-6.c.broad-dsde-methods.internal): java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGSchedul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:154,failure,failure,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,2,['failure'],['failure']
Availability,"Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25G -Xms25G -Djava.io.tmpdir=/Data/data/raja/cfdna/gatk-4.4.0.0/tmp -jar /Data/data/raja/cfdna/nextflow1/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar MarkDuplicatesSpark -I sout_hd.bam -O sout_hd.ctrl.bam.gz; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0; ![Screenshot_2](https://github.com/broadinstitute/gatk/assets/75623749/cdaca619-fc57-4f6b-b350-0402036c099c)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8596:386,Error,Error,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8596,1,['Error'],['Error']
Availability,SIGABRT/error running IntelInflaterDeflaterIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2535:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535,1,['error'],['error']
Availability,"SJDK Defaults.COMPRESSION_LEVEL : 2; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:36:35.392 INFO IndexFeatureFile - Deflater: IntelDeflater; 09:36:35.392 INFO IndexFeatureFile - Inflater: IntelInflater; 09:36:35.392 INFO IndexFeatureFile - GCS max retries/reopens: 20; 09:36:35.392 INFO IndexFeatureFile - Requester pays: disabled; 09:36:35.393 INFO IndexFeatureFile - Initializing engine; 09:36:35.393 INFO IndexFeatureFile - Done initializing engine; 09:36:35.502 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/user1/snp_allsamples.vcf.gz; 09:36:35.518 INFO ProgressMeter - Starting traversal; 09:36:35.518 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 09:36:36.979 INFO IndexFeatureFile - Shutting down engine; [March 21, 2024 at 9:36:36 a.m. CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=1241513984; java.lang.ArrayIndexOutOfBoundsException: Index 37451 out of bounds for length 37451; at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:102); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:92); at htsjdk.tribble.index.IndexFactory.createIndex(IndexFactory.java:529); at htsjdk.tribble.index.IndexFactory.createTabixIndex(IndexFactory.java:476); at htsjdk.tribble.index.IndexFactory.createTabixIndex(IndexFactory.java:502); at htsjdk.tribble.index.IndexFactory.createIndex(IndexFactory.java:403); at org.broadinstitute.hellbender.tools.IndexFeatureFile.createAppropriateIndexInMemory(IndexFeatureFile.java:109); at org.broadinstitute.hellbender.tools.IndexFeatureFile.doWork(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8747:2653,down,down,2653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747,1,['down'],['down']
Availability,"SV pipeline failure on CHM WGS1 with ""two input alignments' overlap on read consumes completely one of them.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:12,failure,failure,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['failure'],['failure']
Availability,"SYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:55:31.186 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:55:31.186 INFO CalibrateDragstrModel - Deflater: IntelDeflater; 13:55:31.186 INFO CalibrateDragstrModel - Inflater: IntelInflater; 13:55:31.186 INFO CalibrateDragstrModel - GCS max retries/reopens: 20; 13:55:31.186 INFO CalibrateDragstrModel - Requester pays: disabled; 13:55:31.187 WARN CalibrateDragstrModel -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CalibrateDragstrModel is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:55:31.187 INFO CalibrateDragstrModel - Initializing engine; 13:55:33.395 INFO CalibrateDragstrModel - Done initializing engine; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:14611,down,down,14611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['down'],['down']
Availability,"SYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:42:34.118 INFO VariantFiltration - Deflater: IntelDeflater; 15:42:34.119 INFO VariantFiltration - Inflater: IntelInflater; 15:42:34.119 INFO VariantFiltration - GCS max retries/reopens: 20; 15:42:34.119 INFO VariantFiltration - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:42:34.119 INFO VariantFiltration - Initializing engine; 15:42:34.634 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/sherlock/dev/Bhatt_lab/crassphage/variants/6753_12-15-2015_first_pass_filtered.vcf; 15:42:34.663 INFO VariantFiltration - Done initializing engine; 15:42:34.750 INFO ProgressMeter - Starting traversal; 15:42:34.750 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 15:42:34.781 INFO VariantFiltration - Shutting down engine; [June 15, 2018 3:42:34 PM PDT] org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=342884352; java.lang.NumberFormatException: For input string: ""26.67""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Long.parseLong(Long.java:589); 	at java.lang.Long.parseLong(Long.java:631); 	at org.apache.commons.jexl2.JexlArithmetic.toLong(JexlArithmetic.java:906); 	at org.apache.commons.jexl2.JexlArithmetic.compare(JexlArithmetic.java:718); 	at org.apache.commons.jexl2.JexlArithmetic.greaterThan(JexlArithmetic.java:790); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.java:796); 	at org.apache.commons.jexl2.parser.ASTGTNode.jjtAccept(ASTGTNode.java:18); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.java:449); 	at org.apache.commons.jexl2.parser.ASTAndNode.jjtAccept(ASTAndNode.java:18); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4921:3496,down,down,3496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921,1,['down'],['down']
Availability,Sample code to show how to modify INFO field combine operation while querying GenomicsDB using the Protobuf API. #4541 . ping @ldgauthier @droazen @lbergelson @jamesemery,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4993:121,ping,ping,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993,1,['ping'],['ping']
Availability,"Scheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_202408111100502620487673658411251_0021.; org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130); at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41); at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862); at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714); at org.apache.spark.util.Utils$$anon$2.write(Utils.scala:160); at com.esotericsoftware.kryo.io.Output.flush(Output.java:185); at com.esotericsoftware.kryo.io.Output.close(Output.java:196); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:8648,failure,failure,8648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['failure'],['failure']
Availability,"SchedulerImpl: Adding task set 2.0 with 2 tasks; 00:59 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, xx.xx.xx.25, executor 2, partition 0, PROCESS_LOCAL, 6010 bytes); 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.16:39037 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.25:41354 (size: 6.4 KB, free: 366.3 MB); **18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:24977,Error,Error,24977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5587:577,error,error,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587,1,['error'],['error']
Availability,See https://gatkforums.broadinstitute.org/gatk/discussion/23793/python-error-using-postprocessgermlinecnvcalls,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852:71,error,error-using-postprocessgermlinecnvcalls,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852,1,['error'],['error-using-postprocessgermlinecnvcalls']
Availability,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8107:126,failure,failures,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107,4,['failure'],"['failure', 'failures']"
Availability,"See some issues---mostly stemming from the HDF5 library and the BLAS library optionally used by MLlib SVD at e.g. https://gatkforums.broadinstitute.org/gatk/discussion/23591/createreadcountpanelofnormals-in-gatk4-1-doesnt-output-valid-hdf5-files#latest; https://gatkforums.broadinstitute.org/gatk/discussion/12537/get-error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation; https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv/p1. Would also be nice to to turn down the verbosity of Spark logging, which emits a ridiculous amount of messages for a simple SVD. I think this is a relatively ancient issue (https://github.com/broadinstitute/gatk/issues/1370), not sure if it's been resolved for other Spark tools since.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5771:318,error,error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5771,2,"['down', 'error']","['down', 'error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation']"
Availability,See the log: https://people.freebsd.org/~pi/logs/gatk-4.1.2.0.43.log. The downstream bug report: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=239901,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6097:74,down,downstream,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6097,1,['down'],['downstream']
Availability,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514:14,failure,failure,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514,3,"['Error', 'error', 'failure']","['Error', 'errors', 'failure']"
Availability,SelectVariants fails with no errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6990:29,error,errors,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6990,1,['error'],['errors']
Availability,"ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@642ec6{/stages/stage/kill,null,AVAILABLE,@Spark}; 10:33:07.389 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3fe5ad73{/metrics/json,null,AVAILABLE,@Spark}; 10:33:07.397",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:46918,AVAIL,AVAILABLE,46918,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,Set channel reopen option and retry options for robustness,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506:48,robust,robustness,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506,1,['robust'],['robustness']
Availability,Set locale to US in Main is not applied to downstream projects Main classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483:43,down,downstream,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483,1,['down'],['downstream']
Availability,"Several experimental changes that improve precision results, and expand possible evaluations, of GATK CNV:. - `combine_tracks.wdl` for post-processing somatic CNV calls. This wdl will perform two operations:; - Increase precision by removing:; - germline segments. As a result, the WDL requires the matched normal segments.; - Areas of common germline activity or error from other cancer studies.; - Convert the tumor model seg file to the same format as AllelicCapSeg, which can be read by ABSOLUTE. This is currently done inline in the WDL. ; - This is not a trivial conversion, since each segment must be called whether it is balanced or not (MAF =? 0.5). The current algorithm relies on hard filtering and may need updating pending evaluation.; - For more information about AllelicCapSeg and ABSOLUTE, see: ; - Carter et al. *Absolute quantification of somatic DNA alterations in human cancer*, Nat Biotechnol. 2012 May; 30(5): 413–421 ; - https://software.broadinstitute.org/cancer/cga/absolute ; - Brastianos, P.K., Carter S.L., et al. *Genomic Characterization of Brain Metastases Reveals Branched Evolution and Potential Therapeutic Targets* (2015) Cancer Discovery PMID:26410082. - Changes to GATK tools to support the above:; - `SimpleGermlineTagger` now uses reciprocal overlap to in addition to breakpoint matching when determining a possible germline event. This greatly improved results in areas near centromeres.; - Added tool `MergeAnnotatedRegionsByAnnotation`. This simple tool will merge genomic regions (specified in a tsv) when given annotations (columns) contain exact values in neighboring segments and the segments are within a specified maximum genomic distance. . - `multi_combine_tracks.wdl` and `aggregate_combine_tracks.wdl` which run `combine_tracks.wdl` on multiple pairs and combine the results into one seg file for easy consumption by IGV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5252:364,error,error,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5252,1,['error'],['error']
Availability,"Several users have run into this issue where GenomicsDBImport errors out due to duplicate fields in their Info, Format, and/or Filter fields. They want to be able to run GenomicsDBImport without having to manually alter their files to remove duplicates. . This is the latest issue reported Sept 6. Here are some others I found that may be related: ; - [Feb 6](https://github.com/bcbio/bcbio-nextgen/issues/2674 ) ; - [April 5](https://gatkforums.broadinstitute.org/gatk/discussion/23824/gatk-4-1-1-0-genomicsdbimport-error-duplicate-field-name-af-found-in-vid-attribute-fields) ; - [July 11](https://gatkforums.broadinstitute.org/gatk/discussion/24212/gatk-4-1-1-0-genomicsdbimport-error-duplicate-fields-exist-in-vid-attribute-fields-and-2-errors). GATK version is 4.1.2.0. **_Command: _**; gatk_megs=$(head -n1 /proc/meminfo | awk '{print int(0.9*($2/1024))}'); ; gatk --java-options ""-Xmx${gatk_megs}m"" GenomicsDBImport --genomicsdb-workspace-path pon_db -V GHS_PT100006_233694007.gvcf.gz -L xgen_plus_spikein.b38.bed --batch-size 50 --reader-threads 5 --tmp-dir=./tmp2. **_Error log:_**; Using GATK jar /mnt/PoN_gvcf/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx14441m -jar /mnt/PoN_gvcf/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenomicsDBImport --genomicsdb-workspace-path pon_db -V GHS_PT100006_233694007.gvcf.gz -L xgen_plus_spikein.b38.bed --batch-size 50 --reader-threads 5 --tmp-dir=./tmp2; 16:20:56.770 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/PoN_gvcf/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:20:57.244 INFO GenomicsDBImport - ------------------------------------------------------------; 16:20:57.244 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.2.0; 16:20:57.245 INFO GenomicsDBImport - For support and do",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6158:62,error,errors,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6158,4,['error'],"['error-duplicate-field-name-af-found-in-vid-attribute-fields', 'error-duplicate-fields-exist-in-vid-attribute-fields-and-', 'errors']"
Availability,"Should the CreateHadoopBamSplittingIndex tool also work on a cram? I am getting this error below which suggests not. What are the benefits of a SplittingIndex for a spark job? On average-how long should it take a spark job to get the splits for a 30x bam or cram? . ```; gatk CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 11:47:53.243 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - The Genome Analysis Toolkit (GATK) v4.0.1.1; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.10.3.el6.x86_64 amd64; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Start Date/Time: March 7, 2018 11:47:52 AM EST; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506:85,error,error,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506,1,['error'],['error']
Availability,Should throw an error if the client tries to write to TileDB in such a way as would corrupt an instance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2536:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2536,1,['error'],['error']
Availability,Shrink NIO buffers sizes for GenomicsDBImport down to the smallest values that still give good performance,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640:46,down,down,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640,1,['down'],['down']
Availability,"Similar to AD, the new annotation (DD) captures the depth of each allele supporting evidence; or reads, however it does so by following a variational Bayes approach looking into the; likelihoods rather than applying a fix threshold. . This turns out to be more robust in some instances. To get the new non-standard annotation in HC you need to add -A AllelePseudoDepth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7303:261,robust,robust,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7303,1,['robust'],['robust']
Availability,"Since the Picard changes in #3620, SamAssertionUtils has been failing silently. See e.g. the Standard error tab for https://storage.googleapis.com/hellbender-test-logs/build_reports/13120.7/tests/test/classes/org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.html:. ```; USAGE: SortSam [arguments]; ...; input is not a recognized option; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3664:102,error,error,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3664,1,['error'],['error']
Availability,"Small PR containing fixes for various issues:; - Move CompareSAMs to picard package (fixes https://github.com/broadinstitute/hellbender/issues/139); - Move most of `CompareSAMs.doWork()` into a separate public method, to be used by external unit tests; - Use HTSJDK's SamFileValidator in assorted unit tests, rather than ValidateSamFile (which is just a CLP wrapper); - Insert `--VERBOSITY ERROR` into CommandLineProgramTest, which suppresses most logging output for CLPs that use HTSJDK-based logging (fixes https://github.com/broadinstitute/hellbender/issues/134)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/171:390,ERROR,ERROR,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/171,1,['ERROR'],['ERROR']
Availability,"Small test that demonstrates numerical error bug, and fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/878:39,error,error,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/878,1,['error'],['error']
Availability,Smith-Waterman parameters do not represent multiple events robustly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2498:59,robust,robustly,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498,1,['robust'],['robustly']
Availability,Snappy-java error when running gatk-launch FastqToSam,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2868:12,error,error,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868,1,['error'],['error']
Availability,So how has the -new-qual error modified? I have used gatk 4.0.4.0 for all my analysis. How badly should I have to re-analyse with the new version of GATK? Is it really important?. __(edited to remove distracting boilerplate text - lbergelson)__,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5000:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5000,1,['error'],['error']
Availability,"So, I have a question that I believe some discussions are necessary, regarding read filtering. The background that triggered this question is when I tried `PrintReads(Spark)` to filter out reads with `ReadFilterLibrary.GoodCigarReadFilter`. As it turns out, that one of the read (long read) have several alignment records, (unfortunately) the primary (i.e. 256 and 2048 flags not turned on) record is the one having a problematic CIGAR, hence filtered out. The alignment records left are&mdash;as expected&mdash;all records with either `not-primary` or `supplementary` flag turned on. . Should one consider such bam valid for analysis?; And more generally, should the collection of alignments after read filtering be checked with `ValidateBam` (assuming the tool checks for that error) as part of best practices?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6433:779,error,error,779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6433,1,['error'],['error']
Availability,Somatic CNV WDL needs to support tumor-only mode where no matched normal is available.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983:76,avail,available,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983,1,['avail'],['available']
Availability,Somatic error correction for low allele fractions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4868:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868,1,['error'],['error']
Availability,"Some factors to consider in making this decision:. -Operations on zero-length intervals are error-prone due to lack of understanding/consensus about expected results (eg., should a query on a zero-length interval return records that abut it on either side?). -We need to determine how a query involving a zero-length interval is supposed to behave in the GA4GH API, as this does not seem to be clearly defined in the API documentation (eg., http://ga4gh.org/documentation/api/v0.5.1/ga4gh_api.html#/schema/%2FUsers%2Fkeenan%2FDropbox%2Fgit-checkouts%2Fschemas%2Fsrc%2Fmain%2Fresources%2Favro%2Ftarget%2Fall.avpr/org.ga4gh.GASearchReadsRequest). The representation is 0-based closed-open (like BED), which means zero-length intervals are possible, but their behavior appears undefined. -None of our current query interfaces (tribble/samtools) support computing overlap with zero-length intervals (although they don't throw an error when given such an interval -- they just never return any records for such queries). -It seems unlikely that we'll be moving anytime soon to representing insertions using zero-length intervals, given that the VCF spec requires insertions to be represented in terms of the preceding reference base.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/317:92,error,error-prone,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/317,2,['error'],"['error', 'error-prone']"
Availability,"Some kinds of interval files contain a sequence dictionary (eg., Picard-style interval files). We should use these sequence dictionaries when they're present and no other sequence dictionaries are available from the other inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/301:197,avail,available,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/301,1,['avail'],['available']
Availability,"Some notes from the 10k tieout:; ### Prepare Step; - ~20 min per full ref_ranges table to insert; - ~7 min per full vet table to insert; - ""bytes scanned"" are same as data table size. ### Extract. **Original Run - 293 min**. - 103 minutes pulling down data, scanning 237 GB; - 43 min on 20m vet records (20:26 - > 21:09); - 60 min on 291m vet records (21:09 -> 22:10). - 190 minutes writing the VCF; ; **Prepare Extract with minor tuning of sorting - 134 min**. - 25 minutes pulling down data ( faster), scanning 10 GB (50x reduction); - 4 min on 20m vet records(02:43 -> 02:47) - NOTE 103s of that was sorting (44s) and spilling to disk (59 s); - 21 min on 291m vet records (02:47 -> 03:08) - NOTE 9 min of that was sorting (6 min) and spilling to disk (3 min). - 109 minutes writing the VCF (this is the change to pre-sort the sample set merged to ah_var_store on 1/12/22) . **Tieout is identical**. ```; kcibul@kc-specops-tiny:~/stroke_tieout$ md5sum gold.jointcallset_0.vcf.gz; 496178eae4afe63c4391d8eba64a9947 gold.jointcallset_0.vcf.gz. kcibul@kc-specops-tiny:~/stroke_tieout$ md5sum trial.full.jointcallset_0.vcf.gz; 496178eae4afe63c4391d8eba64a9947 trial.full.jointcallset_0.vcf.gz; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7640:247,down,down,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7640,2,['down'],['down']
Availability,"Some notes on individual commits:. Updated CallCopyRatioSegments and PreprocessIntervals; reorganized copynumber packages.; -For motivation of changes in CallCopyRatioSegments, see #3825.; -I added the ability to turn off binning in PreprocessIntervals by specifying bin_length = 0.; -I removed the separation between coverage and allelic packages to make the package structure a bit simpler.; -@MartonKN should review, since he wrote PreprocessIntervals and is updating the caller. Added segmentation classes and tests for ModelSegments CNV pipeline.; -I added implementations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:904,avail,available,904,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['avail'],['available']
Availability,"Some of the Docker work from `ah_var_store` needs to be on `EchoCallset` to be able to do the PGEN subsets, particularly the PLINK Docker and GAR changes upon which the PLINK Docker changes depend. I have freshly baked the Variants, PLINK, and Docker images just now for this PR. 👨‍🍳 . Integration run in progress here https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/2585a1b6-c5da-48f0-a196-b5679e7f40a5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8805:60,Echo,EchoCallset,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8805,1,['Echo'],['EchoCallset']
Availability,"Some preliminary evaluation of the new ModelSegments pipeline on CRSP samples has revealed some weaknesses of the ReCapSeg caller (which is simply ported from the old pipeline) to me. I think there are a lot of confusing things going on:. 1) For determining copy-neutral segments, all segments with log2 mean below some threshold are used (rather than absolute log2). There is a comment that this is done to ""mimic the python code"" but I have no idea why this would be sensible, since it includes all deletions.; 2) There is some confusion arising from inconsistent use of z-score and T-statistic. Standard deviation, rather than standard error, is used for calling; i.e., a ""called segment"" is one that has a mean log2 copy ratio that has a z-score above some threshold with respect to the standard deviation of the log2 copy ratios of intervals that fall within copy-neutral segments (note also that these intervals have already been filtered by z-score to remove outliers). That is, any segment with a mean that falls sufficiently within the fuzziness of the caterpillar is not called.; 3) However, even calling using standard error is probably not what we want. This would simply be asking the question: given a population of copy-neutral intervals with a mean and standard deviation, does any non-copy-neutral segment contain intervals with a mean significantly different than the population? We've already answered this question during segmentation!. I think what we want to do instead is ask questions about the population of segment-level copy-ratio estimates, weighted by length.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3825:639,error,error,639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3825,2,['error'],['error']
Availability,"Some recent work by the Green Team as well as some evaluations we have done on our own tools have illuminated that the HaplotypeCaller has a non-deterministic output for some sites (a handful of sites across a typical 30x bam). Typically the differences manifest themselves as minor differences in the annotations at a few sites, for example the qual score and annotations might jitter from run to run like the following two variants: ; `9	103454626	.	A	T	54.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=0.431;DP=4;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=26.85;MQRankSum=1.383;QD=13.65;ReadPosRankSum=0.000;SOR=2.303	GT:AD:DP:GQ:PL	0/1:2,2:4:62:62,0,78`; `9	103454626	.	A	T	52.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=0.431;DP=4;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=26.85;MQRankSum=1.383;QD=13.15;ReadPosRankSum=0.000;SOR=2.303	GT:AD:DP:GQ:PL	0/1:2,2:4:60:60,0,78`; We should track down what is causing this error and shore up our score. I have found a test case that apparently reproduces the non-determinism. It seems to be somehow related to running the input data through the Google Connector. That is, the results appear to be reproducibly deterministic (at least over ~25 trials) when the input bam is local, whereas it starts to jitter when run from a `gs://` URL.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6105:891,down,down,891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6105,2,"['down', 'error']","['down', 'error']"
Availability,Some times in the past I found that it would have been useful to be able to determine whether the user gave a value to an argument or not. I.e. the argument default value may be different based on the circumstances (by different tools if shared across tools or the same tool based on other argument values) and so it is important to make sure that we are not overriding the user request (or at least we can emit the appropriate warning or error message).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/110:439,error,error,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/110,1,['error'],['error']
Availability,"Some tools have had usage examples ported from GATK3 that don't work in GATK4. We should fix ; these. . As well as fixing errors, it would be good to change the javadoc so it references parameters by the constant values instead of hardcoding them. (use `{@value StandardArgumentDefinitions#SOME_NAME}` ). These occur in at least the following tools, (found by `find in path -T`):; - [ ] ValidateVariants; - [ ] VariantFiltration; - [ ] AnalyzeCovariates; - [ ] BaseRecalibrator; - [ ] LeftAlignIndels",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1632:122,error,errors,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1632,1,['error'],['errors']
Availability,"Somewhere between #835 and now, BaseRecalibrator stopped working. When I try to run testBQSRBucket, I get the error below. This test is currently enabled so regression tests should have caught this. ```; java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:131); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:104); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:86); at org.broadinstitute.hellbender.tools.dataflow.pipelines.BaseRecalibratorDataflowIntegrationTest.testBQSRBucket(BaseRecalibratorDataflowIntegrationTest.java:176); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/866:110,error,error,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/866,1,['error'],['error']
Availability,"Sources(FeatureManager.java:182); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:153); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:636); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:160); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:133); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz has invalid uncompressedLength: -795051631, for input source: file:///home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:353); 	... 14 more; Caused by: htsjdk.samtools.util.RuntimeIOException: /home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz has invalid uncompressedLength: -795051631; 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6248:4675,error,error,4675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6248,1,['error'],['error']
Availability,Spaces in a sample name (inside the bam file) will cause a failure in the M2 WDL task.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4355:59,failure,failure,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4355,1,['failure'],['failure']
Availability,Spark Local Runner throws with unhelpful error message over gcs input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369:41,error,error,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369,1,['error'],['error']
Availability,Spark metrics collector refactoring checkpoint.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:36,checkpoint,checkpoint,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,1,['checkpoint'],['checkpoint']
Availability,"Spark tests on gatk-jenkins are currently failing:. Command:. ```; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; Error:. 16/11/29 16:21:01 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:377,Error,Error,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,SparkGenomeReadCounts Error on bai file check,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3909:22,Error,Error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3909,1,['Error'],['Error']
Availability,SparkSharder: ensure robustness in the face of areas of high coverage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2251:21,robust,robustness,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2251,1,['robust'],['robustness']
Availability,SplitNCigarReads error - Attempt to add record to closed writer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522,1,['error'],['error']
Availability,"Stacktrace is below. It looks like the default port (8020) is not being picked up.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 5.0 failed 4 times, most recent failure: Lost task 8.3 in stage 5.0 (TID 82, tw-cluster-2-w-4.c.broad-gatk-collab.internal): java.lang.IllegalArgumentEx; ception: Wrong FS: hdfs://tw-cluster-2-m:-1/user/tom/small_spark_eval/dbsnp_138.b37.20.21.vcf, expected: hdfs://tw-cluster-2-m; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:648); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:194); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106); at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305); at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301); at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426); at hdfs.jsr203.HadoopFileSystem.checkAccess(HadoopFileSystem.java:937); at hdfs.jsr203.HadoopFileSystemProvider.checkAccess(HadoopFileSystemProvider.java:75); at java.nio.file.Files.exists(Files.java:2385); at org.broadinstitute.hellbender.utils.io.IOUtils.assertFileIsReadable(IOUtils.java:551); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:292); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:244); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:218); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:202); at org.broadinstitute.hellbender.engine.spark.KnownSitesCache.loadFromFeatureDataSource(KnownSitesCache.java:43); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3468:147,failure,failure,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3468,2,['failure'],['failure']
Availability,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3140:1190,avail,available,1190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140,2,"['avail', 'down']","['available', 'down']"
Availability,Stop htsjdk CRAM reader from trying to download the reference when the expected reference is not available.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/677:39,down,download,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677,2,"['avail', 'down']","['available', 'download']"
Availability,"StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:2182,heartbeat,heartbeat,2182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['heartbeat'],['heartbeat']
Availability,Store the NCBI build version in the Gencode datasource config files; in order to resolve an issue where this value was not always available; when annotating IGR variants. Resolves #4404,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522:130,avail,available,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522,1,['avail'],['available']
Availability,Stream closed error with gatk 4.1.1.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5919:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919,1,['error'],['error']
Availability,StreamingProcessControllerUnitTest.testSerialCommands (intermittent?) test failure on Travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4014:75,failure,failure,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4014,1,['failure'],['failure']
Availability,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ef747737-4d19-4770-83b7-47715eff8237). tl;dr the only commit really worth looking at is 9ac0befbcc39b9c5a7eb0938dd79a7d5cbd5f297, everything else is a simple merge from master. This is just minor tweaks around recent changes in the JointVariantCalling WDL. I'll need to merge and push this locally to preserve history from master as that option is not available within the GATK GitHub repo.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8537:458,avail,available,458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8537,1,['avail'],['available']
Availability,"Summary of VQSR Changes; 	- ONLY populate AS_RAW_MQRankSum or AS_RAW_ReadPosRankSum for ref-alt genotypes (0/1, 0/2) not 1/1/,1/2,2,2; 	- AS_RAW_MQ for Non AS... Assign full MQ to alternate allele (don't distribute); 	- Compute SUM(AD) for future use; 	- provide command line option to force the AS-Approximate path even when AS annotations are available (for benchmarking/comparisons)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7718:345,avail,available,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7718,1,['avail'],['available']
Availability,"Summary of changes:. - Fixed a minor issue in sampling error estimation that could lead to NaN (as a result of division by zero). - Introduced separate _internal_ and _external_ admixing rates. The _internal_ admixing rate is to be used internally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. Whi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:55,error,error,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['error'],['error']
Availability,"Support for incrementally adding samples to existing genomicsdb workspaces. I've added these comments to the docs, but just wanted to call out again that we recommend making a backup of the existing workspace before trying to update the workspace. Otherwise, if the incremental update fails the workspace may be in a corrupted/inconsistent state. . If the user chooses not to backup (or can't), there is a (somewhat painful, manual) way to restore the workspace on failure IFF the --consolidate option has not been used. The tool will output a backup callset file (suffixed .inc.backup) and a file suffixed .fragmentlist with a list of all the original fragments. In order to roll back to a consistent workspace, the user must; - replace the callset file in the workspace with the one suffixed .inc.backup. That is, something like:; > mv _workspace_/callset.json.inc.backup _workspace_/callset.json; - delete all the directories in the workspace not named genomicsdb_meta_dir or included the file suffixed .fragmentlist",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970:465,failure,failure,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970,1,['failure'],['failure']
Availability,Suppress the `cp: target '/gatk/srcdir' is not a directory` error message that appears in all of the travis logs by creating the target srcdir first in run_unit_tests.sh.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5878:60,error,error,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5878,1,['error'],['error']
Availability,Syntax error on mutect2.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7353:7,error,error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7353,1,['error'],['error']
Availability,"System specs - (RedHat, Java 1.8.0_144); Clean git clone. ```; gradlew --stacktrace; Creating GATK Python package archive...; :createPythonPackageArchive UP-TO-DATE; :compileJava UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :gatkTabComplete; error: error reading /vsc-hard-mounts/leuven-user/304/vsc30484/.gradle/caches/modules-2/files-2.1/org.spire-math/spire_2.11/0.11.0/998b1c1d841baf4fc5d1b119ea55f165f6684ef5/spire_2.11-0.11.0.jar; error in opening zip file; 1 error; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/vsc-hard-mounts/leuven-data/304/vsc30484/git/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkTabComplete'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53); at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155:260,error,error,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155,5,"['FAILURE', 'error']","['FAILURE', 'error']"
Availability,System.exit in Main.java lead to a failure in yarn cluster mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3166:35,failure,failure,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3166,1,['failure'],['failure']
Availability,"System.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.pathExists(SparkUtils.java:100); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.setHadoopBAMConfigurationProperties(ReadsSparkSource.java:241); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:203); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [da63aa3c-e3bc-4893-9f40-42921719a343] entered state [ERROR] while waiting for [DONE].; ```. to reproduce this error, . ```bash; cd /Users/shuang/GATK/gatk. CLUSTER_NAME=""svdev-caller""; MASTER_NODE=""hdfs://svdev-caller-m:8020""; PROJECT_DIR=""user/shuang/NA12878_PCR-_30X"". ./gatk-launch FindBreakpointEvidenceSpark \; -R ""$MASTER_NODE""/reference/Homo_sapiens_assembly38.fasta \; -I ""$MASTER_NODE""/data/smallCram.cram \; -O ""$MASTER_NODE""/""$PROJECT_DIR""/fastq \; --exclusionIntervals gs://sv-data-dsde-dev/reference/GRCh38.kill.intervals \; --kmersToIgnore gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.dups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --cluster svdev-caller; ```. ========================. On the other hand, w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:4257,error,error,4257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['error'],['error']
Availability,"T --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:2206,error,errors,2206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['error'],['errors']
Availability,TK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:13770,ERROR,ERROR,13770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"TRIBBLE : false; 22:49:53.550 INFO CombineGVCFs - Deflater: IntelDeflater; 22:49:53.570 INFO CombineGVCFs - Inflater: IntelInflater; 22:49:53.571 INFO CombineGVCFs - GCS max retries/reopens: 20; 22:49:53.595 INFO CombineGVCFs - Requester pays: disabled; 22:49:53.597 INFO CombineGVCFs - Initializing engine; 22:49:53.869 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/d/projects/sequencing/gvcf/OPG0005F/OPG0005F.GATK.var.g.vcf; 22:49:53.923 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/d/projects/sequencing/gvcf/OPG0005M/OPG0005M.GATK.var.g.vcf; 22:49:53.939 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/d/projects/sequencing/gvcf/OPG0005P/OPG0005P.GATK.var.g.vcf; 22:49:54.132 INFO CombineGVCFs - Done initializing engine; 22:49:54.156 INFO ProgressMeter - Starting traversal; 22:49:54.157 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 22:49:54.193 INFO CombineGVCFs - Shutting down engine; [December 19, 2019 at 10:49:54 PM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=156237824; java.lang.ArrayIndexOutOfBoundsException: Index -2 out of bounds for length 256; at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:120); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:326); at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.java:197); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:216); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:162); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340:3108,down,down,3108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340,1,['down'],['down']
Availability,"T] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinxi_pkuhpc/lus; tre1/ljx/reference_genomes/funcotator_dataSources.v1.7.20200521s/ --ref; -version hg38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7474:1113,down,downloaded,1113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474,2,"['down', 'error']","['downloaded', 'error']"
Availability,"TableCodec has traditionally taken advantage of a quirk of the htsjdk implementation of tabix indexing, where the input stream being indexed was closed and then reopened in between reading of the header and subsequent feature indexing. That quirk had several failure modes (see https://github.com/samtools/htsjdk/issues/393 and https://github.com/samtools/htsjdk/issues/943). These are fixed in https://github.com/samtools/htsjdk/pull/906, and the stream is no longer closed by htsjdk. However, TableCodec required a [modification](https://github.com/broadinstitute/gatk/pull/3403) in order to remain indexable with these fixes, due to its use of a CSV reader (indirectly through TableReader) that buffers input, which thwarts feature-by-feature indexing. We should find a better long term fix for this; either finding a way to prevent OpenCSV from buffering, or possibly using a different CSV implementation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3440:259,failure,failure,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3440,1,['failure'],['failure']
Availability,Tagged files aren't echoed back correctly as args,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3247:20,echo,echoed,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247,1,['echo'],['echoed']
Availability,"TargetCodec is dependent on the old htsjdk indexing scheme whereby the indexer called readActualHeader on the codec first (for the side effect of initializing the codec header state), and then manually processed the feature file contents by:. - re-creating the input SOURCE/stream a second time; - NOT calling readActualHeader; - extracting and passing the features one at a time to the codec's decode method, using the stream position to find the feature file offsets. Although this scheme worked with TargetCodec, it had several other failure modes (see https://github.com/samtools/htsjdk/pull/906). With https://github.com/samtools/htsjdk/pull/906, the SOURCE/stream is only opened once for indexing. However, TargetCodec uses an underlying CSVReader that automatically buffers input, which confounds the indexer. This PR works around that issue for indexing. Note: this won't compile until there is an htsjdk snapshot available with https://github.com/samtools/htsjdk/pull/906.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3403:537,failure,failure,537,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3403,2,"['avail', 'failure']","['available', 'failure']"
Availability,Temporarily swap in Corretto for Temurin as we can't download Temurin.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7969:53,down,download,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7969,1,['down'],['download']
Availability,Test failure is unrelated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6628:5,failure,failure,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6628,1,['failure'],['failure']
Availability,"Tested using gatk-4.beta.6-151-g1ec409c-SNAPSHOT locally and with dataproc. Observed bug while testing commands for documentation updates in https://github.com/broadinstitute/gatk/pull/4068. . CollectInsertSizeMetricsSpark requires the `--histogramPlotFile` (`-H`, file to write insert size Histogram chart to) and current example commands add the `.pdf` extension to these files. The tool errors without this being specified but then doesn't write the file. In CollectBaseDistributionByCycleSpark, `--chart` (`-C`, A file (with .pdf extension) to write the chart to) is optional. When specified, the tool appears to ignore this option and does not write the file. . Metrics files defined by `-O` are written.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4085:390,error,errors,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4085,1,['error'],['errors']
Availability,"Tests are ""failing"" with the ""code is too big"" error on the CNN testTrainingReadModel. I had to update my conda yml template to use a newer Tensorflow @cmnbroad found -- should I add that here too?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6330:47,error,error,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6330,1,['error'],['error']
Availability,"Thanks a lot. I am trying to run `mutect2` in **tumour-only** mode for which I need a panel of normal (PON). I have tried **somatic-hg38_1000g_pon.hg38.vcf.vcf** which gives . `./gatk Mutect2 -R resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta -I /data/Continuum/WES/testAlignmentBROADGenome/results/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.bam -O 3.mt2.vcf -tumor NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.targeted_sequencing.sample_name --af-of-alleles-not-in-resource 2.5e-06 --germline-resource af-only-gnomad.hg38.vcf.gz -pon somatic-hg38_1000g_pon.hg38.vcf.vcf`. A USER ERROR has occurred: Cannot read file:///data/somatic-hg38_1000g_pon.hg38.vcf.vcf because no suitable codecs found. I know **gatk4_mutect2_4136_pon.vcf.gz** locates here for which I should register in GDC but because I am a postdoctoral researcher, I can not register. [1]: https://gdc.cancer.gov/about-data/gdc-data-processing/gdc-reference-files. Could you please help me to run mutect2 in tumour-only mode using another publicly available PON?. Thanks for any help",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8477:612,ERROR,ERROR,612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8477,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability,"The BigQuery library upgrade for extract, broke ingest :/ This fixes that. It also rethrows an exception we were eating that I noticed. The error seen during ingest was. ```; java.lang.IllegalArgumentException: JSONObject does not have a bytes field at root.sample_id.; 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.fillField(JsonToProtoMessage.java:306); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessageImpl(JsonToProtoMessage.java:138); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessage(JsonToProtoMessage.java:86); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:110); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:90); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.writeLoadStatus(CreateVariantIngestFiles.java:202); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.onTraversalSuccess(CreateVariantIngestFiles.java:369); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7620:140,error,error,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7620,1,['error'],['error']
Availability,"The CNN tools launch python immediately even when they're just instantiated, rather than waiting until the tool actually starts executing. This can cause some build tasks (gatkDoc, gatkWDLGen, etc.) to fail if python isn't available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8128:223,avail,available,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8128,1,['avail'],['available']
Availability,"The GATK docker image uses samtools version 0.1.19 instead of the current version 1.9 and can therefore not read `gs://` resources. Samtools is installed in the gatkbase image via apt-get, the recent releases are not available there. Instead, it would have to be built manually (see http://www.htslib.org/download/).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6148:217,avail,available,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6148,2,"['avail', 'down']","['available', 'download']"
Availability,"The GATK4 port of GATK3 VariantEval uses a MultiVariantWalker traversal, along with individual `FeatureInput` arguments for evals, knowns, comps, etc., which are all manually merged together as the walker's driving variants. The resulting variants are then manually processed in groups, by start position. Since the tool needs to know the origin of each variant (eval, comp, dbsnp, known, etc.), and since this isn't preserved by the engine, it re-queries the `FeatureContext` for each input to get the same set of variants grouped by source. Since the inputs are typed as `FeatureInput`, this results in all inputs being both consumed and cached twice; once by `MultiVariantDataSource` and once by `FeatureManager`. Once alternative would be to use a LocusWalker, but that would still require index queries (though the features would be cached), and it would still require manual filtering/aggregation on start position. Proposed fix is to switch the base class to use `MultiVariantWalkerGroupedOnStart` (this would allow removal of `PositionAggregator` class); change the engine to preserve the input source of each variant as proposed in https://github.com/broadinstitute/gatk/pull/4571; and change the input arguments for VariantEval from individual named arguments to tagged feature inputs. This would greatly simplify the initialization code, eliminate redundant reading and caching, and allow the tool to do the input source grouping by just looking at each variant's source field.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439:1359,redundant,redundant,1359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439,1,['redundant'],['redundant']
Availability,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7214:124,error,error,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214,8,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"The Genome Analysis Toolkit (GATK) v4.5.0.0; ## Description; Hi,; Here is my situation, I'm testing the feasibility of incremental GenomicsDB，I have total 400 samples to joint calling, I have no problem directly using `GenomicsDBImport `and `GenotypeGVCFs `for joint calling of all 400 samples. The configuration used is 4c32g for `GenomicsDBImport `and 2c16g for `GenotypeGVCFs`. But when I first built a GenomicsDB of 200 samples using `GenomicsDBImport `successfully, and then use GenomicsDB `--genomicsdb-update-workspace-path` increment 200 samples into the GenomicsDB , use this incremental imported GenomicsDB to `GenotypeGVCFs`. The error happend and report GENOMICSDB_TIMER,Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; Here are my code; ```; gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenomicsDBImport \; --tmp-dir $PWD \; --genomicsdb-workspace-path ~{workspace_dir_name}~{prefix}.~{index} \; --batch-size 50 \; -L ~{intervals} \; --reader-threads 5 \; --merge-input-intervals \; --consolidate \; -V ~{sep = "" -V "" single_sample_gvcfs}. gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenomicsDBImport \; --tmp-dir $PWD \; --genomicsdb-update-workspace-path ~{workspace_dir_name} \; --batch-size 50 \; --reader-threads 5 \; --merge-input-intervals \; --consolidate \; -V ~{sep = "" -V "" single_sample_gvcfs}. gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenotypeGVCFs \; --tmp-dir $PWD \; -R ~{ref} \; -O ~{workspace_dir_name}.vcf.gz \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; -V gendb://~{workspace_dir_name} \; -L ~{intervals} \; --merge-input-intervals \; -all-sites; ```; And I found that before report error the number of threads used by GATK increased, but the memory usage did not exceed the maximum limit of the server.; I also cheched `--max-alternate-alleles` and `--genomicsdb-max-alternate-alleles` to a smaller size but still the same error. I would appreciate some insights in why that is. Thanks,; Yang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8777:1804,error,error,1804,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8777,2,['error'],['error']
Availability,"The Hardy-Weinberg equilibrium (HWE) theorem characterizes the distributions of genotype frequencies in populations that are not evolving. Let’s recall it in its simplest form. [Hardy-Weinberg] Let ( A ) and ( a ) be alleles at a single locus in a non-evolving population with random mating. Let ( p ) and ( q ) be their respective frequencies in that population. ( p ) and ( q ) will remain constant in average from generation to generation. The expected frequencies of the genotypes, ( AA ), ( Aa ) and ( aa ), will also remain constant and are respectively ( p^2 ), ( 2pq ), and (q^2 ). Description:. Use Wigginton’s exact test because it adequately controls type I errors in large and small samples. Calculated by:. Pedstats and vcftools use efficient implementations from Wigginton et al.; use code by Wigginton as your starting point (need to translate to java i think). Remark:. Deviations from HWE can indicate inbreeding, admixture, or population stratification. In order to avoid the latter, HWE tests should be run for each ethnicity/population separately. Typically a variant is filtered out if, for any of the ethnicities, the P-value is lower than (10^\textrm{-6}). HWE tests can also identify loci with systematic genotyping errors, which makes HWE useful for QC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/538:669,error,errors,669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/538,2,['error'],['errors']
Availability,"The SV copy results script has to figure out the total number of workers including standard and preemptibles. The current version of the script had an error in parsing the `gcloud compute dataproc clusters list` command, which left a blank field when no preemptibles were used that couldn't be parsed because it was displayed in fixed-width format. This switches to the CSV format option for the cluster list command, making it easier to parse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4767:151,error,error,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4767,1,['error'],['error']
Availability,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2349:59,reliab,reliably,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349,3,"['failure', 'reliab']","['failure', 'reliably']"
Availability,The TwoBit API can't handle queries beyond the ends of contigs (we get; an assertion failure). This changes ReferenceTwoBitSource to truncate query; intervals at contig ends as necessary. Resolves #1214,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1236:85,failure,failure,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1236,1,['failure'],['failure']
Availability,The [Google Java Style guide](http://google-styleguide.googlecode.com/svn/trunk/javaguide.html) links are dead and give a 404 error. References were changed to [Google Java Style guide](https://google.github.io/styleguide/javaguide.html).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5405:126,error,error,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5405,1,['error'],['error']
Availability,"The `PS` tag should be type `Integer`, not `String` according to the spec, but no error is reported (for me). ```bash; bcftools view https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz chr1:4001310-4001310 > test.vcf; ```. Related: https://github.com/genome-in-a-bottle/giab_latest_release/issues/15",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6762:82,error,error,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6762,1,['error'],['error']
Availability,The `ReferenceBases` annotation fails with an NPE if there is no reference available. It should fail with a helpful UserException instead. ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2799:75,avail,available,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799,1,['avail'],['available']
Availability,"The `google-cloud-nio` library should allow us to customize the errors that trigger a retry/reopen. Specifically, it should allow us to add additional http status codes to trigger a retry, as well as additional exception classes to trigger a reopen. It should be possible to either append to the existing defaults, or to replace the defaults completely with your own values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5306:64,error,errors,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5306,1,['error'],['errors']
Availability,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3068:135,down,downloads,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068,1,['down'],['downloads']
Availability,"The build is failing since 1.21.0-SNAPSHOT is no longer available in any Maven repositories. It looks like 1.21.0 was released last week: https://repo1.maven.org/maven2/com/google/http-client/google-http-client/, and changing the build to use that version seems to fix the problem. Related to #650.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1185:56,avail,available,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1185,1,['avail'],['available']
Availability,The build should give a clear error message explaining how to skip building the native code if it fails to build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1554:30,error,error,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1554,1,['error'],['error']
Availability,"The cloud tests are timing out after 10 minutes without emitting any output. It seems like `ApplyBQSRDataflowIntegrationTest.testPR_Cloud` is responsible. It looks like something is crashing in dataflow but the runner is never stopped so it keeps waiting indefinitely (or at least 10 minutes..) See the dataflow log [here](https://console.developers.google.com/project/broad-dsde-dev/dataflow/job/2015-07-24_12_44_26-17415749601435236766). . Executing locally also seems to hang forever, with messages like . ```; Error: (b65a2091061bf0f9): Workflow failed. Causes: (71540087aac21e37): Unable to create VMs. Causes: (71540087aac21994): Error:; Test: Test method testPR_Cloud[0](ApplyBQSR(args=''))(org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSRDataflowIntegrationTest) produced standard out/err: Message: Value for field 'resource.metadata.items[1].value' is too large; ```. Seems like this is possibly a dataflow bug. If the workflow fails in some way the client should be released.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/750:514,Error,Error,514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/750,2,['Error'],['Error']
Availability,"The code changes here are actually fairly small all things considered after doing some cleaning of the working branches. There are 3 big differences between this branch and the previous version of LinkedDebrujin code: ; 1: Implemented an algorithm to the KBestHaplotypeFinder for coloring ""pivotal edges"" (i.e. edges in which we have made a choice that would be in the junction trees) and then upon fininshing with all of the junctinon tree reachable paths from reference source, we then check for edges that have not been recovered and attempt to rescue them (this fixes the loss of sensitivity from the previous version); 2: Changed the ReadThreadingAssembler to increment the kmer size it uses (when in JT mode) to increment its sizes AFTER it has attempted to recover haplotypes (this catches some new edge cases that causes complicated graphs to fail). This currently is a very rudimentary approach (we simply expand if the KBestHaplotypeFinder failed to find anything at all). ; 3: includes some code to squeeze extra sensitivity out of the junction trees by tolerating SNP errors when threading the junction trees themselves . There are a number of things I think maybe could be tweaked from here:; - I think ""k"" for max haplotypes can be lowered given the new haplotype recovery improvements; - We can and perhaps should revisit the question of how/when to expand the kmer size, as given recent fixes in this branch that could potentially save some sensitivity/specificity that we were losing before. (the code for one approach to this still lives in this branch). . Fixes #5924, #5923, #5828",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6394:523,recover,recovered,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6394,5,"['error', 'recover', 'toler']","['errors', 'recover', 'recovered', 'recovery', 'tolerating']"
Availability,"The current `IsUsingCompressedReferences`; It does not pass project id that hosted dataset. ; When the project_id is missing in a BigQuery SQL query, the bq command will use the --project_id flag specified in the command as the default project for resolving dataset and table references.; Add additional parameter to allow passing dest project. . In our case; Error we saw in GCP console:; ```; Access Denied: Table terra-vpc-sc-dev-7ee328ad:1kg_wgs_2022q1.INFORMATION_SCHEMA.COLUMNS: User does not have permission to query table terra-vpc-sc-dev-7ee328ad:1kg_wgs_2022q1.INFORMATION_SCHEMA.COLUMNS, or perhaps it does not exist.; ```. `terra-vpc-sc-dev-7ee328ad:1kg_wgs_2022q1.INFORMATION_SCHEMA.COLUMNS` is wrong - `terra-vpc-sc-dev-7ee328ad` is the user workspace; It should be `fc-aou-cdr-synth-test-2.1kg_wgs_2022q1` - `fc-aou-cdr-synth-test-2` is the project that contains CDR data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9023:360,Error,Error,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9023,1,['Error'],['Error']
Availability,"The current error handling swallows the detailed part of the error string that gets reported to the user, i.e., without this change you see:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:10000000-10010000. With this change:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The genome loc coordinates 10000000-10010000 exceed the contig size (200000). I originally removed the entire try/catch block, but there is downstream code that throws IllegalArgumentException, which can benefit from being decorated with the context info.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2191:12,error,error,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2191,5,"['ERROR', 'down', 'error']","['ERROR', 'downstream', 'error']"
Availability,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213:82,error,error,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213,1,['error'],['error']
Availability,"The current implementation does not check whether `histo.get(testStatU)` returns a valid pointer or not. If runtime optimization is used (such as the case in J9 JIT), JVM may remove that bin if there is nothing in it. The outcome is a `NullPointer` error at runtime for some corner cases. Adding a pointer check can solve the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5190:249,error,error,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5190,1,['error'],['error']
Availability,"The current runTool method implementation masks exception occurring in either onStartup or doWork if there is a exception in onShutdown. It is more important to report the exception in onStartup or doWork that any occurring in onShutdown. . Apart from this, developers are supposed to make onShutdown robust to failure in onStartup?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/528:42,mask,masks,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/528,3,"['failure', 'mask', 'robust']","['failure', 'masks', 'robust']"
Availability,"The current top-level index document has a bogus tool category called ""tools"" in it (see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/). This is created by the `@DocumentedFeature` annotation on the AlleleFrequency stratifier class. We don't actually have a category for this, and most are not documented. If at some point we want to add those, we can do so with a real category, but for now this removes the annotation. There are also some unnatural line breaks in the index, which are removed here. The html doc could use some more maintenance cleanup, but this fixes those two obvious issues.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6318:561,mainten,maintenance,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6318,1,['mainten'],['maintenance']
Availability,The error : Invalid GZIP header when I run the BaseRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5968:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968,1,['error'],['error']
Availability,The error `Cannot use index file with textual SAM file` was reported by `AssemblyRegionUnitTest.testCreateFromReadShard`when trying to read what was likely a corrupted large file bam on travis. This error does not accurately reflect the problem and is likely caused by a chain of fallbacks in htsjdk. It's possible that the switch from file -> path has changed the sequence of fallbacks causing this error to appear.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2551:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2551,3,['error'],['error']
Availability,"The file extension check in this tool was redundant, since there is a subsequent check; that the input file is in BGZF format. Resolves #5800",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5801:42,redundant,redundant,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5801,1,['redundant'],['redundant']
Availability,"The first issue in this post has been solved by updating GATK versions but there appears to be a bug causing an ""unrecognized argument"" error when running GermlineCNVCaller. . This request was created from a contribution made by Ahmed S. Chakroun on December 05, 2021 13:52 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4412031649691-GermlineCNVCaller-prefix-string-too-short](https://gatk.broadinstitute.org/hc/en-us/community/posts/4412031649691-GermlineCNVCaller-prefix-string-too-short). \--. Hi everybody,. Unfortunately, I am getting stuck because the filename length of my hd5 is too short (< 3) to be used as prefix by the GermlineCNVCaller. So, I am posting just to check that nothing is left for me to do at this level of the analysis I am running other than renaming my input files and re-running everything from the scratch :-(. Accordingly, here is the needed info:. GATK version used: 4.2.0.0 ; ; openjdk version ""11.0.11"" 2021-04-20 ; ; OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04) ; ; OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing). Command:. gatk GermlineCNVCaller \\ ; ; \--run-mode COHORT \\ ; ; \-L Twist\_Exome\_Target\_hg38\_preprocessed\_annotated\_gc-filtered.interval\_list \\ ; ; \-imr OVERLAPPING\_ONLY \\ ; ; \--contig-ploidy-calls ploidy-calls \\ ; ; \--annotated-intervals Twist\_Exome\_Target\_hg38\_preprocessed\_annotated.interval\_list \\ ; ; \-I 13-20.counts.hd5 \\ ; ; \-I 722.counts.hd5 \\ ; ; \-I D19047.counts.hd5 \\ ; ; \-I F24F1.counts.hd5 \\ ; ; \-I NS.counts.hd5 \\ ; ; \-I TBC039.counts.hd5 \\ ; ; \-I VP.counts.hd5 \\ ; ; \-I WES002.counts.hd5 \\ ; ; \-I WES02.counts.hd5 \\ ; ; \-I 17062-T1-.counts.hd5 \\ ; ; \-I 18001-M1-.counts.hd5 \\ ; ; \-I 516.counts.hd5 \\ ; ; \-I 533.counts.hd5 \\ ; ; \-I NBH.counts.hd5 \\ ; ; \-I ADN492.counts.hd5 \\ ; ; \-I WES607.counts.hd5 \\ ; ; \--class-coherence-length 1000.0 \\ ; ; \--cnv-coherence-length 1000.0 \\ ; ; \--enable-bias",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7591:136,error,error,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7591,1,['error'],['error']
Availability,"The fix will enable run gradle build and test on PowerPC while the pairHMM native binary build is being sorted out. It also introduced tolerance when compare two floating point numbers, which caused test failure on PowerPC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1761:135,toler,tolerance,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761,2,"['failure', 'toler']","['failure', 'tolerance']"
Availability,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,11,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"The following command generates an error. Other spark programs work when specifying hdfs://scc/user/farrell/adsp/bams/SRR990385.bam as the input. It seems to be having a problem with testing for the presence of the SRR990385.bai file which is present. I tried running the command with hdfs://scc:**8020**/user/farrell/adsp/bams/SRR990385.bam and that works. . `/share/pkg/gatk/4.beta.5/install/bin/gatk-launch SparkGenomeReadCounts -I hdfs://scc/user/farrell/adsp/bams/SRR990385.bam -o SRR990385.ReadCounts -R /restricted/projectnb/genpro/bundle/2.8/b37/human_g1k_v37.fasta --verbosity ERROR -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-memory 4G --executor-cores 3`. [December 3, 2017 2:56:35 PM EST] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.57 minutes.; Runtime.totalMemory()=982515712; org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 0.0 failed 4 times, most recent failure: Lost task 12.3 in stage 0.0 (TID 14, scc-q09.scc.bu.edu, executor 1): java.lang.IllegalArgumentException: **Wrong FS: hdfs://scc:8020/user/farrell/adsp/bams/SRR990385.bai, expected: hdfs://scc**; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302); at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766); at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3909:35,error,error,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3909,4,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"The following exception was thrown, which looks a lot like a user exception:. ```; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /storageNGS/ngs3/projects/other1/KinderKlinik/Exomes/04_GRCh8_GATK4_cohort/GATK4_testrun/cromwell-executions/HaplotypeCallerGvcf_GATK4/3e5a2ae0-5529-4344-b187-9ceac771e1ed/call-MergeGVCFs/execution/pa.hg38.g.vcf.gz, for input source: file:///storageNGS/ngs3/projects/other1/KinderKlinik/Exomes/04_GRCh8_GATK4_cohort/GATK4_testrun/cromwell-executions/HaplotypeCallerGvcf_GATK4/3e5a2ae0-5529-4344-b187-9ceac771e1ed/call-MergeGVCFs/execution/pa.hg38.g.vcf.gz; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:254); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:101); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:126); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:110); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:620); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getHeaderFromPath(GenomicsDBImport.java:355); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:341); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:296); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275) Caused by: java.nio.file.No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4526:165,error,error,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4526,1,['error'],['error']
Availability,"The following one-liner (which I Googled, so I cannot vouch that this is the best way to do things) revealed 245Mb of files that are unused in master:. for ff in `find src/test/resources -name ""*""`; do file=`basename $ff`; git grep -l $file >/dev/null; rcode=$?; if [[ $rcode -ne 0 ]]; then echo $ff; fi; done. Note I didn't search in all branches, but I figure we can always recommit those files. Also, any index files, etc. should be retained if necessary. CNV team will delete their files, but I'll leave it up to engine and the other teams about how much we want to remove. src/test/resources/dbsnp_132.b36.excluding_sites_after_129.chr1_1k.vcf.idx; src/test/resources/empty.vcf.idx; src/test/resources/exampleFASTA.fasta.fai; src/test/resources/fastaWithoutDict.fasta.fai; src/test/resources/fastaWithoutFai.dict; src/test/resources/hg19micro.dict; src/test/resources/hg19micro.fasta.fai; src/test/resources/hg19mini.dict; src/test/resources/hg19mini.fasta.fai; src/test/resources/Homo_sapiens_assembly19_chr1_1M.dict; src/test/resources/Homo_sapiens_assembly19_chr1_1M.fasta.fai; src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf.idx; src/test/resources/HSA19.dbsnp135.chr1_1M.exome_intervals.modified.vcf.idx; src/test/resources/human_g1k_v37.chr17_1Mb.dict; src/test/resources/human_g1k_v37.chr17_1Mb.fasta.fai; src/test/resources/iupacFASTA.dict; src/test/resources/iupacFASTA.fasta.fai; src/test/resources/joint_calling.chr1_1M.1kg_samples.10samples.noINFO.vcf.idx; src/test/resources/large/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx; src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram.bai; src/test/resources/large/cnv_germline_workflows_test_files/inputs/wes_pon/model_final/bias_covariates_ARD_coefficients.tsv; src/test/resources/large/cnv_germline_workflows_test_files/inputs/wes_pon/model_final/mean_bias_covariates_matrix.tsv; src/test/resources/large/cnv_germline_workflows_test_files/inputs/wes_pon/model_final/mean_bias_cov",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:291,echo,echo,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['echo'],['echo']
Availability,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2357:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357,1,['error'],['error']
Availability,"The gap-opening and gap-continuation parameters of Smith-Waterman realignment depend on PCA slippage and other stuff that depends on the sequencing platform and sample prep. In other words, they are not global parameters (_note: Smith-Waterman is often used to determine sequence similarity between individuals or species in which case its parameters are constants of the population. But a read differs from a candidate haplotype via sequencing error, not mutation_). @ronlevine suggested (and I am reporting because I like the idea) that we probably have sufficient data to learn these parameters for each sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1902:445,error,error,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1902,1,['error'],['error']
Availability,The goal of this PR is to adjust the ingest in two ways:; 1. To update the ingest to loop through all samples (not just the first 10k); 2. To update the ingest to be far more efficient in a few ways:; - To remove the files that are downloaded to each vm so that they do not carry around the extra weight; - To check that the samples in the fofns have not been ingested already so that additional work doesn't need to be done toward processing those samples. There is still work to do around making the bulk ingest process significantly more user-friendly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8197:232,down,downloaded,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8197,1,['down'],['downloaded']
Availability,"The hg19 gencode is missing the transcript `ENST00000515683.1`. It appears in the gtf, but not in the fasta. Using `funcotator_dataSources.v1.2.20180329`. ```; A USER ERROR has occurred: Bad input: Unable to find the given Transcript ID in our transcript list (not in given transcript FASTA file): ENST00000515683.1; ```; ```; gatk Funcotator --output-file-format VCF \; --ref-version hg19 --data-sources-path ~/data/funcotator/funcotator_dataSources.v1.2.20180329/ \; -R ~/data/Homo_sapiens_assembly19.fasta -V 0816201804HC0_R01C01.vcf.gz \; -O 0816201804HC0_R01C01.funcotated.vcf; ```. The error is triggered somewhere near the position`4:60143686`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4739:167,ERROR,ERROR,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,The hstjdk downstream tests are failing and have been since we merged the repos. It looks like the failure are due to missing R dependencies on the worker nodes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3234:11,down,downstream,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3234,2,"['down', 'failure']","['downstream', 'failure']"
Availability,"The htsget.ga4gh.org appears to be down (tests get 404s, ping fails). This output is from my PR https://github.com/broadinstitute/gatk/pull/6799 that prints out the target URI:. ```; org.broadinstitute.hellbender.exceptions.UserException: Invalid request https://htsget.ga4gh.org/reads/A1-B000168-3_57_F-1-1_R2.mus.Aligned.out.sorted.bam, received error code: 404, error type: NotFound, message: The requested resource could not be associated with a registered data source; at org.broadinstitute.hellbender.tools.HtsgetReader.doWork(HtsgetReader.java:266); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:146); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:111); at org.broadinstitute.hellbender.tools.HtsgetReaderIntegrationTest.testSuccessfulParameters(HtsgetReaderIntegrationTest.java:85); ```; Jermey (GA4GH dev) says:. > I recently updated the server, but my understanding was that the gatk build was spinning up a local server from an older image; > 11:41; > so htsget.ga4gh.org is using a newer image, while the gatk tests should pull an older image, spin it up locally, and then request from http://localhost. But based on the output above, it looks like we actually target `https://htsget.ga4gh.org/read...`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6803:35,down,down,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6803,4,"['down', 'error', 'ping']","['down', 'error', 'ping']"
Availability,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3235:11,down,downstream,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235,1,['down'],['downstream']
Availability,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1 — [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2 — [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3 — [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352:233,error,error,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352,3,"['Error', 'error']","['Error', 'error']"
Availability,"The important distinction is not between ""reviewed"" and ""unreviewed"" exceptions (all; use of exceptions should be reviewed in code review, after all), but between user mistakes; and internal sanity check failures. To this end, I've ported UserException from the old GATK codebase (preserving only the; most generally useful subclasses -- we can add more later if needed), removed the silly; ReviewedHellbenderException, and renamed HellbenderException to GATKException, which is; what should be thrown for all errors that are not the user's fault.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/85:204,failure,failures,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/85,3,"['error', 'failure', 'fault']","['errors', 'failures', 'fault']"
Availability,The jenkins spark tests are failing with the following error:. This seems to have been introduced in https://github.com/broadinstitute/gatk/pull/3576. ```; code: 0; message: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:339); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:197); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:194); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:91); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:194); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:162); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:118); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:55,error,error,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,3,"['Error', 'error']","['Error', 'error']"
Availability,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7787:72,error,error,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787,2,['error'],['error']
Availability,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7841:72,error,error,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841,4,['error'],"['error', 'errors']"
Availability,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7601:347,error,errors,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601,1,['error'],['errors']
Availability,"The more the best alignment mismatches its assigned reference location, the more stringent the filter becomes, that is, the lower the threshold for secondary alignments to constitute a multi-mapping becomes. If the best mapping mismatches at one base and the second best mismatches at three, that is very different from the best mismatching at four bases and the second best mismatching at six. @takutosato This corrects a failure to filter some very obvious clustered events mapping artifacts. For example, there were 11 ""events"" within 150 bp in one MC3 sample that were all being called as false negatives for M2 and true positives for M1. If these are real, I will eat dog food.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5501:423,failure,failure,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5501,1,['failure'],['failure']
Availability,"The new version of SplitNCigarReads now sets reads that are split from one original read to be supplementary to each other. Unfortunately, the tool does not respect any existing supplementary reads as being supplementary to each other. Currently the tool clobbers any existing SA tag and sets one read from each read group as primary which is undesirable as it corresponds to loosing information from the aligner. One solution is to use the same ""predicting"" approach for existing SA information to attempt to repair the corresponding SA tag for each read based on how it would be split given its cigar string. Perhaps there is another solution that is more in line with what the SA tag gets used for. . In order to make these tags 100% accurate with overhang fixing on however, the OverhangFixingManager will probably need to be changed to store the mate information for every read it changes between file iterations. Currently it only stores information on a single read from every read group as it would be the only one that affects mate information but this assumption is invalidated as the SA tag needs to keep information on EVERY read in a canonical alignment to be correct, not just the first one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2116:510,repair,repair,510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2116,1,['repair'],['repair']
Availability,The newest release of GenomicsDB treats spanning deletions (spanning; from earlier positions) as deletions in the min PL value computation.; This behavior now matches the behavior of CombineGVCFs. A more detailed description of the issue is provided in; https://github.com/broadinstitute/gatk/pull/4963. * Deleted a couple of files which are no longer necessary.; * Fixed the index of newMQcalc.combined.g.vcf; * Fixes #5045 (error out with a helpful error message); * Fixes #5300,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5397:426,error,error,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397,2,['error'],['error']
Availability,The newly enabled GermlineCNV wdl tests seem to take ~1 hour 8 minutes. The maximum timeout for our travis jobs is 1 hour 10 minutes. This is causing random failures. It's also the slowest of our matrix entry by a large margin which makes our tests even slower than they are. Can these be speed up?. Alternatively we may need to talk to the travis people and get a special dispensation to increase our job timeouts even farther.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4064:157,failure,failures,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4064,1,['failure'],['failures']
Availability,"The packages for codecs is a key feature for downstream tools implementing new codecs for other formats or to include overrides of codecs already included. Nevertheless, the current implementation (at version 4.0.0.0) the only way of configuring this is at the package level using the `codec_packages` configuration. I request support for the following fine-grained configuration:. * Add/Remove concrete codec classes; * Exclude single classes from a concrete `codec_package` specified (this can be done by the previous requirement if it uses fully qualified codec names); * Exclude sub-packages from a concrete `codec_package` specified. Representing this in an YML format, I would like to have the ability to configure the codecs as following:. ```yml; - codecs:; - packages:; - htsjdk.variant; - htsjdk.tribble; - exclude_class: bed.BEDCodec; - org.broadinstitute.hellbender.utils.codecs; - exclude_package: gencode; - org.magicdgs.htsjdk.codecs; - classes:; - org.external.htsjdk_codecs.CustomBedCodec; ```. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:45,down,downstream,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,1,['down'],['downstream']
Availability,"The problem is that when alignments are de-overlapped (i.e. they overlap on the read) in the CPX logic, some resulting alignment could be only 1 base long, leading to problems. This was not caught before because in practice a heuristic alignment filtering step is in place in `AssemblyContigAlignmentsConfigPicker` to filter out, in a post-hoc way, such small alignments.; When I experimented with switching orders of the filtering steps, this behavior was observed. The aim is to make the downstream logic agnostic to upstream filtering detail.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962:490,down,downstream,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962,1,['down'],['downstream']
Availability,"The problem was ported from GATK 3 ([here](https://github.com/broadinstitute/gsa-unstable/pull/1369) and [here](https://github.com/broadinstitute/gsa-unstable/issues/1368)), and seems like more error(s) are introduced. Essentially, the fields`cachePloidyCapacity` and `cacheAlleleCountCapacity` are mixed, and fixing should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1856:194,error,error,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1856,1,['error'],['error']
Availability,"The pull request addresses two issues:. 1. Improved and more robust parsing of FlowBasedReads. Specifically, the code now determines the minimal reportable quality; 2. New tool AddFlowSNVQuality that allows users to convert the flow-based quality format when every base quality reports probability of an insertion or deletion to a conventional format that gives base qualities (total probability of mismatch and probability of each mismatch in separate tags). . We believe that this tool is going to be important for users of the Ultima Genomics data that care about calling SNVs, especially in somatic setting, so the goal was to make documentation more accessible. . Happy to receive feedback about it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8697:61,robust,robust,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8697,1,['robust'],['robust']
Availability,"The quality score sum (QSS) format field reports the sum of base qualities for ref and alt allele, and its correct format field count type is 'R' (see 1.2.2 in the [vcf spec](http://samtools.github.io/hts-specs/VCFv4.2.pdf)). We used to use the wrong count type of 1 and that caused a parsing error in the dream challenge evaluation script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2229:293,error,error,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2229,1,['error'],['error']
Availability,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8494:687,redundant,redundant,687,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494,2,"['down', 'redundant']","['down', 'redundant']"
Availability,"The result of this bug is that if you produce a BAM with Spark, then produce a different BAM with the same name with the walker-framework, then try to read _that_ bam with Spark, it produces checksum (and other more misleading) errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1266:228,error,errors,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1266,1,['error'],['errors']
Availability,"The somatic are labeled as germline and visa-versa. The files themselves must be changed, as well as the data source downloader (to point to the new paths).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5428:117,down,downloader,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5428,1,['down'],['downloader']
Availability,"The symptom is that I'm getting an invalid record when using the Hadoop-BAM probabilistic splitter. I wrote a small program to verify the bug and the record it's happening on. To run this, check out da_key_bug and run something similar to my command. ```; gcloud beta dataproc jobs submit spark \; --cluster high-mem-16-8-sd \; --properties spark.executor.memory=38g,spark.executor.instances=15,spark.executor.cores=7 \; --class org.broadinstitute.hellbender.Main \; --jar build/libs/gatk-all-4.pre-alpha-*-spark.jar \; KeyReadsSpark \; -I hdfs:///user/davidada/CEUTrio.HiSeq.WGS.b37.NA12878.bam \; --bps 4194304 \; --sparkMaster yarn-clien; ```. The important bits are the BAM (`gsutil cp gs://jpmartin/hellbender-test-inputs/CEUTrio.HiSeq.WGS.b37.NA12878.bam .` should work to grab it) and the bps being set to `4194304`. It doesn't happen with a different split. (Yet more evidence pointing to Hadoop-BAM). The bad record is on chromosome 1 and starts at 801305857. I imagine that making a small input would work to find this issue (instead of needing to use the 300 GB BAM). Assigning to Uri (I'd assign it to Tom, but I don't know when he's back).; If you need help cutting down that BAM @droazen and @lbergelson can give some advice.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1098:1179,down,down,1179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1098,1,['down'],['down']
Availability,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1465:138,mask,masks,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465,1,['mask'],['masks']
Availability,"The varianteval package (VariantEval, evaluators/stratifiers and stratification manager) was ported directly from GATK3 to minimize diffs for review, and needs a code-style cleanup pass:. - rename variables with names in ALL_CAPS; - remove redundant type instantiation params in favor of <> operator; - add finals; - revisit the use of generic type params and required casts, etc in StratificationManager and stratifier classes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5440:240,redundant,redundant,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5440,1,['redundant'],['redundant']
Availability,"The way start positions are calculated causes issues when alleles span the boundaries of exons and the coding sequence itself. For exon boundaries:. The error stems from `FuncotatorUtils::getStartPositionInCodingSequence` and how the results of that method call are used in the `GencodeFuncotationFactory`. In addition:; Funcotator must be able to handle indels that span exon start boundaries. For example, in hg19 the following variant is not handled properly:. chr5:71622537-71622538 CA->C. The current workaround is to throw a FuncotatorUtils.TranscriptCodingSequenceException for the transcript causing this problem in Gencode. For Coding sequence boundaries, the following variant in hg38 causes a problem:. ```; chr17 80090386 rs71163918 CAGCACGTGCATGAACAACACAGGACACACACAGCACGTGCATGAACAACACAGGACACACACA C . PASS .; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4307:153,error,error,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4307,1,['error'],['error']
Availability,The work that @tomwhite has done in the HalpotypeCallerSpark has illuminated the fact that currently the downsampler is statefully dependent on the random generator underneath in terms of how it selects reads to be downsampled. This has become an issue since we would like to separate the process of assembly region construction and genotype calling across stages of the spark task. In order to do this successfully there needs to be some way to reproduce the same downsampled results for a given site based solely on the reads present at that site.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5437:105,down,downsampler,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5437,3,['down'],"['downsampled', 'downsampler']"
Availability,There are many useful utilities in there. Some of them should be pushed down to HTSJDK; some of them should be moved to Hellbender; and some of them are redundant with existing GATK utilities (e.g. MathUtil) and should be combined or skipped as necessary.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/215:72,down,down,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/215,2,"['down', 'redundant']","['down', 'redundant']"
Availability,"There are params in Extract Cohort that can be tightened up. Extract Tool has two params that are not used by Extract Features and are _already_ in Extract Cohort. The filter_set_name is used in the BQ filtering tables and looks like we can set it to be completely required for any type of filtering. There are 3 BQ filter tables -- 2 are needed for filtering (no matter what?) and 1 (tranches) is needed for thresholding and sensitivity calculations?. Genotype level filtering is true by default, but this doesn't seem like it should effect things after this cleanup. Though technically it should only be true if a filter_set_name has been specified. I will add another comment in the body of the code, but I would like to add this safety gate explicitly. Disable gnarly doesn't need to be a passed in param---so we'll rip it out for now. SNP and INDEL truth sensitivity and SNP and INDEL Lod scores are cumbersome to have to worry about passing in, but I dont see a better alternative. Should there be additional validation on these (where if they are specified, but no filter_set_name is, then they throw an error?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7293:1111,error,error,1111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7293,1,['error'],['error']
Availability,There are places in the genome where IUPAC bases can actually be decoded because the amino acid code is partially redundant. Add this logic into getMitochondrialAminoAcidByCodon and getEukaryoticAminoAcidByCodon.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6777:114,redundant,redundant,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6777,1,['redundant'],['redundant']
Availability,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862:1086,error,errors,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862,2,"['down', 'error']","['downstream', 'errors']"
Availability,"There are several changes in this PR:. * Re-use Picard's supercategory map and supercategory constants. This will keep in line Picard an GATK supercategories without the need of modifying the code in `HelpConstants`; * Re-use Picard's Testing program group. The `TestProgramGroup` is a duplicate of Picard's `Testing` program group. This duplicated also some constants from the Picard's code. Again, this will maintain in sync GATK.; * Remove the unused **""RNA-Specific Tools""** category constants, which was unused.; * Make public the `getSuperCategoryMap` as in Picard's code, to allow downstream projects to re-use supercategory assignation in their own documentation, and include their own.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4247:588,down,downstream,588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4247,1,['down'],['downstream']
Availability,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5003:302,error,error,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003,3,"['down', 'error']","['downgrade', 'error']"
Availability,There needs to be a validation tool for data sources to ensure that they conform to their formats properly. This tool is envisioned to be run just prior to data source release to fix any silent errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4380:194,error,errors,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380,1,['error'],['errors']
Availability,"There should be a robust mechanism to check whether an index file is up-to-date with respect to the file it indexes (eg., UUIDs, hashes, etc.). Modification time alone is not sufficient, since files can get uploaded out-of-order in cloud environments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5571:18,robust,robust,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5571,1,['robust'],['robust']
Availability,"There were a couple of things I needed to do to get the new Spark code running on a cluster:. i. Go back to using Spark's version of Kryo. Using a different version of Kryo is not actually needed (2.21 used by Spark passes the tests), and actually caused errors on the cluster when run with `--conf spark.driver.userClassPathFirst=true` (which is needed to avoid other library conflicts, like with jopt-simple). ii. Exclude Spark from the JAR file to avoid library conflicts. It's normal to exclude Spark and Hadoop from JAR files since they are supplied by `spark-submit`. Since Gradle doesn't have a 'provided' dependency (see https://github.com/broadinstitute/hellbender/issues/836), I had to do a bit of a workaround with the `shadowJar` target, which is now `sparkJar`. . Here's the command I ran:. ``` bash; NAMENODE=...; SPARK_MASTER=yarn-client; HELLBENDER_HOME=...; spark-submit \; --master $SPARK_MASTER \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; build/libs/hellbender-all-*-spark.jar ReadsPipelineSpark \; --input hdfs://$NAMENODE/user/$USER/bam/NA12878.chr17_69k_70k.dictFix.bam \; --output hdfs://$NAMENODE/user/$USER/out/spark-reads-pipeline \; --reference hdfs://$NAMENODE/user/$USER/fasta/human_g1k_v37.chr17_1Mb.fasta \; --baseRecalibrationKnownVariants $HELLBENDER_HOME/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf \; --sparkMaster $SPARK_MASTER ; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/882:255,error,errors,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/882,1,['error'],['errors']
Availability,There were a few dead links in the GATK to http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam which is still archived here: https://web.archive.org/web/20160720131152/http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam. We should write a new short technical article here: https://gatk.broadinstitute.org/hc/en-us/sections/360007134392-Glossary preserving the knowledge about sort ordering and update the remaining two links in our error messages to be current with that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8272:502,error,error,502,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8272,1,['error'],['error']
Availability,There's been a request from production (https://broadinstitute.atlassian.net/browse/DSDEEPB-2789) to try to ensure that the walker BQSR tools in hellbender can run with less than 3.5 GB of RAM to get costs down.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1460:206,down,down,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460,1,['down'],['down']
Availability,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8087:185,error,error,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087,5,"['down', 'error', 'failure']","['down', 'error', 'failure', 'failures']"
Availability,"These are the changes needed to run on a whole genome in strict mode. We get out of memory errors without these changes. Reads downsampling was missing for the part where `AssemblyRegion`s are filled with reads - this PR adds it in. Downsampling is not deterministic yet, since that depends on #5437, but that's an orthogonal issue so it's OK to merge this change and add #5437 later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5721:91,error,errors,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5721,3,"['Down', 'down', 'error']","['Downsampling', 'downsampling', 'errors']"
Availability,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391:65,down,down,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391,1,['down'],['down']
Availability,"These tools use fread + grep preprocessing (`fread(""grep ..."")`) to quickly read in large TSVs in the backend R scripts. Unfortunately, because of https://github.com/Rdatatable/data.table/issues/1139 and the fact that /dev/shm is limited to 64MB in a standard GATK Docker container, this can yield the following error when running within Docker:. ````; Stderr: grep: write error: No space left on device; Error in fread(sprintf(""grep -v ^@ %s"", tsv_file), sep = ""\t"", stringsAsFactors = FALSE, : ; Expected sep ('	') but new line, EOF (or other non printing character) ends field 2 when detecting types from point 10: 2	229515751	229516; Calls: source ... eval -> eval -> WriteModeledSegmentsPlot -> ReadTSV. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:131); 	at org.broadinstitute.hellbender.tools.copynumber.plotting.PlotModeledSegments.writeModeledSegmentsPlot(PlotModeledSegments.java:289); 	at org.broadinstitute.hellbender.tools.copynumber.plotting.PlotModeledSegments.doWork(PlotModeledSegments.java:206); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ````. Starting a Docker container with a sufficiently large `--shm-size` resolves this, but I am not sure if we ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140:312,error,error,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140,3,"['Error', 'error']","['Error', 'error']"
Availability,"This **CRAM file** and **its reference** are identical to those I have in the cloud because I uploaded these exact files to the cloud after downloading them from the 1000 Genomes Project FTP site. Back then, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154:140,down,downloading,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154,3,"['down', 'error']","['downloading', 'error']"
Availability,"This Barclay [feature](https://github.com/broadinstitute/barclay/issues/25) automatically expands the contents of a file ending in "".list"" whenever the target argument is a collection. This precludes the use of Picard interval list files ending in "".list"" with -L in GATK, since they contain a sam header. The raw sam header lines wind up getting added as interval strings, which then fails parsing: A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: @HD	VN:1.5: Problem parsing start/end value in interval string. Value was: 1.5. A short term GATK workaround is to use a file ending in one of the other known Picard interval list extensions (.interval_list, .intervals, or .picard) instead, but we should find a better fix for this since .list seems to be commonly used. Tools such as GetHetCoverage, which take an interval list in an argument typed as a File (--snpIntervals), are able to consume the interval file because the target argument is not a collection, so the auto-expansion is not triggered. I expect this issue could cause more problems in Picard as well once Barclay is the default parser there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3555:407,ERROR,ERROR,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3555,1,['ERROR'],['ERROR']
Availability,"This PR addresses spec-ops issue #235 - Use -m flag in final gsutil mv of files in ImportGenomes. . Additionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```; ➜ cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt; ➜ cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Ope",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7104:753,error,error,753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104,2,"['error', 'failure']","['error', 'failures']"
Availability,"This PR addresses two phasing bugs, https://github.com/broadinstitute/gatk/issues/6463 and https://github.com/broadinstitute/gatk/issues/6845. https://github.com/broadinstitute/gatk/issues/6463 identified a bug in the phasing algorithm which caused the wrong phase information to be output for scenarios where the first variant in a phase set is homozygous variant and it is followed by het variants in opposite phase. Without this change the het variants were incorrectly placed on the same phase strand because the phase set was tied to the hom var variant, and the algorithm assumed that each het variant could be put in the same phase strand as it because it was on all haplotypes. I've modified the algorithm to keep track, for variants that occur on all haplotypes, of which of the haplotypes have already been used for phasing an upstream ""comp"" variant so that further downstream variants can be checked against the remaining set. https://github.com/broadinstitute/gatk/issues/6845 showed an example of phase sets being disrupted by the presence of an alternate haplotype that supported an additional, uncalled, variant in the region. In this case there was an alternate haplotype supported by two reads that supported a SNP downstream of two pairs of SNPs in alternate phase. The presence of an additional haplotype causes the phasing algorithm to break the phase sets in the region. I've modified the algorithm to only use haplotypes that support the alternate alleles present in called variants in phasing by modifying the number that we pass as `AssemblyBasedCallerUtils.constructPhaseSetMapping()`'s `totalAvailableHaplotypes` parameter. In my opinion this ; fix produces output that is still correct and is much easier to understand (since it only depends on sites that are visible in the output VCF), but if anyone objects to this change please let me know. . Non-test code changes for this PR are in two different commits to try to make it easier to understand the scope of the two cha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7019:877,down,downstream,877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7019,1,['down'],['downstream']
Availability,"This PR converts the Mutect2Filtering engine to be allele specific. This required changes to SomaticClusteringModel and ThresholdCalculator as well as ErrorProbabilities and of course the filters themselves. There are some filters which have not yet been converted, but I am prioritizing the ones in this PR for Sarah Calvo and the mitochondria pipeline. This provides the implementation for dsp-spec-ops tickets 166, 168, 169",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6399:151,Error,ErrorProbabilities,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6399,1,['Error'],['ErrorProbabilities']
Availability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to the same chromosome with strand switch, but NOT significantly overlapping each other. We used to call inversions from such alignments, but it is more appropriate to emit BND records because a lot of times such signal is actually generated from inverted segmental duplications, or simply inverted mobile element insertions. To confidently interpret and distinguish between such events, we need other types of evidence, and is better to be dealt with downstream logic units. Inverted duplications are NOT dealt with in this PR and is going to be in the next. NEEDS TO WAIT UNTIL PART 1 & 2 ARE IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3457:566,down,downstream,566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3457,1,['down'],['downstream']
Availability,"This PR deals with the test failures that were occurring when we ran ALL chromosomes through the integration test, rather than just chr20 and X and Y (the default). It adds another truth set for all chromosomes.; Also two small changes.; - Skip the cost/table size check for the Hail integration, to allow it to get to the hail part if there are spurious test failures in cost.; - Change the name of the files used for table size and cost checking. Makes it easier to install new test data. Passing integration test on all chromosomes [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d8837252-26fa-4d40-bdf1-e42ff8932fd1); Passing integration test on chr20/x/y [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f552e7a3-d245-492d-b5e1-a35ba323fae8).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8787:28,failure,failures,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8787,2,['failure'],['failures']
Availability,"This PR dynamically sets the logging level for command line tools at runtime using the current version of log4j (we were headed down a path of downgrading to a previous version of log4j in order to implement this). However, it uses an API that is normally used in code for extending log4j rather than acting as a client to it, and requires an explicit cast of the value returned from LogManager.getContext. The Apache project site illustrates the use of this api in the first line of code in an example here: https://logging.apache.org/log4j/2.x/manual/customconfig.html#AddingToCurrent. We need to decide if we want to take this and stay on the current version or continue with the downgrade…",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/603:128,down,down,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603,3,['down'],"['down', 'downgrade', 'downgrading']"
Availability,"This PR fixes a critical error that was causing an ""invalid interval"" exception to be thrown while calling imprecise deletion intervals. The problem was a mismatch between the sequence dictionary in the reference and the read metadata's contig ID - contig name mapping. I've modified the code to always use the read metadata when translating the contig ID in `EvidenceTargetLink` intervals into contig names.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3671:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3671,1,['error'],['error']
Availability,"This PR fixes an error in the cost calculation in that we were not handling pre-emption. That is, a shard could get preempted and thus we'd have multiple cost measurements for that shard. ; I've addressed this by looking for a single 'representative' instance of each shard. But despite this, I found there is still non-determinism in the values written to the cost table. So, I've reset the thresholds we use for calculating if the test 'passes'. ; We may need some time to deal with finding real numbers (that is, my threshold may now be too conservative). Passing test runs:; All Chromosomes [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/adc09aff-e6bc-4289-ac34-d74b86eb92e2).; Chromosome 20/X/Y [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1b9a7d34-6d4f-42ca-8434-924ca84ee0ed).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8795:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8795,1,['error'],['error']
Availability,"This PR fixes an exception that was thrown when trying to serialize a ReferenceMultisource object. I was attempting to broadcast a 2bit reference in a Spark tool and found that I received a serialization error of the form:; ...; Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: ReferenceWindowFunctions$$Lambda$1/1599771323. Along with some related error reports (https://github.com/npgall/mobility-rpc/issues/12), this allowed me to track this down to the use of a Lambda function in the ReferenceWindowFunctions class. Replacing it with an explicitly defined function class solves the problem. Added a unit test which used to fail but now passes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1489:204,error,error,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1489,3,"['down', 'error']","['down', 'error']"
Availability,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8113:368,failure,failure,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113,2,"['error', 'failure']","['error', 'failure']"
Availability,"This PR includes two changes:; 1. Provide a command line argument to toggle the overlapping base quality correction (i.e. min(bq, 20)) before reassembly, which happens in FragmentUtils. I've found, however, that by the time SomaticGenotypingEngine runs, those the quality of these bases get bumped up to what they used to be, so this may be a no-op. I included it in case I missed something, and to be consistent with the branch @fleharty and @madduran have been using.; 2. Provide a command line argument to count the two reads in an overlapping pair separately in StrandArtfiact and StrandBiasBySample. This feature is only available in Mutect i.e. it won't affect other tools that use StrandBiasBySample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5286:626,avail,available,626,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5286,1,['avail'],['available']
Availability,This PR is against ah_var_store. Need to make another against EchoCallset. The new reference disk is installed.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/ba18dc3d-0548-48ac-a67b-cec55250de8a) is a passing run of GvsCreateVatFromVDS using quickstart against the new reference.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8746:62,Echo,EchoCallset,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8746,1,['Echo'],['EchoCallset']
Availability,"This PR is intended to introduce several new tools related to the CleanVcf workflow in GATK-SV, which the use of these tools being documented in https://github.com/broadinstitute/gatk-sv/pull/733. These tools are intended to introduce several enhancements over the existing implementation, including but not limited to:; - Introduce various unit and integration tests into the workflow.; - Create more robust and generalizable tools that can be used independent of _CleanVcf_.; - Improve runtime and execution speed by leveraging Java.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8996:402,robust,robust,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8996,1,['robust'],['robust']
Availability,This PR is the culmination of work from myself and @lbergelson to improve the runtime for MarkDuplicatesSpark on a single machine. This involved a rewrite of the tool as well as a number of improvements which should bring it into closer agreement with MarkDuplicates from picard. . Note: this is merely a checkpoint and there is still work that must be done to bring the work into agreement with recent MarkDuplicates development in picard. . Resolves #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4656:305,checkpoint,checkpoint,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656,1,['checkpoint'],['checkpoint']
Availability,"This PR is the initial stage of implementing the calling of IMPRECISE variants in the SV pipeline. It introduces the concept of an evidence-target link, which joins an evidence interval to its distal target. This is an extension of the 'coherent' evidence concept previously used in determining evidence thresholds for assembly. The code in this PR contains the following changes:. - Evidence intervals and distal targets now are treated as stranded, and evidence-target link clustering depends on overlaps between both intervals and strands.; - Evidence target interval and distal target interval calculations have been modified to make sure that evidence supporting the same event clusters together (has overlapping intervals). This includes several changes such as extending the 'rest-of-fragment-size' calculation to try to capture almost all non-outlier fragment sizes in the library; increasing the split read location uncertainty a little; and being more precise about the boundaries of distal target intervals by taking advantage of information in the MD and MC tags if available.; - Evidence target links are gathered for every piece of evidence supporting a high-quality distal target. ; - Evidence target links are clustered together and store the amount of split-read and read-pair evidence that went into each cluster.; - All evidence target link clusters that are composed of at least 1 split read or at least 2 read pairs are collected in the driver and emitted in a BEDPE formatted file specified in the command line parameters.; - A `PairedStrandedIntervalTree` data structure is introduced to allow `SVIntervalTree`-style lookups for paired intervals. To finish this work, future PRs will 1) use the collected evidence target links to annotate our assembly called-variants with the number of split reads and read pairs observed in the original mappings and 2) create IMPRECISE VCF records for events that have enough evidence-target-link support, first for deletions and then possibl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469:1078,avail,available,1078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469,1,['avail'],['available']
Availability,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8507:337,down,down,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507,1,['down'],['down']
Availability,"This PR removes the cap on insert size for calling an 'innie' read pair SV evidence. Previously this was set to a max Z score of 100, which worked out to be around 10kb for some libraries. This was causing us to not gather read-pair based evidence around deletions of over 10kb. The motivation for this limit was to exclude very long read pairs as evidence since they are predominantly mapping errors. I believe that a) this limit was set too low, causing us to miss real events and b) the reads with very long insert sizes likely cover many instances of real variation, even if the actual mappings locations are incorrect, and that the assembly process, variant filters, and correlation with coverage data should be able to filter out assemblies triggered by reads with long mappings. . Removing the limit on insert size does cause the imprecise deletion-calling code to produce many more spurious calls. In practice, I think that we should still evaluate this evidence but not make a call unless it is supported by coverage information, and a long deletion event should be very visible from depth-based calling. Therefore, I've added a limit on the size of imprecise deletions that will be called right now, with the expectation that this hard limit will be removed in favor of depth and other filtering in the future. Manual evaluation of the results shows a small increase in incorrect imprecise variants (mostly in centromeric regions) that were just small enough to slip under the hard threshold, a very small number of incorrect small insertions caused by assembling messy regions, and a number of real 10-25kb deletions that we now assemble and either call an imprecise or precise deletion variant at, so I think this is a small net gain in callset quality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4333:394,error,errors,394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4333,1,['error'],['errors']
Availability,"This addresses issue 569 - the cleanup of format errors in bam and sam files in tests. I will send an archive with a README, validations for the post-modification bams and sams, and diffs for the bams to akiezun.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/809:49,error,errors,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809,1,['error'],['errors']
Availability,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5608:139,error,error,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608,4,['error'],['error']
Availability,"This affects `RunBWAMEMViaCommandLine.java` and `RunSGAViaProcessBuilderOnSpark.java`, where stdio are always captured.; Capturing BWA MEM's stdout is a must because the result is piped to stdout.; For SGA, this helps debugging. But this also slows down the performance. Hence an option would be useful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1878:249,down,down,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1878,1,['down'],['down']
Availability,"This allows the user to write their own description in the FILTER line in the header. Currently a user can define an interval list to filter with and name that filter, but the description line is always ""Doesn't overlap a user-input mask"" or ""Overlaps a user-input mask"", whereas it might be more useful to let the user provide a more detailed description (for example, ""Outside of sequencing target intervals"").",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8831:233,mask,mask,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8831,2,['mask'],['mask']
Availability,This also start the mark down for how to find cost info for the aou prod project,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7236:25,down,down,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7236,1,['down'],['down']
Availability,This branch (or some variation therin) will be necessary to keep track of how well #5980 is doing at bringing down the number of found haplotypes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6049:110,down,down,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6049,1,['down'],['down']
Availability,"This branch adds the ability to perform a Broadcast of the reference and also support for 2bit references. I'm submitting this PR against the `dr_lb_spark_bqsr` branch. I can run the following two commands on my local machine, which produce the same output. Broadcast:. ```; ~/Downloads/spark-1.4.1-bin-hadoop2.4/bin/spark-submit \; --master local[2] \; ~/repos/hellbender/build/libs/hellbender-all-GATK.4.*-spark.jar \; BaseRecalibratorSpark \; --input ~/repos/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam \; --knownSites ~/repos/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf \; --reference ~/repos/hellbender/src/test/resources/large/human_g1k_v37.20.21.2bit \; --joinStrategy BROADCAST \; --output ~/tmp/bqsr.out.2.txt; ```. Shuffle:. ```; ~/Downloads/spark-1.4.1-bin-hadoop2.4/bin/spark-submit \; --master local[2] \; ~/repos/hellbender/build/libs/hellbender-all-GATK.4.*-spark.jar \; BaseRecalibratorSpark \; --input ~/repos/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam \; --knownSites ~/repos/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf \; --reference ~/repos/hellbender/src/test/resources/large/human_g1k_v37.20.21.fasta \; --joinStrategy SHUFFLE \; --output ~/tmp/bqsr.out.3.txt; ```. Still need to try it on a cluster and on bigger input. cc @tomwhite @davidadamsphd for review.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/919:277,Down,Downloads,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/919,2,['Down'],['Downloads']
Availability,"This branch takes the version of gatk-public that gatk-protected currently depends on (4.alpha.2-188-g7332d10) and applies @davidbenjamin 's fix to the `TandemRepeat` annotation to it. The only purpose of this PR is to cause a snapshot to be generated -- do not merge!. This is necessary to unblock @davidbenjamin 's work, because the `HaplotypeCaller` tests are failing if we update protected to the latest public head, and although we've fixed some of the issues there are some unexplained failures in the concordance tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2569:492,failure,failures,492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2569,1,['failure'],['failures']
Availability,"This brings in some additional retries for UnknownHostException and 502 errors,; and moves us from a fork in my personal github repository to the fork in; https://github.com/broadinstitute/google-cloud-java. Resolves #4888; Resolves #5094",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5099:72,error,errors,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5099,1,['error'],['errors']
Availability,"This can cut down the size of the PoN. However, see #4554.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4553:13,down,down,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4553,1,['down'],['down']
Availability,This change is dependent on [this recent change](https://github.com/samtools/htsjdk/commit/4f550e1f1afabf21467957fa672ca2a4ad457897#diff-b678735810949d4263df7bd0fffdecb8L42) in htsjdk (and the build will fail without it). Once htsjdk2.0 is available we'll upgrade it in this branch/pr so the two changes can go in together.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1243:240,avail,available,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1243,1,['avail'],['available']
Availability,"This class is now needed by other pipelines, so let's make it generally available; in the engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/476:72,avail,available,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/476,1,['avail'],['available']
Availability,"This code adds log statements with ""START"" and ""END"" keywords so that we can then process the logs and see (i) how many times a specific operation was called, (ii) how long the operation took (per call, or in aggregate). In addition it has the notion of intermediate ""steps"" so we can drill down individual operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/628:291,down,down,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/628,1,['down'],['down']
Availability,"This code wraps around BaseRecalibrator and presents a very basic interface (set up, add reads, teardown) that's going to be used at each Dataflow worker. Challenges here:; (1) I need to convert the intervals to Features because that's what the BaseRecalibrator class uses, and SimpleInterval is not a subclass of Feature. This may change in the future.; (2) BaseRecalibrator takes Features as inputs - the only simple Feature class I found I could reuse is ArtificialTestFeature. Please let me know if there is a better choice (solving (1) also solves this); (3) I didn't find code to test overlap between a SimpleInterval and a Feature. Rather than roll my own I chose to use the SimpleInterval overlap test and convert to Feature lazily instead of eagerly. This may cause an interval to be converted more than once. So please consider this the start of a discussion on ""here is something that works, but surely there's a better way?"" I'm not so much looking for every performance opportunity, but ideally I'd like to avoid using ArtificialTestFeature if a better candidate is available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511:1079,avail,available,1079,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511,1,['avail'],['available']
Availability,"This constant controls both the maximum number of retries and the maximum number of reopens the GCS NIO library will perform in the face of transient errors. It's currently hardcoded, but should be exposed as an engine argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3315:150,error,errors,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3315,1,['error'],['errors']
Availability,"This enables PoNs to be built with a number of intervals greater than 16777216, which lets us push down to bin sizes <175bp if really necessary. At some point, it is better to scatter and build a separate PoN for each contig so we don't run into memory issues. However, the official WDL is not set up to do the scatter, so one would have to write a custom WDL. @mwalker174 Do you mind taking a look?. Closes #4365.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4528:99,down,down,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4528,1,['down'],['down']
Availability,"This error does not occur with every VCF but at least with the one enclosed below. It passes vcf-validator with default arguments. . To reproduce: . ```; term1$ spark-shell # start you spark local cluster in another screen; ...; term2$ cd /dsde/working/valentin/bugs/gatk-var-walker-ser; term2$ git checkout 58cb99ec ; term2$ ./gradlew sparkJar; term2$ ./gatk-launch ExampleVariantWalkerSpark -V ./in.vcf.gz -- --sparkRunner SPARK --sparkMaster local. ```. ```; The stacktrace starts with:. 17/03/29 16:44:56 INFO SparkContext: Successfully stopped SparkContext; 16:44:56.000 INFO ExampleVariantWalkerSpark - Shutting down engine; [March 29, 2017 4:44:56 PM EDT] org.broadinstitute.hellbender.tools.examples.ExampleVariantWalkerSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=576192512; java.lang.IllegalArgumentException: requirement failed: The partition coalescer passed in must be serializable.; 	at scala.Predef$.require(Predef.scala:224); 	at org.apache.spark.rdd.CoalescedRDD.<init>(CoalescedRDD.scala:84); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:466); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:445); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.coalesce(RDD.scala:445); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.coalesce(SparkSharder.java:321); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:189); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:126); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.shard(SparkSharder.java:99); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.getVariants(VariantWalkerSpark.java:129); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2545:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545,2,"['down', 'error']","['down', 'error']"
Availability,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7983:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983,4,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"This error was reported by a user: . Dear GATK Team, . I am writing you to discuss a an error while using FilterMutectCalls program and it seems a potential bug. ; I wanted to change the default value of **--normal-p-value-threshold **. It looks like the tool doesn't accept this parameter at all. I tried using it with the default value of 0.001 as well. . The program works fine when this parameter is removed. Moreover, the error message also states that **BUG: couldn't set field value** . please see the attached command and the error message. . ** gatk FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz**. Using GATK jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5978:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978,4,['error'],['error']
Availability,This fixes a bug in handling the defaults for setting and using our default cluster initialization script for the SV pipeline. The master version will error if no init script parameters are specified.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3467:151,error,error,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3467,1,['error'],['error']
Availability,"This fixes a bug in the `AlleleFrequencyCalculator` that was causing quality to be overestimated for sites with `*` alleles representing spanning deletions. The bug was causing the calculator to not include homozygous `*` genotypes in the sum of non-site specific variant allele probabilities that is the basis for the qual score. The bug was caused by an off-by-one index error: `IndexRange(0,2)` returns `[0,1]`, not `[0,1,2]` as intended. Not including this genotype inflated the quality score for these sites. . Due to interactions with QUAL-based variant and allele trimming, this causes slightly different behavior when HaplotyeCaller is run in modes where it is forced to emit variants for every locus, as can be seen in the `expected/gvcf.basepairResolution.includeNonVariantSites.vcf` test file for `GenotypeGVCFsIntegrationTest`: 1) Sites spanned by a deletion are now reported with a `*` alt allele and have QUAL 0 and a LowQual filter. Also added a mechanism to `GenotypeGCVFsIntegrationTest` to automatically update the expected result files, similar to what already exists in `HaplotypeCallerIntegrationTest` and `CombineGVCFsIntegrationTest`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6859:373,error,error,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6859,1,['error'],['error']
Availability,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5786:191,recover,recovering,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786,2,['recover'],"['recovering', 'recovery']"
Availability,This fixes a small math error in Permutect that was harming precision in tumor-normal mode.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8668:24,error,error,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8668,1,['error'],['error']
Availability,This fixes the first day's downloads on a new release which previously were set to 0. @jonn-smith This is what I was talking about.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7794:27,down,downloads,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7794,1,['down'],['downloads']
Availability,"This fixes two issues discovered while testing my URI migration branch:. - ReadsSourceSpark.getHeader doesn't propagate the reference at all when a CRAM file input resides on GCS, so it always results in a ""no reference was provided"" error, even when a reference was provided.; - ReadsSourceSpark.checkCramReference always tried to create a Hadoop Path object for the reference no matter what file system it lives on, which fails when using a reference on GCS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6517:234,error,error,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6517,1,['error'],['error']
Availability,This gives better errors in the case of sequence dictionary mismatches.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2615:18,error,errors,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2615,1,['error'],['errors']
Availability,This greatly reduces wall clock time in M2 scatters without affecting sensitivity. It is decoupled from HaplotypeCaller's downsampling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3988:122,down,downsampling,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3988,1,['down'],['downsampling']
Availability,"This includes the fix for the position overflowing in CloudStorageReadChannel; (https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2283), as well; as the fix for the intermittent 503 errors we've already been depending on; (https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3373:194,error,errors,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3373,1,['error'],['errors']
Availability,"This introduces a feature for `GenomicsDBImport` that allows merging multiple contigs into fewer genomicsdb partitions. This should give a huge boost for cases where users have a very large number of contigs (see [here ](https://gatk.broadinstitute.org/hc/en-us/community/posts/360060623952-GenomicsDBImport-very-slow-on-genome-with-many-contigs), for instance). Currently, GenomicsDB would create a separate folder/partition for each contig and this slows down import to a crawl with a large number of contigs. . To use this feature, users should set the flag `--merge-contigs-into-num-partitions` to the number of partitions. Using the feature requires that entire contigs be passed as input intervals -- we don't support merging together an interval list that contains partial contigs. . There's no magic threshold where this would start to be useful - we currently warn users when they specify more than 100 intervals, and I think the same threshold makes sense for when they should consider using this flag. Choosing the right value for `--merge-contigs-into-num-partitions` would be dependent on amount of parallelism users want to use (for example, do they want to want to import using `max-num-intervals-to-import-in-parallel`). If no parallelism is envisioned either on import or query, setting `--merge-contigs-into-num-partitions` to `1` should work as well -- though the user may find it more reassuring to break up the work into more partitions just so you can see some progress being made....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6681:457,down,down,457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6681,1,['down'],['down']
Availability,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:10,checkpoint,checkpoint,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,1,['checkpoint'],['checkpoint']
Availability,"This is a follow up from a discussion from the GATK Office Hours meeting. The user has not found that the --dont-use-soft-clip-bases parameter is the culprit of this difference. The user is sending in a bug report. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078111952-The-depth-of-SNV-InDels-calculated-by-Mutect2-in-GATK4-2-0-0-is-much-lower-than-real-sequencing-depth-why-](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078111952-The-depth-of-SNV-InDels-calculated-by-Mutect2-in-GATK4-2-0-0-is-much-lower-than-real-sequencing-depth-why-). \--. Hi GATK team, . GATK version: 4.2.0.0. Sample: Target region sequencing, human cancer, everage depth 1000X. I'm using Mutect2 in GATK4.2.0.0 to call somatic SNV/InDels. What confused me is that I found the depth of each location emitted from ""AD"" field in vcf is much lower than the real depth. I think I have disabled downsampling by set ""--max-reads-per-alignment-start"" to 0. The command line I used is as follow: . gatk  Mutect2  -R  reference.fa  -I  tumor.bam  --panel-of-normals  pon.vcf.gz  -L  target.bed  -O  sample.snvIndels.vcf  --callable-depth  30  --f1r2-tar-gz  sample.f1r2.tar.gz  --min-base-quality-score  25  --max-reads-per-alignment-start  0  --minimum-allele-fraction  0.002  --dont-use-soft-clipped-bases  --force-active  --mitochondria-mode  --enable-all-annotations . For example, a mutated point information in vcf called by GATK4.2.0.0-Mutect2 is: . 1 24868045 . A G . . AC=1;AF=0.500;AN=2;AS\_MQ=60.00;AS\_SB\_TABLE=51,50|46,23;AS\_UNIQ\_ALT\_READ\_COUNT=69;BaseQRankSum=0.561;ClippingRankSum=-1.473;DP=179;ECNT=2;FS=13.849;LikelihoodRankSum=-0.392;MBQ=37,37;MFRL=236,239;MMQ=60,60;MPOS=44;MQ=60.00;MQ0=0;MQRankSum=0.000;NCC=0;NCount=0;OCM=0;PON;POPAF=7.30;REF\_BASES=GCTCAGCAGAACAGACCCAGA;ReadPosRankSum=1.335;SOR=1.545;Samples=HD786\_4-1;TLOD=230.09 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:**101,69**:0.408:170:54,30:45,39:51,50,46,23. But the information of the same point in vcf called by GATK4.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7285:903,down,downsampling,903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7285,1,['down'],['downsampling']
Availability,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8737:66,Echo,EchoCallset,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737,4,['Echo'],['EchoCallset']
Availability,"This is a port of the GATK3 version I actually ran. Some differences in the genotyping engines and the fixed median calculation in MathUtils between GATK3 and GATK4 make the results _slightly_ different in some cases, but still within tolerances.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4940:235,toler,tolerances,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4940,1,['toler'],['tolerances']
Availability,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6034:524,recover,recovery,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034,1,['recover'],['recovery']
Availability,"This is based on some digging for a user query here: https://gatk.broadinstitute.org/hc/en-us/community/posts/360060283092-GenotypeGVCFs-java-lang-NullPointerException. The background here is that GenomicsDB added support for 64 bit INFO fields starting 4.1.5.0. When using bcf codec, GenomicsDB will pass a 64 bit type to GATK, which isn't supported by GATK. But instead of flagging that as a missing type, it seems like GATK still tries to decode the missing type hitting the NPE. We probably want to throw a better error so the user knows what the issue is.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6548:518,error,error,518,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6548,1,['error'],['error']
Availability,"This is due to`GATKVAriantContextUtils::makeGenotypeCall` using `GenotypeLikelihoods.getAlleles`, which throws an error because you have to explicitly tell `GenotypeLikelihoods` to initialize ploidies != 2. We can fix this by explicitly initializing but it seems that `GenotypeLikelihoodCalculators` has the same functionality but more robust.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2214:114,error,error,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2214,2,"['error', 'robust']","['error', 'robust']"
Availability,This is important for Infogain and other potential hybrid data because it will allow Permutect to separately normalize within each read group and otherwise keep track of different read groups downstream. It also makes the data structures in Permutect much more convenient because numeric data is easier to write to a memory map.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8860:192,down,downstream,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8860,1,['down'],['downstream']
Availability,"This is part of the effort to port GATK3 VariantEval to GATK4. Because VariantEval is a pretty big tool, I thought it would help to separate changes in the VariantEval package from core GATK changes. If you would prefer I could split this PR into several, based on what they target. However, here is a summary:. 1) I added a method to FeatureInput so downstream code and determine if the name is actually user-supplied or whether this is the default one assigned by GATK. 2) I split MultiVariantWalker into a base class that does not have arguments specified. This is comparable to what already exists for VariantWalkerBase - this is a pretty minor change. 3) I added a method to VariantWalkerBase to return the set of intervals potentially being iterated. The intent is to return either the full genome, or the user-supplied intervals. . 4) I restored some code to MendelianViolation that is used by components of VariantEval. This is very close to identical from GATK3. 5) Likewise to SampleDBBuilder (code had to be tweaked for GATK4). 6) IntervalUtils/MathUtils/Utils: I restored from GATK3 methods not ported to GATK4. These should be exact copies of GATK3 unless a change was needed. Once these are closed out, I will make a separate PR with VariantEval itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495:351,down,downstream,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495,1,['down'],['downstream']
Availability,"This is probably complicated by a bug in the htsjdk warning from previous versions, which should be fixed in the latest master now. There's probably still a bug, but the error will be more informative now. There may be a ploidy-related bug since the somatic genotypes are a little funky that way. I don't like the fact that this is calling a biallelic method. @fleharty if you still care about this, can you run it again with the latest master?. _Originally posted by @ldgauthier in https://github.com/broadinstitute/gatk/issues/6689#issuecomment-898580381_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7729:170,error,error,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7729,1,['error'],['error']
Availability,"This is rebased off of https://github.com/broadinstitute/gatk/pull/3716, since it depends on code there. Hence, only the second commit needs to be reviewed in this PR. The code and tests are quite similar to that for PlotSegmentedCopyRatio/PlotACNVResults. However, I've changed the R scripts to be more efficient (WGS plots no longer take several hours). Furthermore, PlotModeledSegments is more flexible than PlotACNVResults in that it plots CR, AF, or both on the fly depending on the available inputs. I've also added some more input validation, changed some terminology, and moved over to data.table for reading TSVs in R.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3729:488,avail,available,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3729,1,['avail'],['available']
Availability,"This is set to an unexposed 250 samples. Since the sampling and posterior estimation is done online to prevent the entire samples x intervals x dCR samples matrix from causing OOM, I think we suffer from lack of vectorization. However, the good news is that we can probably get by with far fewer samples, say ~20) if all we want are estimates of posterior mean and standard deviation---not even sure if the latter will be used by downstream analyses anytime soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754:430,down,downstream,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754,1,['down'],['downstream']
Availability,"This is something we got to only in a rather basic way in GATK but is very useful to enable in order to save users from themselves. This would involve three components:. 1) Hard min/max values that correspond to limits beyond which values could cause errors/program failures; violation should throw a User Exception;. 2) Recommended min/max values that correspond to limits beyond which values do not make sense for a given analysis functionality for standard use cases; violation should log a WARN entry. 3) Behavior-disabling value if applicable. Let's say we have an argument that provides a threshold for filtering; and it takes min. 4, max. 20. We may want to set it up so that passing -1 disables the behavior controlled by the argument (so in the filtering case, ""-1"" means ""don't filter at all"") without tripping the min value check. . These should all be accessible to the GATKDoclet (or equivalent) for documentation purposes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/143:251,error,errors,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143,2,"['error', 'failure']","['errors', 'failures']"
Availability,"This isn't just a matter of changing the number. [RegisterCoder was made more stringent](https://cloud.google.com/dataflow/release-notes/java) and this will force some code changes. Hopefully only little ones, but I got only as far as getting an internal Java error and I think that's a sign I should go to bed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/754:260,error,error,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/754,1,['error'],['error']
Availability,"This issue affect version of GATK4 from version 4.0.10.1 and onwards, and is related to github feature/pull request [#4969: ""Improve MQ calculation accuracy""](https://github.com/broadinstitute/gatk/pull/4969). GVCFs with large `RAW_MQ` sum of sumSquaredMQs values end up with an error like:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. The error happens when the sumSquaredMQs element value is greater than Java's. `Integer.MAX_VALUE`. See the following [github pull request comment for more details](https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813). . Should a `java.lang.Long`, `java.math.BigInteger`, `java.lang.Double`, or `java.math.BigDecimal` be used here instead of a `java.lang.Integer` for method: [`parseRawDataString` inside `org.broadinstitute.hellbender.tools.walkers.annotator.RMSMappingQuality`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java#L251) ?. I've noticed this error when using GATK4 version 4.0.11.0 on the [GATK4 Germline SNPs-INDELs WDL workflow](https://github.com/gatk-workflows/gatk4-germline-snps-indels). It happens on the [`JointGenotyping.GenotypeGVCFs` task](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl#L127).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5433:279,error,error,279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5433,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"This issue came up on the forum and Ted Brookings took a look at the stack trace to verify that it is most likely not an issue with the input data. The user also specified that they did not get this issue with GATK 4.2.0.0. This request was created from a contribution made by Quentin Chartreux on September 10, 2021 12:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException). \--. I try to perform joint genotyping with genotypeGVCF. . Before i run haplotypecaller then Genomicsdbiimport and now genotypesGVCF. . I used gatk 4.2.2.0. . I run Genomicsdbiimport by interval so i would like to perform genotypesGVCF by interval. . the command use for genotypesgvcf is : . gatk \\ ; ; \--java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" \\ ; ; GenotypeGVCFs \\ ; ; \-R ${REF\_Genome} \\ ; ; \-V gendb://${vcf\_database\_tmp} \\ ; ; \-O ${TMP\_DIR}/gentaumix\_interval\_${SLURM\_ARRAY\_TASK\_ID}\_raw.vcf.gz \\ ; ; \-D ${DBSNP} \\ ; ; \--sequence-dictionary ${Dict} \\ ; ; \-L ${Interval} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation \\ ; ; \--only-output-calls-starting-in-intervals \\ ; ; \--merge-input-intervals \\ ; ; 2> ${log\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}. And the log (for one interval but it's the same for all): . Using GATK jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7465:420,error,error-IndexOutOfBoundsException,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465,2,['error'],['error-IndexOutOfBoundsException']
Availability,"This issue has come up during my work on #6634 and has resulted in the decision to introduce a new argument to GATK `--use-original-alignments-for-genotyping-overlap` in order to better match DRAGEN for concordance. Reads in the GATK undergo a number of modifications before they are used for genotyping that I have listed down below. (Note: between each of these steps some reads get lost to various filtering code and this is not an exhaustive list). 1. Reads undergo modification in `AssemblyBasedCallerUtils.finalizeRegion()` where the reads have their soft-clipped bases reverted, low quality ends removed, mate overlapping base qualities modified, and overhangs outside of the active region removed. Then these reads are used for assembly to discover haplotypes. ; 2. Once we have discovered haplotypes the whole assembly region (reads, haplotypes and all) gets trimmed down to a smaller span that ~overlaps the variants discovered the haplotypes plus either 75+ or 20 bases of padding depending on what type of events are seen. ; 3. These clipped reads (with reads below 10 bases in length being removed) have their base qualities farther modified in `PairHMMLikelihoodCalculationEngine.createQualityModifiedRead()` in various ways. This modification does not stick however since the base qualities are all modified on a clean partial copy of the read.; 4. Following this the reads (the ones from step 2) are realigned to the reference according to their best haplotypes. Sometimes this means as few as 11 bases of ""read"" are being realigned at this stage. . It is these realigned reads that are used for genotyping, where the only reads that are actually used to contribute likelihoods for calls are reads that overlap the variant event within 2 bases of overlap on either side. In DRAGEN they do something different that we had to replicate to achieve concordance. Dragen still performs equivalent modifications for steps 1-3 as they apply to the reads but rather than performing step 4 and u",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6706:323,down,down,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6706,2,['down'],['down']
Availability,"This issue is for tracking https://github.com/samtools/samtools/issues/704. The samtools-generated .idx and .dict entries for a reference sequence that contains a ""."" have different lengths (the .dict appears to not count the "".""), and the MD5s are different. This causes GATK to throw:. > A USER ERROR has occurred: Couldn't read file /Users/cnorman/projects/htsjdk/src/test/resources/htsjdk/samtools/cram/amb.fa. Error was: Index length does not match dictionary length for contig: iupac with exception: Index length does not match dictionary length for contig: iupac. and causes failures when reading a CRAM file using that reference. It works fine if the .dict is regenerated using GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3306:297,ERROR,ERROR,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3306,3,"['ERROR', 'Error', 'failure']","['ERROR', 'Error', 'failures']"
Availability,This method (validateSequenceDictionaries) in GATKTool needs to be modified so that the vcf file names associated with each sequence dictionary are passed into validateDictionaries() to make error messages more useful.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/660:191,error,error,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/660,1,['error'],['error']
Availability,This migrates ONLY that part of the code from ah_var_store to EchoCallset branch (there are other changes in ah_var_store),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8755:62,Echo,EchoCallset,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8755,1,['Echo'],['EchoCallset']
Availability,"This moves us to a snapshot of google-cloud-java based off of a branch in my fork here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. This patch wraps many more operations within retries, and in our tests resolves the intermittent 503/SSL errors completely when running at scale. This PR also migrates us from setting retry settings per-Path to setting it globally, using a new API from that google-cloud-java branch. This fixes an issue where the number of reopens was getting set to 0 deep in the google-cloud-java library. Resolves #2749; Resolves #2685; Resolves #3118; Resolves #3120; Resolves #3253",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3295:287,error,errors,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295,1,['error'],['errors']
Availability,"This new PathSeq WDL redesigns the workflow for improved performance in the cloud. Downsampling can be applied to BAMs with high microbial content (ie >10M reads) that normally cause performance issues. . Other improvements include:. * Removed microbial fasta input, as only the sequence dictionary is needed.; * Broke pipeline down to into smaller tasks. This helps reduce costs by a) provisioning fewer resources at the filter and score phases of the pipeline and b) reducing job wall time to minimize the likelihood of VM preemption.; * Filter-only option, which can be used to cheaply estimate the number of microbial reads in the sample.; * Metrics are now parsed so they can be fed as output to the Terra data model.; * CRAM-to-BAM capability; * Updated WDL readme; * Deleted unneeded WDL json configuration, as the configuration can be provided in Terra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6536:83,Down,Downsampling,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6536,2,"['Down', 'down']","['Downsampling', 'down']"
Availability,"This read filter removes unnmapped reads and reads with unmapped mates. When used in combination with `MateOnSameContigOrNoMappedMateReadFilter` this subsets down to reads only on chrM whose mate is also on chrM. If we only used the `MateOnSameContigOrNoMappedMateReadFilter` we end up with reads whose mate is unmapped still in the BAM, but not the unmapped read, which causes problems downstream in the mitochondria pipeline. This read filter will make the subsetting step faster when we no longer need the NuMTs. I would really appreciate this getting in before the next release (on Tuesday). (fyi @droazen) @ldgauthier @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5826:158,down,down,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5826,2,['down'],"['down', 'downstream']"
Availability,"This release contains important bugfixes, including a fix for https://github.com/broadinstitute/gatk/issues/8141 (intermittent failure to properly compress outputs)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8409:127,failure,failure,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8409,1,['failure'],['failure']
Availability,This removes some symlinks that were checked into git-lfs and puts them back into normal git. It stops lfs from outputting failure messages on checkout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5229:123,failure,failure,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5229,1,['failure'],['failure']
Availability,"This request removes all calls to getChr() in the code by replacing calls to getChr() with calls to getContig() and removing a redundant assertion that contains a call to getChr(). To complete the removal of getChr() from the code, we could:. Remove the getChr() method overrides in the TableFeature and ArtificialTestFeature classes and convert these classes from implementing the Feature interface to implementing the Locatable interface. Remove the definition of the Feature interface from htsjdk.tribble (since all Feature does is add getChr() to the Locatable interface), and replace all references to the Feature interface in the tools and libraries with references to the Locatable interface. These steps will also remove the deprecation warnings for getChr() (part of #377).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/478:127,redundant,redundant,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/478,1,['redundant'],['redundant']
Availability,"This request was created from a contribution made by ABours on May 29, 2020 18:23 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360067695771-GenotypeGvcfs-has-formatting-issues-in-both-v4-1-6-0-as-v4-1-7-0. --. Hi,. I'm using v4.1.6.0 of GenotypeGvcfs to make a vcf, out of whole genome data from 19 samples (following your recommendations). When I run ValidateVariants to check the output of GenotypeGvcfs I get a error message, which states that one or more of the ALT allele are actually not in the samples provided. A previous user already found a similar error in ValidateVariants (https://gatk.broadinstitute.org/hc/en-us/community/posts/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-), but then for Haplotypecaller, and you have opened a bugreport to add a feature to ValidateVariants: https://github.com/broadinstitute/gatk/issues/6553. However, it would be nice if you could actually investigate the formatting error. Unfortunately my formatting error isn't the same as reported in the other post. I have 105 error in which the 1st alternative allele is a spanning deletion and the 2nd (and 3rd) is either an indel or snp. It's true that the 2nd and 3rd allele is actually not found in my samples. I even have 7 occurances in which the 1st allele (spanning deletion) has allele frequency 1.00. my code is the following for GenotypeGVCFs:. java -Xms32G -Xmx32G -jar ${gatk4} GenotypeGVCFs -R ${ref} -V ${pipeline}/${name}\_v4.1.6.0.g.vcf.gz -O ${vcf}/${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list 2> ${log}/${name}\_v4.1.6.0\_genotype.log. for ValidateVariants:. java -Xms10G -Xmx10G -jar ${gatk4} ValidateVariants -R ${ref} -V ${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list --warn-on-errors 2> ${log}/${name}\_v4.1.6.0\_genotype\_valivar.log. the warning in ValidateVariants and the site look like this:. 14:12:15.126 WARN ValidateVariants - \*\*\*\*\* Input 1st\_v4.1.6.0.vcf.gz fails strict validation of type ALL: on",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:437,error,error,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,3,['error'],['error']
Availability,"This request was created from a contribution made by D B on January 17, 2020 14:43 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360056339072-Mutect2-Need-one-or-two-reads-to-construct-a-fragment. --. The command I used below is able to generate the .vcf output along with its index and stats file, but my snakemake run fails to complete due to exit status (3 instead of 0). I wonder if below error is caused by trying to split the runs by chromosome and setting improper interval padding. Thank you for your time. ------------------------------------------------------------------------------------------------------------------------------------. a) GATK version used. _The Genome Analysis Toolkit (GATK) v4.1.4.1_. b) Exact GATK commands used. _/usr/bin/time -v gatk --java-options ""-Xmx10G"" Mutect2 -R ../reference/indices\_010920/GRCh38.d1.vd1.fa -L chr4.bed -I chr4.bam --max-mnp-distance 0 --interval-padding 100 -O chr4.vcf.gz_. c) The entire error log if applicable. _java.lang.IllegalArgumentException: Need one or two reads to construct a fragment_ ; _at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725)_ ; _at org.broadinstitute.hellbender.utils.read.Fragment.create(Fragment.java:43)_ ; _at org.broadinstitute.hellbender.utils.read.Fragment.createAndAvoidFailure(Fragment.java:58)_ ; _at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)_ ; _at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1376)_ ; _at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)_ ; _at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)_ ; _at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)_ ; _at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)_ ; _at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)_ ; _at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.groupEvidence(AlleleLikeliho",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6419:415,error,error,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6419,2,['error'],['error']
Availability,"This request was created from a contribution made by Duo Xie on August 20, 2022 16:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --know",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:337,error,errors,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,2,['error'],"['error', 'errors']"
Availability,"This request was created from a contribution made by Elizabeth Lee on October 19, 2022 17:47 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/9761457082907-JointGenotyping-ImportGvcfs-terminates-without-an-active-exception](https://gatk.broadinstitute.org/hc/en-us/community/posts/9761457082907-JointGenotyping-ImportGvcfs-terminates-without-an-active-exception). \--. JointGenotyping fails in ImportGvcfs with the c++ error ""terminate called without an active exception"", which occurs when a thread goes out of scope without calling join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the wh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:439,error,error,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['error'],['error']
Availability,"This request was created from a contribution made by FranBC on July 23, 2021 18:59 UTC.; This user is receiveing a NullPointerException error when running CollectvariantCallingMetrics and has verified that the vcf headers match the reference. The user uploaded their file to the GATK FTP server as ""CollectVariantCallingMetrics_dbsnp155_FranBC2.tar.gz"". Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community\_comment\_4404056347547](https://gatk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community_comment_4404056347547). \--. Dear GATK Team,. I am having a similar issue to Yenan, when using CollectVariantCallingMetrics, was a cause/workaround ever found for this?. I have no problem when using this tool with the GRCh38 dbSNP (build 138) vcf file provided in the resource bundle, however whenever I try a different dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7383:136,error,error,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383,2,['error'],['error']
Availability,"This request was created from a contribution made by Jay Singh on May 09, 2020 12:47 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360062649991-java-lang-IllegalArgumentException-Invalid-interval-Contig-chr1-start-9-end-10464. --. Can you please provide ; a) GATK version used - **gatk-4.1.7.0** ; b) Exact GATK commands used. **./gatk Funcotator --variant /home/deepak/software\_library/gatk-4.1.7.0/SAMPLE3\_Haplo.g.vcf.gz --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR\_DATA/DATA\_SOURCES --output SAMPLE\_3\_variants.funcotated.vcf --output-file-format VCF** ; **Using GATK jar /home/deepak/software\_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar**. I am using Funcotator and have used the above commands but I am not getting the correct annotated output file. Also I am getting the following error "" **java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:-9 end:10464** "". I have also attached the screenshot of the output result. Thanks. ![](https://gatk.broadinstitute.org/hc/user_images/ra0laiOjkI-jPJAnYIbjpw.png) ![](https://gatk.broadinstitute.org/hc/user_images/L_-FFyP26boUMsHYjvf_Vg.png)<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/5579'>Zendesk ticket #5579</a>)<br>gz#5579</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598:906,error,error,906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598,1,['error'],['error']
Availability,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7865:171,Error,Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865,6,"['Error', 'down', 'error']","['Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations', 'downloaded', 'error', 'errors']"
Availability,"This request was created from a contribution made by Lucas Kopecky Bobadilla on June 05, 2020 15:53 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360067974992-AnalyzeCovariates. --.  . Hello I am using the current 4.17 GATK version and I am trying to run AnalyzeCovariates. I ran the baserecalibrator to get the table file to run AnalyzeCovariates but I am getting this following error in the R command:. [June 5, 2020 10:45:08 AM CDT] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.02 minutes. Runtime.totalMemory()=1233649664. org.broadinstitute.hellbender.utils.R.RScriptExecutorException: . Rscript exited with 1. Command Line: Rscript -e tempLibDir = '/tmp/Rlib.323393943272793217';source('/tmp/BQSR.1306882797239975225.R'); /tmp/AnalyzeCovariates5750988274473323663.csv /media/brent/lucas\_SSD/dicamba\_rnaseq/GATK/qlty\_recalibration/trim\_DIC\_CHR\_F2\_R\_104\_1.table /media/brent/lucas\_SSD/dicamba\_rnaseq/GATK/qlty\_recalibration/AnalyzeCovariates.pdf. Stdout: . Stderr: . Attaching package: ‘gplots’.  . The following object is masked from ‘package:stats’:.  .     lowess.  . Error in distributeGraphRows(list(a, b, c), c(1, 1, 1)) : .   object 'a' not found. Calls: source -> withVisible -> eval -> eval -> distributeGraphRows. Execution halted.  . Any idea what is going on?.  . Thanks!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/5868'>Zendesk ticket #5868</a>)<br>gz#5868</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6650:402,error,error,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6650,3,"['Error', 'error', 'mask']","['Error', 'error', 'masked']"
Availability,"This request was created from a contribution made by Mark Godek on May 28, 2020 12:43 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360067471451-Funcotator-cannot-complete-funcotaion-for-variant-due-to-alternate-allele. --. I'm attempting to annotate germline variants after VQSR with Funcotator using GATK 4.1.4.1. GATK command is:. gatk Funcotator \ ; -R ${REFERENCE\_GENOME} \ ; -V ${OUT}/germline.filtered.vcf.gz \ ; -O ${OUT}/annotated.germline.vcf \ ; --output-file-format VCF \ ; --data-sources-path /mnt/data/rbueno/analysis\_files/MedGenome\_FamilialMPMs/Annotation\_data\_sources/funcotator\_dataSources.v1.6.20190124s \ ; --ref-version hg19 ; ; I get many warnings and it terminates with a String index out of range error. Any help is appreciated.  . The tail end of the output follows: ; ; ; 07:33:14.569 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756762-69756762 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:17.957 INFO VcfFuncotationFactory - dbSNP 9606\_b150 cache hits/total: 521/453691 ; 07:33:18.138 INFO Funcotator - Shut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651:749,error,error,749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651,1,['error'],['error']
Availability,"This request was created from a contribution made by Matt Johnson on July 05, 2021 21:23 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community\_comment\_4403173344411](https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community_comment_4403173344411). \--. Hello, I am also having this issue. \[This page\](/hc/en-us/articles/360035530752-What-types-of-variants-can-GATK-tools-detect-or-handle-) indicates the following definition of SYMBOLIC:. SYMBOLIC (such as the  `<NON-REF>`  allele used in GVCFs produced by HaplotypeCaller, the  `*`  allele used to signify the presence of a  [spanning deletion](https://gatk.zendesk.com/hc/en-us/articles/360035531912), or undefined events like a very large allele or one that's fuzzy and not fully modeled; i.e. there's some event going on here but we don't know what exactly). Therefore I would expect SelectVariants --select-type-to-exclude SYMBOLIC to not have any calls containing spanning deletions. However, the output VCFs still do (in gatk4 4.2.0.0). This causes problems for downstream tools like FastaAlternateReferenceMaker:. java.lang.IllegalArgumentException: the input sequence contains invalid base calls like: \*. Is there any way to force GATK to exclude spanning deletions when filtering a VCF?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/168722'>Zendesk ticket #168722</a>)<br>gz#168722</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7341:1214,down,downstream,1214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7341,1,['down'],['downstream']
Availability,"This request was created from a contribution made by Min-Hwan Sohn on March 05, 2020 01:00 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360057956031-PathseqPipelineSpark-stop-with-error-message-regarding-com-esotericsoftware-kryo-KryoException-Buffer-underflow-. --. Hi GATK team. I recently used PathseqPipelineSpark embedded in GATK v4.1.4.1 (installed from anaconda) to identify potential microbial composition of human tissue Whole-Genome samples. . NovaSeq-sequenced paired-end reads (2X151bp) were aligned (onto hg19 reference), duplicate-removed, base quality score-recalibrated and BQSR-applied, which eventually used as an input to the PathseqPipelineSpark. . Since I failed to find hg19 host reference in the GATK resource bundle, first I created a BWA image file and a Kmer file originated from hg19 reference fasta with the command below. But for microbe-related files, I used ones that were contained in the bundle.  . **'''** ; ; **gatk --java-options ""-Xmx50G"" BwaMemIndexImageCreator -I ./ref.fasta** ; **gatk --java-options ""-Xmx50G"" PathSeqBuildKmers --reference ./ref.fasta -O ref.hss** ; ; **'''**.  . And then I ran PathSeq with the following command.  . **'''** ; ; **gatk --java-options ""-Xmx200G"" PathSeqPipelineSpark \** ; **--input sample.bam \** ; **--filter-bwa-image ref.fasta.img \** ; **--kmer-file ref.hss \** ; **--is-host-aligned true \** ; **--min-clipped-read-length 70 \** ; **--microbe-fasta pathseq\_microbe.fa \** ; **--microbe-bwa-image pathseq\_microbe.fa.img \** ; **--taxonomy-file pathseq\_taxonomy.db \** ; **--output sample.pathseq.bam \** ; **--scores-output sample.pathseq.txt** ; ; ; **'''**.  . and unfortunately it was shut down by this error message. **09:27:43.974 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so** ; **Mar 05, 2020 9:27",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:203,error,error-message-regarding-com-esotericsoftware-kryo-KryoException-Buffer-underflow,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['error'],['error-message-regarding-com-esotericsoftware-kryo-KryoException-Buffer-underflow']
Availability,"This request was created from a contribution made by Rahul Gupta on February 03, 2022 19:42 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4417613488411-In-Mutect2-force-calling-allele-via-alleles-does-not-force-call-the-allele](https://gatk.broadinstitute.org/hc/en-us/community/posts/4417613488411-In-Mutect2-force-calling-allele-via-alleles-does-not-force-call-the-allele). \--. If not an error, choose a category for your question(REQUIRED): ; ; c) Why do I see (......)?. GATK version: 4.2.4.1. Hi all,. I'm running Mutect2 over samples for which I have provided a VCF of alleles that I would like force-called. In 99% of cases this works great, but in a few instances a couple of alleles that I have included for force-calling do not actually make it into the output calls VCF. My command is:. gatk --java-options -Xmx3000m Mutect2 \\ ; ; \-R /data\_in/MY1776/MY1776.self.ref.shifted\_by\_8000\_bases.fasta \\ ; ; \-I /data\_in/MY1776/self\_realigned\_shifted.bam \\ ; ; \--read-filter MateOnSameContigOrNoMappedMateReadFilter \\ ; ; \--read-filter MateUnmappedAndUnmappedReadFilter \\ ; ; \-O /out\_default.vcf \\ ; ; \--alleles /data\_in/MY1776/MY1776.self.ref.reversed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7672:414,error,error,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672,1,['error'],['error']
Availability,"This request was created from a contribution made by Yangyxt on August 30, 2021 08:18 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.2.0 ; ; b) Exact command used:. ${gatk} PostprocessGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:487,error,error,487,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['error'],['error']
Availability,"This request was created from a contribution made by Yanis Chrys on August 19, 2021 11:35 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-). \--. Hi, ; ; I am working on haploid bacterial data and I ran into a limitation of the program that I either can't solve or it would be nice to add a funtion for it in the future. I'll explain the issue:. Let's say I have (low coverage) data that I want to turn into an alternate fasta reference where: ; ; REF: A. ALT: AAGT,T,CA. If I want to keep variants where the AD > \[threshold\] I can't do. \-select 'vc.getGenotype(""sample"").getAD.1'. because for my sample it could be that the called ALT is getAD.2 and so far I haven't been able to use anything other than a number as an index to getAD. This would be solved if we could do:. getAD.getGT OR getAD.IndexOfAlleleWithHighestCount. but to my knowledge none of these will work because JEXL will give an error. Maybe extending JEXL java operation to the AD array could fix it? Because even getAD\[0\] gives an error. Do you have a solution to this?. PS. I am sorry if this should have been under General Questions<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/177956'>Zendesk ticket #177956</a>)<br>gz#177956</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7448:1167,error,error,1167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7448,2,['error'],['error']
Availability,"This request was created from a contribution made by tc on February 09, 2022 17:49 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:964,error,error,964,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,1,['error'],['error']
Availability,"This seems to be a simple typo. The minimal data to calculate the segmentation cost should be `2 * windowSize`, rather than `windowSize`, as the error message indicates. In the current logic, the segmentation cost at a particular point is calculated as the difference between the sum of costs of two windows to the left and right of that point and the cost of a big window of size `2 * windowSize`. If the # of the data points is less than the `2 * windowSize`, the cost for the full window will be wrong in the circular buffer representation; it will get the wrong cost of a window of size `2 * windowSize - data_size`, instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6835:145,error,error,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6835,1,['error'],['error']
Availability,This seems to resolve the issue (which I had no problem reproducing) locally. Interestingly when I run the test suite locally I get test failures here but apparently we never caught this on travis. There must be something different about the tests on travis... I am happy for advice as to how to write a test for this fix though because the error seems to be occurring under a dizzying array of spark/hadoop internal serialization code and I'm not sure how to test that properly. . Fixes #6513 #6738,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6741:137,failure,failures,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6741,2,"['error', 'failure']","['error', 'failures']"
Availability,"This set of optimizations brings the GATK4 HaplotypeCaller performance into line; with GATK3.x performance. Note that HaplotypeCallerSpark is not touched by this PR (that is for a future PR). Summary of changes:. * AssemblyRegionWalker: query all intervals on each contig simultaneously, rather than individually; * GATKRead: Cache adaptor boundary, soft start/end, and cigar length; * GATKRead: add getBasesNoCopy() / getBaseQualitiesNoCopy(); * ReadPileup: speed up stratified constructor; * LIBS.lazyLoadNextAlignmentContext(): don't keep pileup elements unnecessarily separated by sample during pileup creation; * Restore faster GATK3 version of ReferenceConfidenceModel.sumMismatchingQualities(); * RefVsAnyResult: nest within ReferenceConfidenceModel, and allow direct field access; * Remove redundant getBases() call in ReadThreadingGraph; * Fix BaseGraph Utils.validateArg() call; * ReadPileup: replace Collections.unmodifiableList(pileupElements).iterator() with direct return of an iterator that forbids removal; * Kill expensive bounds checking in GATKRead getBase()/getBaseQuality()/getCigarElement(); * Kill nonNull checks in PileupElement; * Kill expensive PileupElement and ReadPileup arg validation; * GATKRead adapter: clear cached values upon mutation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031:798,redundant,redundant,798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031,1,['redundant'],['redundant']
Availability,"This should emit a big file with lines consisting of: a kmer, and the count in the pileup of each of the following to occur at the *center* of the kmer:. * no error; * A substitution; * C substitution; * G substitution; * T substitution; * *beginning* of deletion; * *beginning* of insertion. We might later get fancier and distinguish between different lengths of indels.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3092:159,error,error,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3092,1,['error'],['error']
Availability,This should fix the failures.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/217:20,failure,failures,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/217,1,['failure'],['failures']
Availability,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7682:27,failure,failure,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682,3,"['down', 'error', 'failure']","['downloaded', 'error', 'failure']"
Availability,"This should hopefully fix the out of memory errors on CircleCI, although it's hard to test since they happen kind of randomly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/654:44,error,errors,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/654,1,['error'],['errors']
Availability,This should resolve #650. This may require a change to the hellbender-protected build to include sonatype snapshots as an available maven repo if it doesn't already. This is the same issue as #779.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/801:122,avail,available,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/801,1,['avail'],['available']
Availability,This should work around a rare error in `_calculate_new_intervals` that could generate invalid partitioners in a way that; `calculate_new_intervals` cannot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8603:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8603,1,['error'],['error']
Availability,"This test input is malformed. When I try to read it with the Dataflow code, I get this error:. htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 129, Read name 809R9ABXX101220:5:6:17918:145992, Mate Alignment start should be 0 because reference name = *. Here's the corresponding read:. 809R9ABXX101220:5:6:17918:145992 97 17 69400 37 67M9S \* 71202348 0 ACTCCCCACCTTACCTGACTCCTTCCAGGGTTTGTCGCCTTTCCGGTCCCTGACCCCAGTGGATGGGAGTCTGTCC ?ABDDEEABEECBDBDAB=DEDCDEEBFADABCEAD?EEEDCFE?ABEEE@FCDEEEBF@F?C<E@########## MD:Z:67 PG:Z:BWA RG:Z:809R9.5 AM:i:0 NM:i:0 SM:i:37 MQ:i:0 OQ:Z:DGEGGGGBFGGGGGDF8@@FGFBGGGBGCECCEEDFGGGFGFGGGBDGGF9DBFFGFBF;@>A4@@########## UQ:i:0. @droazen confirms that Picard's ValidateSAMFile utility reports that this bam has multiple errors. We should replace it with a clean input, and update the ""known good"" output accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/568:87,error,error,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/568,4,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3173:51,error,errors,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173,4,['error'],"['error', 'errors']"
Availability,"This ticket opens a discussion between @samuelklee, @davidbenjamin and anyone else who is interested towards deciding if this is useful. . I think it would be useful for (i) the outputs of CollectAllelicCounts and GetPileupSummaries and other similar tools to be in VCF format and for (ii) downstream tools that take these results to then also take in VCF format. I think it's good to adhere to standard formats, for results that already nearly resemble variant calls, for versatility. At the least, it would be great if the _format_ that is output by the two tools and accepted by downstream tools is unified. There is also discussion on whether these two particular tools should be merged. There may be other tickets on this particular discussion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717:290,down,downstream,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717,2,['down'],['downstream']
Availability,"This tool creates a data file containing a map from reference accessions to NCBI taxonomic IDs, and the taxonomic tree, which includes parent/child relationships as well as other metadata like the reference length and scientific name of each node. . The input files are available from the NCBI FTP server. One is a ""catalog"" file that gives the mapping from reference contig accession to taxonomic ID. There are catalog files available for RefSeq and for Genbank - the tool can take in either. . There are two other files - a ""names"" and ""nodes"" file contained in a single tarball - that contain the scientific names of each node and parent/child relationships. For convenience, the tool takes in the path to the tarball and extracts the two files automatically. The resulting database size is minimized using the given reference. Once the full NCBI taxonomy tree is built, any organism node that is neither in the reference nor an ancestor of a reference organism is removed. The resulting datafile is read in by the ClassifyReads tool (coming in a future PR) to assign relative abundance scores to each taxonomic node. Also made some changed to the way the PSTree and PSTreeNode are serialized (using Kryo read/writeObject instead of read/writeClassAndObject) so that loading old files won't break if these classes change packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2730:270,avail,available,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2730,2,['avail'],['available']
Availability,"This user is noticing that when running BQSR with the default inflater, some blocks have compression errors that result in issues while running HaplotypeCaller. This request was created from a contribution made by Jacob Wang on November 02, 2021 08:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:101,error,errors,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,4,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"This user is receiving a memory allocation error when running MarkDuplicatesSpark. The user has tried limiting the number of executors and raising and lowering the memory allocations but continues to get the same error. . This request was created from a contribution made by Udi L on August 04, 2021 07:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360058452072-MarkDuplicatesSpark-consumes-enormous-amount-of-RAM#community\_comment\_4404649417755](https://gatk.broadinstitute.org/hc/en-us/community/posts/360058452072-MarkDuplicatesSpark-consumes-enormous-amount-of-RAM#community_comment_4404649417755). \--. Hi,. Did you figure it?. I have the same problem. I am using --java-options ""-Xmx80G""<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/173588'>Zendesk ticket #173588</a>)<br>gz#173588</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7406:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7406,2,['error'],['error']
Availability,"This user is receiving an error in their workflow when using GATK4.0.3.0. The error in the particular step can be resolved using a newer GATK version but the user has used the older version for the rest of the workflow and would like a solution that allows them to continue with the older version.; This request was created from a contribution made by HT on August 23, 2021 05:44 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405613731739-GATK4-0-3-0-GenotypeGVCFs-Could-not-open-array-genomicsdbarray](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405613731739-GATK4-0-3-0-GenotypeGVCFs-Could-not-open-array-genomicsdbarray). \--. a) GATK version used:  **GATK 4.0.3.0**. b) Exact command used:. **\[Tool\]: GenomicsDBImport**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx85g -Xms85g"" GenomicsDBImport \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \--sample-name-map ${dir\_CombineGVCFs}/S2\_cohort.sample\_map \\ ; ; \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4 \\ ; ; \--TMP\_DIR ${dir\_CombineGVCFs}/temporary \\ ; ; \--intervals ${dir\_CombineGVCFs}/intervals/bed3\_tmp.intervals \\ ; ; \--reader-threads 5 \\ ; ; \--batch-size 50. **\[output\]**:. folders and files in; ====================. \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4; ================================================================. callset.json ; ; genomicsdb\_array ; ; \_\_tiledb\_workspace.tdb ; ; vcfheader.vcf ; ; vidmap.json. **\[Tool\]: GenotypeGVCFs**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx4g"" GenotypeGVCFs \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \-V gendb://${dir\_GenomicsDBImport}/tmp4 \\ ; ; \-O ${dir\_GenotypeVCFs}/tmp4.vcf.gz. c) Entire error log:. Using GATK jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7442:26,error,error,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442,2,['error'],['error']
Availability,"This user received an ArrayIndexOutofBoundsException error when running GenotypeGVCFs. The user confirmed that the headers of their vcf files and the their fasta files have matching IDs and contig lengths. The user also tried running ValidateVariants and received the following error: A USER ERROR has occurred: Input MA1.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position 1A:3456221 are not observed at all in the sample genotypes. This request was created from a contribution made by Alon Ziv on July 07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge  java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used  this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7348:53,error,error,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"This user was getting a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2 – GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2) for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is: . gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7479:95,error,error,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479,2,['error'],['error']
Availability,"This version introduces a change that (at least on my machine) fixes the mysterious ""happens only on the command line"" test failure. Also uses a newer version of genomics-dataflow because I had to fix a bug there for API_KEY to work in our setting. Finally, this version also moves the files around so they match the local tree, and changes the environment variables naming scheme to be a little more consistent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535:124,failure,failure,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535,1,['failure'],['failure']
Availability,"This was a bug I uncovered while finalizing #5607, fixing it seems to change the GVCF blocking by enough that I would like somebody to take a look at it @ldgauthier. Specifically look at the last commit on this branch as that is what actually changes the test/code to no longer duplicate the old behavior. For a description of the error this fixes, see #5646. . Fixes #5646 . Blocked by #5607",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5665:331,error,error,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5665,1,['error'],['error']
Availability,"This was a very useful debug tool when working on issue #2685. It sends many parallel reads for a long time. This makes sure that the combination of the cloud provider's throttling and our own retry parameters allows us to eventually read everything to completion and not fail with disconnection errors. This test is disabled by default, because it takes too long to be run every time. But if there's a doubt about retries we can dust it off and run it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3070:296,error,errors,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3070,1,['error'],['errors']
Availability,This will eliminate the transient Spark errors that look like this:. ```; java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3; ```. (Reported by @akiezun.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/886:40,error,errors,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/886,1,['error'],['errors']
Availability,"Three major changes here.; 1. Added in logic to create the ploidy table during ingest (with necessary supporting class) and use it during extract automatically as part of the default joint workflow. Also removed a column that we won't need when creating it automatically.; 2. Rearranged the PAR checking logic to consolidate it in its own class (PloidyUtils.java). Successful run against tiny sample set ""PLOIDY_TEST"" in echo callset project:. https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%20Echo%20Callset%20v2/job_history/a93aa2ef-9cef-451d-8cf8-b31f1c6a8407. You'll need your aou credentials to see the results. Successful integration run on XY:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/6a9a5fdf-ffaa-4dcb-af73-56a4b25e69a4. This run shows all of the OTHER integration tests running successfully except BGE, due to the test data needing an updates for BGE X and Y:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/21664810-7516-49f2-a60c-51b2e05faf06. The only difference between those two tests running was an update to the expected values for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8994:421,echo,echo,421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8994,1,['echo'],['echo']
Availability,Tiny improvement that was languishing in an ancient and redundant PR https://github.com/broadinstitute/gatk/pull/6736,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8725:56,redundant,redundant,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8725,1,['redundant'],['redundant']
Availability,To avoid quota issues when pulling down the base image during tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7102:35,down,down,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7102,1,['down'],['down']
Availability,"To test I ran this before and after the fix. Before it would sit there after the line . ```; Tool returned:; 0; ```; for ~10 minutes before it would time out and exit. After the fix it shuts down cleanly. ```; gatk --java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 \; CreateVariantIngestFiles \; -V gs://fc-e2f6ffa2-4033-4517-98fc-889bee4cc7a6/5e6b194b-5f69-40f2-a6de-f4f3f80ce05a/ReblockGVCF/702cdbf7-0666-4ee5-b889-91ba0ffa90bd/call-Reblock/HG00405.haplotypeCalls.er.raw.vcf.gz.rb.g.vcf.gz \; -L chr20:1-100000 \; -IG FORTY \; --ignore-above-gq-threshold false \; --project-id broad-dsp-spec-ops \; --dataset-name gvs_qs_v2_kc \; --output-type BQ \; --enable-reference-ranges true \; --enable-pet false \; --enable-vet true \; -SN ERS4367795 \; --gvs-sample-id 99 \; --ref-version 38; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7571:191,down,down,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7571,1,['down'],['down']
Availability,"To upgrade ApplyBQSR for cloud execution I had to:. (i) change the input to remove reads with the unaligned flag; (ii) load the recalibration report from GCS instead of shipping it as a serialized object, because Dataflow explodes if we ship too much (error is: ""malformed JSON"").; (iii) for the case of a remote execution with a local output file name, add logic to copy the output via GCS to the client's machine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/595:252,error,error,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595,1,['error'],['error']
Availability,ToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:14167,Recover,Recovered,14167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,Tool to merge M2 calls into MC3 vcf for downstream processing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5007:40,down,downstream,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5007,1,['down'],['downstream']
Availability,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2449:2776,ERROR,ERROR,2776,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449,2,['ERROR'],['ERROR']
Availability,Tools touched:. Mutect2; CreateSomaticPanelOfNormals; GetPileupSummaries; CalculateContamination; FilterMutectCalls; Picard CollectSequencingArtifactMetrics; FilterByOrientationBias; Concordance . The Mutect2 (How to) tutorial and various supporting documents are now public and so I've cleaned up now redundant information we temporarily placed in the tool docs. I also made a pass at unifying this set of tool docs in terms of presentation elements.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4310:302,redundant,redundant,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4310,1,['redundant'],['redundant']
Availability,Transient GCS Permission Errors when using 4.beta.6,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:25,Error,Errors,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,1,['Error'],['Errors']
Availability,"Travis rolled out updated images that caused our builds to start failing. It looks like some of the WDL tests had been failing for a [while](https://github.com/broadinstitute/gatk/issues/3558), but that wasn't causing the **builds** to fail until the new Travis images were rolled out, at which point we started running out of space logging the errors. We're temporarily requesting to use the old Travis image (https://github.com/broadinstitute/gatk/pull/3557), but we should revert that once we address the underlying issues.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3559:345,error,errors,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3559,1,['error'],['errors']
Availability,"Trying to generate the documentation with gatk-protected to explore how a dependent-project could include ReadFilters and other utilities, running in my local computer it throws the following error:. ```; javadoc: error - In doclet class org.broadinstitute.hellbender.utils.help.GATKHelpDoclet, method start has thrown an exception java.lang.reflect.InvocationTargetException; java.lang.RuntimeException: Could not find the field corresponding to java.lang.Throwable.backtrace, presumably because the field is inaccessible; at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:604); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:622); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDoc(DefaultDocWorkUnitHandler.java:591); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.getFieldDocForCommandLineArgument(DefaultDocWorkUnitHandler.java:403); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processNamedArgument(DefaultDocWorkUnitHandler.java:357); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.lambda$addCommandLineArgumentBindings$5(DefaultDocWorkUnitHandler.java:287); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.addCommandLineArgumentBindings(DefaultDocWorkUnitHandler.java:287); at org.broadinstitute.barclay.help.DefaultDocWorkUnitHandler.processWorkUnit(DefaultDocWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2739:192,error,error,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739,2,['error'],['error']
Availability,Tweak extract values based on Echo Runs [VS-1432],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8979:30,Echo,Echo,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8979,1,['Echo'],['Echo']
Availability,Tweak ingest messaging and failure mode [VS-267],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7680:27,failure,failure,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7680,1,['failure'],['failure']
Availability,"Two annotations, allele depth and total depth, consider whether reads are informative relative to the alleles in the output `VariantContext`, which is in general a subset of the alleles contains in the `ReadLikelihoods`. In PR #2185 I overlooked this and forgot to subset the likelihoods' alleles to those of the vc, which was the previous behavior (see the diff from that PR for the two annotations in this PR). This PR duplicates the old behavior. This fixes failures in the HaplotypeCaller integration tests introduced by the previous PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2239:461,failure,failures,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239,1,['failure'],['failures']
Availability,"Two primary sets of changes. 1. split out the combined ""CREATE TABLE AS... SELECT... join PET + VET"" into 3 separate items. CREATE, INSERT vet, INSERT pet; 2. To keep our shuffle down we are not joining in sample_id at query time, since we already have the id -> name mapping in ExtractCohort... we just needed to use it (should reduce costs slightly also). Testing. Tested on the GVS tieout set. As expected the only difference in the cohort extract tables is that we are no longer seeing mis-joined VET information at `*` sites (which is a nice side benefits). Otherwise tables tie out exactly in SQL. In addition, I ran a full GIAB tieout before and after and the results are identical",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7288:179,down,down,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7288,1,['down'],['down']
Availability,"Two users have reported similar errors on the forums: ; https://gatk.broadinstitute.org/hc/en-us/community/posts/360057963511-HaplotypeCaller-error-Read-is-malformed-read-starts-with-deletion; https://gatk.broadinstitute.org/hc/en-us/community/posts/360058147791-read-starts-with-deletion-Cigar-4D150M. When examining the stacktrace, it appears the error is happening at the end of time when we are filling in reference confidence for a site based on reads from the AlleleLikelihoods object. Reads in that pool have been realigned based on their best Haplotype already so its feasible that we have introduced a regression somewhere in that code:; ```; org.broadinstitute.hellbender.exceptions.UserException$MalformedRead: Read A01023:5:HMV7LDMXX:2:2473:11406:29324 chrY:13433535-13433720 is malformed: read starts with deletion. Cigar: 37D68M1I81M. Although the SAM spec technically permits such reads, this is often indicative of malformed files.; at org.broadinstitute.hellbender.utils.locusiterator.AlignmentStateMachine.stepForwardOnGenome(AlignmentStateMachine.java:274); at org.broadinstitute.hellbender.utils.locusiterator.ReadStateManager.addReadsToSample(ReadStateManager.java:258); at org.broadinstitute.hellbender.utils.locusiterator.ReadStateManager.collectPendingReads(ReadStateManager.java:177); at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.lazyLoadNextAlignmentContext(LocusIteratorByState.java:288); at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.hasNext(LocusIteratorByState.java:225); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.getPileupsOverReference(AssemblyBasedCallerUtils.java:443); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.calculateRefConfidence(ReferenceConfidenceModel.java:195); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:645); at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6490:32,error,errors,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6490,3,['error'],"['error', 'error-Read-is-malformed-read-starts-with-deletion', 'errors']"
Availability,"UG ScriptExecutor - --convergence_snr_averaging_window=500; 21:07:11.897 DEBUG ScriptExecutor - --convergence_snr_trigger_threshold=1.000000e-01; 21:07:11.897 DEBUG ScriptExecutor - --convergence_snr_countdown_window=10; 21:07:11.897 DEBUG ScriptExecutor - --max_calling_iters=10; 21:07:11.897 DEBUG ScriptExecutor - --caller_update_convergence_threshold=1.000000e-03; 21:07:11.897 DEBUG ScriptExecutor - --caller_internal_admixing_rate=7.500000e-01; 21:07:11.897 DEBUG ScriptExecutor - --caller_external_admixing_rate=1.000000e+00; 21:07:11.897 DEBUG ScriptExecutor - --disable_caller=false; 21:07:11.897 DEBUG ScriptExecutor - --disable_sampler=false; 21:07:11.897 DEBUG ScriptExecutor - --disable_annealing=false; Traceback (most recent call last):; File ""/paedyl01/disk1/louisshe/tmp/gatk/cohort_denoising_calling.418897092082188314.py"", line 179, in <module>; warm_up_task.engage(); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/inference_task_base.py"", line 346, in engage; converged_continuous = self._update_continuous_posteriors(); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/inference_task_base.py"", line 403, in _update_continuous_posteriors; raise ConvergenceError; gcnvkernel.tasks.inference_task_base.ConvergenceError. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/paedyl01/disk1/louisshe/tmp/gatk/cohort_denoising_calling.418897092082188314.py"", line 184, in <module>; sys.exit(gcnvkernel.io_consts.diverged_inference_exit_code); NameError: name 'sys' is not defined; 21:09:00.147 INFO GermlineCNVCaller - Shutting down engine; [August 13, 2024 9:09:00 PM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 3.36 minutes.; Runtime.totalMemory()=3168796672; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1. ```. Much appreciated for your help indeed!. Best regards,; Louis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:13938,down,down,13938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['down'],['down']
Availability,"UIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:2964,ERROR,ERROR,2964,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['ERROR'],['ERROR']
Availability,Unable to load bwa index error when running BwaSpark under version alpha.2-45-ga30af5a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,1,['error'],['error']
Availability,Unable to run the spark code in Scala Ide getting error as below,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8587:50,error,error,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587,1,['error'],['error']
Availability,Uninformative error when GATK doesn't have permission to check whether a GCS bucket is Requester Pays,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6349:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6349,1,['error'],['error']
Availability,Unknown error when running SVAnnotate on GTF file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394,1,['error'],['error']
Availability,Update EchoCallset gatk docker to most recent build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8803:7,Echo,EchoCallset,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8803,1,['Echo'],['EchoCallset']
Availability,"Update GvsPrepareCallset step so that, instead of creating and inserting all the data into the `_pet_new` table in one step (which errors out with large callsets), add the data in smaller sections that correspond to each `pet_` in the GVS. Closes https://broadworkbench.atlassian.net/browse/VS-48",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7395:131,error,errors,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7395,1,['error'],['errors']
Availability,Update To handle if no data error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7084:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7084,1,['error'],['error']
Availability,Update ValidateSamFileIntegrationTest once htsjdk #369 CRAM bug fix is available,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1138:71,avail,available,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1138,1,['avail'],['available']
Availability,Update error message based on https://github.com/broadinstitute/gatk/issues/4669.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4678:7,error,error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4678,1,['error'],['error']
Availability,Updated arguments and tests to use a datasources directory.; ; Fixed the way dna repair genes get parsed.; Fixed a coding sequence bug when dealing with multiple gencode sources.; Fixed a bug in COSMIC annotaitons when GENCODE annotations are in IGRs.; ; Fixes #3999,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4020:81,repair,repair,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4020,1,['repair'],['repair']
Availability,"Updates (EchoCallset Version):. Changes to scatter width of VCFs generated has changed the amount of data generated in tests, so need to update truth; Adding a new field to extracted VCF Header EXCESS_ALLLELES and that will break the tests.; And why not validate our VCFs for jollies.; Updates 'truth' path for data to match these changes. Integration tests failed due to different number of output VCFs now. So I cherry-picked Miguel's commit on ah_var_store that changed the scatter.; Integration tests *still* [failing](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8f7b0cc9-4a31-404b-99b3-89e182707e8b) due to the change in VCF Header:. > 6,7d5; < ##FILTER=<ID=high_CALIBRATION_SENSITIVITY_INDEL,Description=""Site failed INDEL model calibration sensitivity cutoff (0.99)"">; < ##FILTER=<ID=high_CALIBRATION_SENSITIVITY_SNP,Description=""Site failed SNP model calibration sensitivity cutoff (0.997)"">; 9c7; < ##FORMAT=<ID=FT,Number=1,Type=String,Description=""Genotype Filter Field"">; ---; > ##FORMAT=<ID=FT,Number=1,Type=String,Description=""Sample Genotype Filter Field"">; 3388a3387,3388; > ##high_CALIBRATION_SENSITIVITY_INDEL=Sample Genotype FT filter value indicating that the genotyped allele failed INDEL model calibration sensitivity cutoff (0.99); > ##high_CALIBRATION_SENSITIVITY_SNP=Sample Genotype FT filter value indicating that the genotyped allele failed SNP model calibration sensitivity cutoff (0.997)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8848:9,Echo,EchoCallset,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8848,1,['Echo'],['EchoCallset']
Availability,Updating current Git LFS download size in README.md,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6933:25,down,download,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6933,1,['down'],['download']
Availability,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7609:81,down,download,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609,1,['down'],['download']
Availability,"Upon running Mutect2 using the germline and pon provided by gatk inside a pipeline while running in parallel. If I use an intervals BED file I get the following error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:383); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:335); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:246); 	at org.broadinstitute.hellbender.engine.FeatureManager.initializeFeatureSources(FeatureManager.java:209); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:156); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:488); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7059:161,error,error,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059,2,"['Error', 'error']","['Error', 'error']"
Availability,Use the sequence dictionary when reading from HDFS if one's available.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/924:60,avail,available,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/924,1,['avail'],['available']
Availability,Use womtool to get errors faster in our WDL tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4802:19,error,errors,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4802,1,['error'],['errors']
Availability,"User Question: I'm trying to speed up the process of calling variants using SPARK. I have access to a slurm HPC cluster, so I guess it's not that straightforward to run GATK in a proper distributed master-slave architecture (if there is any tutorial on how to setup slurm jobs to use GATK Spark tools on multiple nodes I would appreciate it a lot). ; Therefore, I run GATK in local mode with some SPARK threads, eventually speeding up the process by parallelising the number of samples processed simultaneously with GNU parallel. But then, I'm having troubles because some samples crash due to SPARK errors. Perhaps you could send my logs to the developers ? I'm trying to run 8 parallel GATk jobs (8 samples) using 5 Spark cpus on each in a node with 40 cpus. . Best,; Pedro. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/56193#Comment_56193",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5717:600,error,errors,600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5717,1,['error'],['errors']
Availability,"User Report:; ------------; Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360055990891-CollectGcBiasMetrics-Array-Index-Out-Of-Bounds-Exception](https://gatk.broadinstitute.org/hc/en-us/community/posts/360055990891-CollectGcBiasMetrics-Array-Index-Out-Of-Bounds-Exception); ------------. Hello,. When running CollectGcBiasMetrics on a moderately sized sam file (~500Mb), picard gives ArrayIndexOutOfBoundsException and ""Exception counting mismatches for read ..."". The SCAN\_WINDOW\_SIZE=1000. When it's set to default value 100, the error message is slightly different but ArrayIndexOutOfBoundsException persists. I have also experimented with different window sizes, all values >1000 give same error at the same read on chrX (details below). The reference fasta file is taken from UCSC: [https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz](https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz). Any feedback leading to resolving the issue is greatly appreciated. a) Picard version:. 2.21.6-SNAPSHOT. b) Command script:. java -jar picard.jar CollectGcBiasMetrics \\ ; ; I=sorted.sam \\ ; ; O=gc\_bias\_metrics.txt \\ ; ; CHART=gc\_bias\_metrics.pdf \\ ; ; S=summary\_metrics.txt \\ ; ; R=hg19.fa \\ ; ; SCAN\_WINDOW\_SIZE=1000. c) Error log:. MINIMUM\_GENOME\_FRACTION=1.0E-5 IS\_BISULFITE\_SEQUENCED=false METRIC\_ACCUMULATION\_LEVEL=\[ALL\_READS\] ALSO\_IGNORE\_DUPLICATES=false ASSUME\_SORTED=true STOP\_AFTER=0 VERBOSITY=INFO QUIET=false VALIDATION\_STRINGENCY=STRICT COMPRESSION\_LEVEL=5 MAX\_RECORDS\_IN\_RAM=500000 CREATE\_INDEX=false CREATE\_MD5\_FILE=false GA4GH\_CLIENT\_SECRETS=client\_secrets.json USE\_JDK\_DEFLATER=false USE\_JDK\_INFLATER=false ; ; \[Tue Jan 07 16:48:19 PST 2020\] Executing as [akoch@hpc5-0-3.local](mailto:akoch@hpc5-0-3.local) on Linux 2.6.32-431.11.2.el6.x86\_64 amd64; OpenJDK 64-Bit Server VM 1.8.0\_181-b13; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.21.6-SNAPSHOT ; ; INFO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6372:552,error,error,552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6372,2,['error'],['error']
Availability,"User has reported default settings in other countries cause tools to error because of the switch between commas and periods. The tools themselves generate the data with the commas that then error downstream tools. User reports a workaround solution of adding `-Duser.language=en -Duser.country=US` to commands but this is cumbersome. . Should our tools should be internationally compatible?. ---; Hi @shlee, @slee,. yes it is working when I replace the commas with points, but since I am using the wdl-pipeline which was provided by slee on gitHub, this is not a sufficient workaround. Due to my expectation that the problem (commas instead of points) arise from my language settings I try different thing to change my actual language on the system. To make the long story short the solution was to start CalculateTargetCoverage with the flags (I added them in the cnv_somatic_tasks.wdl):; ```; java -Duser.language=en -Duser.country=US -Xmx${default=4 mem}g -jar ${gatk_jar} CalculateTargetCoverage; ```; And now the cnv_somatic_panel_workflow.wdl runs smoothly to the end, with points instead of commas :). An other solution would be to running the pipeline in a DockerContainer wich have an english java-version installed, like the one which is provide by the gatk-team. I hope this helps other user too which are working with non-englisch java versions. Greetings EADG. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40481#Comment_40481",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3297:69,error,error,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3297,3,"['down', 'error']","['downstream', 'error']"
Availability,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3301:812,error,error,812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301,1,['error'],['error']
Availability,"User is running Mutect2 on GATK 4.1.3.0 and gets a NullPointerException error. Their bam did run using different variant callers. However, they got these [results twice](https://gatkforums.broadinstitute.org/gatk/discussion/comment/60577#Comment_60577) on two different bams. Here is the latest error below. . If possible, can the error message improve? The user was able to continue after creating a new index and dictionary for their reference:. ./gatk --java-options ""-Dsamjdk.sra_libraries_download=true"" Mutect2 -R hg19.fa -I test3.bam -O unfiltered.vcf; Using GATK jar /c/linux/gatk/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Dsamjdk.sra_libraries_download=true -jar /c/linux/gatk/gatk-package-4.1.3.0-local.jar Mutect2 -R hg19.fa -I test3.bam -O unfiltered.vcf; 10:33:35.145 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c/linux/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 28, 2019 10:33:37 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:33:37.506 INFO Mutect2 - ------------------------------------------------------------; 10:33:37.507 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.3.0; 10:33:37.507 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:33:37.508 INFO Mutect2 - Executing as ascott3@LAPTOP-L1C565MP on Linux v4.4.0-17763-Microsoft amd64; 10:33:37.508 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v11.0.4+11-post-Ubuntu-1ubuntu218.04.3; 10:33:37.508 INFO Mutect2 - Start Date/Time: August 28, 2019 at 10:33:35 AM GMT; 10:33:37.509 INFO Mutect2 - ------------------------------------------------------------; 10:33:37.509 INFO Mutect2 - -------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6142:72,error,error,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6142,3,['error'],['error']
Availability,"User is seeing non-ref blocks with GQ=0 even though bamout shows reads covering that region. Example: chr1	14155446	.	C	<NON_REF>	.	.	END=14155446	GT:DP:GQ:MIN_DP:PL	0/0: 100:0: 100 :0,0,0. Discussed this at the gatk office hrs and seems like this might be a bug. . Please find below the user report:; 1) I'm using 4.1.3.0; 2) java -Xmx6g -jar ../gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar HaplotypeCaller -R ~/gatk/hg19.unmasked.fa -G StandardAnnotation -mbq 17 --standard-min-confidence-threshold-for-calling 25 --max-reads-per-alignment-start 0 --disable-read-filter NotDuplicateReadFilter --min-dangling-branch-length 5 --allow-non-unique-kmers-in-ref --kmer-size 30 --kmer-size 10 --kmer-size 15 -ERC GVCF -I read1.sampe.bam -O small.i.vcf -L small.intervals --recover-all-dangling-branches --assembly-region-out test.txt --dont-trim-active-regions --min-assembly-region-size 28. 3) The bamout does seem to include the region of interest where GT=0 (see image https://us.v-cdn.net/5019796/uploads/editor/ye/ty6e3xa8vqom.png """"). 4) this it the relevant region in the VCF:; chr1	14155328	.	G	<NON_REF>	.	.	END=14155401	GT:DP:GQ:MIN_DP:PL	0/0:10:30:10:0,30,385; chr1	14155402	.	G	A,<NON_REF>	414.02	.	DP=10;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQandDP=36000,10	GT:AD:DP:GQ:PL:SB	1/1:0,10,0:10:30:428,30,0,428,30,428:0,0,10,0; chr1	14155403	.	T	<NON_REF>	.	.	END=14155440	GT:DP:GQ:MIN_DP:PL	0/0:10:30:10:0,30,426; chr1	14155441	.	G	<NON_REF>	.	.	END=14155596	GT:DP:GQ:MIN_DP:PL	0/0:0:0:0:0,0,0. This is a region if I'm altering the read to include a SNP in the region where there is a GT=0:; chr1	14155328	.	G	<NON_REF>	.	.	END=14155401	GT:DP:GQ:MIN_DP:PL	0/0: 100:99:1 00:0,120,1800; chr1	14155402	.	G	A,<NON_REF>	4486.03	.	DP=100;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQandDP=360000,100	GT:AD:DP:GQ:PL:SB	1/1:0,100,0: 100:99:4500,301,0,4500,301,4500:0,0,100,0; chr1	14155403	.	T	<NON_REF>	.	.	END=14155445	GT:DP:GQ:MIN_DP:PL	0/0: 100:99: 100:0,120,1800; chr1	14155446	.	C	<NON_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6100:767,recover,recover-all-dangling-branches,767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6100,1,['recover'],['recover-all-dangling-branches']
Availability,User pointed out minor error.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4524:23,error,error,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4524,1,['error'],['error']
Availability,"User report:. I am using GATK 4.1.4.0 and noticed the following : if Mutect2 is run with parameter --bam-output it makes a BAM file that contains :; @PG ID:HalpotypeBAMWriter; (note the typo); If then FilterAlignmentArtifacts is run with this file as input and BAM output is asked, it says; java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!; and does not create the file. Command Used:; gatk Mutect2 --input normal.recalibrated.vcf --input tumor.recalibrated.vcf -normal mormal -tumor tumor --reference /data/Homo_sapiens.UCSC.hg38.fa --output tumor.recalibrated.mutect2/vcf --f1r2-tar-gz f1r2.tar.gz --native-pair-hmm-threads 4 --bam-output tumor.recalibrated.realigned.bam --add-output-sam-program-record false -bam-output. The log of the command that generated the error was :. Using GATK jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar. Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar FilterAlignmentArtifacts --variant tumor.recalibrated.filtered.vcf --input tumor.recalibrated.realigned.bam --reference /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/genome/Homo_sapiens.UCSC.hg38.fa --bwa-mem-index-image /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/bwa_index_img/Homo_sapiens.UCSC.hg38.img --output tumor.recalibrated.filtered2.vcf --bam-output tumor.recalibrated.realigned2.bam --verbosity ERROR --tmp-dir TMP --QUIET true. 14:38:44.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so. 14:38:44.103 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation. java.lang.I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6287:835,error,error,835,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6287,1,['error'],['error']
Availability,"User report:. I test this in 4.1.4.0 and 4.1.4.1. ```shell; gatk CountBasesSpark \; -I input_reads.bam \; -O base_count.txt; ```. When run this cmd, it is OK, and get a right output base_count.txt.; But I want compute bases located in a interval file, so:; ```; gatk CountBasesSpark \; -I input_reads.bam \; -O base_count.txt\; -L interval.file; ```. This cmd cannot run successfully, with some errors I find like this:. ```shell; ......; 9/11/28 17:44:01 INFO NewHadoopRDD: Input split: file:/disks/disk1/data_sample/19NGS14; 2/19NGS142.bam:1476395008+33554432; 19/11/28 17:44:01 INFO NewHadoopRDD: Input split: file:/disks/disk1/data_sample/19NGS14; 2/19NGS142.bam:1509949440+33554432; 19/11/28 17:44:01 INFO NewHadoopRDD: Input split: file:/disks/disk1/data_sample/19NGS14; 2/19NGS142.bam:704643072+33554432; 19/11/28 17:44:02 ERROR Executor: Exception in task 6.0 in stage 1.0 (TID 7); java.util.NoSuchElementException: next on empty iterator; at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); at scala.collection.Iterator$$anon$13.next(Iterator.scala:469); ......; ```; The interval.file is fine because I use it for the whole GATK pipeline. ; The CountReadsSpark has the same error. Please check this. Thanks.; Chris. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24645/countbasesspark-doesnt-work-with-l-opt/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6319:395,error,errors,395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6319,3,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"User report:. ValidateVariants causes the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i -gvcf; ```; And it causes the following error for all my files:; ```; ***********************************************************************. A USER ERROR has occurred: In a GVCF all records must ordered. Record: [VC Unknown @ Super-Scaffold_2:1-4 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=4} filters= covers a position previously traversed. ***********************************************************************; ```. This doesn't cause the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i; ```. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/59104#Comment_59104",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6023:42,error,error,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,UserException.BadTmpDir has an out of date error message,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4709:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4709,1,['error'],['error']
Availability,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5780:1147,down,downsample,1147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780,2,['down'],"['downsample', 'downsampling']"
Availability,Users tend to feed GVCF files to these 3 tools primarily that ends up giving error messages of missing annotations or else. I added these warning messages.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9008:77,error,error,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9008,1,['error'],['error']
Availability,Using 4.0.4.0 and got the following rather unhelpful error from a ImportGenomicsDB. I suspect it might be a variant in one of my VCFs being imported causing an issue but the error message doesn't even hint at which file it was working on. ```; ~/gatk-4.0.4.0/gatk GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; Using GATK jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar GenomicsDBImport -R hs37d5.fa --sample-name-map gvcfs.samplemap --genomicsdb-workspace-path outputsb.workspace -L 6:29691241-33054015 --batch-size 50 --consolidate true; 17:01:47.406 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cloud-user/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:01:47.522 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.522 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.4.0; 17:01:47.522 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:01:47.522 INFO GenomicsDBImport - Executing as cloud-user@lustre-assembly-1 on Linux v3.10.0-862.3.3.el7.x86_64 amd64; 17:01:47.522 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 17:01:47.523 INFO GenomicsDBImport - Start Date/Time: 17 July 2018 17:01:47 BST; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 17:01:47.523 INFO GenomicsDBImport - Picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5064:53,error,error,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064,2,['error'],['error']
Availability,Using PathSeqFilterSpark --ignore-alignment-contigs raises error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8766:59,error,error,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8766,1,['error'],['error']
Availability,"Using VCFTools to validate:; ```; vcf-validator SAMPLE7T-vs-SAMPLE7N-filtered.vcf; ```. I get a massive amount of messages:; ```; .....snip.......; column SAMPLE7N at 19:49136721 .. Could not validate the float [NaN],FORMAT tag [MPOS] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MBQ] expected different number of values (expected 1, found 2); .....snip.......; column SAMPLE7T at 19:45901415 .. FORMAT tag [MBQ] expected different number of values (expected 1, found 2),FORMAT tag [MMQ] expected different number of values (expected 1, found 2),FORMAT tag [MCL] expected different number of values (expected 1, found 2),FORMAT tag [MFRL] expected different number of values (expected 1, found 2),FORMAT tag [MPOS] expected different number of values (expected 1, found 2); .....snip.......; ```. Sure enough, the header does not match the values for those fields (in the header number=""A""), so the validation errors are correct. Not sure what is the deal with FOXOG, but that may not be a big deal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3296:1161,error,errors,1161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3296,1,['error'],['errors']
Availability,"Using a .list file for intervals doesn't trim white space appropriately and causes interval parsing to fail with strange errors. For example the following line ending in a space causes an error in an interval .list file:; ""20:1-100 "" . This fails with the following error:; ```. A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:1-100 : For input string: ""100 "". ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:1-100 : For input string: ""100 ""; 	at org.broadinstitute.hellbender.utils.GenomeLocParser.parseGenomeLoc(GenomeLocParser.java:328); 	at org.broadinstitute.hellbender.utils.IntervalUtils.intervalFileToList(IntervalUtils.java:375); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:279); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:226); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:174); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:155); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeIntervals(GATKTool.java:513); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:708); 	at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:50); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6371:121,error,errors,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6371,4,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"Using https://jitpack.io/#broadinstitute/gatk for building SNAPSHOTS (independent on your jfrog repository, which has a expired date for snapshots - https://github.com/broadinstitute/gatk/issues/4565) is not working any longer due to the requirement of git-lfs (e.g., https://jitpack.io/com/github/broadinstitute/gatk/4.0.4.0/build.log). It will be nice to add a `jitpack.yml` file (see https://jitpack.io/docs/BUILDING/#custom-commands) to install dependencies - this will be useful for downstream project depending on SNAPSHOTS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4819:488,down,downstream,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4819,1,['down'],['downstream']
Availability,"Using the `--gcs-project-for-requester-pays` argument to access a requester-pays bucket, I tried `broad-dsde-methods`, `""broad-dsde-methods""`, and `222581509023`, but no dice. The log shows that the engine is reading the argument, but it doesn't seem to be passed to the cloud utils correctly.; ```; 14:23:16.753 INFO PrintReads - GCS max retries/reopens: 20; 14:23:16.753 INFO PrintReads - Requester pays: enabled. Billed to: broad-dsde-methods; 14:23:16.753 INFO PrintReads - Initializing engine; 14:23:18.501 INFO PrintReads - Shutting down engine; [September 23, 2019 2:23:18 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=375914496; code: 400; message: Bucket is requester pays bucket but no user project provided.; reason: required; location: null; retryable: false; com.google.cloud.storage.StorageException: Bucket is requester pays bucket but no user project provided.; ```. `gsutil -u 222581509023 stat gs://fc-secure-2011b97c-a9c9-4a13-8911-f3833be31253/CCDG_WashU_CVD_EOCAD_METSIM_WGS_all/2893803451.cram` works and `gsutil stat gs://fc-secure-2011b97c-a9c9-4a13-8911-f3833be31253/CCDG_WashU_CVD_EOCAD_METSIM_WGS_all/2893803451.cram` produces; ```; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```. I tried the above variations on `export GOOGLE_CLOUD_PROJECT=` in the shell, but that didn't change things. It's possible I missed some combination of the above, but at the very least our docs need clarification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6179:539,down,down,539,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6179,1,['down'],['down']
Availability,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985:152,error,error,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985,2,['error'],['error']
Availability,"Using underlying functionality from GenomicsDB to validate/specify cloud url's for GenomicsDB workspaces. This allows for the specification of s3 and azure blob storage uri's in addition to gcs for GenomicsDB workspaces. Currently, there are no tests for s3/az uri's, this is just experimental functionality available if needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7271:308,avail,available,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7271,1,['avail'],['available']
Availability,"V39_SCAF000680, LMJLV39_SCAF000681, LMJLV39_SCAF000682, LMJLV39_SCAF000683, LMJLV39_SCAF000684, LMJLV39_SCAF000685, LMJLV39_SCAF000686, LMJLV39_SCAF000687, LMJLV39_SCAF000688, LMJLV39_SCAF000689, LMJLV39_SCAF000690, LMJLV39_SCAF000691, LMJLV39_SCAF000692, LMJLV39_SCAF000693, LMJLV39_SCAF000694, LMJLV39_SCAF000695, LMJLV39_SCAF000696, LMJLV39_SCAF000697, LMJLV39_SCAF000698, LMJLV39_SCAF000699, LMJLV39_SCAF000700, LMJLV39_SCAF000701, LMJLV39_SCAF000702, LMJLV39_SCAF000703, LMJLV39_SCAF000704, LMJLV39_SCAF000705, LMJLV39_SCAF000706, LMJLV39_SCAF000707, LMJLV39_SCAF000708, LMJLV39_SCAF000709, LMJLV39_SCAF000710, LMJLV39_SCAF000711, LMJLV39_SCAF000712, LMJLV39_SCAF000713, LMJLV39_SCAF000714, LMJLV39_SCAF000715, LMJLV39_SCAF000716, LMJLV39_SCAF000717, LMJLV39_SCAF000718, LMJLV39_SCAF000719, LMJLV39_SCAF000720, LMJLV39_SCAF000721, LMJLV39_SCAF000722, LMJLV39_SCAF000723, LMJLV39_SCAF000724, LMJLV39_SCAF000725, LMJLV39_SCAF000726, LMJLV39_SCAF000727, LMJLV39_SCAF000728, LMJLV39_SCAF000729, LMJLV39_SCAF000730, LMJLV39_SCAF000731, LMJLV39_SCAF000732, LMJLV39_SCAF000733, LMJLV39_SCAF000734, LMJLV39_SCAF000735, LMJLV39_SCAF000736, LMJLV39_SCAF000737, LMJLV39_SCAF000738, LMJLV39_SCAF000739, LMJLV39_SCAF000740, LMJLV39_SCAF000741, LMJLV39_SCAF000742, LMJLV39_SCAF000743, LMJLV39_SCAF000744, LMJLV39_SCAF000745, LMJLV39_SCAF000746, LMJLV39_SCAF000747, LMJLV39_SCAF000748, LMJLV39_SCAF000749, LMJLV39_SCAF000750, LMJLV39_SCAF000751, LMJLV39_SCAF000752, LMJLV39_SCAF000753, LMJLV39_SCAF000754, LMJLV39_SCAF000755, LMJLV39_SCAF000756, LMJLV39_SCAF000757, LMJLV39_SCAF000758, LMJLV39_SCAF000759, LMJLV39_SCAF000760, LMJLV39_SCAF000761, LMJLV39_SCAF000762, LMJLV39_SCAF000763, LMJLV39_SCAF000764, LMJLV39_SCAF000765, LMJLV39_SCAF000766, LMJLV39_SCAF000767, LMJLV39_SCAF000768, LMJLV39_SCAF000769, LMJLV39_SCAF000770, LMJLV39_SCAF000771, LMJLV39_SCAF000772, LMJLV39_SCAF000773]; ##### ERROR ------------------------------------------------------------------------------------------; how should I fix it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6798:20585,ERROR,ERROR,20585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6798,1,['ERROR'],['ERROR']
Availability,VAT Performance / Reliability Improvements,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7828:18,Reliab,Reliability,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7828,1,['Reliab'],['Reliability']
Availability,VCF row validation error on gCNV results,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834,1,['error'],['error']
Availability,VCFCodec to read file file:///local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Mills_and_1000G_gold_standard.indels.hg38.vcf; 01:51:51.457 INFO FeatureManager - Using codec VCFCodec to read file file:///local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.dbsnp138.vcf; 01:51:51.507 INFO BaseRecalibrationEngine - The covariates being used here: ; 01:51:51.507 INFO BaseRecalibrationEngine - ReadGroupCovariate; 01:51:51.507 INFO BaseRecalibrationEngine - QualityScoreCovariate; 01:51:51.507 INFO BaseRecalibrationEngine - ContextCovariate; 01:51:51.507 INFO BaseRecalibrationEngine - CycleCovariate; 01:51:51.517 INFO FeatureManager - Using codec VCFCodec to read file file:///local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf; 20/04/29 01:51:51 ERROR Executor: Exception in task 581.0 in stage 0.0 (TID 581); org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf; at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:383); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:335); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:238); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:222); at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$nul,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6578:2577,ERROR,ERROR,2577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6578,1,['ERROR'],['ERROR']
Availability,"VICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:31:00.557 INFO SplitNCigarReads - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - Defaults.REFERENCE_FASTA : null; 15:31:00.557 INFO SplitNCigarReads - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2026:3296,down,down,3296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026,1,['down'],['down']
Availability,VQSR plot legend is masking part of the histogram,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6585:20,mask,masking,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6585,1,['mask'],['masking']
Availability,VS-1029 Fix naming errors in hail_gvs_import script,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8487:19,error,errors,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8487,1,['error'],['errors']
Availability,"VS-1092 - Fix for GvsCreateFilterSet OOD; Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f2dc8eca-1fc2-4ab3-ac38-ab430fb1d60a).; One failure - in the Exome test on AssertCostIsTrackedAndExpected. Doesn't seem related to this code change, so am going to allow it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8573:186,failure,failure,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8573,1,['failure'],['failure']
Availability,VS-1184 Echo callset branch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8653:8,Echo,Echo,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8653,1,['Echo'],['Echo']
Availability,VS-1288 Updates to ah_var_store from echo callset.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8991:37,echo,echo,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8991,1,['echo'],['echo']
Availability,VS-1291. Quick Fix to QuickstartIntegration test in EchoCallSet branch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8737:52,Echo,EchoCallSet,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737,1,['Echo'],['EchoCallSet']
Availability,VS-1310 - VS-1310 changes to echo callset branch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8797:29,echo,echo,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8797,1,['echo'],['echo']
Availability,VS-1310. Update gatk docker on EchoCallset,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8803:31,Echo,EchoCallset,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8803,1,['Echo'],['EchoCallset']
Availability,VS-1324 Investigate integration test failures,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8787:37,failure,failures,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8787,1,['failure'],['failures']
Availability,VS-1327 ensure sample ids are unique echo callset version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8821:37,echo,echo,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8821,1,['echo'],['echo']
Availability,VS-1335 bgzip on extract Echo Callset version (#8809),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8820:25,Echo,Echo,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8820,1,['Echo'],['Echo']
Availability,VS-1358. Have EchoCallset Branch Support Hail wheel.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8910:14,Echo,EchoCallset,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8910,1,['Echo'],['EchoCallset']
Availability,VS-1368 The tarball is too damn big (#8829) -- Echo Callset Version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8830:47,Echo,Echo,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8830,1,['Echo'],['Echo']
Availability,VS-1379 into echo branch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8879:13,echo,echo,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8879,1,['echo'],['echo']
Availability,VS-1395 have create vat from vds optionally take sites only vcf echo callset version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8866:64,echo,echo,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8866,1,['echo'],['echo']
Availability,VS-1397 update tests echo callset version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8848:21,echo,echo,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8848,1,['echo'],['echo']
Availability,VS-1413 Fix Pgen merge chrom encoding - EchoCallset Version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8880:40,Echo,EchoCallset,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8880,1,['Echo'],['EchoCallset']
Availability,VS-1422 Merge VAT changes in echo into ah_var_store,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8977:29,echo,echo,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8977,1,['echo'],['echo']
Availability,VS-1433.; This PR adds the tool vcf-validator to our variants docker and uses it in our integration test.; It validates that the VCFs have no errors in the `AD` field (which were previously reported by AoU friends).; It also modifies the Beta integration test to only run on WGS samples (previously ran on all samples). Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/0c9fb830-7831-4bee-a82c-d0146b250e59).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8903:142,error,errors,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8903,1,['error'],['errors']
Availability,VS-1506 - Pull changes from EchoCallset into ah_var_store branch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8993:28,Echo,EchoCallset,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8993,1,['Echo'],['EchoCallset']
Availability,VS-1526 - Fix error in Extract Manifest Creation causing it to run forever.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9034:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9034,1,['error'],['error']
Availability,VS-341 improve test cost calc echo callset version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8813:30,echo,echo,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8813,1,['echo'],['echo']
Availability,VS-430 Remove yng status from vds echo callset version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8864:34,echo,echo,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8864,1,['echo'],['echo']
Availability,VS-942. Fix an error where NO variants were found,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8388:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8388,1,['error'],['error']
Availability,VS-974 - Better error on GvsCreateFilterSet.PopulateFilterSetInfo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8530:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8530,1,['error'],['error']
Availability,"ValidateVariants gVCF mode error ""covers a position previously traversed""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6023:27,error,error,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023,1,['error'],['error']
Availability,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:226,error,error,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,1,['error'],['error']
Availability,ValidateVariants: Error reports last (not first) overlapping interval,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103:18,Error,Error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103,1,['Error'],['Error']
Availability,VariantAnnotator cmd line arg parsing error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4604:38,error,error,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604,1,['error'],['error']
Availability,VariantEval Error if output file does not pre-exist,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5674:12,Error,Error,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674,1,['Error'],['Error']
Availability,"VariantEval compares input VCF(s) to comparison tracks(s). One can execute the tool without providing comparison tracks. In GATK3, when no comps are provided, the tool creates a dummy RODBinding, not backed by a file. This is passed through the downstream code in the tool. In GATK4, RODBinding has become FeatureInput. Is there a way to replicate this dummy FeatureInput behavior in GATK4? I dont see a way to do this in GATK4, since FeatureInput needs a valid filepath. I think I can rework VariantEval to come up w/ a different solution than this dummy binding; however, thought I'd ask here first.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4440:245,down,downstream,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4440,1,['down'],['downstream']
Availability,VariantRecalibrator Reference alleles not same position error message,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6963:56,error,error,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6963,1,['error'],['error']
Availability,VariantRecalibrator should throw an error when an annotation is specified twice,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2221:36,error,error,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2221,1,['error'],['error']
Availability,"VariantWalker/MultiVariantWalker create two FeatureDataSources for each input, which results in redundant dynamic discovery of which codec to use. For a MultiVariantWalker with a lot of inputs, like VariantRecalibrator, this can be a lot of path/stream opening/closing. Propose this small change to cache the codec in the FeatureInput. Ideally FeatureManager would keep track of this, but thats bigger refactor as not all of the FeatureDataSources are created by FeatureManager.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2737:96,redundant,redundant,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737,1,['redundant'],['redundant']
Availability,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103:1797,error,error,1797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103,3,['error'],['error']
Availability,Vs 629 failure to retrieve job information during ingest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8047:7,failure,failure,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8047,1,['failure'],['failure']
Availability,"WRITE_FOR_SAMTOOLS : true; 15:47:25.649 INFO VariantsToTable - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:25.650 INFO VariantsToTable - Deflater: JdkDeflater; 15:47:25.651 INFO VariantsToTable - Inflater: JdkInflater; 15:47:25.651 INFO VariantsToTable - GCS max retries/reopens: 20; 15:47:25.651 INFO VariantsToTable - Requester pays: disabled; 15:47:25.651 INFO VariantsToTable - Initializing engine; 15:47:25.751 INFO FeatureManager - Using codec VCFCodec to read file file:///media/glier_ubuntu/4TB/Javad_Final/6bwa/2/filtered_merged_firstpass.vcf.gz; 15:47:31.284 INFO VariantsToTable - Done initializing engine; 15:47:31.293 WARN VariantsToTable - Allele-specific fields will only be split if splitting multi-allelic variants is specified (`--split-multi-allelic` or `-SMA`; 15:47:31.294 INFO ProgressMeter - Starting traversal; 15:47:31.294 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 15:47:31.296 INFO VariantsToTable - Shutting down engine; [May 13, 2020 at 3:47:31 p.m. EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.VariantsToTable done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=3286237184; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/VariantTransformer. Method 'htsjdk.variant.variantcontext.VariantContext lambda$identity$76d6cab0$1(htsjdk.variant.variantcontext.VariantContext)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.VariantTransformer.identity(VariantTransformer.java:32); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.makePreVariantFilterTransformer(VariantWalkerBase.java:131); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.getTransformedVariantStream(VariantWalkerBase.java:155); 	at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6604:11343,down,down,11343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6604,1,['down'],['down']
Availability,"We are currently plagued with cryptic intermittent failures coming from the BWA and FML bindings in Travis CI. These usually manifest as a simple ""exited with code 137"" (ie., killed by signal 9) error, but sometimes we get an explicit segfault or out-of-memory error. Examples:. ```; �[31mFAILURE: �[39m�[31mBuild failed with an exception.�[39m; * What went wrong:; Execution failed for task ':test'.; �[33m> �[39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3209:51,failure,failures,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209,5,"['error', 'failure']","['error', 'failures']"
Availability,"We are in the process of trying to get [The Mutect2 WDL worfkow](https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl) working. When we run it on an input tsv file, we get the following error on transitioning from Mutect2 to MergeVCFs:. `Unable to evaluate parameters: parameter \""m2_multi.m2.MergeVCFs.input_vcfs-9\"" has invalid value:`. This error I think is due to a confusion between a string output and a file output in the Mutect2 call. And in fact, there I see that `File output_vcf = read_string(""vcf_name.txt"")` - `read_string` is not terribly well documented, but I would guess that it in fact outputs a string, rather than a file. . I am by no means an expert on WDL, but it does appear that this output might need a fix. Thoughts? (This issue has been seen before btw, [here](https://gatkforums.broadinstitute.org/wdl/discussion/10046/string-vs-file-mistake-caused-cromwell-26-workflow-to-hang), and I just found [this old forum post](https://gatkforums.broadinstitute.org/wdl/discussion/8683/what-is-the-best-way-to-specify-a-file-output-when-the-file-name-is-not-known-ahead-of-the-run) which seems to ask the question too)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4017:204,error,error,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4017,2,['error'],['error']
Availability,"We are unable to run joint genotyping on the cloud when it includes a sample with a space in the name; it fails when calling GATK's GenomicsDBImport functionality (see error log below). . This practice (samples with spaces in the name) is unfortunately not uncommon, so we've had to modify our various workflows one by one to add support for this case. Please update GenomicsDBImport to handle samples with a space in the name. . Example error log: ; gsutil cat gs://broad-jg-dev-cromwell-execution/JointGenotyping/6918095f-ca06-4883-bcb5-f5c2e343bb6d/call-ImportGVCFs/shard-0/ImportGVCFs-0-stderr.log. Using GATK jar /usr/gitc/gatk-package-4.beta.6-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx4g -Xms4g -jar /usr/gitc/gatk-package-4.beta.6-local.jar GenomicsDBImport --genomicsDBWorkspace genomicsdb --batchSize 50 -L chr1:1-391754 --sampleNameMap /cromwell_root/broad-jg-dev-storage/freimer_dutch_fin_wgs_v1/v1/sample_map --readerThreads 5 -ip 500; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.H9t5pC; [December 14, 2017 7:41:30 PM UTC] GenomicsDBImport --genomicsDBWorkspace genomicsdb --batchSize 50 --sampleNameMap /cromwell_root/broad-jg-dev-storage/freimer_dutch_fin_wgs_v1/v1/sample_map --readerThreads 5 --intervals chr1:1-391754 --interval_padding 500 --genomicsDBSegmentSize 1048576 --genomicsDBVCFBufferSize 16384 --overwriteExistingGenomicsDBWorkspace false --consolidate false --validateSampleNameMap false --interval_set_rule UNION --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 0 -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3979:168,error,error,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3979,2,['error'],['error']
Availability,"We blow up with the following scary error:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.IOException: unexpected exception type; at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1538); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1110); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1810); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315:36,error,error,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315,3,"['error', 'failure']","['error', 'failure']"
Availability,"We can do this in the GenomicsDBImport walker -- it doesn't have to be in GenomicsDB. Not every version is backward compatible, so when there are cryptic errors it would be nice to know what version was used to create the GDB.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4508:154,error,errors,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4508,1,['error'],['errors']
Availability,We can simplify it by removing thread safety guarantees and then push it down into htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/89:73,down,down,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/89,1,['down'],['down']
Availability,"We created a GenomicsDB workspace using GenomicsDBImport, followed by 2 rounds of adding new samples using GenomicsDBImport. We then ran GenotypeGVFs on it and received this NPE. Is there any information or debugging you can suggest that would help diagnose this?. The command run was approximately this (filepaths have been simplified):. java -Xmx48g -Xms48g -Xss2m GenomeAnalysisTK4.jar GenotypeGVCFs -R 128_Mmul_10.fasta --variant gendb://CombinedGenotypes_WES.gdb -O Test_WES_variantcalling.vcf.gz --annotate-with-num-discovered-alleles -stand-call-conf 30 --max-alternate-alleles 12 --allow-old-rms-mapping-quality-annotation-data. and the error:. 17 Jun 2020 15:49:59,410 DEBUG: java.lang.NullPointerException; 17 Jun 2020 15:49:59,412 DEBUG: at htsjdk.variant.bcf2.BCF2Decoder.decodeInt(BCF2Decoder.java:226); 17 Jun 2020 15:49:59,413 DEBUG: at htsjdk.variant.bcf2.BCF2Decoder.decodeSingleValue(BCF2Decoder.java:157); 17 Jun 2020 15:49:59,414 DEBUG: at htsjdk.variant.bcf2.BCF2Decoder.decodeTypedValue(BCF2Decoder.java:146); 17 Jun 2020 15:49:59,416 DEBUG: at htsjdk.variant.bcf2.BCF2Decoder.decodeTypedValue(BCF2Decoder.java:130); 17 Jun 2020 15:49:59,417 DEBUG: at htsjdk.variant.bcf2.BCF2Decoder.decodeTypedValue(BCF2Decoder.java:125); 17 Jun 2020 15:49:59,419 DEBUG: at htsjdk.variant.bcf2.BCF2Codec.decodeInfo(BCF2Codec.java:410); 17 Jun 2020 15:49:59,420 DEBUG: at htsjdk.variant.bcf2.BCF2Codec.decodeSitesExtendedInfo(BCF2Codec.java:298); 17 Jun 2020 15:49:59,422 DEBUG: at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); 17 Jun 2020 15:49:59,423 DEBUG: at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); 17 Jun 2020 15:49:59,425 DEBUG: at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:183); 17 Jun 2020 15:49:59,426 DEBUG: at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); 17 Jun 2020 15:49:59,428 DEBUG: at java.util.Iterator.forEachRemaining(Iterator.java:116); 17 Jun 2020 15:49:59,42",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667:645,error,error,645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667,1,['error'],['error']
Availability,"We encountered an error with git-lfs in #1378. This was worked around by simply pulling twice in `scripts/install_git_lfs.sh`, but it shouldn't have to do that. . Remove this extra pull once a better solution is found in https://github.com/github/git-lfs/issues/904",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1404:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1404,1,['error'],['error']
Availability,We get errors like these:. Error: (3760208c5e4fb47a): java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at com.google.cloud.genomics.dataflow.readers.bam.Reader.process(Reader.java:66); at com.google.cloud.genomics.dataflow.readers.bam.ReadBAMTransform$ReadFn.processElement(ReadBAMTransform.java:96),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/637:7,error,errors,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/637,2,"['Error', 'error']","['Error', 'errors']"
Availability,"We had a little bit of trouble with AllelicCNV at the Finland workshop last week. Apologies that this isn't the most complete bug report, but the hands-on portion of the workshop moved pretty fast. Soo Hee took a picture of the error with her phone:; ![image](https://user-images.githubusercontent.com/6578548/30697898-6dff7e24-9eae-11e7-8ec3-d876483fec1a.png); The rest of the relevant line is ""undefined symbol: cblas_daxpy"". The version was the GATK4 beta 4 release and the command was:; ```; gatk-launch AllelicCNV \; --tumorHets tumor_hets.tsv \; --tangentNormalized tumor_C.tn.tsv \; --segments tumor_C.seg \; --outputPrefix acnv \; --intervalThresholdCopyRatio 5.0; ```; The inputs are in the AllelicCNV workshop bundle in Google Drive: https://drive.google.com/drive/folders/0BzI1CyccGsZiU1dkcndQMkRmTTQ. I think the host institution was running Red Hat, but it might have been Ubuntu. Like I said, sorry this is a pretty sad bug report. I haven't tried to reproduce the error since it seems platform-specific, but maybe some weirdo who doesn't use a Mac (@LeeTL1220) would give it a shot?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3599:228,error,error,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3599,2,['error'],['error']
Availability,We had removed it because it was shutting down and our tests showed it was unnecessary. It turns out that it's only unnecessary because we're getting jcenter results cached in the artifactory. JCenter is now planning to remain online for the immediate future in read only mode so it should be fine to put it back. Fixes https://github.com/broadinstitute/gatk/issues/7636,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7665:42,down,down,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7665,1,['down'],['down']
Availability,"We have WGS for more than 800 samples of cod. We are trying to genotype them with GATK, but we cannot get GenomicsDBImport to complete. It seems to run fine but we get the following message at the end of our slurm output file: ""slurmstepd: error: Exceeded step memory limit at some point."" We're running it one chromosome at a time.; We're running it with:; #SBATCH --mem-per-cpu=100G; #SBATCH --partition=hugemem; #SBATCH --cpus-per-task=2. --java-options ""-Xmx190g -Xms190g"" ; --batch-size 50 --consolidate; ; Could you help us optimize it so it actually runs and completes?; Thanks a lot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6321:240,error,error,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6321,1,['error'],['error']
Availability,"We have a problematic read that goes in as cigar `14S45M92S` (or something similar) and turns `137H14S` when clipped by `ReadClipper.hardClipLowQualEnds` `(AssemblyBasedCallerUtils::L73)`. This is because the first 137 are base quality 2. M2 (and HC) fails later on because we miscalculate the start position as -4. This only occurs when a specific interval file is used. When run with `-L chrM`, the we don't get the error, for example. Problematic bam: `/seq/picard_aggregation/RP-1490/WGS/OCB006937/v2/OCB006937.bam`; Read Name: `H3FWGCCXY170920`; Position: chrM:147-146 (this is what I wrote down but I'm not sure if it's right); Reference: `/seq/references/Mus_musculus_assembly10/v0/Mus_musculus_assembly10.fasta` (this is a mouse reference). ```; java.lang.IllegalArgumentException: contig must be non-null and not equal to *, and start must be >= 1; contig = chrM; start = -4; 	at org.broadinstitute.hellbender.utils.read.SAMRecordToGATKReadAdapter.setPosition(SAMRecordToGATKReadAdapter.java:92); 	at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHARDCLIP_BASES(ClippingOp.java:381); 	at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:73); 	at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:145); 	at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:126); 	at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:330); 	at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:333); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:82); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:242); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:211); 	at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3813:418,error,error,418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3813,2,"['down', 'error']","['down', 'error']"
Availability,"We have determined that a sites-only VCF causes ASEReadCounter to only output a header. There are warnings in the stack trace but it is not clear that the tool found no genotypes in the file. We can look into adding a check in ASEReadCounter to exit out if the VCF has no genotype fields. The documentation for this tool should also be more specific. This request was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota excee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7327:921,error,error,921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327,1,['error'],['error']
Availability,"We have the basic DREAM somatic challenge, but there's also an RNA challenge, and perhaps others. If it's a similar format of bams, masks, and truth vcfs it would be really easy to set up a validation like the one we currently have on Firecloud.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5427:132,mask,masks,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5427,1,['mask'],['masks']
Availability,We might be able to compute the microhomology/microsinertion length based on the two cigars and nail it down to an exact breakpoint or range specific to that clip.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1925:104,down,down,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1925,1,['down'],['down']
Availability,"We need a framework for easily comparing annotation values in test suite outputs (actual vs. expected), with a per-annotation configurable tolerance before failure. Exact-match comparisons are too brittle for our needs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3275:139,toler,tolerance,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3275,2,"['failure', 'toler']","['failure', 'tolerance']"
Availability,"We need a plan for this. It would be nice to use the SV team's in-memory BWA-mem binding, but it's not clear that tweaking settings would be enough to capture possible alignment errors in a BWA-aligned bam file. CGA has used Novo-align and BLAT in the past. We'll want to talk with Chris, Heng, and Julian about this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3089:178,error,errors,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3089,1,['error'],['errors']
Availability,"We need a tool to compare multiple references and spit out a TSV (or similar) detailing what the differences are. Additionally it should be able to spit out a liftover file that will properly move a variant from one reference to another. We should first compare the sequence dictionaries in the references to see if they have equal lengths and checksums - the names may differ and we should track this so we can definitively say which contigs are equivalent. After this, we should walk the references and find out specifically which bases differ between contigs that have different checksums (with some limits on the number of differences between them so we don't get bogged down by `hg19` vs `hg38` comparisons). ; Then it should create a liftover file from those comparisons so the data can be easily converted between the references given. . Additionally, it should be able to take a variant file and a set of references and say:. - whether the variant file ""belongs"" to one of the given references; - if it isn't exactly from one of the given references, which reference is closest; - _optionally_: a lifted-over version of that VCF to the closest reference (with a bunch of warnings, if applicable). This will finally lay to rest the questions raised by [my blog post about ""HG19""](https://gatk.broadinstitute.org/hc/en-us/articles/360035890711). I believe Adam Phillipy had created a perl script that does something similar to this, but a brief view of his github page doesn't show anything like that anymore (maybe it was called `refdiff` or similar). I created a bash script that does something similar to this (see attached), but it only looks at the sequence dictionaries. It produces a table similar to that in the above blog post. For example:. |MD5 | HG38(Homo_sapiens_assembly38.dict) | HG38_WEIRD(genome.hg38rg.fa.dict)|; | --- | --- | --- |; |1e95e047b98ed92148dd84d6c037158c|chr1_KI270708v1_random|1_KI270708v1_random|; |42f7a452b8b769d051ad738ee9f00631|chr1_KI270714v1_random|1_KI270",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6837:675,down,down,675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6837,1,['down'],['down']
Availability,"We need a way to install GATK Python modules onto the docker image, from repo source, in a way that doesn't assume a repo clone is present on the docker image (there currently is one, but we want to remove it to recover space), and that also doesn't make the conda environment dependent on a repo clone. This PR adds a build task that creates a zip archive of the GATK Python source; propagates that to the docker image, and then pip installs the contents of the archive into the conda environment on the docker. Since we don't have an actual python module in the repo at the moment, there is a second, temporary, commit that contains a dummy python module used only to trigger and test that the installation works.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3964:212,recover,recover,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3964,1,['recover'],['recover']
Availability,"We need to make sure the `SparkSharder` can handle coverage spikes. We should profile to see how it performs in regions of high coverage, and potentially come up with a partition-level downsampling strategy that could discard partitions with extraneous coverage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2251:185,down,downsampling,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2251,1,['down'],['downsampling']
Availability,We need to produce a script that will make it easy to evaluate what changes to the HalpotypeCallerSpark will result in the biggest performance impact. To that end we want to write wdls and associated scripts that will make it easier for us to evaluate what each incremental change to the tool will change about accuracy and runtime for the machine configurations we care about. We should probably also hammer down what the machine types we consider important are as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5396:409,down,down,409,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5396,1,['down'],['down']
Availability,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1771:588,failure,failure,588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771,1,['failure'],['failure']
Availability,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2706:324,error,error,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706,1,['error'],['error']
Availability,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2622:38,error,error,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622,11,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"We see a massive (18x) slowdown of our spark performance tests on jenkins. The failures started on July 20th. The last good build was of a21447f. Which leaves one of:; - 4c697e06ea33c9179840c81c843658442c82a951: Move to google-cloud-java snapshot with more robust retries, and set … ; - 1bc0bbfc5a2240e85fd4b9f9010673c7242552a0 Filter Mutect2 artifacts that arise from apparent-duplicate reads. as the culprit. It seems more likely that the google cloud changes are causing the slowdown.; It seems like the slowdown is happening because of a change in the scattering, going from many partitions to fewer partitions which then all get scheduled on the same shard.; It's not immediately obvious what's causing this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3437:79,failure,failures,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3437,2,"['failure', 'robust']","['failures', 'robust']"
Availability,"We should add a runtime check, probably at the ScriptExecutor level, to verify that the version of the python package we're running matches the version of GATK. Otherwise subtle failure modes could ensue when a user upgrades GATK but does not re-establish the conda env.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4995:178,failure,failure,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4995,1,['failure'],['failure']
Availability,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377:306,error,error,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377,3,['error'],['error']
Availability,"We should be able to use this single implementation to satisfy both https://github.com/broadinstitute/gatk/issues/4347 and https://github.com/broadinstitute/gatk/issues/4351. Questions: @vdauwera What other tools that were dropped from GATK3 should be added to the list now? GATK3 also has a deprecated annotations list, which is included here, but is empty. Are there any annotations that should be listed ? I can't really implement/test that part unless I populate it with something. Also, trying to run a missing tool is currently handled as an error, and surfaces in the context of a usage message. Perhaps that hides it too much:. <img width=""918"" alt=""screen shot 2018-03-07 at 11 25 40 am"" src=""https://user-images.githubusercontent.com/10062863/37104403-a15635ae-21fa-11e8-985a-94ff0e203cf8.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4505:548,error,error,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4505,1,['error'],['error']
Availability,"We should figure out how to fail the travis build if the before_install or install blocks fail. We have confusing test failures when earlier stages fail, it would be better to error early.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3444:119,failure,failures,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3444,2,"['error', 'failure']","['error', 'failures']"
Availability,"We should have an installer script checked in to the repo for downloading the Funcotator datasources. Ideally the script would have a trivial Java frontend in the form of a simple GATK tool, to make it more discoverable by users.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4549:62,down,downloading,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4549,1,['down'],['downloading']
Availability,"We should run all of our dataflow tests on spark local runner as well as the dataflow direct pipeline runner. The plan is to run these as ""optional"" tests so that spark failure doesn't block our builds but we will be aware of any differences between the spark and google implementations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/551:169,failure,failure,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/551,1,['failure'],['failure']
Availability,We think we're hitting disk space caps on travis in some builds. Travis support suggested we move to their minimal image which has more available space. We should investigate if this is possible without major pain. Particularly it may have problems with git-lfs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3236:136,avail,available,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3236,1,['avail'],['available']
Availability,"We usually use a `ReadsDownsamplingIterator` with a `PositionalDownsampler` that has a `ReservoirDownsampler` inside of it, which works as intended. If, however, you create a `ReadsDownsamplingIterator` with a raw `ReservoirDownsampler` directly, no items get downsampled. This is because the implementation of `ReservoirDownsampler.hasFinalizedItems()` violates the assumptions of the `ReadsDownsamplingIterator`. Reported by @cmnbroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4768:260,down,downsampled,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4768,1,['down'],['downsampled']
Availability,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1426:341,avail,available,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426,1,['avail'],['available']
Availability,"We want to downsample pathological regions at the Hadoop-BAM level, before paying the cost of loading all the reads into an RDD. Probably we want something like a `ReservoirDownsampler` that downsamples on a per-alignment-start basis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1644:11,down,downsample,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644,2,['down'],"['downsample', 'downsamples']"
Availability,We were seeing failures on spark clusters that manifested as being; unable to find the Main class while running spark submit. The caused was the accidental introduction of a jar signature file; and key from the transitive gnu.getopt dependency. This was causing; signature validation failures since our uber jar did not match the; expected hashes. Fixed by excluding .SF and .RSA files from our jars.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618:15,failure,failures,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618,2,['failure'],['failures']
Availability,We're going to trim down to ILLUMINA only for now. We can delete a bunch of code that is for other kinds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/187:20,down,down,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/187,1,['down'],['down']
Availability,"We're running into an error with GATK 4.0beta3 on some read pairs that appears related to processing clipping on a read. The BAM file itself gets processed fine on GATK 3.8 and the problematic read appears okay on a manual inspection. This is a self-contained reproducible test case that demonstrates the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk_cigar_error.tar.gz. The error is:; ```; java.lang.IllegalArgumentException: contig must be non-null and not equal to *, and start must be >= 1; at org.broadinstitute.hellbender.utils.read.SAMRecordToGATKReadAdapter.setPosition(SAMRecordToGATKReadAdapter.java:92); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHARDCLIP_BASES(ClippingOp.java:381); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:73); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:147); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:128); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:332); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:335); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:84); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:238); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:480); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:221); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:244); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:217); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GAT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3466:22,error,error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3466,2,['error'],['error']
Availability,We're seeing a high rate of failures running BQSR pipelines in production with `ExecutionException`s. . The root cause seems to be an `UnknownHostException` thrown in the google storage api client. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:567); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:556); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094:28,failure,failures,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094,1,['failure'],['failures']
Availability,We've been encountering transient 403 errors when using GCS NIO.; It seems that some GCS-related service is incorrectly returning 403 in; certain cases where we do actually have permission to access a resource.; This commit moves us to a google-cloud-java snapshot that retries upon; 403 errors:. https://github.com/droazen/google-cloud-java/commit/6d11bef1c81f885c26b2b56c8616b7a705171e4f. Resolves #3735,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3766:38,error,errors,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3766,2,['error'],['errors']
Availability,"We've been experiencing intermittent, inexplicable failures in the test suite.; In a recent travis build, an assertion in MisencodedBaseQualityReadTransformerUnitTest; failed for no reason at all, then passed on the re-run (with no code changes). My initial suspect is the new test suite parallelism, which based on my experience; with TestNG is a potentially inexhaustible source of weird, impossible-to-reproduce errors.; Let's turn this off for now and see if these intermittent failures go away. The; extra speed isn't worth it if we can't trust the results of our travis builds!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/307:51,failure,failures,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/307,3,"['error', 'failure']","['errors', 'failures']"
Availability,We've been noticing a number of sporadic wdl test failures recently. They typically resolve after rerunning. Some of them seem to be the travis timeout after 10 minutes of inactivity. It would be good if we could make that happen less often. Here's an example of a mutect2 wdl failure [travis log.txt](https://github.com/broadinstitute/gatk/files/1623864/travis.log.txt),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4129:50,failure,failures,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4129,2,['failure'],"['failure', 'failures']"
Availability,"We've been using 4.beta.6 to generate new callsets because it has the GenomicsDBImport batching fix and it seems to have introduced transient Auth errors that production was not seeing before. This happens a maybe one shard at every task level and when rerun usually succeeds but as you can imagine is pretty annoying. This happens across multiple tools (GenomicsDBImport, GatherVcfs). Sometimes we get this as the only response from GATK when this happens. ```; ***********************************************************************. A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-gotc-prod-storage/pipeline/G101956/gvcfs/DDP_ATCP_42_1.4afb46bb-4009-47c4-9aa0-407e92de0db8.g.vcf.gz. ***********************************************************************; ```. and other times we get a nice stacktrace for this issue. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.IOUtil.transferByStream(IOUtil.java:141); 	at org.broadinstitute.hellbender.tools.GatherVcfsCloud.gatherWithBlockCopying(GatherVcfsCloud.java:394); 	at org.broadinstitute.hellbender.tools.GatherVcfsCloud.doWork(GatherVcfsCloud.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:147,error,errors,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,4,"['ERROR', 'Error', 'Failure', 'error']","['ERROR', 'Error', 'Failure', 'errors']"
Availability,"We've discovered a confusing git-lfs bug that effects people who have a git fork of gatk that started before the gatk/gatk-protected merge. . The symptom is that if you try to update your fork from the gatk master branch, and then push to your master, you'll see git errors like this:. ```; $ git push; open/Users/louisb/tmp/gatk-1/src/test/resources/large-protected/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx: no such file or directory; ```. This will prevent you from updating your master. The solution is to checkout the commit `32a2b39` which will manually trigger a git-lfs download of 1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx. Then checkout master again and push. @magicDGS This probably affects you. . We're going to open a git-lfs issue since it seems to be a pretty serious git-lfs bug, I'll update this ticket with more information when we have it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3038:267,error,errors,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3038,2,"['down', 'error']","['download', 'errors']"
Availability,"We've discovered a number of bam files being used in tests which are not valid BAM files. We should go through all the checked in BAMs, validate them, and replace broken ones. (Except ones that are intentionally broken for testing.) . (added later by @akiezun); In particular, copied from https://github.com/broadinstitute/hellbender/issues/568, NA12878.chr17_69k_70k.dictFix.bam has a problem:; whoever fixes this ticket needs to take care of this input. `htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 129, Read name 809R9ABXX101220:5:6:17918:145992, Mate Alignment start should be 0 because reference name = *.`. Here's the corresponding read:; `809R9ABXX101220:5:6:17918:145992 97 17 69400 37 67M9S * 71202348 0 ACTCCCCACCTTACCTGACTCCTTCCAGGGTTTGTCGCCTTTCCGGTCCCTGACCCCAGTGGATGGGAGTCTGTCC ?ABDDEEABEECBDBDAB=DEDCDEEBFADABCEAD?EEEDCFE?ABEEE@FCDEEEBF@F?CA4@@########## UQ:i:0`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569:508,error,error,508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"We've had reports of two kinds of intermittent network errors when using NIO that are not currently caught by google-cloud-nio:. * `UnknownHostException`; * 502 Bad Gateway. We should patch the library to add these errors to the list of retryable/reopenable errors. We can start by patching the GATK fork of `google-cloud-java`, and then concurrently submit a PR against `google-cloud-java` proper.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4888:55,error,errors,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888,3,['error'],['errors']
Availability,"We've identified a site that has a genotype with pls of 0,0,1 and reports GQ0. GVCF is `/humgen/gsa-hpprojects/dev/mshand/palantir/Analysis/535_GenotypeGVCFs/shard-314.vcf.gz` which cannot be made available publicly. site is 43291092. Gatk3 outputs a GQ0 hom-ref call while gatk4 outputs a no call. From discussion with @ldgauthier it seems like the no-call is a reasonable output, but it's strange that a) we have GQ0 when the pl difference is 1, and b) that there's a difference between gatk3 and 4. | | `GT:AD:DP:GQ:PGT:PID:PL` | ; | ----|------|; | gatk3 | `0/0:18,0:18:0:.:.:0,0,1 ` |; | gatk4 | `./.:18,0:18:.:.:.:0,0,1 ` |",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2712:197,avail,available,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2712,1,['avail'],['available']
Availability,We've made progress and the [Jenkins Tests](	at java.nio.file.Files.readAttributes(Files.java:1737) are failing with a new error now. . ```; com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:208); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:161); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:117); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:86); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:183); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$run,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517:123,error,error,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517,1,['error'],['error']
Availability,We've seen 2 instances of this error now:; ```; java.lang.ArrayIndexOutOfBoundsException: 15; 	at htsjdk.variant.bcf2.BCF2Utils.decodeType(BCF2Utils.java:122); 	at htsjdk.variant.bcf2.BCF2Decoder.decodeInt(BCF2Decoder.java:220); 	at htsjdk.variant.bcf2.BCF2Decoder.decodeNumberOfElements(BCF2Decoder.java:205); 	at htsjdk.variant.bcf2.BCF2Decoder.decodeTypedValue(BCF2Decoder.java:129); 	at htsjdk.variant.bcf2.BCF2Decoder.decodeTypedValue(BCF2Decoder.java:125); 	at htsjdk.variant.bcf2.BCF2LazyGenotypesDecoder.parse(BCF2LazyGenotypesDecoder.java:75); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148); 	at htsjdk.variant.variantcontext.GenotypesContext.getMaxPloidy(GenotypesContext.java:431); 	at htsjdk.variant.variantcontext.VariantContext.getMaxPloidy(VariantContext.java:785); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:405); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:92); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:212); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.strea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3412:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3412,1,['error'],['error']
Availability,What is driving the transcript error messages? Do we need to worry about these?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4882:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882,1,['error'],['error']
Availability,"What is the best Sparkified way to recapitulate our germline Best Practices workflow? Is there one pipeline that encompasses all steps from BWA alignment to HaplotypeCaller calling?. Please see forum thread as I have answered the user question tentatively with these two options:. [1] BwaSpark --> SortReadFileSpark --> ReadsPipelineSpark; [2] BwaAndMarkDuplicatesPipelineSpark --> SortReadFileSpark --> BQSRPipelineSpark --> HaplotypeCallerSpark. Thanks. ---; Hi @shlee ,; I am really sorry for the delay but I was busy in the last weeks. Anyway I will try to be clearer with this picture:. ![](https://us.v-cdn.net/5019796/uploads/editor/3x/9bu9fsvbgjrh.png """"). as you can see I would like to combine the tools `BwaAndMarkDuplicatesPipelineSpark` and `BQSRPipelineSpark` in one single tool, in order to improve efficiency of the pipeline (avoiding for example a disk writing). ; I tried to do it with [this](https://pastebin.com/XEqvpKmG ""this"") naive approach as I reported in previous comments, but executing this code I obtain this error (as you can see at the end of this [stack-trace](https://paste.ee/p/dMod1 ""stack-trace"") ) : ; ```; 17/11/03 13:02:14 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Reference index for 'chr11' not found in sequence dictionary.; ```. Do you think is better if I speak directly with developers in the GitHub repository?. Best regards,; Nicholas. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/44143#Comment_44143",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3878:1038,error,error,1038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3878,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"What is the current behavior when OpenMP is not available or is the wrong version on a Linux system? If the answer is ""it blows up with a gross error"", then let's patch the code so that it gracefully falls back to the single-threaded version, as on a Mac.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1819:48,avail,available,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1819,2,"['avail', 'error']","['available', 'error']"
Availability,When I am trying to run gatk HallotypeCaller I got error below. Can someone give me some advices?. INFO: Failed to detect whether we are running on Google Compute Engine.; 12:55:36.413 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.413 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:55:36.414 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:36.414 INFO HaplotypeCaller - Executing as linux@Paulina on Linux v5.4.0-67-generic amd64; 12:55:36.414 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.10+9-Ubuntu-0ubuntu1.18.04; 12:55:36.414 INFO HaplotypeCaller - Start Date/Time: 24 April 2021 at 12:55:36 CEST; 12:55:36.414 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.414 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Version: 2.24.0; 12:55:36.414 INFO HaplotypeCaller - Picard Version: 2.25.0; 12:55:36.414 INFO HaplotypeCaller - Built for Spark Version: 2.4.5; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical p,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7229:51,error,error,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229,1,['error'],['error']
Availability,"When I ingested more non- allele specific GVCFs, I ran into some edge cases. Most of these missing annotations occurred at weird sites where the GQ was 0 (but it wasn't a ref block). These changes keep it from throwing an error when it hits these sites.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6945:222,error,error,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6945,1,['error'],['error']
Availability,"When I tried to run the latest build of the spark gatk I got the following error message on both dataproc and the onprem cluster.; `File Not Found: [/Users/emeryj/IdeaProjects/gatk/build/libIntelDeflater.so]`. Making the following changes to gatk-launch seems to fix it. ```; @@ -23,14 +23,13 @@ BUILD_LOCATION = script +""/build/install/"" + projectName + ""/bin/""; GATK_RUN_SCRIPT = BUILD_LOCATION + projectName; BIN_PATH = script + ""/build/libs"". -EXTRA_JAVA_OPTIONS=""-Dsamjdk.intel_deflater_so_path=libIntelDeflater.so -Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ""; +EXTRA_JAVA_OPTIONS=""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "". DEFAULT_SPARK_ARGS = [""--conf"", ""spark.kryoserializer.buffer.max=512m"",; ""--conf"", ""spark.driver.maxResultSize=0"",; ""--conf"", ""spark.driver.userClassPathFirst=true"",; ""--conf"", ""spark.io.compression.codec=lzf"",; ""--conf"", ""spark.yarn.executor.memoryOverhead=600"",; -""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ""--conf"", ""spark.driver.extraJavaOptions="" + EXTRA_JAVA_OPTIONS,; ""--conf"", ""spark.executor.extraJavaOptions="" + EXTRA_JAVA_OPTIONS]; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1930:75,error,error,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1930,1,['error'],['error']
Availability,"When I use gatk haplotypecaller to get vcf from WES bam file, it stopped at somewhere of chr4. The error information is ""java.lang.IllegalStateException: Never found start 0 or stop -1 given cigar 23M90D98M"". Counld you help me to debug it?. My gatk version is 4.1.6.0. All error information is as bellows:. 14:47:36.134 INFO ProgressMeter - chr3:195451651 47.5 141130 2974.2; 14:47:46.204 INFO ProgressMeter - chr3:195937445 47.6 141450 2970.4; 14:47:56.209 INFO ProgressMeter - chr3:197401760 47.8 142070 2973.0; 14:48:06.317 INFO ProgressMeter - chr4:944918 48.0 142740 2976.5; 14:48:12.491 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.403408197; 14:48:12.491 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 110.672423397; 14:48:12.491 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 120.18 sec; 14:48:12.492 INFO HaplotypeCaller - Shutting down engine; [September 13, 2024 at 2:48:12 PM CST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 48.27 minutes.; Runtime.totalMemory()=708837376; java.lang.IllegalStateException: Never found start 0 or stop -1 given cigar 23M90D98M; at org.broadinstitute.hellbender.utils.read.AlignmentUtils.getBasesCoveringRefInterval(AlignmentUtils.java:204); at org.broadinstitute.hellbender.utils.haplotype.Haplotype.trim(Haplotype.java:95); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyResultSet.trimDownHaplotypes(AssemblyResultSet.java:135); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyResultSet.calculateOriginalByTrimmedHaplotypes(AssemblyResultSet.java:111); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyResultSet.trimTo(AssemblyResultSet.java:73); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:558); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(Hap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8984:99,error,error,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8984,3,"['down', 'error']","['down', 'error']"
Availability,"When I use gatk to CombineGVCFs, the following error occurs: ""Cannot read file because no suitable codecs found"".; How can I solve this problem？",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8255:47,error,error,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8255,1,['error'],['error']
Availability,"When WDL tests fail on travis it can be hard to determine the cause of the failure. We should either always retain the cromwell workflow logs, maybe in a gcs bucket, or have some easy way to opt-in for retention to make it easier to diagnose.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5098:75,failure,failure,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5098,1,['failure'],['failure']
Availability,"When [ReadsDataflowSource.getReadPCollection](https://github.com/broadinstitute/hellbender/blob/master/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReadsDataflowSource.java#L129) calls Google Dataflow's [ReadBAMTransform.getReadsFromBAMFilesSharded](https://github.com/googlegenomics/dataflow-java/blob/master/src/main/java/com/google/cloud/genomics/dataflow/readers/bam/ReadBAMTransform.java#L104) we get a **java.lang.VerifyError**. The full error looks like this:. ```; Exception in thread ""main"" java.lang.VerifyError: Bad type on operand stack; Exception Details:; Location:; com/google/cloud/genomics/dataflow/readers/bam/ReadBAMTransform.getReadsFromBAMFilesSharded(Lcom/google/cloud/dataflow/sdk/Pipeline;Lcom/google/cloud/genomics/utils/GenomicsFactory$OfflineAuth;Ljava/lang/Iterable;Lcom/google/cloud/genomics/dataflow/readers/bam/ReaderOptions;Ljava/util/List;)Lcom/google/cloud/dataflow/sdk/values/PCollection; @25: invokevirtual; Reason:; Type 'com/google/cloud/dataflow/sdk/transforms/Create' (current frame, stack[2]) is not assignable to 'com/google/cloud/dataflow/sdk/transforms/PTransform'; Current Frame:; bci: @25; flags: { }; locals: { 'com/google/cloud/dataflow/sdk/Pipeline', 'com/google/cloud/genomics/utils/GenomicsFactory$OfflineAuth', 'java/lang/Iterable', 'com/google/cloud/genomics/dataflow/readers/bam/ReaderOptions', 'java/util/List', 'com/google/cloud/genomics/dataflow/readers/bam/ReadBAMTransform' }; stack: { 'com/google/cloud/dataflow/sdk/values/TupleTag', 'com/google/cloud/dataflow/sdk/Pipeline', 'com/google/cloud/dataflow/sdk/transforms/Create' }; Bytecode:; 0x0000000: bb00 0159 2db7 0002 3a05 1905 2bb6 0003; 0x0000010: b200 042a 1904 b800 05b6 0006 c000 07b8; 0x0000020: 0008 b600 09b8 000a b200 0b2a 2cb8 0005; 0x0000030: b600 06c0 0007 120c b800 0db6 0009 b600; 0x0000040: 0e3a 0619 0519 06b6 000f b0 . at org.broadinstitute.hellbender.engine.dataflow.datasources.ReadsDataflowSource.getReadPCollection(ReadsDataflowSource.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/791:471,error,error,471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/791,1,['error'],['error']
Availability,"When a large number of intervals is specified at import time, a large number of arrays are created, which can lead to exhausting available open file handles. In addition, my informal tests indicate that querying a workspace created from an import that used a large number of intervals is pretty slow. @kgururaj suggests we might want issue a warning at a threshold of 100 intervals. See discussion in https://github.com/broadinstitute/gatk/pull/4997.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066:129,avail,available,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066,1,['avail'],['available']
Availability,"When an error occurs, we currently print this:. > Use -DGATK_STACKTRACE_ON_USER_EXCEPTIONto print the stack trace. In addition to missing a space, this doesn't work with gatk-launch. Replace this with:. > Use the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3307:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3307,1,['error'],['error']
Availability,"When hard clipping a read resulted in an empty read, and the read was sufficiently; close to the start of a contig, we could end up trying to set the read to a negative; start position and blowing up. This patch causes us to return an empty read earlier,; preventing us from setting an invalid start. Resolves the ""IllegalArgumentException during clipping: contig must be non-null and not; equal to *, and start must be >= 1"" error reported by many users of the GATK4; HaplotypeCaller. Resolves #3466",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4080:426,error,error,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4080,1,['error'],['error']
Availability,"When reading BAM or VCF files using the NIO code, sometimes it fails with a 503 ""server unavailable"" error. In this case we should retry instead of giving up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2436:101,error,error,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2436,1,['error'],['error']
Availability,"When running BQSR with incompatible references we get an ""out of memory"" error (see #668). This isn't what we were expecting so this issue is a reminder to investigate in case the underlying cause is a bug.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/679:73,error,error,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/679,1,['error'],['error']
Availability,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2793:87,failure,failure,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793,2,"['error', 'failure']","['error', 'failure']"
Availability,"When running a Picard tool within GATK, the exit code of that Picard tool is not preserved when GATK exits. For example, running `ExtractSequences` with no arguments produces an error and the Picard return code of `1`, but GATK returns a 0. This seems to be around `Main.java:196` (`mainEntry`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4329:178,error,error,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4329,1,['error'],['error']
Availability,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1491:54,error,error,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:73,error,error,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,7,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"When running the current version of the SV pipeline against the NA12878_PCR-_30X bam file aligned to hg19/b37, stage 10 is held up by a single assembly task that takes much longer than the others. Unfortunately I don't have exact timings yet, and haven't identified the assembly issue that's causing the problem, but I wanted to log this as an issue so we don't forget about it. It's possible that other samples will have similar outlier assemblies so it would be good to track down cases like this so that we can try to identify and remove pathological intervals. @tedsharpe if you find yourself with free time could you take a look at this? Whatever is going wrong with it might be fixed or relevant to your work on debugging assemblies and kmer gathering (if we're lucky perhaps your 'diffuse k-mer' fix will solve this issue).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3607:478,down,down,478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3607,1,['down'],['down']
Availability,"When running the following command. gatk4 BaseRecalibratorSpark -I xx_markduplicatespark.bam -knownSites dbsnp_138.b37.vcf -knownSites Mills_and_1000G_gold_standard.indels.b37.vcf -O xx_baserecalibratespark.table -R humann_g1k_v37.2bit --TMP_DIR tmp. I got error message,. Using GATK wrapper script /curr/tianj/software/gatk/build/install/gatk/bin/gatk; Running:; /curr/tianj/software/gatk/build/install/gatk/bin/gatk BaseRecalibratorSpark -I A15_markduplicatespark.bam -knownSites ref/Mills_and_1000G_gold_standard.indels.b37.vcf -O A15_baserecalibratespark.table -R /curr/tianj/data/humann_g1k_v37.2bit; 17:19:00.338 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/software/gatk/build/instabgkl_compression.so; [May 17, 2017 5:19:00 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark --knownSites /genome/ref/db_1000G_gold_standard.indels.b37.vcf --output A15_baserecalibratespark.table --reference /curr/tianj/data/humann_g1k_v37.2bp --joinStrategy BROADCAST --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_defdeletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_lles false --useOriginalQualities false --defaultBaseQualities -1 --readShardSize 10000 --readShardPadding 1000 --readValid-interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --sharl[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inf; [May 17, 2017 5:19:00 PM UTC] Executing as tianj@ip-172-31-78-66 on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM:4.alpha.2-261-gb8d32ee-SNAPSHOT; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.COMPRESSION_LEVEL : 1; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.CREATE_INDEX : false; 17:19:00.371 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2732:257,error,error,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732,1,['error'],['error']
Availability,"When splitting up samples over regions to pass to HaplotypeCallerSpark, we ran into an edge case where it will die on regions not containing any reads, with a empty collection error. It would be great if we could catch this cleanly and generate a VCF without any calls. Here is a small self contained test case which demonstrates the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_hcspark_noreads.tar.gz. and the full error message:; ```; java.lang.UnsupportedOperationException: empty collection; at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1004); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1004); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1004); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384); at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCaller(HaplotypeCallerSpark.java:229); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:182); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCallerSpark.java:143); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4234:176,error,error,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4234,2,['error'],['error']
Availability,"When the gvcf was merged by ""bcftools concat"", the following error will be happen. more info: ; https://gatkforums.broadinstitute.org/gatk/discussion/10817/gatk-runtime-error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer. **java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer**; 	at java.lang.Integer.compareTo(Integer.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:52); 	at java.util.Comparators$NaturalOrderComparator.compare(Comparators.java:47); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351); 	at java.util.TimSort.sort(TimSort.java:216); 	at java.util.Arrays.sort(Arrays.java:1507); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:302); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.Utils.getMedianValue(Utils.java:1137); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:277); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:101); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:340); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:189); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:134); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.java:73); 	at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4633:61,error,error,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4633,2,['error'],"['error', 'error-on-genotypegvcfs-java-lang-double-cannot-be-cast-to-java-lang-integer']"
Availability,"When the user includes an unknown argument in a tool command line the error message only indicates the first letter of the unknown argument as supposed to the full name. . At the very least the message should read ""unknown argument STARTING with 'x'"". However I would say that is far better if the whole argument name is output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1751:70,error,error,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1751,1,['error'],['error']
Availability,When trying to index a gzipped VCF at `/humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/hybrid.m38.vcf.gz`I get the error `A USER ERROR has occurred: This tool does not supports indexing of block-compressed files for ProgressReportingDelegatingCodec`. Indexing this file with tabix is speedy and successful.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4184:115,error,error,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4184,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"When using GVCFs with Mutect2 (for example with the Mitochondria mode), in the filtering step ADs for symbolic alleles are set to 0 so it doesn't contribute to overall AD. There was an off-by-one error that removed the alt allele AD rather than the <NON_REF> allele AD. This led to `NaN`s and errors when a site had no ref reads (for example a GT of [ref,alt,<NON_REF>] and AD of [0,300,0] would accidentally be changed to an AD of [0,0,0] if the alt index was removed instead of the <NON_REF> index). . The test changes the AD in one of the sites of the NA12878.MT.g.vcf to have 0 ref reads which fails without the fix in `SomaticClusteringModel`. This addresses #8455.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8458:196,error,error,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8458,2,['error'],"['error', 'errors']"
Availability,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonçalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4434:68,down,downloaded,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434,6,"['FAILURE', 'down', 'error']","['FAILURE', 'downloaded', 'error', 'errors']"
Availability,"When we copy a Google read (ie., a `com.google.api.services.genomics.model.Read`), we currently call into the `GenericJson.clone()` method, which uses reflection to deep copy the fields. However, this method seems to incorrectly try to instantiate a `java.util.Arrays$ArrayList` instead of a `java.util.ArrayList`, causing it to blow up during BQSR runs (as reported by @jean-philippe-martin):. ```; [Jul-13 1:36 PM] JP Martin: ; 2015-07-13T17:36:00.151Z: Error: (5361981f0726257c): java.lang.IllegalArgumentException: unable to create new instance of class java.util.Arrays$ArrayList possibly because it is not public; at com.google.api.client.util.Types.handleExceptionForNewInstance(Types.java:165); at com.google.api.client.util.Types.newInstance(Types.java:120); at com.google.api.client.util.Data.clone(Data.java:220); at com.google.api.client.util.Data.deepCopy(Data.java:307); at com.google.api.client.util.Data.clone(Data.java:222); at com.google.api.client.util.Data.deepCopy(Data.java:293); at com.google.api.client.util.GenericData.clone(GenericData.java:172); at com.google.api.client.json.GenericJson.clone(GenericJson.java:90); at com.google.api.services.genomics.model.Read.clone(Read.java:548); at ; org.broadinstitute.hellbender.utils.read.GoogleGenomicsReadToGATKReadAdapter.copy(GoogleGenomicsReadToGATKReadAdapter.java:619); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/650:456,Error,Error,456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/650,1,['Error'],['Error']
Availability,"When you're looking for it, it's easily found:. ```; normal_command_line=""-I ${normal_bam} -normal `cat normal_name.txt`""; ```; If `normal_name.txt` has a space in it, the M2 command is later rendered as:; `.... -normal HCC1143 BL ...` . This will cause a failure, since ""BL"" is not a parameter. We should render as: `.... -normal ""HCC1143 BL"" ...` . or `.... -normal HCC1143_BL ...` . Please note that this affects the tumor sample lines as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4355:256,failure,failure,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4355,1,['failure'],['failure']
Availability,"While @ldgauthier was presenting LobSTR, an STR indel caller, @fleharty and I were discussing how their probabilistic model was in some sense trying to convey a gap opening penalty and a gap continuation penalty for PCR slippage and related errors. This sort of suggests that one could go whole hog and build this into an enhanced pair-HMM that is aware of multiple error modes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1869:241,error,errors,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1869,2,['error'],"['error', 'errors']"
Availability,"While Funcotator does not support GVCF files, using it with one illustrated the following issue:. In a multiallelic variant, it is possible to have a ""best"" transcript that matches one allele, and a different ""best"" transcript that matches another allele. The following variant is such a case:. ```; <HG38>; #CHROM POS ID REF ALT QUAL FILTER INFO; chr1	55063514	.	G	A,<NON_REF>	1000	.	.; ```. For this case, the Gencode funcotations come out as:; `A` -> `PCSK9_MISSENSE_p.R670H`; `<NON_REF>` -> `USP24_COULD_NOT_DETERMINE`. This becomes a problem for the `VcfOutputRenderer`. ----. This phenomenon has to do with symbolic alleles. The fix is to change the logic for when we create funcotations for masked / symbolic alleles.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5822:698,mask,masked,698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5822,1,['mask'],['masked']
Availability,"While trying to run a tool with a CRAM input located on a hdfs attached to a dataproc cluster, we are hit by an error message described below:. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from hdfs://svdev-caller-m:8020/data/smallCram.cram; Caused by:Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:206); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:112,error,error,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"While using a custom tool that validates arguments (e.g., certain range for an `Integer` parameter), a way of handling it is override `CommandLineProgram.customCommandLineValidation()` and return an String with the error found. Nevertheless, if parsing the arguments throws a `UserException.CommandLineException`, the error is printed with a concrete format (`printDecoratedUserExceptionMessage`) after the usage. This is different from the custom validation, which is printed without any decoration and before the usage. Although this behavior could be desirable, I expect that if my custom validation thrown an `UserException.CommandLineException` it is printed in the same way as other exceptions, and the exception is re-thrown in the same way (for testing purposes, for instance). But the current behaviour just exit without any error printed because the exception is catched in `Main`. A very minor change is include in the `try` block the custom validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2225:215,error,error,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2225,3,['error'],['error']
Availability,While working on #9012 I tried to update the gencode v28 datasource snippets in the Funcotator integration tests to V43. In doing so I found that it broke the MAF vs. VCF output render tests with errors of the following nature: . ```; java.lang.AssertionError: Failed Matching VCF and MAF fields:; 	VCF (Gencode_43_variantClassification): 	RNA[0]	RNA[1]	RNA[2]	RNA[3]	RNA[4]	RNA[5]	RNA[6]	RNA[7]	RNA[8]	RNA[9]	RNA[10]; 	MAF (Variant_Classification): 	LINCRNA[0]	LINCRNA[1]	LINCRNA[2]	LINCRNA[3]	LINCRNA[4]	LINCRNA[5]	LINCRNA[6]	LINCRNA[7]	LINCRNA[8]	LINCRNA[9]	LINCRNA[10]; ----; 	VCF (Gencode_43_otherTranscripts): 	[0]	[1]	[2]	[3]	[4]	[5]	[6]	[7]	[8]	[9]	[10]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[11]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[12]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[13]	PIK3CA_ENST00000643187.1_INTRON/PIK3CA-DT_ENST00000435560.1_FIVE_PRIME_FLANK[14]	[48]	[49]	[50]	[51]	[52]	[53]	[54]	[55]	[56]	[57]	[58]	[59]	[60]	[61]	[62]	[63]	[64]	[65]	[66]	[67]	[68]	[69]	[70]	[71]	[72]	[73]	[74]	[75]	[76]	[77]	[78]	[79]	[80]	[81]	[82]	[83]	[84]	[85]	[86]	[87]	[88]	[89]	[90]	[91]	[92]	[93]	[94]	[95]	[96]	[97]	[98]	[99]	[100]	[101]	[102]	[103]; 	MAF (Other_Transcripts): 	[0]	[1]	[2]	[3]	[4]	[5]	[6]	[7]	[8]	[9]	[10]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[11]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[12]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[13]	PIK3CA_ENST00000643187.1_INTRON|PIK3CA-DT_ENST00000435560.1_FIVE_PRIME_FLANK[14]	[48]	[49]	[50]	[51]	[52]	[53]	[54]	[55]	[56]	[57]	[58]	[59]	[60]	[61]	[62]	[63]	[64]	[65]	[66]	[67]	[68]	[69]	[70]	[71]	[72]	[73]	[74]	[75]	[76]	[77]	[78]	[79]	[80]	[81]	[82]	[83]	[84]	[85]	[86]	[87]	[88]	[89]	[90]	[91]	[92]	[93]	[94]	[95]	[96]	[97]	[98]	[99]	[100]	[101]	[102]	[103]; ----; ```. Its unclear what is the most correct output ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9013:196,error,errors,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9013,1,['error'],['errors']
Availability,"While working on a GATK-3 bug, broadinstitute/gsa-unstable#855, @SHuang-Broad noticed that the natural place to avoid an explosion of genotypes (i.e. the size of the GLs vector) for a large number of alleles is right after computing haplotype likelihoods. In GATK-4 this is in `HaplotypeCallerEngine::callRegion` after the line `final ReadLikelihoods<Haplotype> readLikelihoods =; likelihoodCalculationEngine.computeReadLikelihoods(assemblyResult, samplesList, reads)`. . However, at this point there is the following comment:. ``` java; // Note: we used to subset down at this point to only the ""best"" haplotypes in all samples for genotyping, but there; // was a bad interaction between that selection and the marginalization that happens over each event when computing; // GLs. In particular, for samples that are heterozygous non-reference (B/C) the marginalization for B treats the; // haplotype containing C as reference (and vice versa). Now this is fine if all possible haplotypes are included; // in the genotyping, but we lose information if we select down to a few haplotypes. [EB]; ```. If I understand EB's (@eitanbanks?) point right, subsetting here is dangerous only because of our downstream hack to turn multiallelic genotyping into a biallelic problem. The new AF/QUAL model will perform honest multiallelic genotyping, hence it makes sense to revisit whether to subset haplotypes at this point.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1868:565,down,down,565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1868,3,['down'],"['down', 'downstream']"
Availability,"With GATKv4.0.0.0, when I try to run GenomicsDBimport, it crashes and gives this error message:. > 19:29:01.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/gatk/4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 19:29:01.391 INFO GenomicsDBImport - ------------------------------------------------------------; 19:29:01.391 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.0.0; 19:29:01.391 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:29:01.391 INFO GenomicsDBImport - Executing as gowens@cdr619.int.cedar.computecanada.ca on Linux v3.10.0-693.5.2.el7.x86_64 amd64; 19:29:01.391 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 19:29:01.392 INFO GenomicsDBImport - Start Date/Time: January 10, 2018 7:29:01 PST PM; 19:29:01.392 INFO GenomicsDBImport - ------------------------------------------------------------; 19:29:01.392 INFO GenomicsDBImport - ------------------------------------------------------------; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 19:29:01.392 INFO GenomicsDBImport - Picard Version: 2.17.2; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:29:01.393 INFO GenomicsDBImport - Deflater: IntelDeflater; 19:29:01.393 INFO GenomicsDBImport - Inflater: IntelInflater; 19:29:01.393 INFO GenomicsDBImport - GCS max retries/reopens: 20; 19:29:01.393 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 19:29:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124:81,error,error,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124,1,['error'],['error']
Availability,With the changes in #7754 the experience for forked-PR tests has improved somewhat given that now the jacoco test reports are uploaded as artifacts that can be downloaded. Unfortunately this is confusing and the reports don't appear in the UI until the entire test suite has finished executing AND there is no comment besides the actions API comments on the merge widiget about the tests failing and where to find out why. This can be improved if we pull out an action that runs on test completion to comment about the finished tests for all users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7799:160,down,downloaded,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7799,1,['down'],['downloaded']
Availability,Worker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:3769,ERROR,ERROR,3769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"Would it be possible to expose the [`READ_QUALITY_FILTER_THRESHOLD`](https://github.com/broadinstitute/gatk/blob/9d5727df8db3a475b1ba5f9bff6bc92a322f5633/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerEngine.java#L729) on the command line for 4.9.0.1? I know on the latest branch we have [`--mapping-quality-threshold`](https://github.com/broadinstitute/gatk/blob/7e3d8a1e0c56206345128e3a6125ecc30427deda/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerArgumentCollection.java#L153). For data and regions where we get low mapping qualities (eg. PacBio), a hard-filter on mapq 20 is onerous. I'd also echo the comment in the latter TODO that the interplay between `----mapping-quality-threshold` (new) and ` --minimum-mapping-quality` (old) is confusing upon first inspection.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7034:683,echo,echo,683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7034,1,['echo'],['echo']
Availability,Wrap file-not-found SAMExceptions and other user errors from htsjdk in UserExceptions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/86:49,error,errors,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/86,1,['error'],['errors']
Availability,"WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:35053,failure,failure,35053,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['failure'],['failure']
Availability,"Wrapping a prefetcher into another would be an error, check; for it and report it. The commit also includes a test. Also shut down the executor when the channel is closed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2643:47,error,error,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643,2,"['down', 'error']","['down', 'error']"
Availability,"Writing output via NIO is currently broken, as reported by @jean-philippe-martin:. ```; /home/jpmartin/build/install/gatk/bin/gatk PrintReads -I inputs/CEUTrio.HiSeq.WEx.b37.NA12892.bam -L 10:1000000-2000000 -O gs://jpmartin/staging/out_tmp.bam; htsjdk.samtools.util.RuntimeIOException: Error opening file: /home/jpmartin/gs:/jpmartin/staging/out_tmp.bam; at htsjdk.samtools.SAMFileWriterFactory.makeBAMWriter(SAMFileWriterFactory.java:246); ```. Now that the read support is in good shape, let's get the writing end working -- should hopefully be an easy fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2422:287,Error,Error,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2422,1,['Error'],['Error']
Availability,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3015:1716,robust,robust,1716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015,1,['robust'],['robust']
Availability,Xbyak: Error: Code is too big,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6307:7,Error,Error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6307,1,['Error'],['Error']
Availability,"Yes, what are the plans? I too would like to know. ---; What are the plans for the tools available in Picard 2.9.2 or GATK3.7 that are not in GATK4 alpha? Is the plan eventually to port everything to GATK4? Or are some being permanently sent out to pasture?. I have specifically noticed as missing:; - CollectVariantCallingMetrics; - SetNmMdAndUqTags; - the -gvcf option for ValidateVariants. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/9736/gatk4-status-of-some-picard-gatk3-7-tools-missing-from-alpha/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3084:89,avail,available,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3084,1,['avail'],['available']
Availability,Yf make BaseUtils methods more robust to handle all possible byte values,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7010:31,robust,robust,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7010,1,['robust'],['robust']
Availability,"[--cnv_coherence_length float]; [--class_coherence_length float]; [--max_copy_number int]; [--num_calling_processes int]; [--learning_rate float]; [--adamax_beta1 float]; [--adamax_beta2 float]; [--log_emission_samples_per_round int]; [--log_emission_sampling_median_rel_error float]; [--log_emission_sampling_rounds int]; [--max_advi_iter_first_epoch int]; [--max_advi_iter_subsequent_epochs int]; [--min_training_epochs int]; [--max_training_epochs int]; [--initial_temperature float]; [--num_thermal_advi_iters int]; [--convergence_snr_averaging_window int]; [--convergence_snr_trigger_threshold float]; [--convergence_snr_countdown_window int]; [--max_calling_iters int]; [--caller_update_convergence_threshold float]; [--caller_internal_admixing_rate float]; [--caller_external_admixing_rate float]; [--disable_sampler str_to_bool]; [--disable_caller str_to_bool]; [--disable_annealing str_to_bool]; cohort_denoising_calling.6786136740079319091.py: error: unrecognized arguments: --random_seed=1984 --num_samples_copy_ratio_approx=200; 23:44:54.590 INFO GermlineCNVCaller - Shutting down engine; [August 3, 2024 at 11:44:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 1.04 minutes.; Runtime.totalMemory()=2147483648; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 2; Command Line: python /tmp/cohort_denoising_calling.6786136740079319091.py --ploidy_calls_path=/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/ploidy/ploidy-calls --output_calls_path=/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/cohort_all/cohort_30-calls --output_tracking_path=/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/cohort_all/cohort_30-tracking --random_seed=1984 --modeling_interval_list=/tmp/intervals15539986661449841065.tsv --output_model_path=/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:39954,error,error,39954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['error'],['error']
Availability,[BUG?] GATK4.1.3.0 Mutect2 enable-all-annotations option error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6314:57,error,error,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6314,1,['error'],['error']
Availability,"[CNNScoreVariants] ValueError('Error! Unknown code:', '\x00')",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4727:31,Error,Error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727,1,['Error'],['Error']
Availability,[ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:1847,ERROR,ERROR,1847,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,[EchoCallset] Retry with more memory VCF extract [VS-1351],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8814:1,Echo,EchoCallset,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8814,1,['Echo'],['EchoCallset']
Availability,[EchoCallset] Support > 100 vet tables [VS-1377],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8840:1,Echo,EchoCallset,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8840,1,['Echo'],['EchoCallset']
Availability,[EchoCallset] VCF max alt alleles [VS-1334] [VS-1357],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8816:1,Echo,EchoCallset,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8816,1,['Echo'],['EchoCallset']
Availability,[Echo] PGEN-non bonkers default disk size [VS-1279],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8826:1,Echo,Echo,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8826,1,['Echo'],['Echo']
Availability,[Echo] bq query audit [VS-1396] (#8847),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8854:1,Echo,Echo,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8854,1,['Echo'],['Echo']
Availability,[Mutect2] Mutect2 error when use --disable-tool-default-read-filters,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8224:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224,1,['error'],['error']
Availability,[feature request and question] downsample to coverage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5075:31,down,downsample,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5075,1,['down'],['downsample']
Availability,"[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:30:55 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@418f0534{/jobs,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@134a8ead{/jobs/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54247647{/jobs/job,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5463f035{/jobs/job/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Start",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:7946,AVAIL,AVAILABLE,7946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"[log](https://freebsd.org/~yuri/gatk-4.6.0.0-tests-5.log). Examples of test failures from the above log:; ```; Running Test: Test method testRequirePythonEnvironment(org.broadinstitute.hellbender.utils.python.StreamingPythonExecutorIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.python.StreamingPythonExecutorIntegrationTest > testRequirePythonEnvironment FAILED; java.lang.NullPointerException: Cannot invoke ""Object.getClass()"" because the return value of ""java.lang.RuntimeException.getCause()"" is null; at org.broadinstitute.hellbender.utils.python.StreamingPythonExecutorIntegrationTest.testRequirePythonEnvironment(StreamingPythonExecutorIntegrationTest.java:34); ```. Error messages in another test case:; ```; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0xE2) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x80) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x99) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; ```. This test is skipped without any apparent reason:; ```; Running Test: Test method loadIndex(o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:76,failure,failures,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,3,"['Error', 'error', 'failure']","['Error', 'error', 'failures']"
Availability,"[mutect2] ""no normal.pileup"" error in mutect2.wdl tumor only mode",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494,1,['error'],['error']
Availability,"[org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:13550,ERROR,ERROR,13550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"\* Opening on behalf of a user on an HPC cluster, my knowledge in this field is a bit limited. ### Affected tool(s) or class(es); gatk HaplotypeCaller. ### Affected version(s); Latest 4.6.0.0 release. ### Description ; When running command, ~16 hours into the run the program crashes. Below is the start of the Java error report file. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f06ed243291, pid=1058615, tid=1058616; #; # JRE version: OpenJDK Runtime Environment (17.0.2+8) (build 17.0.2+8-86); # Java VM: OpenJDK 64-Bit Server VM (17.0.2+8-86, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf291] __memset_avx2_erms+0x11; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/ramadugulab/luy/SNPcallingBreeding/core.1058615); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. --------------- S U M M A R Y ------------. Command Line: -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /bigdata/operations/pkgadmin/opt/linux/centos/8.x/x86_64/pkgs/gatk/4.6.0.0/gatk-package-4.6.0.0-local.jar HaplotypeCaller -R /rhome/luy/bigdata/genomes/Cclementina_182_v1_2.fa -I AlignedCalToCcl_Scaffolds_MarkDupOut.bam -O AlignedCalToCcl_Scaffolds.vcf.gz -ERC GVCF. Host: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz, 64 cores, 20G, Rocky Linux release 8.8 (Green Obsidian); Time: Sat Sep 28 04:11:19 2024 PDT elapsed time: 58592.788414 seconds (0d 16h 16m 32s). --------------- T H R E A D ---------------. Current thread (0x00007f06e4025b70): JavaThread ""main""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988:316,error,error,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988,2,['error'],['error']
Availability,"] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:1647,ERROR,ERROR,1647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,1,['ERROR'],['ERROR']
Availability,"] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:13375,ERROR,ERROR,13375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6012:1799,Avail,Available,1799,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012,1,['Avail'],['Available']
Availability,"_/_/ /_/\_\ version 2.4.5; /_/; ; Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_252; Branch HEAD. No matter how high I set the max file descriptors (even to 1M); mwiewior@Mareks-MacBook-Pro ~ % ulimit -a; -t: cpu time (seconds) unlimited; -f: file size (blocks) unlimited; -d: data seg size (kbytes) unlimited; -s: stack size (kbytes) 8192; -c: core file size (blocks) 0; -v: address space (kbytes) unlimited; -l: locked-in-memory size (kbytes) unlimited; -u: processes 2048; -n: file descriptors 1000000. I'm keep on getting the following error:. 20/06/06 14:56:35 ERROR Utils: Aborting task; org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file file:///private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta. Error was: Fasta index file could not be opened: /private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta.fai; at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:159); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:125); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:110); at org.broadinstitute.hellbender.engine.ReferenceFileSource.<init>(ReferenceFileSource.java:35); at org.broadinstitute.hellbender.engine.spark.LocusWalkerSpark.lambda$getAlignmentsFunction$a99dbf6a$1(LocusWalkerSpark.java:113); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6642:1339,Error,Error,1339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6642,1,['Error'],['Error']
Availability,"_34\_proteinChange, Gencode\_34\_gcContent, Gencode\_34\_referenceContext, Gencode\_34\_otherTranscripts, ACMGLMMLof\_LOF\_Mechanism, ACMGLMMLof\_Mode\_of\_Inheritance, ACMGLMMLof\_Notes, ACMG\_recommendation\_Disease\_Name, ClinVar\_VCF\_AF\_ESP, ClinVar\_VCF\_AF\_EXAC, ClinVar\_VCF\_AF\_TGP, ClinVar\_VCF\_ALLELEID, ClinVar\_VCF\_CLNDISDB, ClinVar\_VCF\_CLNDISDBINCL, ClinVar\_VCF\_CLNDN, ClinVar\_VCF\_CLNDNINCL, ClinVar\_VCF\_CLNHGVS, ClinVar\_VCF\_CLNREVSTAT, ClinVar\_VCF\_CLNSIG, ClinVar\_VCF\_CLNSIGCONF, ClinVar\_VCF\_CLNSIGINCL, ClinVar\_VCF\_CLNVC, ClinVar\_VCF\_CLNVCSO, ClinVar\_VCF\_CLNVI, ClinVar\_VCF\_DBVARID, ClinVar\_VCF\_GENEINFO, ClinVar\_VCF\_MC, ClinVar\_VCF\_ORIGIN, ClinVar\_VCF\_RS, ClinVar\_VCF\_SSR, ClinVar\_VCF\_ID, ClinVar\_VCF\_FILTER, LMMKnown\_LMM\_FLAGGED, LMMKnown\_ID, LMMKnown\_FILTER ; ; 02:00:35.778 ERROR FuncotationMap - Values:  , , , , , , , , , , , , , , , , , , , , , , , , , , , , false, ,  ; ; 02:00:35.793 INFO  FilterFuncotations - Shutting down engine ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53 ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(St",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7865:6676,down,down,6676,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865,1,['down'],['down']
Availability,"_From @lbergelson on July 20, 2015 18:22_. `PrintReads` allows `--BQSR` to be specified even if it is compiled with out BQSR available. This means that a public only compilation of gatk will look like it can perform BQSR, but silently fails to do so. A check should be added so that `PrintReads` will throw a `UserException` if BQSR is requested but not available.; - [ ] When implemented (if not already fixed in G4), open an issue in gsa-unstable to have the fix backported to G3. . _Copied from original issue: broadinstitute/gsa-unstable#1058_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1384:125,avail,available,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1384,2,['avail'],['available']
Availability,_N.bam:469762048+33554432** ; **20/03/05 09:27:56 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:973078528+33554432** ; **20/03/05 09:27:56 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:1174405120+33554432** ; **20/03/05 09:27:56 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:335544320+33554432** ; **20/03/05 09:27:56 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:905969664+33554432** ; **20/03/05 09:27:56 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:134217728+33554432** ; **20/03/05 09:27:56 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:503316480+33554432** ; **20/03/05 09:28:58 ERROR Executor: Exception in task 34.0 in stage 0.0 (TID 34)** ; **com.esotericsoftware.kryo.KryoException: Buffer underflow.** ; **at com.esotericsoftware.kryo.io.Input.require(Input.java:199)** ; **at com.esotericsoftware.kryo.io.Input.readLong(Input.java:686)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet.<init>(LongHopscotchSet.java:83)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:527)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:519)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:712)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LargeLongHopscotchSet.<init>(LargeLongHopscotchSet.java:55)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LargeLongHopscotchSet$Serializer.read(LargeLongHopscotchSet.java:172)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LargeLongHopscotchSet$Se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:26038,ERROR,ERROR,26038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['ERROR'],['ERROR']
Availability,"_all_nio_fixes; 12:18:42.093 INFO HaplotypeCaller - Initializing engine; 12:18:42.597 INFO FeatureManager - Using codec VCFCodec to read file file:///beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf; 12:18:42.723 INFO HaplotypeCaller - Done initializing engine; 12:18:42.732 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 12:18:42.732 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:18:43.546 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:18:43.549 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:18:43.599 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002b5f92e39fab, pid=85482, tid=0x00002b5f56e60ae8; #; # JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12); # Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops); # Derivative: IcedTea 3.6.0; # Distribution: Custom build (Tue Nov 21 11:22:36 GMT 2017); # Problematic frame:; # C [libgomp.so.1+0x7fab] omp_get_max_threads+0xb; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /beegfs/work/iiipe01/Exome-Test/work/1e/fc972c6b14c8006857230849630a49/hs_err_pid85482.log; #; # If you would like to submit a bug report, please include; # instructions on how to reproduce the bug and visit:; # http://icedtea.classpath.org/bugzilla; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:3825,error,error,3825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['error'],['error']
Availability,"_module_cache().module_from_key(; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 48, in get_module_cache; return cmodule.get_module_cache(config.compiledir, init_args=init_args); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1587, in get_module_cache; _module_cache = ModuleCache(dirname, **init_args); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 703, in __init__; self.refresh(); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 794, in refresh; files = os.listdir(root); FileNotFoundError: [Errno 2] No such file or directory: '/spin1/home/linux/gatk_users1/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.5.1804-Core-x86_64-3.6.2-64/tmpmy0w17z3'; 00:34:39.396 DEBUG ScriptExecutor - Result: 1; 00:34:39.397 INFO DetermineGermlineContigPloidy - Shutting down engine; [October 27, 2019 12:34:39 AM EDT] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.66 minutes.; Runtime.totalMemory()=2151677952; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python /tmp/cohort_determine_ploidy_and_depth.3351404099122294482.py --sample_coverage_metadata=/tmp/samples-by-coverage-per-contig8898090777596224038.tsv --output_calls_path=/gpfs/gsfs7/users/gatk_users1/0-Project/1-gCNV-Lung/z-bak/z-2019-10-26-1-Test-gCNV/2-Output/1-Contig-Ploidy/22.Contig_Ploidy_Dir/ploidy-calls --mapping_error_rate=1.000000e-02 --psi_s_scale=1.000000e-04 --mean_bias_sd=1.000000e-02 --psi_j_scale=1.000000e-03 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.990000e-01 --log_emission_samples_per_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235:4730,down,down,4730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235,1,['down'],['down']
Availability,"_nio_fixes; 11:48:09.328 INFO GenomicsDBImport - Initializing engine; 11:48:14.819 INFO IntervalArgumentCollection - Processing 43270923 bp from intervals; 11:48:14.846 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/g/ubuntushare/sequence/C271_sentieon_gvcf/my_database.chr01; 11:48:14.919 INFO GenomicsDBImport - Vid Map JSON file will be written to my_database.chr01/vidmap.json; 11:48:14.919 INFO GenomicsDBImport - Callset Map JSON file will be written to my_database.chr01/callset.json; 11:48:14.919 INFO GenomicsDBImport - Complete VCF Header will be written to my_database.chr01/vcfheader.vcf; 11:48:14.919 INFO GenomicsDBImport - Importing to array - my_database.chr01/genomicsdb_array; 11:48:14.924 INFO ProgressMeter - Starting traversal; 11:48:14.924 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:48:19.709 INFO GenomicsDBImport - Importing batch 1 with 348 samples; 11:48:24.549 INFO GenomicsDBImport - Shutting down engine; [November 26, 2023 11:48:24 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.27 minutes.; Runtime.totalMemory()=12916359168; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:74); at com.intel.genomicsdb.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:1289); at com.intel.genomicsdb.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:1212); at com.intel.genomicsdb.GenomicsDBImporter.<init>(GenomicsDBImporter.java:597); at com.intel.genomicsdb.GenomicsDBImporter.<init>(GenomicsDBImporter.java:512); at com.intel.genomicsdb.GenomicsDBImporter.<init>(GenomicsDBImporter.java:472); at com.intel.genomicsdb.GenomicsDBImporter.<init>(GenomicsDBImporter.java:358); at org.broadinstitute.hellbender.tools.genomi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8593:4020,down,down,4020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593,1,['down'],['down']
Availability,"_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.addGivenAlleles(AssemblyBasedCallerUtils.java:350); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:291); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. Somehow the HaplotypeCaller seems to have some bug in GENOTYPE_GIVEN_ALLELES mode and when the VCF file for the given alleles contains a very large indel it ends up giving a cryptic error, regardless of what is contained in the bam file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6037:3479,error,error,3479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037,1,['error'],['error']
Availability,"` averageHQSoftClips.mean() > AVERAGE_HQ_SOFTCLIPS_HQ_BASES_THRESHOLD`, that it will mark the `ActivityProfileState` object it constructs as `ActivityProfileState.Type.HIGH_QUALITY_SOFT_CLIPS`. Subsequently in `ActivityProfile.processState()` a `HIGH_QUALITY_SOFT_CLIPS` will end up getting copied twice per the average number of high quality soft clips at a site. What this means in the end is that each HQ softclip site gets its activity likelihood multiplied by the number of softclips found in average at that site, currently the code centers that ""bright spot"" on the base corresponding to the base that corresponds to the edge of the softclip bases. Its worth noting that the way the average is computed in the HaplotypeCallerEngine is that the high quality softclips are averaged against each other with no heed paid to non-softclipped bases. (see `ReferenceConfidenceModel.applyPileupElementRefVsNonRefLikelihoodAndCount()`) It is conceivable that a site could trigger `AVERAGE_HQ_SOFTCLIPS_HQ_BASES_THRESHOLD` with just a single high quality softclipped read of a sufficient length at a site. This may result in the entire site having its activity likelihood multiplied by at least 12x. This is very old behavior dating back to the original HaplotypeCaller so far as I can tell. Its possible since we liberal about our Activity threshold and since we are already smoothing our probability thresholds out so wide that this doesn't make a substantial difference one way or another. It is worth evaluating two questions related to this problem:; 1. Should the probability density around a high quality softclip be smoothed to distribute the probability density farther away than the current maximum of 50 bases? (Effectively what #5767 accomplishes); 2. Should we even be creating these ""bright spots"" at all for what may ultimately be a very small number of high quality softclips at a location? We would need to evaluate what impact toning this down has on active region selection downstream.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5897:2183,down,down,2183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5897,2,['down'],"['down', 'downstream']"
Availability,`./gatk-launch --help`. prints the available tools and ends with . ```; A USER ERROR has occurred: '--help' is not a valid command.; ```. this is weird,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1301:35,avail,available,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1301,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability,`./gatk-launch CompareBaseQualities -I HCC1143.bqsr.bam -I2 HCC1143.bqsr.cram -R ../human_g1k_v37.fasta --disableSequenceDictionaryValidation`. results in ; `A USER ERROR has occurred: A reference file is required when using CRAM files.`. which is bogus because i passed in a reference,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1321:165,ERROR,ERROR,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1321,1,['ERROR'],['ERROR']
Availability,`./gatk-launch CountVariants --variant some.vcf -L 21`. i get. `A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals. This restriction may be removed in the future.`. the message should tell me what to do. References to the future are less useful than help in what argument to pass,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1305:71,ERROR,ERROR,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1305,1,['ERROR'],['ERROR']
Availability,"`./gatk-launch FlagStat --input fred`. ```; A USER ERROR has occurred: Couldn't read file /local/dev/akiezun/alphaTesting/gatk/fred because Cannot read non-existent file: /local/dev/akiezun/alphaTesting/gatk/fred; ```. ""Couldn't read file because cannot read non-existent file"" ? Should be improved",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1297:51,ERROR,ERROR,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1297,1,['ERROR'],['ERROR']
Availability,`/gatk-launch CountVariants -V src/test/resources/large/dbsnp_138.b37.20.21.vcf -L 20`. the input file does have a sequence dictionary yet i get this . ```; A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals.; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1310:164,ERROR,ERROR,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1310,1,['ERROR'],['ERROR']
Availability,`; gatk --java-options '-Xmx4g -Djava.io.tmpdir=./tmp' CombineGVCFs -R ../genome.fasta --variant ./s1_blood.g.vcf --variant ./s1.g.vcf -O ./test.merge.g.vcf; `; running the above code will encounter an error:; ![image](https://github.com/broadinstitute/gatk/assets/61311726/e2c2ac75-e9ad-440d-86cc-823d40ea7f03),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8869:202,error,error,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8869,1,['error'],['error']
Availability,"`A USER ERROR has occurred: Invalid command line: Argument -L/-XL has a bad value: unmapped. Currently the only way to view unmapped intervals is to perform a traversal of the entire file without specifying any intervals`. ""unmapped intervals""?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1309:8,ERROR,ERROR,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1309,1,['ERROR'],['ERROR']
Availability,"`CloudStorageReadChannel.create()` appears to do a GCS access outside of the retry mechanism in `CloudStorageReadChannel.read()`. It calls the constructor, which calls `CloudStorageReadChannel.fetchSize()`, which does a `gcsStorage.get(file)` followed by a `getSize()`. . We are seeing 503 failures specifically from the GCS access in `CloudStorageReadChannel.fetchSize()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:202); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:234); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:78); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:68); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:304); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:265); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:101); at htsjdk.tribble.readers.Ta",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253:290,failure,failures,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253,1,['failure'],['failures']
Availability,"`GatherVcfsIntegrationTest.testBlockGather()` is currently disabled in master, since when running on travis with it enabled the JVM appears to run out of memory, resulting in errors like the following:. ```; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5293240000, 65536, 1) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 65536 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11418.log. [error occurred during error reporting , id 0xb]; ```. ```; :test[M::bwa_idx_load_from_disk] read 0 ALT contigs; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000715180000, 719847424, 0) failed; error='Cannot allocate memory' (errno=12). #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. This often manifests as a generic ""exited with code 137"" error, which appears to mean ""JVM killed with signal 9"":. ```; [31mFAILURE: [39m[31mBuild failed with an exception.[39m; * What went wrong:; Execution failed for task ':test'.; [33m> [39mProcess 'Gradle Test Executor 1' finished with non-zero exit value 137; ```. See https://travis-ci.org/broadinstitute/gatk/builds/248893875 for an example failed travis build. We should either modify the test to use less memory, or make more memory available to it on travis. I've tried increasing `maxHeapSize` for the test suite in `build.gradle` from `4G` to `6G`, but this did not help (see https://travis-ci.org/broadinstitute/gatk/builds/249355697)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3208:175,error,errors,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3208,9,"['avail', 'error']","['available', 'error', 'errors']"
Availability,"`GvsExtractCallset` based the scatter count on the number of samples loaded, while `GvsExtractCohortFromSampleNames` based the scatter count on the number of samples being extracted. The latter was leading to severe memory issues when trying to extract a small cohort using a filter that had been built from a very large number of samples (e.g. 4 sample cohort from a 400K+ sample Echo filter). Successful run with these changes [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%20Echo%20Callset%20v2/job_history/cdaeb192-f4b9-406d-b90c-4f253f08b7ba) (requires PMI ops). Although the extract went smoothly with these changes, the CreateManifestAndOptionallyCopyOutputs task had some major issues as described in VS-1526 that caused it to run for days and need to be addressed before Foxtrot.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9028:381,Echo,Echo,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9028,1,['Echo'],['Echo']
Availability,`JavaSparkContext.addFile` seems to be a runtime version of --files. According to this [documentation](https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/SparkContext.html#addFile%28java.lang.String%29) it is used in concert with `SparkFIles.get()` on the node to lookup the downloaded file location. It may be possible to persist these files (and those passed with `--files`) by setting 'spark.yarn.preserve.staging.files true`. We should investigate incorporating calls to these methods as a way of sharing small-medium sized inputs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2083:286,down,downloaded,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2083,1,['down'],['downloaded']
Availability,"`PrintWriter` explicitly doesn't throw IOExceptions when it fails to write. This could cause silent corruption of output files. We should probably not use it because it's hard to remember to correctly check it's error status. Some places that use it: MafOutputRenderer, MetricsUtils, others...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5458:212,error,error,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5458,1,['error'],['error']
Availability,"`ReadsSparkSource.putPairsInSamePartition()` is currently rather naive -- it assumes that in queryname-sorted data, read pairs are always sorted with the first of pair first and the second of pair second, that there are no secondary or supplementary alignments, and that there are always two reads per fragment. We should make this method more robust:. 1. It should not assume that each pair is sorted with the first of pair first and the second of pair second. This is not an invariant of queryname sort order, which requires only that reads with the same name be together. 2. It should work in the presence of secondary and supplementary alignments. 3. It should work even if there are more than two reads per fragment (which is the case with some sequencing technologies). All of these points could be resolved by simply iterating until the read name changes when peeking into each partition. Once we make that change, we should add good unit tests to cover the above 3 cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2442:344,robust,robust,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2442,1,['robust'],['robust']
Availability,"`SamFileValidator`'s `isValid()` flags certain reads as being errors when they should not be. This happens when a read that has flag 0x1 ( read has a mate) set to false has flags 0x2, 0x8, 0x20, 0x40 or 0x80 set to true. . According to sam spec, if flag 0x1 is unset than those fields are uninterpretable and should be ignored. This was identified in #569",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/592:62,error,errors,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/592,1,['error'],['errors']
Availability,"`UserException.BadTmpDir` has the error message: ; `""Failure working with the tmp directory %s. Override with -Djava.io.tmpdir=X on the command line to a bigger/better file system.""`. This should be changed refer to either the `--java-options` command or `--TMP_DIR`. Also, it doesn't attach the casual exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4709:34,error,error,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4709,2,"['Failure', 'error']","['Failure', 'error']"
Availability,"`ValidateVariants` performs several checks that go above and beyond what the VCF spec requires for VCF files (e.g. throwing an exception if a variant has an alt allele but has a genotype of hom ref [as found by this user](https://gatk.broadinstitute.org/hc/en-us/community/posts/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-)). This is good - it helps catch logic errors in our and others' pipelines. . However, we should add a flag to `ValidateVariants` that will cause it to validate solely based on the VCF spec and not the more strict guidelines.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6553:381,error,errors,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6553,1,['error'],['errors']
Availability,`VariantFiltration` should accept multiple `mask` bed files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8161:44,mask,mask,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8161,1,['mask'],['mask']
Availability,"`VariantWalker` and `VariantContext` do not do variant validation at parse-time. This causes awkward errors on invalid files like the one found here:; https://gatkforums.broadinstitute.org/gatk/discussion/23809/oncotator-for-build-hg38. It would be best if there was a way to properly validate the variants before parsing them. `ValidateVariants` currently doesn't properly work when given default options (#5862). When #5862 is fixed, this may be ignored - I think validating at run-time when iterating over variants may add too much overhead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5867:101,error,errors,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5867,1,['error'],['errors']
Availability,"`VectorPairHMMUnitTest` checks `isAVXSupported()` and skips tests if it's not. However, if AVX is supported on the machine, but the library isn't available it fails with . ```; Caused by: java.lang.IllegalArgumentException: Resource not found relative to class org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM: /lib/libVectorLoglessPairHMM.so; at org.broadinstitute.hellbender.utils.io.Resource.getResourceContentsAsStream(Resource.java:60); at org.broadinstitute.hellbender.utils.io.IOUtils.writeResource(IOUtils.java:234); at org.broadinstitute.hellbender.utils.io.IOUtils.writeTempResource(IOUtils.java:222); at org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.<init>(VectorLoglessPairHMM.java:106); at org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.<init>(VectorPairHMMUnitTest.java:22); ... 46 more; ```. This should be fixed so that the tests are skipped instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1575:146,avail,available,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1575,1,['avail'],['available']
Availability,`__pthread_tpp_change_priority` error in GenotypeGVCFs (GATK 4.0.0.0),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518:32,error,error,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518,1,['error'],['error']
Availability,"```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS for that. #### Steps to reproduce; Run the `StructuralVariationDiscoveryPipelineSpark` pipeline on a site with SV event having different homology length around the breakpoints. #### Expected behavior; Breakpoint inference taking into account of small indels in the micro-homology surrounding the breakpoints. #### Actual behavior; Breakpoint inference assuming homologous sequence surrounding the breakpoints having the same length. #### What could be done:; The inference code could use CIGAR's in the alignment to infer how much to left adjust the breakpoint. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4883:1867,avail,available,1867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883,2,['avail'],['available']
Availability,```; ./gatk-launch CompareBaseQualitiesSpark -I HCC1143.bqsr.bam -I2 HCC1143.bqsr.cram -R ../human_g1k_v37.fasta --disableSequenceDictionaryValidation; ```. blows up with. ```; htsjdk.samtools.SAMException: Error opening file: ../human_g1k_v37.fasta; at htsjdk.samtools.util.IOUtil.openFileForReading(IOUtil.java:523); at htsjdk.samtools.reference.FastaSequenceFile.<init>(FastaSequenceFile.java:59); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:129); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:82); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:70); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:59); at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.getReferenceSequenceDictionary(ReferenceFileSource.java:52); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceSequenceDictionary(ReferenceMultiSource.java:110); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:354); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.ma,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1593:207,Error,Error,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1593,1,['Error'],['Error']
Availability,```; ./gatk-launch CountVariants -V fred -L 20; ```. blows up with a message . ```; A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals. This restriction may be removed in the future.; ```. which makes no sense because `fred` does not even exist,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1896:91,ERROR,ERROR,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1896,1,['ERROR'],['ERROR']
Availability,```; ./gatk-launch FlagStat --input ../HCC1143_BL.cram; ```. prints. ```; A USER ERROR has occurred: A reference file is required when using CRAM files.; ```. two problems:; 1 `USER ERROR` is super unfriendly; 2 it should say what argument to use to pass in a reference,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1288:81,ERROR,ERROR,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1288,2,['ERROR'],['ERROR']
Availability,```; ./gatk-launch FlagStat; ```. gives me this. ```; Running:; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk FlagStat; ***********************************************************************. A USER ERROR has occurred: Invalid command line: Argument input was missing: Argument 'input' must be specified at least once.; ```. it needs to add something about -h to see all arguments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1285:219,ERROR,ERROR,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1285,1,['ERROR'],['ERROR']
Availability,```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:242,ERROR,ERROR,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1447:91,error,error,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447,1,['error'],['error']
Availability,"```; /opt/jdk1.8.0_172/bin/java -jar /data-ddn/software/GATK4/4.0.3.0/gatk-package-4.0.3.0-local.jar Mutect2 -h | head. USAGE: Mutect2 [arguments]. Call somatic SNVs and indels via local assembly of haplotypes; Version:4.0.3.0. Required Arguments:. --input,-I:String BAM/SAM/CRAM file containing reads This argument must be specified at least once.; Required. --output,-O:File File to which variants should be written Required. --reference,-R:String Reference sequence file Required. --tumor-sample,-tumor:String BAM sample name of tumor. May be URL-encoded as output by GetSampleName with -encode; argument. Required. ...; ```. while version 4.0.5.0 raises an error :. ```; /opt/jdk1.8.0_172/bin/java -jar /data-ddn/software/GATK4/4.0.5.0/gatk-package-4.0.5.0-local.jar Mutect2 -h; java.lang.IllegalArgumentException: Allowed values request for unrecognized string argument: input; 	at org.broadinstitute.hellbender.cmdline.GATKPlugin.GATKAnnotationPluginDescriptor.getAllowedValuesForDescriptorHelp(GATKAnnotationPluginDescriptor.java:246); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.usageForPluginDescriptorArgumentIfApplicable(CommandLineArgumentParser.java:870); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.makeArgumentDescription(CommandLineArgumentParser.java:847); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.printArgumentUsage(CommandLineArgumentParser.java:791); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.lambda$printArgumentUsageBlock$2(CommandLineArgumentParser.java:276); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:352); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4875:661,error,error,661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4875,1,['error'],['error']
Availability,"```; 15:52:58.620 INFO ProgressMeter - chr18:3309146 21.0 6473000 308149.3; 15:53:02.175 ERROR GencodeFuncotationFactory - Unable to create a GencodeFuncotation on transcript ENST00000536353.2 for variant: chr18:9887391-9887436(GGAAGCCATCCAGCCCAAGGAGGGTGACATCCCCAAGTCCCCAGAA* -> G); ```. If we do not need to worry about these, can we downgrade to a warning?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4882:89,ERROR,ERROR,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882,2,"['ERROR', 'down']","['ERROR', 'downgrade']"
Availability,"```; 18:25:10.198 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 18:25:10.198 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 18:25:10.217 INFO FileOutputCommitter - Saved output of task 'attempt_20200121182129_0016_r_000700_0' to file:/home/shuang/analysis/peregrine_maternal.long.cigar.reprint.bam.parts/_temporary/0/task_20200121182129_0016_r_000700; ```; Repeated many times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6400:228,failure,failures,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6400,1,['failure'],['failures']
Availability,"```; :compileJava/usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5934:153,error,error,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934,3,['error'],['error']
Availability,"```; A USER ERROR has occurred: Bad input: BAM header sample names [TCGA-D7-6520-01A-11D-1800-08, TCGA-D7-6520-10A-01D-1800-08]does not contain given tumor sample name C440.TCGA-D7-6520-10A-01D-1800-08.5; ```; Possible solutions:; - (hacky) A fix needs to be put into the WDL that calls samtools to alter the provided sample name:; ```; if [[ ""_${normal_bam}"" == *.bam ]]; then; samtools view -H ${normal_bam} | sed -n ""/SM:/{s/.*SM:\\(\\)/\\1/; s/\\t.*//p ;q};"" > normal_name.txt; normal_command_line=""-I ${normal_bam} -normal `cat normal_name.txt`""; fi; ....snip....; samtools view -H ${tumor_bam} | sed -n ""/SM:/{s/.*SM:\\(\\)/\\1/; s/\\t.*//p ;q};"" > tumor_name.txt; java -Xmx4g -jar $GATK_JAR Mutect2 \; -R ${ref_fasta} \; -I ${tumor_bam} \; -tumor `cat tumor_name.txt` \; $normal_command_line \; ${""--dbsnp "" + dbsnp} \; ${""--cosmic "" + cosmic} \; ${""--normal_panel "" + pon} \; ${""-L "" + intervals} \; -O ""${output_vcf_name}.vcf""; ```; Pro: Already tested.; Con: Requires samtools.; Con: Annoying for users not running the WDL.; Con: Easy to forget/bad logistically. - Use labeled parameters if supported in GATK. @davidbenjamin @vdauwera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3265:12,ERROR,ERROR,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3265,1,['ERROR'],['ERROR']
Availability,"```; ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.; ```. shows up at the top of every run, we should fix this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/216:5,ERROR,ERROR,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/216,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,```; Using GATK 4.0.2.1. gatk FastqToSam --FASTQ=test_R1.fq --FASTQ2=test_R2.fq --RUN_DATE=2011-04-30T01:00:00+0100 --OUTPUT=test.bam --SM=test; Gives the error No value found for tagged argument: RUN_DATE=2011-04-30T01:00:00+0100. Using Picard 2.18.1-SNAPSHOT. java -jar XXXX/build/libs/picard.jar FastqToSam FASTQ=test_R1.fq FASTQ2=test_R2.fq RUN_DATE=2011-04-30T01:00:00+0100 OUTPUT=test.bam SM=test; Works without issues; ```. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/51988#Comment_51988,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5189:155,error,error,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5189,1,['error'],['error']
Availability,"```; error: unmappable character for encoding ASCII; * @author Daniel G??mez-S??nchez (magicDGS); ```. affected files: IntervalOverlappingIterator.java:17, LocusWalker.java:27, ExampleLocusWalker.java:22. @magicDGS can you fix this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1841:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1841,1,['error'],['error']
Availability,"```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2039:63,failure,failure,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039,2,['failure'],['failure']
Availability,"`ah_var_store` edition: Allows hard-filtering based on a maximum number of alt alleles [VS-1334], as well as fixing GATK Docker image building to use image IDs rather than git hashes [VS-1357]. Integration test _mostly_ successful [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5d021859-5971-4fd7-8451-086c224fdb00). `GvsQuickstartIntegration` failed with:. ```; The bytes observed (89733530) for 'ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned' differ from those expected (85119360); FAIL!!! The relative difference between these is 0.0514208, which is greater than the allowed tolerance (0.05); ```. Successful tieout run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/04b840f9-9779-48d6-8faa-4425d67ddadb). [VS-1334]: https://broadworkbench.atlassian.net/browse/VS-1334?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [VS-1357]: https://broadworkbench.atlassian.net/browse/VS-1357?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8806:624,toler,tolerance,624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8806,1,['toler'],['tolerance']
Availability,"`gatk-launch DownsampleSam --help`. ```; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk DownsampleSam --help |; org.broadinstitute.hellbender.exceptions.GATKException$CommandLineParserInternalException: [R, reference] has already been used. |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.handleArgumentAnnotation(CommandLineParser.java:620) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.createArgumentDefinitions(CommandLineParser.java:149) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.<init>(CommandLineParser.java:134) |; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:186) |; at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:41) |; at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66) |; at org.broadinstitute.hellbender.Main.main(Main.java:81); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1303:13,Down,DownsampleSam,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1303,2,['Down'],['DownsampleSam']
Availability,a Bam file is to be traversed in a sliding window fashion - 2 parameters are widow size and skip length. All reads in the window are then made available to the user. HaplotypeCaller will be the main user of this functionality,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/10:143,avail,available,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/10,1,['avail'],['available']
Availability,"a bug fix suggested by Louis.; @lbergelson I don't know how to add a test. Without this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:162,failure,failure,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,2,['failure'],['failure']
Availability,a bunch of methods in the GATKSAMRecord are generally useful and should be pushed down to SAMRecord,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/77:82,down,down,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/77,1,['down'],['down']
Availability,"a conda environment. $ conda env create -n gatk_gpu -f gatkcondaenv.yml. Activate this environment and execute GATK CNNScoreVariants(1D model). $ gatk-4.0.3.0/gatk --java-options '-Xmx8G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' CNNScoreVariants -V INPUT_VCF -R REF -O OUTPUT_VCF. Then, GATK outputs the following:. ```; 18:30:57.468 INFO CNNScoreVariants - Initializing engine; 18:30:57.985 INFO FeatureManager - Using codec VCFCodec to read file ...; 18:30:58.183 INFO IntervalArgumentCollection - Processing 48129895 bp from intervals; 18:30:58.188 INFO CNNScoreVariants - Done initializing engine; 18:31:00.188 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:.../1d_cnn_mix_train_full_bn.2560984151625233621.json and weights:.../1d_cnn_mix_train_full_bn.2397909300265264152.hd5; 18:31:19.873 INFO ProgressMeter - Starting traversal; 18:31:19.874 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 18:31:50.095 INFO CNNScoreVariants - Shutting down engine; [2018/04/24 18:31:50 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.88 minutes.; Runtime.totalMemory()=2141716480; ***********************************************************************; A USER ERROR has occurred: A timeout ocurred waiting for output from the remote Python command.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A timeout ocurred waiting for output from the remote Python command.; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4696:1073,down,down,1073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696,1,['down'],['down']
Availability,"a.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:601); at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1098); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306); Job 610777 stderr output:; ```; My command is as follows: ; ```; ref=/share/org/YZWL/yzwl_hanxt/leizhou/ref/ARS1.2_chr30_2.fasta; mkdir /share/org/YZWL/yzwl_hanxt/leizhou/variant/temp; cd /share/org/YZWL/yzwl_hanxt/leizhou/variant/H-3; gatk --java-options ""-Xmx60g"" GenotypeGVCFs \; -R ${ref} \; -V H-3.g.vcf.gz \; -O /share/org/YZWL/yzwl_hanxt/leizhou/variant/temp/H-3.vcf.gz; ```; I have also tried commands like:; ```; ref=/share/org/YZWL/yzwl_hanxt/leizhou/ref/ARS1.2_chr30_2.fasta; mkdir /share/org/YZWL/yzwl_hanxt/leizhou/variant/temp; cd /share/org/YZWL/yzwl_hanxt/leizhou/variant/H-3; gatk --java-options ""-Xmx1600g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref} \; -V H-3.g.vcf.gz \; -O /share/org/YZWL/yzwl_hanxt/leizhou/variant/temp/H-3.vcf.gz \; --genomicsdb-max-alternate-alleles 10 \; --max-alternate-alleles 4; ```; But it still reports the same error. Could you help me figure out what the problem is? How can I solve it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8969:7902,error,error,7902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8969,1,['error'],['error']
Availability,"a361a9181d69679af	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:11N	LN:3374	M5:b9ad3338cc73e2a99888a36e04c29f75	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:14	LN:3095	M5:0385be87eb49df4c59d7487495e3b1b4	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:16N	LN:2985	M5:10150ad21301a29f92e1521530fdd3f5	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:20	LN:3105	M5:05dc0384da2f751afe549a9bfdbc3037	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; ```. If the contig name is the issue, then this is a bug of sorts that, depending on your project, may or may not be an issue. If you expect all of your samples (PoN normals and tumor samples) to have coverage for specific alternate contigs and need to include such contigs in your analysis, then we can request a fix for this parsing error. I think this case would be unusual. If alternate contigs are not needed for your research, or you expect sporadic coverage for the contigs across samples, then you can move ahead by limiting your analysis to the [primary assembly](https://gatkforums.broadinstitute.org/gatk/discussion/7857/reference-genome-components). For a typical somatic CNV analysis, because of the way the PoN is pruned, when working with GRCh38 alignments, you want to be sure to limit your counting to the primary assembly. You want to use the `-L` argument with an intervals file that only lists the primary assembly and excludes alternate and decoy contigs. This is really important. Any apparent arm/contig level event with variable coverage across your samples will cause CreatePanelOfNormals to raise a _red flag_ for the sample. The tool considers the sample suspect, in that it interprets the arm/contig level ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3357:3222,error,error,3222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357,1,['error'],['error']
Availability,"a:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:237); at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:107); at org.apache.spark.broadcast.TorrentBroadcast.(TorrentBroadcast.scala:86); at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56); at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1387); at org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:646); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.j; at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:67); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:93); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220). However, if I use only Mills_and_1000G_gold_standard.indels.b37.vcf instead of both of them. It ran through. Use only dbsnp_138.b37.vcf causes the same error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2732:6110,error,error,6110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732,1,['error'],['error']
Availability,"a:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 2; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Cancelling stage 1; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) failed in 10.702 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:29151,failure,failure,29151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['failure'],['failure']
Availability,"a:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:6135,down,down,6135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['down'],['down']
Availability,ablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.intern,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:13273,ERROR,ERROR,13273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"ach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://gith",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2988:3787,error,error,3787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988,1,['error'],['error']
Availability,achOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). ### Error log 2.; [2021年12月20日 下午08时36分52秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 7.54 minutes.; Runtime.totalMemory()=4957667328; htsjdk.samtools.util.RuntimeIOException: java.util.zip.DataFormatException: invalid code lengths set; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:161); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7614:4325,Error,Error,4325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614,1,['Error'],['Error']
Availability,"ache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist.; 	at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:8394,ERROR,ERROR,8394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['ERROR'],['ERROR']
Availability,"ackages/theano/gof/cmodule.py"", line 1118, in module_from_key; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1017, in _get_from_key; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 691, in _get_module; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 302, in dlimport; File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load; File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked; File ""<frozen importlib._bootstrap>"", line 655, in _load_unlocked; File ""<frozen importlib._bootstrap_external>"", line 674, in exec_module; File ""<frozen importlib._bootstrap_external>"", line 780, in get_code; File ""<frozen importlib._bootstrap_external>"", line 832, in get_data; OSError: [Errno 23] Too many open files in system: '/gatk/local_mnt/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/tmp_yxu5we5/__init__.py'; Error in atexit._run_exitfuncs:; Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1492, in _on_atexit; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1295, in clear_old; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 746, in refresh; OSError: [Errno 23] Too many open files in system: '/gatk/local_mnt/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'; 23:39:07.445 DEBUG ScriptExecutor - Result: 1; 23:39:07.447 INFO GermlineCNVCaller - Shutting down engine; [February 22, 2019 11:39:07 PM UTC] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.14 minutes.; Runtime.totalMemory()=2305818624; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:21637,Error,Error,21637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Error'],['Error']
Availability,actTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:4870,ERROR,ERROR,4870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"adcast_0_piece0 stored as bytes in memory (estimated size 52.9 KB, free 15.8 GB); 17/07/21 16:52:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.41.105.80:34818 (size: 52.9 KB, free: 15.8 GB); 17/07/21 16:52:00 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:109; 17/07/21 16:52:00 INFO FileInputFormat: Total input paths to process : 1; 17/07/21 16:52:08 INFO SparkUI: Stopped Spark web UI at http://192.41.105.80:4040; 17/07/21 16:52:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/07/21 16:52:08 INFO MemoryStore: MemoryStore cleared; 17/07/21 16:52:08 INFO BlockManager: BlockManager stopped; 17/07/21 16:52:08 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/07/21 16:52:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/07/21 16:52:08 INFO SparkContext: Successfully stopped SparkContext; 16:52:08.357 INFO SparkGenomeReadCounts - Shutting down engine; [21 July 2017 16:52:08 BST] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=4171235328; java.lang.NumberFormatException: For input string: ""A*01""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at org.seqdoop.hadoop_bam.BAMInputFormat.getIntervals(BAMInputFormat.java:104); at org.seqdoop.hadoop_bam.BAMInputFormat.filterByInterval(BAMInputFormat.java:284); at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:158); at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:252); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:121); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3360:6363,down,down,6363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360,1,['down'],['down']
Availability,"add ""id_loaded"" column to schema; add ""partition_id != '__UNPARTITIONED__'"" to query so we don't get int conversion error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7416:116,error,error,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7416,1,['error'],['error']
Availability,add a clear error message if native code fails to build,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1554:12,error,error,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1554,2,['error'],['error']
Availability,add echocallset as an option for integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8757:4,echo,echocallset,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8757,1,['echo'],['echocallset']
Availability,add error message for when tree-score-threshold is set in ReblockGVCF…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8218:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8218,1,['error'],['error']
Availability,add logging and validate vat to echo callset branch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8770:32,echo,echo,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8770,1,['echo'],['echo']
Availability,"adding a test for large files existance in the ""large"" test group. large files should be available for tests on travis after this pull request",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/907:89,avail,available,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/907,1,['avail'],['available']
Availability,"adding tests for NPR when there's no output specified; this effects CountBasesSpark, CountReadsSpark, FlagStatSpark, and CountVariantsSpark; fixes #1523. fixing an error in CountVariantsSpark where the vcf input was optional. adding some convinience functions to ArgumentsBuilder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1535:164,error,error,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1535,1,['error'],['error']
Availability,"addresses specops issues:; - #268 https://github.com/broadinstitute/dsp-spec-ops/issues/268; - #270 https://github.com/broadinstitute/dsp-spec-ops/issues/270. similar changes as the PR for ExtractCohort:; - added custom classes `ExtractFeaturesRecord` that implements `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites). also:; - enable using intervals input (-L) rather than specifying min-location and max-location. updated WDL to support scattering using SplitIntervals (based off of CohortExtract); - add back AS_QD to headers (currently headers are shared between ExtractCohort and ExtractFeatures - AS_QD not needed for cohort but is needed for features)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7184:420,down,down,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7184,1,['down'],['down']
Availability,adingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.48,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:17326,Recover,Recovered,17326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,adle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4004,ERROR,ERROR,4004,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,adle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildAc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:6773,ERROR,ERROR,6773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,adle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.ac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:14661,ERROR,ERROR,14661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"ady stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSche",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:2009,failure,failure,2009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['failure'],['failure']
Availability,"age.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/01/09 18:30:55 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/01/09 18:30:55 INFO storage.DiskBlockManager: Created local directory at /tmp/sun/blockmgr-b03058dc-763a-449c-bd05-18f3304c01ea; 18/01/09 18:30:55 INFO memory.MemoryStore: MemoryStore started with capacity 2004.6 MB; 18/01/09 18:30:55 INFO spark.SparkEnv: Registering OutputCommitCoordinator; 18/01/09 18:30:55 INFO util.log: Logging initialized @25356ms; 18/01/09 18:30:55 INFO server.Server: jetty-9.3.z-SNAPSHOT; 18/01/09 18:30:55 INFO server.Server: Started @25495ms; 18/01/09 18:30:55 INFO server.AbstractConnector: Started ServerConnector@283ab206{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:30:55 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@418f0534{/jobs,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@134a8ead{/jobs/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54247647{/jobs/job,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5463f035{/jobs/job/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.Servle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:7167,AVAIL,AVAILABLE,7167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"aling with large ploidies (typically for pooled experiments) run into limitations where the combination of the ploidy and numbers of alleles in the high teens and above produces too many possible genotypes for GenotypeGVCFs to handle under its current architecture. . For example, in the case reported here, the ploidy is 19 and the number of alternate alleles is 21, so GenotypeGVCFs cannot handle the large number of possible genotypes that result from all the possible combinations. A reasonable way to deal with this would be to cull the possible combinations dynamically at runtime to eliminate the most unlikely combinations up front. ; #### Test data. Has been provided by the user ; #### [Original forum post](http://gatkforums.broadinstitute.org/discussion/4954/combination-of-ploidy-and-number-of-alleles-error-when-running-genotypegvcfs/p1). ---. @vruano commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-77993425). The error message explain the reason well ... a possibility to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:1242,error,error,1242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['error'],['error']
Availability,"alkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5934:1840,error,error,1840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934,1,['error'],['error']
Availability,all picard tools should be synched before we go to alpha release. We are now at some version and need to sync to the latest available by the alpha release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/705:124,avail,available,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/705,1,['avail'],['available']
Availability,"aller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:55:36.588 INFO ProgressMeter - unmapped 0.0 1 3333.3; 12:55:36.588 INFO ProgressMeter - Traversal complete. Processed 1 total regi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7229:2577,Avail,Available,2577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229,1,['Avail'],['Available']
Availability,"alse --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:3855,ERROR,ERROR,3855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['ERROR'],['ERROR']
Availability,alse numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at ch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:20842,Recover,Recovered,20842,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,also updating build to extract the intelDeflater from the jar instead of downloading it; removed the htsjdkVersion property I had recently added since it was no longer necessary with the update to include the .so in the jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1706:73,down,downloading,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1706,1,['down'],['downloading']
Availability,alt allels error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8953:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8953,1,['error'],['error']
Availability,"aluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:1734,recover,recovered,1734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['recover'],['recovered']
Availability,"am.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```; #### Expected behavior; - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,78,88,96,104,111:78; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,80,85,88,90,93:80; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,89,101,110,119,126:89; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,96,101,105,108:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,94,96,98:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,83,89,94,99,103:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628:2584,down,down,2584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628,1,['down'],['down']
Availability,an error of filtered SNP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7042:3,error,error,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7042,1,['error'],['error']
Availability,"anager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:39354,failure,failure,39354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['failure'],['failure']
Availability,"analysis_set_plus_decoy_hla.fa --WARN_ON_MISSING_CONTIG false --LOG_FAILED_INTERVALS true --WRITE_ORIGINAL_POSITION false --WRITE_ORIGINAL_ALLELES false --LIFTOVER_MIN_MATCH 1.0 --ALLOW_MISSING_FIELDS_IN_HEADER false --RECOVER_SWAPPED_REF_ALT false --TAGS_TO_REVERSE AF --TAGS_TO_DROP MAX_AF --DISABLE_SORT false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jul 26, 2020 10:20:35 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Sun Jul 26 10:20:35 EDT 2020] Executing as farrell@scc-hadoop.bu.edu on Linux 3.10.0-1062.12.1.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0; INFO 2020-07-26 10:20:35 LiftoverVcf Loading up the target reference genome.; INFO 2020-07-26 10:20:56 LiftoverVcf Lifting variants over and sorting (not yet writing the output file.); [Sun Jul 26 10:20:56 EDT 2020] picard.vcf.LiftoverVcf done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=5861015552; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.tribble.TribbleException: Badly formed variant context at location chr1:596697; getEnd() was 596797 but this VariantContext contains an END key with value 532177; at htsjdk.variant.variantcontext.VariantContext.validateStop(VariantContext.java:1401); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1383); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); at htsjdk.variant.variantcontext.Variant",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:2410,avail,available,2410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,1,['avail'],['available']
Availability,"andler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409732fb{/executors/threadDump/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e99e2cb{/static,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@478967eb{/,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f2b39a{/api,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18c880ea{/jobs/job/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:9271,AVAIL,AVAILABLE,9271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"anges will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 17:04:15.948 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 17:04:15.948 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 17:04:15.960 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 17:04:15.962 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 17:04:16.002 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 17:04:16.003 INFO IntelPairHmm - Available threads: 40; 17:04:16.003 INFO IntelPairHmm - Requested threads: 4; 17:04:16.003 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 17:04:16.052 INFO ProgressMeter - Starting traversal; 17:04:16.052 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 17:04:16.589 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 17:04:17.126 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.126 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.128 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.130 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.132 WARN StrandBiasB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6260:4435,Avail,Available,4435,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6260,1,['Avail'],['Available']
Availability,"ant it is unclear if this matters and perhaps the latter check can be done away with? ; - I have added a genotype debugging stream that closely matches the debug output stream from DRAGEN (which itself was a reflection of the GATK3 debug out stream). This involved a lot of threading output writers through the codebase and perhaps this is better handled by the ""--debug"" argument like it used to? Thoughts? . Notes: ; - It should be noted that by design all of the added changes to HaplotypeCaller are opt-in, barring errors in implementation.; - This code is measurably slower than vanilla HaplotypeCaller. In particular FRD is a very expensive step that corresponds to ~5-7% of the runtime. This is in part because it has to duplicate many of the steps in the genotyper based on the number of unique mapping qualities present at a site as well as the fact that it performs an O(n^2) number of operations at sites with many possible alleles. There are options to cut down on the cost of this algorithm that moderately impact the results relative to DRAGEN. . This implementation is intended to produce results close to the results on DRAGEN 3.4.12 without stripping away the major improvements made in GATK4, as a result there are a number of areas in which we know we are producing different results: ; - In GATK4 variants that overlap with an upstream deletion will have added to their alleles list a sybmolic '*' deletion alleles which are genotyped as part of the allele array in the genotyeper. This is not the case in DRAGEN and it interacts with FRD in such a way as to produce a number of variants that in DRAGEN would have been called as 0/1 heterozygous calls with capped QUAL scores, in gatk they are called as 1/2 calls with uncapped quality scores.; - While we have added the option to use the legacy assembly region creation code, it is not part of the expected pipeline for running DRAGEN. This includes a number of arguments that were done away with in the recent refactoring pass. ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:4359,down,down,4359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['down'],['down']
Availability,"ant normal3.vcf \\ ; ; \--variant normal4.vcf \\ ; ; \--variant normal5.vcf \\ ; ; \--variant normal6.vcf \\ ; ; \--variant normal7.vcf \\ ; ; \--variant normal8.vcf \\ ; ; \--variant normal9.vcf \\ ; ; \--variant normal10.vcf \\ ; ; \--variant normal11.vcf \\ ; ; \--variant normal12.vcf \\ ; ; \--variant normal13.vcf \\ ; ; \--variant normal14.vcf \\ ; ; \--variant normal15.vcf \\ ; ; \--variant normal16.vcf \\ ; ; \--variant normal17.vcf \\ ; ; .... ; ; \--variant normal80.vcf \\ ; ; \--genomicsdb-workspace-path pon\_db \\ ; ; \--tmp-dir /tmp1 \\ ; ; \-L /gatk\_bundle/hglft\_genome\_3bc14\_d6f440.bed \\ ; ; \--sequence-dictionary /gatk\_bundle/hg19\_v0\_Homo\_sapiens\_assembly19.dict \\ ; ; \--reader-threads 15 \\ ; ; \--java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true'; ```. Here For interval list, I have downloaded the hg38 target interval from GATK resource bundle and converted into hg19 format using UCSC liftover utility. GenomicsDBImport is not reporting any error related to command but also not reporting any results. Here are the details from GenomicsDBImport log file:. ```; 17:16:16.069 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jan 12, 2021 5:16:16 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 17:16:16.329 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 17:16:16.329 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 17:16:16.329 INFO GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 17:16:16.330 INFO GenomicsDBImport - Executing as akansha@sbilab on Linux v4.4.0-169-generic amd64 ; ; 17:16:16.330 INFO GenomicsDBImport - Jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037:2066,error,error,2066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037,1,['error'],['error']
Availability,"antWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mapping with BWA in memory to *those* might be very helpful. So, we need to: 1) get our hands on a decoy genome that will play nicely with BWA, and 2) talk to the SV team. ---. @ldgauthier commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296675311). To be pedantic, the mapping quality also considers how well the read aligns; to its",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2930:1390,redundant,redundant,1390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930,1,['redundant'],['redundant']
Availability,"ants(VariantLocusWalker.java:76); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: GenomicsDB JNI Error: VariantQueryProcessorException : Could not open array 1$1$188260577 at workspace: /data1/EquCab/GenomicsDB/ECA3_GenomicsDB_260/1; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed; 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:407); 	... 12 more. I'm assuming it is something in the array 1$1$188260577 files, and possibly the _book_keep.tbs.gz file, although I'm not sure how to go about trouble shooting the issue. I also recreated the dat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012:5074,Error,Error,5074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012,3,"['Error', 'error']","['Error', 'error']"
Availability,"apping qualities of the reads are low; compared to reads that contain fewer SNPs. I've been mulling over the; conflation of these two aspects of mapping quality for a while because it; biases our VQSR results, but maybe the new filtering models will be able to; figure it out. The b37 reference with decoy contigs is here:; /humgen/1kg/reference/human_g1k_v37_decoy.fasta.I believe that the; reference issue that required the decoy in the b37 1000G work was resolved; in the hg38 reference. This is an excellent topic to discuss with Heng; during his office hours when he gets back from China in a few weeks, but I; expect the SV team will also be helpful in the meantime. On Sun, Apr 23, 2017 at 11:14 PM, David Benjamin <notifications@github.com>; wrote:. > So. . . given that our pipeline aligns with BWA, it might seem like this; > is just a redundant and laborious rehashing of the mapping quality score.; >; > *However*, the mapping quality only considers multi-mapping within the; > reference, and therefore doesn't account for mapping errors due to; > incompleteness of the reference. That is, reads from genomic regions that; > are not part of the reference (because they're hard to assemble, like; > centromeres etc) might map well to a unique regions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCJQob4WqdwDN0R8jvbNGT1l0vSCks5rzBOmga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2930:3524,error,errors,3524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930,1,['error'],['errors']
Availability,"ar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET; 20:54:45.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; INFO	2021-02-12 20:54:45	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-12 20:54:45	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-12 20:54:45	FilterSamReads	6 SAMRecords written to stdout. ```; Check file:; `gunzip -c -d -f test_stdout.bam | head -n 5`; ```; Tool returned:; 0. ?[[lW?m?$?^?q???k????zg?x}?s???mE?ޖ?r#U???ԑ/Qm'܄dkUM???????zCBB?!*?V*?#; <Q!QU?; ```; Bam file using -0; `gunzip -c -d -f test_outbam.bam | head -n 5`; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapienUR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```. It looks like this issue has been discussed at https://github.com/broadinstitute/gatk/issues/4433 and https://github.com/broadinstitute/gatk/issues/4329 but this issue seems to still exist in GATK 4.1.9.0. Please let me know if you want any of these test files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7080:2102,down,downloads,2102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080,4,['down'],['downloads']
Availability,"aracter '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; \[E::vcf\_parse\_format\] Invalid character '.' in 'AF' FORMAT field at 1:883625 ; ; 11:20:40.460 INFO GenomicsDBImport - Done importing batch 1/1 ; ; 11:20:40.463 INFO ProgressMeter - unmapped 0.0 1 30.2 ; ; 11:20:40.463 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 0.0 minutes. ; ; 11:20:40.463 INFO GenomicsDBImport - Import completed! ; ; 11:20:40.464 INFO GenomicsDBImport - Shutting down engine ; ; \[July 13, 2021 11:20:40 AM EDT\] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.09 minutes. ; ; Runtime.totalMemory()=2418016256 ; ; Tool returned: ; ; true<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/169705'>Zendesk ticket #169705</a>)<br>gz#169705</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7362:27809,down,down,27809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7362,1,['down'],['down']
Availability,"ark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf: Too many open files, for input source: /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:263); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:102); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:127); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:121); at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:380); ... 39 more. How many files does it need to open? As user I can not ulimit to more than 4096 files. Best; Dietmar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6578:6188,error,error,6188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6578,1,['error'],['error']
Availability,"ark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:1882,failure,failure,1882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['failure'],['failure']
Availability,"arnReadOrientationModel - Context CCG: with 143118 ref and 4274 alt examples, EM converged in 14 steps; 16:21:13.361 INFO LearnReadOrientationModel - Context CGA: with 74445 ref and 2480 alt examples, EM converged in 14 steps; 16:21:14.122 INFO LearnReadOrientationModel - Context CGC: with 115334 ref and 3741 alt examples, EM converged in 14 steps; 16:21:14.667 INFO LearnReadOrientationModel - Context CTA: with 173673 ref and 1472 alt examples, EM converged in 13 steps; 16:21:15.238 INFO LearnReadOrientationModel - Context CTC: with 439622 ref and 3855 alt examples, EM converged in 12 steps; 16:21:15.709 INFO LearnReadOrientationModel - Context GAA: with 333082 ref and 2101 alt examples, EM converged in 11 steps; 16:21:16.222 INFO LearnReadOrientationModel - Context GAC: with 227508 ref and 2090 alt examples, EM converged in 12 steps; 16:21:17.015 INFO LearnReadOrientationModel - Context GCA: with 340865 ref and 6507 alt examples, EM converged in 14 steps; 16:21:17.906 INFO LearnReadOrientationModel - Context GCC: with 404793 ref and 8210 alt examples, EM converged in 14 steps; 16:21:18.677 INFO LearnReadOrientationModel - Context GGA: with 409382 ref and 6784 alt examples, EM converged in 13 steps; 16:21:19.182 INFO LearnReadOrientationModel - Context GTA: with 153202 ref and 1568 alt examples, EM converged in 12 steps; 16:21:19.680 INFO LearnReadOrientationModel - Context TAA: with 208518 ref and 1131 alt examples, EM converged in 12 steps; 16:21:20.459 INFO LearnReadOrientationModel - Context TCA: with 325628 ref and 6437 alt examples, EM converged in 13 steps; 16:21:21.008 INFO LearnReadOrientationModel - Context AAA: with 426193 ref and 2055 alt examples, EM converged in 12 steps; 16:21:21.044 INFO LearnReadOrientationModel - Shutting down engine; [November 26, 2018 4:21:21 PM UTC] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 0.39 minutes.; Runtime.totalMemory()=780140544; Tool returned:; SUCCESS; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447:10359,down,down,10359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447,1,['down'],['down']
Availability,"arp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where HaplotypeCaller is using more memory across longer shard lengths. Given that these are not throwing java garbage collection exceptions makes me suspicious that this might be related to the non-java gkl code thats being run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7693:1764,failure,failures,1764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693,1,['failure'],['failures']
Availability,aryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:55:36.588 INFO ProgressMeter - unmapped 0.0 1 3333.3; 12:55:36.588 INFO ProgressMeter - Traversal complete. Processed 1 total regions in 0.0 minutes.; 12:55:36.588 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 12:55:36.589 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 12:55:36.589 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 12:55:36.589 INFO HaplotypeCaller - Shutting down engine; [24 April 2021 at 12:55:36 CEST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=528482304,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7229:3912,down,down,3912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229,1,['down'],['down']
Availability,"ase sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result (due to the lower overall count because of the pairing of reads) probably causes us to miss this event. Also, although CollectFragmentOverlaps initially looks pretty good, I think the bin-to-bin correlations that are evident here negatively affect segmentation. This is not an extremely rigorous evaluation, but it suggests that we should consider switching ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:2295,recover,recovered,2295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['recover'],['recovered']
Availability,"aseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:46:27.296 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:46:27.296 INFO BaseRecalibrator - Deflater: IntelDeflater; 15:46:27.296 INFO BaseRecalibrator - Inflater: IntelInflater; 15:46:27.296 INFO BaseRecalibrator - GCS max retries/reopens: 20; 15:46:27.296 INFO BaseRecalibrator - Requester pays: disabled; 15:46:27.297 INFO BaseRecalibrator - Initializing engine; 15:46:28.062 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/dbsnp_146.hg38.vcf; 15:46:28.075 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/1000G_phase1.snps.high_confidence.hg38.vcf; 15:46:28.127 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/Mills_and_1000G_gold_standard.indels.hg38.vcf; 15:46:28.213 INFO BaseRecalibrator - Done initializing engine; 15:46:28.216 INFO BaseRecalibrator - Shutting down engine; [2021年1月8日 下午03时46分28秒] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.52 minutes.; Runtime.totalMemory()=1488977920; ***********************************************************************. A USER ERROR has occurred: Number of read groups must be >= 1, but is 0. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Number of read groups must be >= 1, but is 0; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.<init>(BaseRecalibrationEngine.java:96); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.onTraversalStart(BaseRecalibrator.java:144); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7031:3779,down,down,3779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7031,1,['down'],['down']
Availability,"aseq/gatk_output/CDL-164-04P/log/CDL-164-04P-1_0_249250621_genomicsdb; 11:49:26.339 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 11:49:26.339 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 11:49:26.339 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 11:49:26.339 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 11:49:26.339 INFO ProgressMeter - Starting traversal; 11:49:26.340 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:49:26.697 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f9935bac359, pid=8320, tid=0x00007f99881bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb1661720680664773125.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/hs_err_pid8320.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:4901,error,error,4901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['error'],['error']
Availability,"askArtifacts/taskArtifacts.bin: Size{2}, CacheStats{hitCount=8, missCount=2, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=0}; :createPythonPackageArchive (Thread[Daemon worker Thread 2,5,main]) started.; :createPythonPackageArchive; Executing task ':createPythonPackageArchive' (up-to-date check took 0.003 secs) due to:; Output property 'archivePath' file /home/axverdier/Tools/GATK4/git/gatk/build/gatkPythonPackageArchive.zip has changed.; Output property 'archivePath' file /home/axverdier/Tools/GATK4/git/gatk/build/gatkPythonPackageArchive.zip has been removed.; Creating GATK Python package archive...; Created GATK Python package archive in /home/axverdier/Tools/GATK4/git/gatk/build/gatkPythonPackageArchive.zip; :createPythonPackageArchive (Thread[Daemon worker Thread 2,5,main]) completed. Took 0.058 secs.; :compileJava (Thread[Daemon worker Thread 2,5,main]) started.; :compileJava; Executing task ':compileJava' (up-to-date check took 0.044 secs) due to:; No history is available.; All input files are considered out-of-date for incremental task ':compileJava'.; Compiling with JDK Java compiler API.; /home/axverdier/Tools/GATK4/git/gatk/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SuspectedTransLocDetector.java:13: warning: [unchecked] unchecked conversion; import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;; ^; required: List<String>; found: List; error: warnings found and -Werror specified; 1 error; 1 warning; :compileJava FAILED; :compileJava (Thread[Daemon worker Thread 2,5,main]) completed. Took 4.116 secs. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileJava'.; > Compilation failed; see the compiler error output for details. * Try:; Run with --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':compileJava'.; at org.gradle.api.internal.tasks.execu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:4539,avail,available,4539,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,1,['avail'],['available']
Availability,"ast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.form",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:4936,failure,failure,4936,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['failure'],['failure']
Availability,"ast_3_piece0 stored as bytes in memory (estimated size 25.4 KB, free 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on mpcb006.cm.cluster:46741 (size: 25.4 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO SparkContext: Created broadcast 3 from newAPIHadoopFile at SamSource.java:108; 18:35:30.930 INFO FileInputFormat - Total input files to process : 1; 20/10/08 18:35:30 INFO SparkUI: Stopped Spark web UI at http://mpcb006.cm.cluster:4040; 20/10/08 18:35:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 20/10/08 18:35:30 INFO MemoryStore: MemoryStore cleared; 20/10/08 18:35:30 INFO BlockManager: BlockManager stopped; 20/10/08 18:35:30 INFO BlockManagerMaster: BlockManagerMaster stopped; 20/10/08 18:35:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 20/10/08 18:35:30 INFO SparkContext: Successfully stopped SparkContext; 18:35:30.994 INFO MarkDuplicatesSpark - Shutting down engine; [October 8, 2020 at 6:35:30 PM CEST] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2579496960. ### Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875:8810,down,down,8810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875,1,['down'],['down']
Availability,async BAM writing errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1699:18,error,errors,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1699,1,['error'],['errors']
Availability,"at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); > at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); > at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); > at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); > at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); > at java.util.Iterator.forEachRemaining(Iterator.java:116); > at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); > at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); > at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); > at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); > at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); > at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); > at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); > at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); > at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); > at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); > at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); > at org.broadinstitute.hellbender.Main.main(Main.java:292); > . I have also tried to use older version of funcotator data sources, funcotator_dataSources.v1.6.20190124s, then the resulting error is:. > org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: NCRNA00115",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:22576,error,error,22576,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['error'],['error']
Availability,at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.base/java.lang.Thread.run(Thread.java:834); [TileDB::FileSystem] Error: hdfs: Error getting hdfs connection; [TileDB::StorageManagerConfig] Error: Error getting hdfs connection: Connection refused.; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest > testWriteToAndQueryFromGCS FAILED; java.io.IOException: GenomicsDB JNI Error: VCFAdapterException : Could not copy contents of VCF header filename gs://hellbender-test-logs/staging/703469fc-52fe-441d-b6e0-8092a114fe2c/vcfheader.vcf to temporary file /tmp/TileDBVoWFeM; at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:176); at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:80),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:7490,Error,Error,7490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,4,['Error'],['Error']
Availability,"at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:12494,down,down,12494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['down'],['down']
Availability,at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:22358,FAILURE,FAILURE,22358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['FAILURE'],['FAILURE']
Availability,"atches/Minute; 11:49:26.697 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f9935bac359, pid=8320, tid=0x00007f99881bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb1661720680664773125.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/hs_err_pid8320.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Then, I tried to enable core dumping:. ```; [rathik@reslnrefo01 CDL-164-04P]$ ulimit -c unlimited. [rathik@reslnrefo01 CDL-164-04P]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/joint/gatk-haplotype-joint/CDL-164-04P/1/bcbiotx/tmp1SAfjS' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annota",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:5485,error,error,5485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['error'],['error']
Availability,ateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:6440,Error,ErrorProbabilities,6440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['Error'],['ErrorProbabilities']
Availability,ateErrorProbability(TumorEvidenceFilter.java:27); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:11309,Error,ErrorProbabilities,11309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Error'],['ErrorProbabilities']
Availability,"ating read-count file 0345-20.hdf5 (34 / 44); 17:30:33.666 INFO GermlineCNVCaller - Aggregating read-count file 0641-18.hdf5 (35 / 44); 17:30:36.381 INFO GermlineCNVCaller - Aggregating read-count file 0949-20.hdf5 (36 / 44); 17:30:39.115 INFO GermlineCNVCaller - Aggregating read-count file 1081-20.hdf5 (37 / 44); 17:30:41.937 INFO GermlineCNVCaller - Aggregating read-count file 1416-20.hdf5 (38 / 44); 17:30:44.527 INFO GermlineCNVCaller - Aggregating read-count file 1491-20.hdf5 (39 / 44); 17:30:47.177 INFO GermlineCNVCaller - Aggregating read-count file 1553-18.hdf5 (40 / 44); 17:30:49.901 INFO GermlineCNVCaller - Aggregating read-count file 1577-20.hdf5 (41 / 44); 17:30:52.733 INFO GermlineCNVCaller - Aggregating read-count file 1600-20.hdf5 (42 / 44); 17:30:55.582 INFO GermlineCNVCaller - Aggregating read-count file 1720-20.hdf5 (43 / 44); 17:30:58.449 INFO GermlineCNVCaller - Aggregating read-count file 1995-20.hdf5 (44 / 44); 17:40:55.029 INFO GermlineCNVCaller - Shutting down engine; [April 27, 2021 at 5:40:55 PM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 12.44 minutes.; Runtime.totalMemory()=11144265728; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 139; Command Line: python /media/Data/tmp/cohort_denoising_calling.4407709016674150942.py --ploidy_calls_path=/media/Data/AnnotationDBs/CNV/Genom/ploidy-calls --output_calls_path=/media/Data/AnnotationDBs/CNV/Genom/CNV-calls --output_tracking_path=/media/Data/AnnotationDBs/CNV/Genom/CNV-tracking --random_seed=1984 --modeling_interval_list=/media/Data/tmp/intervals8808982738430140650.tsv --output_model_path=/media/Data/AnnotationDBs/CNV/Genom/CNV-model --enable_explicit_gc_bias_modeling=True --read_count_tsv_files /media/Data/tmp/0028-2117281826103999737636.tsv /media/Data/tmp/0045-2114178578414165773973.tsv /media/Data/tmp/0098-187303293573572392847.tsv /media/Data/tmp/0156-2116276973316025582383.tsv /media/Data/tmp/042",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7234:9653,down,down,9653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234,1,['down'],['down']
Availability,"atk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8432:1055,error,error,1055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432,1,['error'],['error']
Availability,"aults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; *********************************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:3291,ERROR,ERROR,3291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['ERROR'],['ERROR']
Availability,"aults.CREATE_MD5 : false; 12:11:34.899 INFO PlotACNVResults - Defaults.CUSTOM_READER_FACTORY : ; 12:11:34.899 INFO PlotACNVResults - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:11:34.899 INFO PlotACNVResults - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:11:34.899 INFO PlotACNVResults - Defaults.REFERENCE_FASTA : null; 12:11:34.899 INFO PlotACNVResults - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_CRAM_REF_DOWNLOAD : false; 12:11:34.900 INFO PlotACNVResults - Deflater IntelDeflater; 12:11:34.900 INFO PlotACNVResults - Initializing engine; 12:11:34.900 INFO PlotACNVResults - Done initializing engine; 12:11:35.009 INFO PlotACNVResults - Shutting down engine; [February 15, 2017 12:11:35 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1502085120; java.lang.IllegalArgumentException: There must be at least one contig above the threshold length in the sequence dictionary.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:673); 	at org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults.doWork(PlotACNVResults.java:120); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2941:2596,down,down,2596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941,1,['down'],['down']
Availability,"aults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:10:10.530 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:10:10.530 INFO FilterMutectCalls - Deflater: IntelDeflater; 10:10:10.530 INFO FilterMutectCalls - Inflater: IntelInflater; 10:10:10.530 INFO FilterMutectCalls - GCS max retries/reopens: 20; 10:10:10.530 INFO FilterMutectCalls - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:10:10.530 INFO FilterMutectCalls - Initializing engine; 10:10:10.877 INFO FeatureManager - Using codec VCFCodec to read file file:///scratch/dberaldi/projects/20180204_combine_callers/gatk/TT001T03.snv.vcf.gz; 10:10:10.981 INFO FilterMutectCalls - Done initializing engine; 10:10:11.035 INFO ProgressMeter - Starting traversal; 10:10:11.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:10:11.331 INFO FilterMutectCalls - Shutting down engine; [February 7, 2018 10:10:11 AM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=933756928; org.broadinstitute.hellbender.exceptions.GATKException: INFO annotation 'MFRL' contains a non-int value '7.97254e+06'; 	at org.broadinstitute.hellbender.utils.GATKProtectedVariantContextUtils.lambda$attributeValueToIntArray$1(GATKProtectedVariantContextUtils.java:134); 	at java.util.stream.ReferencePipeline$4$1.accept(ReferencePipeline.java:210); 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.IntPipeline.toArray(IntPipeline.java:502); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4363:3896,down,down,3896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4363,1,['down'],['down']
Availability,"ava HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5220:5434,error,error,5434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220,1,['error'],['error']
Availability,"ava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:16992,failure,failure,16992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['failure'],['failure']
Availability,"ava:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster yarn-client --executor-memory 8g --driver-memory 8g --conf spark.broadcast.blockSize=1g; ```. Running against commit f2b3bae of branch https://github.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1491:5652,Error,Error,5652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491,1,['Error'],['Error']
Availability,available resource bundle is lacking INFO fields for best practice VariantRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6715:0,avail,available,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6715,1,['avail'],['available']
Availability,avoid error due to finite precision error in M2 pon creation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5797:6,error,error,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5797,2,['error'],['error']
Availability,"aw.vcf -stand_call_conf 50 -A RMSMappingQuality -A BaseCounts; INFO 14:49:42,892 HelpFormatter - Executing as yanhs3941@compute-0-76 on Linux 2.6.32-696.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12.; INFO 14:49:42,892 HelpFormatter - Date/Time: 2021/10/09 14:49:42; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,922 NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/Onc_Soft_DB/software/GATK3.8/GenomeAnalysisTK.jar!/com/intel/gkl/native/libgkl_compression.so; INFO 14:49:42,957 GenomeAnalysisEngine - Deflater: IntelDeflater; INFO 14:49:42,958 GenomeAnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - Downsampling Se. the error is :; maxAltAlleles (6), the following will be dropped: TAAC.; WARN 14:59:10,944 HaplotypeCallerGenotypingEngine - location chr12:21623284-21623286: too many alternative alleles found (8) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: C, CA.; INFO 14:59:13,453 ProgressMeter - chr12:21624342 1.0358131E7 9.5 m 55.0 s 49.5% 19.2 m 9.7 m; WARN 14:59:37,613 HaplotypeCallerGenotypingEngine - location chr12:133237753-133237756: too many alternative alleles found (9) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: G, TAAA, GAAAAAAA.; INFO 14:59:43,454 ProgressMeter - chr13:32892450 1.0955099E7 10.0 m 54.0 s 54.0% 18.5 m 8.5 m; INFO 15:00:13,456 ProgressMeter - chr13:32912251 1.0976428E7 10.5 m 57.0 s 55.2% 19.0 m 8.5 m; ##### ERROR --; ##### ERROR stack trace; java.lang.IllegalStateException: Never found start -1 or stop -1 given cigar 17I; at org.broadinstitute.gatk.utils.sam.AlignmentUtils.getBasesCoveringRefInterval(AlignmentUtils.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7499:2086,error,error,2086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499,1,['error'],['error']
Availability,"awTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`. ### bump test coverage. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence` serialization test. - [x] `NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes()` (but only related to `SimpleSvType`'s, BND's will wait for a later PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663:1515,avail,available,1515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663,2,"['avail', 'down']","['available', 'downstreamBreakpointRefPos']"
Availability,"axverdier/Tools/GATK4/git/gatk/build/gatkPythonPackageArchive.zip has been removed.; Creating GATK Python package archive...; Created GATK Python package archive in /home/axverdier/Tools/GATK4/git/gatk/build/gatkPythonPackageArchive.zip; :createPythonPackageArchive (Thread[Daemon worker Thread 2,5,main]) completed. Took 0.058 secs.; :compileJava (Thread[Daemon worker Thread 2,5,main]) started.; :compileJava; Executing task ':compileJava' (up-to-date check took 0.044 secs) due to:; No history is available.; All input files are considered out-of-date for incremental task ':compileJava'.; Compiling with JDK Java compiler API.; /home/axverdier/Tools/GATK4/git/gatk/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SuspectedTransLocDetector.java:13: warning: [unchecked] unchecked conversion; import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;; ^; required: List<String>; found: List; error: warnings found and -Werror specified; 1 error; 1 warning; :compileJava FAILED; :compileJava (Thread[Daemon worker Thread 2,5,main]) completed. Took 4.116 secs. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileJava'.; > Compilation failed; see the compiler error output for details. * Try:; Run with --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':compileJava'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:4993,error,error,4993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,2,['error'],['error']
Availability,"b given intervals. ~197/200 jobs finished, but several gave an odd error. I'm probably not giving you enough information to debug, but maybe this is enough to ask questions. the command is below:. ```. java8 -Xmx48g -Xms48g -Xss2m \; -jar GenomeAnalysisTK4.jar GenotypeGVCFs \; -R REF.fasta \; --variant gendb://<genomicsdb_path> \; -O OUTPUT.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 --max-alternate-alleles 12 \; -L <Repeated ~40 times for small contigs>. ```; ; The error is the following:. ```. 21:58:51.873 WARN  MinimalGenotypingEngine - Attempting to genotype more than 50 alleles. Site will be skipped at location QNVO02001146.1:1343; --; 21:59:01.308 INFO  ProgressMeter -  QNVO02001146.1:1679            425.0               1264000           2974.3; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),6.825391631999999,Cpu time(s),6.825079531999995; #; # A fatal error has been detected by the Java Runtime Environment:; #; #  SIGSEGV (0xb) at pc=0x00007fcca9a2ea19, pid=36873, tid=140574431450880; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode linux-amd64 ); # Problematic frame:; # C  [libtiledbgenomicsdb3086049122144672414.so+0x3e3a19]  ArraySchema::tile_num(void const*) const+0x79; #; # Core dump written. Default location: /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-10-06_16-46-33/Job734/core or core.36873; #; # An error report file with more information is saved as:; # /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-10-06_16-46-33/Job734/hs_err_pid36873.log; #; # If you would like to submit a bug report, please visit:; #   http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. Do you have an debugging suggestions based on this? Than",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910:1092,error,error,1092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910,1,['error'],['error']
Availability,b/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7085:1090,recover,recoverDanglingHeads,1090,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085,1,['recover'],['recoverDanglingHeads']
Availability,"bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 02:57:15.774 INFO GenomicsDBImport - Initializing engine; 02:57:18.389 INFO IntervalArgumentCollection - Processing 11228744 bp from intervals; 02:57:18.437 INFO GenomicsDBImport - Done initializing engine; Created workspace ../RAW_VCF/my_database; 02:57:18.583 INFO GenomicsDBImport - Vid Map JSON file will be written to ../RAW_VCF/my_database/vidmap.json; 02:57:18.583 INFO GenomicsDBImport - Callset Map JSON file will be written to ../RAW_VCF/my_database/callset.json; 02:57:18.583 INFO GenomicsDBImport - Complete VCF Header will be written to ../RAW_VCF/my_database/vcfheader.vcf; 02:57:18.583 INFO GenomicsDBImport - Importing to array - ../RAW_VCF/my_database/genomicsdb_array; 02:57:18.583 INFO ProgressMeter - Starting traversal; 02:57:18.583 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 02:57:31.082 INFO GenomicsDBImport - Shutting down engine; [July 10, 2018 2:57:31 AM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.26 minutes.; Runtime.totalMemory()=4116185088; Exception in thread ""main"" java.lang.StackOverflowError; 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:95); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104); 	at com.intel.genomicsdb.model.ImportConfig.isThereChromosomeIntervalIntersection(ImportConfig.java:104). This Issue was generated from y",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4994:3460,down,down,3460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994,1,['down'],['down']
Availability,"bender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:233); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). #### Steps to reproduce; Get gs bucket with files from @bhanugandham ; To get error, do not use interval list (i.e. run the command as shown above), but we were able to avoid the error by using this interval list:; @HD VN:1.6; @SQ SN:gi|395136682|gb|CP003248.1| LN:4411708 M5:26f1f5c8a8a8c6e33c79d3fc6d40373e UR:file:/Users/bgandham/variant/mtb/Mycobacterium_tuberculosis_H37Rv.fasta; gi|395136682|gb|CP003248.1| 2074300 2074800 + target_1. #### Expected behavior; The tool should produce a vcf. #### Actual behavior; Fails with an ArrayIndexOutOfBounds Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7085:3139,error,error,3139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085,3,"['Error', 'error']","['Error', 'error']"
Availability,better Gnarly error message,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8270:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8270,1,['error'],['error']
Availability,better error message,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3812:7,error,error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3812,1,['error'],['error']
Availability,better error message for JEXL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1498:7,error,error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1498,1,['error'],['error']
Availability,"biguous call; I don't think is totally correct to think of these as *hom*-ref calls but if that is what the user wants... Handling ambiguous calls in the reads... I presume that these have low quality and thus are ignored, and if not we should force them to. . ---. @vdauwera commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85098560). I assume ambiguous basecalls in reads are ignored and therefore not an issue. It's really what to do with ambiguous ref bases that concerns me. Currently it seems that HC just accepts them as legitimate bases in certain conditions at least. . I'm not sure I understand this part:. > Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. That comes down to randomly assigning a ref allele at that site, doesn't it? I'm not sure I'm comfortable with that. ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85121261). Not random, A has priority, then C, then G and finally T amongst those that are compatible with the ambiguous code. For Example for N it would be A, but for B would C (as B means C/G/T(U)). ---. @vdauwera commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85123717). Well, I understand that it's alphabetical, but I mean it's not really meaningful -- it's even worse than random since the choice is biased by whatever accident of history caused A to be earlier in the alphabet than C. To be clear I don't have a better idea, but this one bugs me. ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85134672). I think this bias is nothing compare with the one introduced by the reference itself. I don",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2914:3199,down,down,3199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914,1,['down'],['down']
Availability,"blish different components in different artifacts. At least I would like to have a different artifact for pure-java components separate from the rest, to be sure that python (for example) is not required. Does some of this makes sense for you? A proposed scheme will be the following:. * `common`/`engine`: this should include the engine, utils, and everything that it can be useful by itself. This should be a dependency for every other module. Components in other sub-modules might be proposed to be moved to this one if they might be useful out of their own. If the package names does not change, the interface and usage will be unmodified, and then there is no change in the API.; * `spark`: I think that this is a nice separation from other components. In this case, this can include all code related with Spark classes for removal of the huge Spark dependency in sub-projects that does not require them.; * `tools` and `spark-tools`: this can be even split in more fine grained sub-modules depending on the pipeline (e.g. CNV, Mutect, etc., if it makes sense). In addition, the separation between normal tools and spark-tools will make easier for downstream projects to support or not spark in their code.; * `experimental`: this might contain prototype code that might change in the future, and that will be nice in terms of documentation purposes (always annotated with `@BetaFeature` or `@Experimental`, etc.). In addition, code shouldn't rely on the code in this package for anything, allowing to have experimental code for play around and remove if required, without any major version bump.; * `testing`: this will contain the testing framework. It is related with #1481 and #3567. ; * `documentation`: this might be useful for code dependent on `com.sun.javadoc` to do not interact with other classes if code for documenting a downstream project is not necessary.; * Other modules might be useful for concrete components: e.g, ., the gCNV python computational kernel implemented in #3838.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3900:1602,down,downstream,1602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3900,2,['down'],['downstream']
Availability,blow up on gatk-launch DownSampleSam --help,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1303:23,Down,DownSampleSam,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1303,1,['Down'],['DownSampleSam']
Availability,"blyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1098); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:4402,failure,failure,4402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['failure'],['failure']
Availability,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7121:194,error,errored,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121,2,['error'],"['error', 'errored']"
Availability,"broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource@78eac8f2); - element of array (index: 1); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases, functionalInterfaceMethod=org/apache/spark/api/java/function/PairFlatMapFunction.call:(Ljava/lang/Object;)Ljava/lang/Iterable;, implementation=invokeStatic org/broadinstitute/hellbender/engine/spark/ShuffleJoinReadsWithRefBases.lambda$addBases$cff38836$1:(Lorg/broadinstitute/hellbender/utils/SerializableFunction;Lorg/broadinstitute/hellbender/engine/datasources/ReferenceMultiSource;Lscala/Tuple2;)Ljava/lang/Iterable;, instantiatedMethodType=(Lscala/Tuple2;)Ljava/lang/Iterable;, numCaptured=2]); - writeReplace data (class: java.lang.invoke.SerializedLambda); - object (class org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases$$Lambda$74/1217660878, org.broadinstitute.hellbender.engine.spark.ShuffleJoinReadsWithRefBases$$Lambda$74/1217660878@663d7f24); - field (class: org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1, name: f$5, type: interface org.apache.spark.api.java.function.PairFlatMapFunction); - object (class org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1, <function1>); at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101); at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301); ... 30 more; 16/10/17 16:03:47 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2216:5904,down,down,5904,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216,1,['down'],['down']
Availability,"broadinstitute.org/hc/en-us/community/posts/360067695771-GenotypeGvcfs-has-formatting-issues-in-both-v4-1-6-0-as-v4-1-7-0. --. Hi,. I'm using v4.1.6.0 of GenotypeGvcfs to make a vcf, out of whole genome data from 19 samples (following your recommendations). When I run ValidateVariants to check the output of GenotypeGvcfs I get a error message, which states that one or more of the ALT allele are actually not in the samples provided. A previous user already found a similar error in ValidateVariants (https://gatk.broadinstitute.org/hc/en-us/community/posts/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-), but then for Haplotypecaller, and you have opened a bugreport to add a feature to ValidateVariants: https://github.com/broadinstitute/gatk/issues/6553. However, it would be nice if you could actually investigate the formatting error. Unfortunately my formatting error isn't the same as reported in the other post. I have 105 error in which the 1st alternative allele is a spanning deletion and the 2nd (and 3rd) is either an indel or snp. It's true that the 2nd and 3rd allele is actually not found in my samples. I even have 7 occurances in which the 1st allele (spanning deletion) has allele frequency 1.00. my code is the following for GenotypeGVCFs:. java -Xms32G -Xmx32G -jar ${gatk4} GenotypeGVCFs -R ${ref} -V ${pipeline}/${name}\_v4.1.6.0.g.vcf.gz -O ${vcf}/${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list 2> ${log}/${name}\_v4.1.6.0\_genotype.log. for ValidateVariants:. java -Xms10G -Xmx10G -jar ${gatk4} ValidateVariants -R ${ref} -V ${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list --warn-on-errors 2> ${log}/${name}\_v4.1.6.0\_genotype\_valivar.log. the warning in ValidateVariants and the site look like this:. 14:12:15.126 WARN ValidateVariants - \*\*\*\*\* Input 1st\_v4.1.6.0.vcf.gz fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position chr\_1:1088200 are not observed at all in the sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:1056,error,error,1056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,1,['error'],['error']
Availability,"broadinstitute/gatk-protected/issues/942). @vdauwera commented on [Sat Mar 07 2015](https://github.com/broadinstitute/gsa-unstable/issues/855). #### Issue description / case details. Users who are dealing with large ploidies (typically for pooled experiments) run into limitations where the combination of the ploidy and numbers of alleles in the high teens and above produces too many possible genotypes for GenotypeGVCFs to handle under its current architecture. . For example, in the case reported here, the ploidy is 19 and the number of alternate alleles is 21, so GenotypeGVCFs cannot handle the large number of possible genotypes that result from all the possible combinations. A reasonable way to deal with this would be to cull the possible combinations dynamically at runtime to eliminate the most unlikely combinations up front. ; #### Test data. Has been provided by the user ; #### [Original forum post](http://gatkforums.broadinstitute.org/discussion/4954/combination-of-ploidy-and-number-of-alleles-error-when-running-genotypegvcfs/p1). ---. @vruano commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-77993425). The error message explain the reason well ... a possibility to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:1074,error,error-when-running-genotypegvcfs,1074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['error'],['error-when-running-genotypegvcfs']
Availability,"bugfix for gnarly qualapprox calculation, tolerate missing VarDP, sup…",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7061:42,toler,tolerate,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7061,1,['toler'],['tolerate']
Availability,"build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be do",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985:1063,error,error,1063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985,1,['error'],['error']
Availability,build.gradle finds the tool provider with the following line:. ```; final javadocJDKFiles = files(((URLClassLoader) ToolProvider.getSystemToolClassLoader()).getURLs()); ```; ToolPrivider.getSystemToolClassLoader() returns null on jre and certain other java installations. This causes a confusing null pointer exceptions. We should have a better error message when this happens.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4532:345,error,error,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4532,1,['error'],['error']
Availability,build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:14339,ERROR,ERROR,14339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"build_docker.sh creates unzippedJar and testsJar, but it does not remove them and it fails as a result in subsequent runs. . I ran ./build_socker.sh -e <GIT LOG HASH> and I got the error message ; mv: rename ./build/bundle-files-collected to ./unzippedJar/bundle-files-collected: Directory not empty. Only after removing unzippedJar and testJar could I build the image again successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5369:181,error,error,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5369,1,['error'],['error']
Availability,build_docker.sh needs to download and mount the large files when running unit tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3191:25,down,download,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191,1,['down'],['download']
Availability,"but 3 parallele jobs failed due to java core dump:; - Syntax I ran was pretty basic, I also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7515:1088,error,error,1088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515,1,['error'],['error']
Availability,"by IGV by removing or reordering columns, but I don't think this is unreasonable. (I think this is preferable to outputting additional 4-column segment files specifically for use with IGV, right?). ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275259864). Started a branch. Will have to cook up some new test data but should hopefully be relatively quick. Note that we will lose the dotted centromere indicators unless we require their locations as an additional input. ---. @sooheelee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275262070). I think IGV's default heatmap coloring is centered around 0 or 1, whichever CNV data isn't. As for the centromere locations, I'm not sure but perhaps [this format](http://software.broadinstitute.org/software/igv/Cytoband) can help define those for people who want to define them. I'd have to do some digging through UCSC Golden paths to see what is commonly available. ---. @samuelklee commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275279714). According to the page linked above, users should be able to set data range and log/linear scale in IGV?. ---. @samuelklee commented on [Fri Jan 27 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275760865). @LeeTL1220 @achevali I would like to get rid of the per-segment ACNV plotting, unless there are any strong objections. @dlivitz Do you guys find this functionality useful?. ---. @LeeTL1220 commented on [Fri Jan 27 2017](https://github.com/broadinstitute/gatk-protected/issues/495#issuecomment-275761180). Whatever you like. On Fri, Jan 27, 2017 at 3:04 PM, samuelklee <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> @achevali; > <https://github.com/achevali> I would like to get rid of the per-segment; > ACNV plotting, unless there are any strong obje",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2853:5693,avail,available,5693,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2853,1,['avail'],['available']
Availability,"by the caller; using TRUE; 3: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 4: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 5: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; Execution halted. at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:18); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:125); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generatePlots(RecalUtils.java:360); at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.generatePlots(AnalyzeCovariates.java:329); at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.doWork(AnalyzeCovariates.java:341); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). The above error occurs when generating plot using:; ```; gatk AnalyzeCovariates \; -before /sample_analysis/SRR25308851/SRR25308851_before_recal_data.table \; -after /sample_analysis/SRR25308851/SRR25308851_after_recal_data.table \; -plots /sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; ```. SRR25308851_before_recal_data.table and SRR25308851_after_recal_data.table files are proper, generated using gatk BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8456:5509,error,error,5509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456,1,['error'],['error']
Availability,"c.uk/ena/cram/md5/%s; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.REFERENCE_FASTA : null; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; > > 15:10:22.794 INFO PrintReadsSpark - Deflater IntelDeflater; > > 15:10:22.794 INFO PrintReadsSpark - Initializing engine; > > 15:10:22.794 INFO PrintReadsSpark - Done initializing engine; > > 15:10:23.180 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; > > 15:10:25.800 INFO PrintReadsSpark - Shutting down engine; > > [October 18, 2016 3:10:25 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.05 minutes.; > > Runtime.totalMemory()=467140608; > > org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:252); > > at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); > > at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:3319,down,down,3319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,1,['down'],['down']
Availability,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2739:3154,error,error,3154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739,2,['error'],['error']
Availability,cala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(Map,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:4181,Error,Error,4181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['Error'],['Error']
Availability,"capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:1199,ERROR,ERROR,1199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183:5823,ERROR,ERROR,5823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183,2,['ERROR'],['ERROR']
Availability,ce file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:47.107 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_rec/hg38/acmg59_test_cleaned.txt; 06:42:47.109 INFO Funcotator - Initializing Funcotator Engine...; 06:42:47.139 INFO Funcotator - Creating a MAF file for output: file:/data/nws/WES/GenomicsDBImport/200923_A00268_0517_AHKL37DSXY/Set20-5_L2_159A59.somatic.filterMutectCalls.funcotator.maf; 06:42:47.186 INFO ProgressMeter - Starting traversal; 06:42:47.187 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 06:42:47.233 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; 06:42:47.233 INFO VcfFuncotationFactory - LMMKnown 20180618 cache hits/total: 0/0; 06:42:47.287 INFO Funcotator - Shutting down engine; [2021年2月21日 上午06时42分47秒] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.62 minutes.; Runtime.totalMemory()=1988100096; java.lang.IllegalArgumentException: Unexpected value: lncRNA; 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1016); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature.<init>(GencodeGtfFeature.java:144); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:729); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfFeature.create(GencodeGtfFeature.java:299); 	at org.broadinstitute.hellbender.utils.codecs.gencode.GencodeGtfCodec.decod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7090:9362,down,down,9362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090,1,['down'],['down']
Availability,"ce output. 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported. 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; ```. Since the calculation takes quite long, I checked the WARN messages of the output above. Especially the last one about the AVX instruction set where it says that a **MUCH** slower implementation will be used. From the few WARN messages it seems like the root cause is the failure to load libgkl and that again seems to be related to my platform. From another thread/topic I concluded that the instruction set problem might be gone if libgkl could be loaded. Does anyone know more about this issue or how to work around it?. Best regards,; Robert",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:5727,failure,failure,5727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['failure'],['failure']
Availability,ce sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to /tmp/libgkl_utils9151568277466250840.so; 11:35:41.777 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:35:41.802 DEBUG NativeLibraryLoader - Extracting libgkl_pairhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache mi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:6337,Avail,Available,6337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Avail'],['Available']
Availability,"ception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configurati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:1649,ERROR,ERROR,1649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,ceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.toolin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:6423,ERROR,ERROR,6423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"changes to build.gradle; R package installation is now part of the gradle build; install_R_packages.R no longer reinstalls existing packages; a warning will be emitted if this fails. compilation no longer depends on R installation, installation does. test run in parallel now; this is set to use 2 cores on travis and 4 locally. adding a note about our R dependency to the readme. travis changes; adding caching to travis for dramatic R installation speedup; updating gradle download because it was using an out of date link. misc changes:; adding an additional flag to mark duplicates to avoid the garbage collection statistics while integration testing; tagging tests that depend on R for future use",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/296:475,down,download,475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/296,1,['down'],['download']
Availability,changing the default behavior of `RScriptExecutor` from logging a warning on a failure to crashing on failure. the exception message will include output from the failed Rscript; closes #223,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/237:79,failure,failure,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/237,2,['failure'],['failure']
Availability,"cheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 17/10/11 14:19:37 INFO scheduler.TaskSetManager: Starting task 0.3 in stage 1.0 (TID 4, com2, executor 2, partition 0, NODE_LOCAL, 1990 bytes); 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Disabling executor 2.; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: Executor lost: 2 (epoch 1); 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, com2, 46254); 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removed 2 successfully in removeExecutor; 17/10/11 14:19:38 ERROR cluster.YarnScheduler: Lost executor 2 on com2: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:24681,ERROR,ERROR,24681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['ERROR'],['ERROR']
Availability,cher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:11630,ERROR,ERROR,11630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"ciMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion with user; http://gatkforums.broadinstitute.org/gatk/discussion/comment/36309#Comment_36309; - related dsde-docs issue; https://github.com/broadinstitute/dsde-docs/issues/1744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2946:11838,ERROR,ERROR,11838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946,12,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"cial ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool should end successfully without the specified error, and generated consistent result. #### Actual behavior; Both tool ends throwing out the same error, but the alignment ratio of the bam file from bwaspark is the same as the original bwa, and seems to be ok. The vcf variant number changes between each runs, and different from the result of stable non-spark official GATK 4 version. ``` bash; 2018-12-03 13:19:45 ERROR CoarseGrainedExecutorBackend:43 - RECEIVED SIGNAL TERM; ```. The vcf line number for several runs of the ReadsPipelineSpark,; ``` bash; 94488 test2.vcf.gz; 97426 test3.vcf.gz; 82279 test4.hg19.vcf.gz; 99403 test4.vcf.gz; 86384 test7_hg19.vcf.gz; 104854 test7.vcf.gz; 68824 test9.vcf.gz; 97810 test.vcf.gz; ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5481:2046,error,error,2046,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"cker container as described here: https://gatk.broadinstitute.org/hc/en-us/articles/360035889991--How-to-Run-GATK-in-a-Docker-container. ``` bash; ./gatk Mutect2 -I:tumor 1st.chr1.bam -I:normal 2nd.chr1.bam -O variants.vcf.gz --min-pruning 8 -R reference.chr1.fa; ```; I repeated this three times, the last time to make sure whether the .vcf.stats is being generated or not. This was a test run using the input filtered for chr1 using `samtools view -b`. I though this was the reason for getting the error message about Contig 2 not being present. ```bash; ...; 18:39:03.207 INFO ProgressMeter - 1:282722440 448.7 1529870 3409.8; 18:39:06.592 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 19.218222963000002; 18:39:06.592 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 5376.604473962; 18:39:06.593 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 12201.77 sec; 18:39:06.594 INFO Mutect2 - Shutting down engine; [August 25, 2020 6:39:06 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 448.75 minutes.; Runtime.totalMemory()=12349079552; ***********************************************************************. A USER ERROR has occurred: Contig 2 not present in the sequence dictionary [1]. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```; Is there any way of getting around this and generating .vcf.stats without repeating a lengthy variant calling `Mutect2`? Is this a problem introduced by running the truncated input files?. Looking up online, it seems this seemed to be an issue in the previous versions of GATK:https://github.com/broadinstitute/gatk/issues/6102. Thanks!. #### Expected behavior; I would expect .vcf.stats to be automatically generated with the output. #### Actual behavior; No .vcf.stats can be located",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6768:1559,ERROR,ERROR,1559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6768,1,['ERROR'],['ERROR']
Availability,clearer error when values are missing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7939:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7939,2,['error'],['error']
Availability,"code, the `phaseGT` in the code and `PGT` format field on each genotype can be interpreted as being an indicator of which of the two phased haplotypes in the sample contains the site-specific alternate allele at the site (ie. excluding `*` which represents variation that beings upstream of the current variant. NB that this results in cases where `PGT` is not the same as the phased `GT` field. For example, in the case of a spanned SNP site with REF allele `A` and alt alleles `C` and `*`, `GT` may be set to `1|2` to represent the spanned SNP, while PGT would be set to `1|0` to represent the fact that it is the first haplotype in the pair of phased haplotypes that contains the site-specific alt allele (in this case `C`). If reviewers agree with this interpretation, I think we should create a new ticket to clarify documentation around the PGT and PID tags to reflect it. . After discussions with @ldgauthier I believe that there may be downstream issues in preserving phasing after passing gVCFs through CombineGVCFs, GenomicsDBImport, and/or GenotypeGVCFs, especially if the gVCFs are emitted without GT fields. In that case, `GenotypeGVCFs` should probably have logic to reconstruct the phased genotype for each sample based on the PGT and PID tags when possible. I will create a new ticket describing the issue. There still may be cases where HaplotypeCaller does not emit phasing information for spanning deletions due to the presence of extra haplotypes that contradict diploid phasing, as in https://github.com/broadinstitute/gatk/issues/6845. A fix to that issue would likely reduce the number of those cases. The integration test result file `src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/withOxoGReadCounts.vcf` does not have any changes that have to do with this PR -- it was automatically updated by GenotypeGVCFsIntegrationTest, which included some new jitter in QUAL scores as described in https://github.com/broadinstitute/gatk/pull/6859, but neve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6937:1171,down,downstream,1171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6937,1,['down'],['downstream']
Availability,"code, theoretically, allows for arbitrarily complex rearrangement; shown above on the right is a table of inversion calls (TP/FP 12/7 using PacBio calls on CHM-1 & 13 cell lines as truth) extracted from the `<CPX>` calls, they were extracted by the tool proposed in PR #4602. #### stages:. * Primitive filter on breakpoints ; * low MQ of assembly contigs' mappings that evidenced the BND records, ; * suspiciously large distance between mates (mate pairs whose distance are over $10^5$bp (~1/3 of input, see blelow) are more likely to be artifact or dispersed/segmental duplications). <p align=""center""><img src=""https://user-images.githubusercontent.com/16310888/40271740-6daa2b9e-5b6f-11e8-9dbb-89085450db6d.png"" width=""420"" height=""420"" ></p>; * if overlaps with CPX (supposedly they should be captured already, or is more complex than what can be comprehended by the logic proposed here). The mates are then converted to intervals bounded by the mates' locations. These ""normal sized"" variants are sent down for further analysis. * Filtering based on overlap signatures. Here we have several possible scenarios (total ~130 pairs of mates):; * no overlappers (~ 50 mate pairs, balanced between ++/--); * multiple overlappers (~ 10 mate pairs, balanced between ++/--); * unique overlapping pairs of mate pairs (~ 60 mate pairs); * which overlaps with same type (++/++ or --/--, ~ 10 mate pairs); * which overlaps with opposite type (++/-- overlap, ~ 50 mate pairs). These are the overlapping pairs sent down for breakpoint linking, expecting a maximum of 20~30 inversion calls. * Type inference. Here we have four possible cases, each signaling what could be involved (primed block is inverted):; * INV55 interval left/right boundary upstream of INV33 interval's left/right boundary: `ABC -> B'`; * INV33 interval left/right boundary upstream of INV55 interval's left/right boundary: `ABC -> AC'B'A'C`; * INV33 interval contains INV55 interval: `ABC -> ABA' or AB'A'`; * INV55 interval contains INV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789:3257,down,down,3257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789,1,['down'],['down']
Availability,command line parsing errors should be more specific,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/169:21,error,errors,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/169,1,['error'],['errors']
Availability,"csDBImport` jobs write 50 DBs to a GCS bucket. 49 of them worked successfully, however one have failed with the following error:. ```sh; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7653:1371,error,error,1371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653,1,['error'],['error']
Availability,"cs_max_retries 20 --disableToolDefaultReadFilters false; [August 22, 2017 2:52:59 PM UTC] Executing as root@fb0704c97258 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.3-41-g9d05dd8-SNAPSHOT; [August 22, 2017 3:06:11 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSR done. Elapsed time: 13.19 minutes.; Runtime.totalMemory()=3040870400; htsjdk.samtools.FileTruncatedException: Premature end of file: /PairedEndSingleSampleWorkflow/4a87f12f-014e-438a-9a10-260c70bf3584/call-SortSampleBam/attempt-4/NA12878.aligned.duplicate_marked.sorted.bam; 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:530); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:366); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:209); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3481:4010,avail,available,4010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3481,1,['avail'],['available']
Availability,"ct is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:4791,failure,failure,4791,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['failure'],['failure']
Availability,"cted HaplotypeCallerSpark -I XX_BQSRappliedspark.bam -O XX_525.gvcf -R /curr/data/humann_g1k_v37.2bit --emitRefConfidence BP_RESOLUTION --TMP_DIR tmp"". And it runs on Amazon instance m4.2xlarge. 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:1223,ERROR,ERROR,1223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['ERROR'],['ERROR']
Availability,"ctory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:17.957 INFO VcfFuncotationFactory - dbSNP 9606\_b150 cache hits/total: 521/453691 ; 07:33:18.138 INFO Funcotator - Shutting down engine ; [May 28, 2020 7:33:18 AM EDT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 34.35 minutes. ; Runtime.totalMemory()=3822059520 ; java.lang.StringIndexOutOfBoundsException: String index out of range: 545 ; at java.lang.String.substring(String.java:1963) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForInsertion(ProteinChangeInfo.java:256) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:93) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:371) ; at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2003) ; at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotationForProteinCodingFeature(GencodeFuncotationFactory.java:1193) ; at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651:2006,down,down,2006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651,1,['down'],['down']
Availability,"cumentation for this tool should also be more specific. This request was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7327:1294,down,downsample,1294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327,1,['down'],['downsample']
Availability,cuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:9320,ERROR,ERROR,9320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,cutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:491); 	... 12 more; Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRecord.read(InputRecord.java:505); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	... 25 more; ```; The error seems to appear after `org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 19.15 minutes.` is logged which is surprising.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685:7784,down,down,7784,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685,2,"['down', 'error']","['down', 'error']"
Availability,"d features are; {gene, transcript, exon, CDS, Selenocysteine, start_codon,; stop_codon and UTR}; start - start position of the feature, with sequence numbering ; starting at 1.; end - end position of the feature, with sequence numbering ; starting at 1.; score - a floating point value indiciating the score of a feature; strand - defined as + (forward) or - (reverse).; frame - one of '0', '1' or '2'. Frame indicates the number of base pairs; before you encounter a full codon. '0' indicates the feature ; begins with a whole codon. '1' indicates there is an extra; base (the 3rd base of the prior codon) at the start of this feature.; '2' indicates there are two extra bases (2nd and 3rd base of the ; prior exon) before the first codon. All values are given with; relation to the 5' end.; attribute - a semicolon-separated list of tag-value pairs (separated by a space), ; providing additional information about each feature. A key can be; repeated multiple times. Attributes. The following attributes are available. All attributes are semi-colon; separated pairs of keys and values. - gene_id: The stable identifier for the gene; - gene_version: The stable identifier version for the gene; - gene_name: The official symbol of this gene; - gene_source: The annotation source for this gene; - gene_biotype: The biotype of this gene; - transcript_id: The stable identifier for this transcript; - transcript_version: The stable identifier version for this transcript; - transcript_name: The symbold for this transcript derived from the gene name; - transcript_source: The annotation source for this transcript; - transcript_biotype: The biotype for this transcript; - exon_id: The stable identifier for this exon; - exon_version: The stable identifier version for this exon; - exon_number: Position of this exon in the transcript; - ccds_id: CCDS identifier linked to this transcript; - protein_id: Stable identifier for this transcript's protein; - protein_version: Stable identifier version for th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6488:3790,avail,available,3790,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6488,1,['avail'],['available']
Availability,"d file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 2022 2:14:09 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.49 minutes.; Runtime.to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:3765,avail,available,3765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['avail'],['available']
Availability,"d from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.build",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:1021,ERROR,ERROR,1021,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"d in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618:2300,error,error,2300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618,1,['error'],['error']
Availability,"d it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:994,ERROR,ERROR,994,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,3,['ERROR'],['ERROR']
Availability,"d no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:1101,ERROR,ERROR,1101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"d successfully, however one have failed with the following error:. ```sh; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7653:1450,error,error,1450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653,1,['error'],['error']
Availability,d time: 0.01 minutes.; Runtime.totalMemory()=2252865536; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(Filte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:10599,Error,ErrorProbabilities,10599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Error'],['ErrorProbabilities']
Availability,"d(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D__contig; _readvv_sieve_cb(): block read failed; major: Dataset; minor: Read failed; #008: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fio.c line 120 in H5F_block_read(; ): read through metadata accumulator failed; major: Low-level I/O; minor: Read failed; #009: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Faccum.c line 263 in H5F__accum_r; ead(): driver read request failed; major: Low-level I/O; minor: Read failed; #010: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDint.c line 204 in H5FD_read(): ; driver read request failed; major: Virtual File Layer; minor: Read failed; #011: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5FDsec2.c line 725 in H5FD_sec2_re; ad(): file read failed: time = Wed Apr 14 11:52:33 2021; , filen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7202:1820,error,error,1820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202,1,['error'],['error']
Availability,"d.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused by: java.io.IOException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.throwIfFalseEOF(NetHttpResponse.java:202); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:171); at java.io.FilterInputStream.read(FilterInputStream.java:107); at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:638); ... 13 more. Subsequent reruns on 4.0.4.0 yielded only 1 failure out of 3 walkers x 59 samples; no failures were observed on 4.0.9.0 or 4.0.11.0. So the problem is not unique to 4.0.12.0, but the rate of failure is much higher. Additional reruns on 4.0.12.0 suggest that the original failures were not intermittent; one run showed 6/59 samples failing, with many of those having more than one of the 3 walkers fail. Additional reruns with a branch of 4.0.12.0 that reverted to the htsjdk version used in 4.0.11.0 still showed a high rate of failure. There were at least a couple of instances where the same BAM appeared to fail in roughly the same spot as in the 4.0.12.0 runs, and other instances where the same BAM failed in roughly the same spot, only in two different walkers. However, the set of BAMs that failed was not consistent across all runs. Leaving to @droazen to delegate. The FC CNV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631:7891,down,download,7891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631,1,['down'],['download']
Availability,"dClassDesc(ObjectInputStream.java:1713); 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000); 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); 	at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); 	... 20 more; 17/11/15 19:43:35 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@5917b44d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/11/15 19:43:35 WARN org.apache.spark.ExecutorAllocationManager: No stages are running, but numRunningTasks != 0; 19:43:35.858 INFO PrintVariantsSpark - Shutting down engine; [November 15, 2017 7:43:35 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=823132160; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; Serialization trace:; genotypes (org.seqdoop.hadoop_bam.VariantContextWithHeader); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:8294,Error,Error,8294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['Error'],['Error']
Availability,dExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:6962,ERROR,ERROR,6962,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,dExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:7814,ERROR,ERROR,7814,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,dExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:10618,ERROR,ERROR,10618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,dExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:9360,ERROR,ERROR,9360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"dLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:149); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:190); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:107); at org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest.testVCFModeIsConcordantWithGATK3_8Results(HaplotypeCallerSparkIntegrationTest.java:143); Caused by:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 26, localhost, executor driver): java.util.ConcurrentModificationException; at java.util.ArrayList.sort(ArrayList.java:1464); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:3947,failure,failure,3947,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['failure'],['failure']
Availability,"dLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275) Caused by: java.nio.file.NoSuchFileException: /storageNGS/ngs3/projects/other1/KinderKlinik/Exomes/04_GRCh8_GATK4_cohort/GATK4_testrun/cromwell-executions/HaplotypeCallerGvcf_GATK4/3e5a2ae0-5529-4344-b187-9ceac771e1ed/call-MergeGVCFs/execution/pa.hg38.g.vcf.gz; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.tribble.util.ParsingUtils.openInputStream(ParsingUtils.java:110); at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:244) ... 13; ```. It looks like we intended to catch and convert to user exception in:; `at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getHeaderFromPath(GenomicsDBImport.java:355)` but we weren't expecting a `TribbleException`. ```; private VCFHeader getHeaderFromPath(final Path variantPath) {; try(final AbstractFeatureReader<VariantContext, LineIterator> reader = getReaderFromPath(variantPath)) {; return (VCFHeader) reader.getHeader();; } catch (final IOException e) {; throw new UserException(""Error while reading vcf header from "" + variantPath.toUri(), e);; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4526:3431,Error,Error,3431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4526,1,['Error'],['Error']
Availability,"dPosRankSum=0.433	GT:AD:DP:GQ:PL:SB	0/2:414,2,357,0:773:99:14672,11361,50781,0,41338,45124,13972,52387,44158,56529:206,208,177,182; chr13	32944608	.	T	A,*,<NON_REF>	0	.	BaseQRankSum=5.453;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:5950,down,download,5950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,1,['down'],['download']
Availability,"dd(ImmutableList.java:748); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableCollection$Builder.addAll(ImmutableCollection.java:456); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.addAll(ImmutableList.java:847); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList.copyOf(ImmutableList.java:275); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractRecordCollection.<init>(AbstractRecordCollection.java:83); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.<init>(AbstractLocatableCollection.java:58); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractSampleLocatableCollection.<init>(AbstractSampleLocatableCollection.java:44); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection.<init>(AllelicCountCollection.java:58); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments$$Lambda$168/595759572.apply(Unknown Source); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments.readOptionalFileOrNull(ModelSegments.java:599); 	at org.broadinstitute.hellbender.tools.copynumber.ModelSegments.doWork(ModelSegments.java:481); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). My allelicCounts.tsv is about 69 G. I know it's array size problem, java can not load allelicCounts file that big. I wonder if you know some ways to solve this error. thaks!. Peng Pu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7633:5447,error,error,5447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633,1,['error'],['error']
Availability,"ded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022年5月20日 上",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866:2052,Redundant,Redundant,2052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866,1,['Redundant'],['Redundant']
Availability,deleted redundant method lnToLog10. Closes #2166.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2650:8,redundant,redundant,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2650,1,['redundant'],['redundant']
Availability,"denceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Tim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437:1470,Redundant,Redundant,1470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437,1,['Redundant'],['Redundant']
Availability,"der.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:58); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:706); 	at org.broadinstitute.hellbender.engine.VariantWalker.onStartup(VariantWalker.java:45); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Did not inflate expected amount, for input source: /data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:372); 	... 12 more; Caused by: htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7114:4408,error,error,4408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114,1,['error'],['error']
Availability,devents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.Defa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2758,ERROR,ERROR,2758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"different in HC invocation was that the first one used `--dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5791:1932,recover,recover,1932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791,1,['recover'],['recover']
Availability,dle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:12357,ERROR,ERROR,12357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,dle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:11099,ERROR,ERROR,11099,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"dler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409732fb{/executors/threadDump/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e99e2cb{/static,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@478967eb{/,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Start",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:9006,AVAIL,AVAILABLE,9006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"dlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1491:5503,ERROR,ERROR,5503,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491,2,"['ERROR', 'Error']","['ERROR', 'ErrorMonitor']"
Availability,doWork should not return error code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/71:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/71,1,['error'],['error']
Availability,"domly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfully. . #### Actual behavior; _Tell us what happens instead_. job dies with this error:. `malloc(): unaligned tcache chunk detected`. ```; 23:45:26.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/gpfs_de6000/home/dalegre/miniconda3/e; nvs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:45:26.822 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45:26.824 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 23:45:26.824 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:45:26.824 INFO GenomicsDBImport - Executing as dalegre@amd4103.hpc.eu.lenovo.com on Linux v5.14.0-284.11.1.el9_2.x86_64 amd6; 4; 23:45:26.824 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 23:45:26.824 INFO GenomicsDBImport - Start Date/Time: February 6, 2024 at 11:45:26 PM CET; 23:45:26.824 INFO GenomicsDBImport - -------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683:2081,error,error,2081,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683,1,['error'],['error']
Availability,downgrading gcloud to 207.0.0 from 208.0.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5004:0,down,downgrading,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5004,1,['down'],['downgrading']
Availability,dromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:282); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:993); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036:1666,down,downsampling,1666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036,1,['down'],['downsampling']
Availability,"ds but `VariantAnnotator` only has the reads. Several annotations had fallback code to annotate a likelihoods object that had no likelihoods. @vruano This is the class that you disliked so much in your recent code review of #5783 . A few issues with this state of things:. * Torturing the definition of `AlleleLikelihoods`, which forced the class to have methods like `hasLikelihoods()`.; * `VariantAnnotator` only applied the few annotations that had custom pileup-based fallback code.; * Lots more annotation code for the fallback mode. So the first step was the option that @lbergelson and @jamesemery liked most: create a regular likelihoods object in `VariantAnnotator` by hard-assigning of each read to the allele it best matches. This is exactly what all the custom fallback modes were doing in effect, but now it's implemented in one place instead of six or so. This lets us delete `UnfilledLikelihoods` and also lets `VariantAnnotator` apply any annotation. @ldgauthier Since the most non-trivial aspect is the new integration test I'm inclined to assign you the review, but a case could be made for someone on the engine team. This completely broke the `VariantAnnotator` tests, which were based on exact matches. This had been an issue before and has always been a bit of a nuisance, but now overhauling the tests became completely unavoidable. So, I rewrote all the tests and wrote a rigorous test based on concordance with annotations from `Mutect2`. If I were reviewing I would start with the new code in `VariantAnnotator` that constructs the likelihoods object from the reads and verify that it is just a more polished version of the fallback code that several annotations used to have. Then I would look at the new `VariantAnnotator` integration tests. Some of the tolerances are fairly liberal but it's worth noting that much of the old exact match ""truth"" annotations were completely bogus. This is better than what we had before by a long shot but it's still use at your own risk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6172:2027,toler,tolerances,2027,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6172,1,['toler'],['tolerances']
Availability,"ds-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6`; ```. The germline resource is a VCF of approximately 80 million SNPs and indels (including multi allelic sites) called from a large number of canine WGS. It is formatted as a VCF with no sample information:; ```; chr1 240 . TG T 464.40 PASS AC=4;AF=0.011;AN=332;BaseQRankSum=0.674;ClippingRankSum=0;DP=14798;ExcessHet=0.0026;FS=5.63;InbreedingCoeff=-0.005;MLEAC=14;MLEAF=0.017;MQ=7.49;MQRankSum=-0.967;QD=22.11;ReadPosRankSum=0.967;SOR=3.18; ```. The VCF for variants for contamination is a subset of this VCF, with only biallelic SNPs with AF between 0.01 and 0.2. Initially, it was formatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:4929,error,error,4929,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,2,['error'],['error']
Availability,"due to error `Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects` during RevertSam step in MitochondriaPipeline.wdl @meganshand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8406:7,error,error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8406,1,['error'],['error']
Availability,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3335:278,error,error,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335,1,['error'],['error']
Availability,"dy stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in stage 25.0 (TID 43224, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.sca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:6296,failure,failure,6296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['failure'],['failure']
Availability,"e ""org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.addTranscript(org.banscriptFeature)"" because ""gene"" is null; at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.aggregateRecordsIntoGeneFeature(AbstractGtfCodec.java:339); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:170); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:23); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.readNextRecord(TribbleIndexedFeatureReader.java:376); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.<init>(TribbleIndexedFeatureReader.java:343); at htsjdk.tribble.TribbleIndexedFeatureReader.iterator(TribbleIndexedFeatureReader.java:310); at org.broadinstitute.hellbender.engine.FeatureDataSource.iterator(FeatureDataSource.java:531); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.buildIntervalTreesFromGTF(SVAnnotate.java:297); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.onTraversalStart(SVAnnotate.java:227); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Seems like these errors are from compiled code and i couldn't locate the exact script. ; It will be great if you could help me with the following questions so i can fix these issues myself:; **what does the `gene` refer to? Could you please provide a template input `.gtf` file so i can better know how to call `SVAnnotate`?**",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394:2001,error,errors,2001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394,1,['error'],['errors']
Availability,e coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:15455,Recover,Recovered,15455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"e from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since options 3 and 4 seem to be the only options with votes, let's sit down next week and discuss in detail the pain points of these two options, and make a choice between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:6505,down,down,6505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['down'],['down']
Availability,"e likely to be artifact or dispersed/segmental duplications). <p align=""center""><img src=""https://user-images.githubusercontent.com/16310888/40271740-6daa2b9e-5b6f-11e8-9dbb-89085450db6d.png"" width=""420"" height=""420"" ></p>; * if overlaps with CPX (supposedly they should be captured already, or is more complex than what can be comprehended by the logic proposed here). The mates are then converted to intervals bounded by the mates' locations. These ""normal sized"" variants are sent down for further analysis. * Filtering based on overlap signatures. Here we have several possible scenarios (total ~130 pairs of mates):; * no overlappers (~ 50 mate pairs, balanced between ++/--); * multiple overlappers (~ 10 mate pairs, balanced between ++/--); * unique overlapping pairs of mate pairs (~ 60 mate pairs); * which overlaps with same type (++/++ or --/--, ~ 10 mate pairs); * which overlaps with opposite type (++/-- overlap, ~ 50 mate pairs). These are the overlapping pairs sent down for breakpoint linking, expecting a maximum of 20~30 inversion calls. * Type inference. Here we have four possible cases, each signaling what could be involved (primed block is inverted):; * INV55 interval left/right boundary upstream of INV33 interval's left/right boundary: `ABC -> B'`; * INV33 interval left/right boundary upstream of INV55 interval's left/right boundary: `ABC -> AC'B'A'C`; * INV33 interval contains INV55 interval: `ABC -> ABA' or AB'A'`; * INV55 interval contains INV33 interval: `ABC -> C'BC or C'B'C`. Note that for the last two cases, where the inverted dispersed duplication is guaranteed, the two possible alternate alleles are reverse complement&mdash;inversion&mdash;of each other, hence signatures of contig alignments along is not enough, and alignments of short reads within the affected region cannot break the degeneracy either.; So we need to attach left and right flanking regions to the affected region, and align short reads back to these two haplotypes and study the pair o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789:3755,down,down,3755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789,1,['down'],['down']
Availability,"e lists one by one until enough allele's are emptied, so that the number of genotypes does not surpasses a maximum based on the largest ploidy amongst the input samples. Of course, one would need to create some temporary data-structure to make the operation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267297). Those haplotype scores have not been throughly analyzed but we are already using them to discard haplotypes beyond the maximum allowed per graph kmer size so I don't see the harm in using the for further reduction. . Certainly is a step forward from just throwing an exception back to the user. However, we should output a Warning every time we need to do such a reduction just to keep track. ---. @sooheelee commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217443170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of further interest to their aims. For example, either for more exact mapping then variant calling or separate variant calling. I say this because a quick glance at the literature suggests this is potentially a highly variable region that may be present in multiple copies depending on species. It's a ribosomal DNA locus, that is, a site from which rRNA is transcribed. In mammals (humans & mice) it looks like it is a tandemly repeated locus residing on several chromosomes:. <img width=""424"" alt=""screenshot 2016-05-06 09 37 12"" src=""https://cloud.githubusercontent.com/assets/11543866/15074654/264e199a-136e-11e6-852e-431d8690f2aa.png"">. Some random references:; - [Concerted copy number variation balances ribosomal; DNA dosage in human and mouse genomes](http://www.pnas.org/content/112/8/2485.full.pdf); - [Haplotype Detection from Next-Generation Sequencing in High-Ploidy-Level Species: 45S rDNA Gene Copies in the Hexaploid Spartina maritima.](http:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:6990,mask,mask,6990,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['mask'],['mask']
Availability,"e mapping quality also considers how well the read aligns; to its best mapping. In places where a sample has a lot of nearby SNPs; compared to the reference the mapping qualities of the reads are low; compared to reads that contain fewer SNPs. I've been mulling over the; conflation of these two aspects of mapping quality for a while because it; biases our VQSR results, but maybe the new filtering models will be able to; figure it out. The b37 reference with decoy contigs is here:; /humgen/1kg/reference/human_g1k_v37_decoy.fasta.I believe that the; reference issue that required the decoy in the b37 1000G work was resolved; in the hg38 reference. This is an excellent topic to discuss with Heng; during his office hours when he gets back from China in a few weeks, but I; expect the SV team will also be helpful in the meantime. On Sun, Apr 23, 2017 at 11:14 PM, David Benjamin <notifications@github.com>; wrote:. > So. . . given that our pipeline aligns with BWA, it might seem like this; > is just a redundant and laborious rehashing of the mapping quality score.; >; > *However*, the mapping quality only considers multi-mapping within the; > reference, and therefore doesn't account for mapping errors due to; > incompleteness of the reference. That is, reads from genomic regions that; > are not part of the reference (because they're hard to assemble, like; > centromeres etc) might map well to a unique regions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2930:3327,redundant,redundant,3327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930,1,['redundant'],['redundant']
Availability,e(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.la,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:9135,ERROR,ERROR,9135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"e(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist.; 	at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:8060,ERROR,ERROR,8060,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['ERROR'],['ERROR']
Availability,"e-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2 – GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2) for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is: . gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processi ng\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gat k/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common \_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA \_B07.table ; ; 01:03:32.752 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar: file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compressi on.so ; ; Sep 12, 2021 1:03:32 AM shaded.cloud\_nio.com.goo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7479:1503,error,error,1503,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479,1,['error'],['error']
Availability,"e.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:4849,failure,failure,4849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['failure'],['failure']
Availability,e.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegrationTest.java:33); Caused by:; java.io.IOException: Error getting access token for service account: ; at shaded.cloud-nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:319); at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:149); at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:135); at shaded.cloud-nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); at com.google.cloud.HttpTransportOptions$1.initialize(HttpTransportOptions.java:149); at shaded.cloud-nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googlea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514:2053,Error,Error,2053,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514,1,['Error'],['Error']
Availability,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:2591,error,error,2591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,2,"['avail', 'error']","['available', 'error']"
Availability,"e.hellbender.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:54); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:41); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:432); 	at org.broadinstitute.hellbender.engine.VariantWalker.onStartup(VariantWalker.java:43); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); Caused by: shaded.cloud-nio.com.google.api.client.http.HttpResponseException: 400 Bad Request; {; ""error"" : ""invalid_grant"",; ""error_description"" : ""Token has been expired or revoked.""; }; 	at shaded.cloud-nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1061); 	at shaded.cloud-nio.com.google.auth.oauth2.UserCredentials.refreshAccessToken(UserCredentials.java:121); 	at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:97); 	at shaded.cloud-nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:74); 	at shaded.cloud-nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:65); 	at com.google.cloud.HttpServiceOptions$1.initialize(HttpServiceOptions.java:233); 	at shaded.cloud-nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415:2805,error,error,2805,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415,1,['error'],['error']
Availability,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287:2655,down,down,2655,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287,4,"['ERROR', 'down']","['ERROR', 'down']"
Availability,e.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecuto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4672,ERROR,ERROR,4672,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,e.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:11808,ERROR,ERROR,11808,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,e.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:10550,ERROR,ERROR,10550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"e/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad commented on [Mon Dec 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-268016064). Looking more closely, isn't this done already in #1377 ? @vruano ?. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287822737). @SHuang-Broad @vruano Status update on this?. ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287916452). @SHuang-Broad this is not fixed by #1377 as this makes reference to the selection executed by the AFCalculator... it might be that @davidbenjamin now AF calculator addressed the issue, but is also possible that he avoid it it entirely and just focused in the new QUAL calculation. . ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287921195). Looking at the code I made reference in GATK3, it seem that it is still faulty... I guess we need to take a look on whether in GATK4 has been fixed and then back-ported if people are interested. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287952396). Alright, thanks for the update. At this point we don't care too much about fixing it in GATK3; we're all about moving forward with GATK4. Do you want me to move this issue to GATK or do you already have an issue for this there?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288598906). The new qual score doesn't subset alleles at all because it doesn't need to. `AlleleSubsettingUtils` handles this upstream of the new qual. We're waiting on the HaplotypeCaller tie-out to eliminate the old qual from GATK 4, however. ---. @vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomme",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2958:4510,fault,faulty,4510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958,1,['fault'],['faulty']
Availability,e/libgkl_utils.so ; 1009 17:07:52.859 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/rds/project/rds-cyiwgCzJok8/WES_snakemake/.snakemake/conda/773770bb2edb9f4c58fb17b5017e ; 1010 1fbe_/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so ; 1011 17:07:52.860 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions ; 1012 17:07:52.860 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation ; 1013 17:07:52.867 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/rds/project/rds-cyiwgCzJok8/WES_snakemake/.snakemake/conda/773770bb2edb9f4c58fb17b5017e1f ; 1014 be_/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 1015 17:07:52.881 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions ; 1016 17:07:52.881 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 17:07:52.882 INFO IntelPairHmm - Available threads: 20 ; 17:07:52.882 INFO IntelPairHmm - Requested threads: 20 ; 17:07:52.882 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 17:07:52.941 INFO ProgressMeter - Starting traversal ; 17:07:52.941 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 17:07:54.578 WARN Fragment - More than two reads with the same name found. Using two reads randomly to combine as a fragment. ; 17:07:54.812 WARN Fragment - More than two reads with the same name found. Using two reads randomly to combine as a fragment. ; 17:07:54.812 WARN Fragment - More than two reads with the same name found. Using two reads randomly to combine as a fragment. ; 17:07:54.812 WARN Fragment - More than two reads with the same name found. Using two reads randomly to combine as a fragment. ; 17:07:54.812 WARN Fragment - More than two reads with the same name found. Using two reads randomly to combine as a fragment. ; 17:07:54.812 WARN Frag,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8966:4848,Avail,Available,4848,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8966,1,['Avail'],['Available']
Availability,"e; [November 7, 2019 11:30:15 AM CET] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2252865536; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:10454,error,errorProbability,10454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['error'],['errorProbability']
Availability,"eCaller - ------------------------------------------------------------; 14:39:56.485 INFO HaplotypeCaller - ------------------------------------------------------------; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; 14:39:56.486 INFO HaplotypeCaller - Picard Version: 2.23.3; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:39:56.486 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:56.486 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:39:56.486 INFO HaplotypeCaller - Inflater: IntelInflater; 14:39:56.486 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:39:56.487 INFO HaplotypeCaller - Requester pays: disabled; 14:39:56.487 INFO HaplotypeCaller - Initializing engine; 14:39:57.467 INFO HaplotypeCaller - Shutting down engine; [February 10, 2021 2:39:57 PM EST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1681391616; htsjdk.samtools.cram.CRAMException: Attempt to unmapped with non zero alignment start (0) or span (150); at htsjdk.samtools.cram.BAIEntry.<init>(BAIEntry.java:60); at htsjdk.samtools.cram.BAIEntry.<init>(BAIEntry.java:83); at htsjdk.samtools.cram.CRAIIndex.openCraiFileAsBaiStream(CRAIIndex.java:89); at htsjdk.samtools.SamIndexes.asBaiSeekableStreamOrNull(SamIndexes.java:91); at htsjdk.samtools.CRAMFileReader.initWithStreams(CRAMFileReader.java:202); at htsjdk.samtools.CRAMFileReader.<init>(CRAMFileReader.java:193); at htsjdk.samtools.SamReaderFactory$SamReaderFactoryImpl.open(SamReaderFactory.java:422); at htsjdk.samtools.SamReaderFactory.open(SamReaderFactory.java:105); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.<init>(ReadsPathDataSource.java:245); at org.broadinstitute.hellbender.engi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:3042,down,down,3042,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['down'],['down']
Availability,"eHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Version: 2.17.2; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Deflater: IntelDeflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Inflater: IntelInflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - GCS max retries/reopens: 20; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Initializing engine; 11:47:53.458 INFO CreateHadoopBamSplittingIndex - Done initializing engine; 11:47:53.463 INFO CreateHadoopBamSplittingIndex - Shutting down engine; [March 7, 2018 11:47:53 AM EST] org.broadinstitute.hellbender.tools.spark.CreateHadoopBamSplittingIndex done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1115160576; ***********************************************************************. A USER ERROR has occurred: Bad input: A splitting index is only relevant for a bam file, but a file with extension cram was specified. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506:3052,down,down,3052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"eSources(FeatureManager.java:208) ; ;     at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:155) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.initializeFeatures(ReadWalker.java:72) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:726) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). And I will get the same error when I assign the temp directory in another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:7345,error,error,7345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['error'],['error']
Availability,eWalker.java:99) ; ;     at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ;     at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ;     at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.traverse(FeatureWalker.java:97) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). Here's how my called segment file looks like:. CONTIG    START    END    NUM\_POINTS\_COPY\_RATIO    MEAN\_LOG2\_COPY\_RATIO    CALL ; ; 1    14645    13839497    2764    -0.121225    0 ; ; 1    13839498    55529537    8713    -0.060943    0 ; ; 1    55534430    142797736    6763    0.050711    0 ; ; 1    142803161    143164144    9    -1.797248    - ; ; 1    143186822    156929235    3970    -0.077460    0 ; ; 1    156929872    224009136    8811    0.024671    0 ; ; 1    224116102    224116470    1    -4.545156    - ; ; 1    224124170    249230997    3307    0.004490    0 ; ; 2    41203    137402680    14122    -0.000470    0 ; ; 2    137402681    215911009    8594    0.077261    0 ; ; 2    215914005    243081349    4299    -0.032370    0. I used GATK/4.2.4.1. Would you please kindly let me know the cause for the invalid interval error?Thanks a lot!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/271141'>Zendesk ticket #271141</a>)<br> gz#271141</i>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:6972,error,error,6972,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,1,['error'],['error']
Availability,"eader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:384); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:97); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; `gatk --java-options ""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" CollectReadCounts -R [...].fasta -L [...].interval_list -I Sample.bam --interval-merging-rule OVERLAPPING_ONLY -O Sample.counts.hdf5` where `Sample.bam` is a symlink to a BAM-file from an NFS-mounted location. #### Possible workaround; You can create symlink for the parent directory of the target BAM-file instead. #### Expected behavior; I would expect a better error message that would help to indicate the cause behind ""Size exceeds Integer.MAX_VALUE"" exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7579:3179,error,error,3179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579,1,['error'],['error']
Availability,"eaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:13997,failure,failure,13997,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['failure'],['failure']
Availability,eads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:10910,Recover,Recovered,10910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,eam.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7298:7538,Error,ErrorProbabilities,7538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298,1,['Error'],['ErrorProbabilities']
Availability,"eblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly available samples to demonstrate the issue. Let me know if this is needed. . #### Expected behavior; A large number of variants should be called as hom ref (`0/0`). #### Actual behavior; Many of these variants are left as not called / missing genotypes (`./.`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8208:2662,avail,available,2662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208,1,['avail'],['available']
Availability,"ecause I uploaded these exact files to the cloud after downloading them from the 1000 Genomes Project FTP site. Back then, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154:1037,ERROR,ERROR,1037,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154,1,['ERROR'],['ERROR']
Availability,ecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:4530,ERROR,ERROR,4530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"edulerBackend: Shutting down all executors; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Stopped; 17/10/11 14:19:38 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/10/11 14:19:38 INFO storage.MemoryStore: MemoryStore cleared; 17/10/11 14:19:38 INFO storage.BlockManager: BlockManager stopped; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 17/10/11 14:19:38 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/10/11 14:19:38 INFO spark.SparkContext: Successfully stopped SparkContext; 14:19:38.600 INFO PrintReadsSpark - Shutting down engine; [October 11, 2017 2:19:38 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.48 minutes.; Runtime.totalMemory()=986185728; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:31980,failure,failure,31980,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['failure'],['failure']
Availability,eems to be an `UnknownHostException` thrown in the google storage api client. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:567); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:556); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReade,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094:1102,avail,available,1102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094,1,['avail'],['available']
Availability,"egion, modifying reads as a side effect; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(assemblyActiveRegion. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(MTAC.maxMnpDistance);. // when we trim on the originalAssemblyRegion, the trimmingResult takes its un-modified reads!; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(originalAssemblyRegion, allVariationEvents, referenceContext);. // now the assemblyResult gets the unmodified reads of the trimmingResult!; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. If we want things like `-dont-use-soft-clipped-bases` to work, we should call `trimmer.trim` on `untrimmedAssemblyResult`. I think that change alone may be all we need. Let's look at the corresponding code in `HaplotypeCallerEngine`:. ```; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(region. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(hcArgs.maxMnpDistance);. // same things as Mutect2 — we trim on the unmodified region; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(region, allVariationEvents, referenceContext);. // same as Mutect2; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. In addition to the proposed simple fix, this brings up a few code smells:. * One would expect assembly not to modify its input reads, but it does through the side effect of `finalizeRegion`.; * Assembly has both the permanent changes of finalize region and the temporary changes of read error correction.; * `AssemblyResultSet` stores the reads but so does `AssemblyRegion`. Without doing any serious refactoring, perhaps `finalizeRegion` could at least be split off from assembly so that the latter does not stealthily modify reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6686:2719,error,error,2719,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6686,1,['error'],['error']
Availability,"egions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCJQob4WqdwDN0R8jvbNGT1l0vSCks5rzBOmgaJpZM4Lb8pz>; > .; >. ---. @davidbenjamin commented on [Wed May 03 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-298946022). Copying comments from closed issue #993. Instead of running an aligner in memory, let's first try preprocessing an alignability (to the ref + decoy) resource file. Then we can simply query this file at each called variant. > ENCODE used a kmer size of 36 bp, which is seriously obsolete and will tend to underestimate alignability. However, the GEM program (paper here: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030377 and binary here: http://algorithms.cnag.cat/wiki/The_GEM_library#Documentation and blog post on how to run it here: http://blog.kokocinski.net/index.php/sequence-mappability-alignability?blog=2) was used by ENCODE to produce this track and we can easily produce it ourselves with any kmer size and any mismatch threshold. > Furthermore, once we make this track we can store this track in memory eg as a `HashedListTargetCollection` and therefore we can query it for every read to get an annotation for the number of uniquely mappable reads (up to some error tolerance). > One more thing: we can also query based on the start position of each read's mate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2930:5634,error,error,5634,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930,2,"['error', 'toler']","['error', 'tolerance']"
Availability,"elSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:00:18.546 INFO ModelSegments - Deflater: IntelDeflater; 10:00:18.546 INFO ModelSegments - Inflater: IntelInflater; 10:00:18.546 INFO ModelSegments - GCS max retries/reopens: 20; 10:00:18.546 INFO ModelSegments - Requester pays: disabled; 10:00:18.546 INFO ModelSegments - Initializing engine; 10:00:18.546 INFO ModelSegments - Done initializing engine; 10:00:18.549 INFO ModelSegments - Reading file (/lustre/home/acct-medliuyb/medliuyb-user1/PP_JOB/HiC_PC/seq-data/GATK_practise/CNV/WGS_1_T.denoisedCR.tsv)...; 10:00:22.289 INFO ModelSegments - Reading file (/lustre/home/acct-medliuyb/medliuyb-user1/PP_JOB/HiC_PC/seq-data/GATK_practise/CNV/CollectAllelicCounts/WGS_1_T.allelicCounts.tsv)...; 10:19:45.750 INFO ModelSegments - Shutting down engine; [January 10, 2022 at 10:19:45 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 19.46 minutes.; Runtime.totalMemory()=2628889083904; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; 	at java.base/java.util.Arrays.copyOf(Arrays.java:3688); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.getReadyToExpandTo(ImmutableList.java:768); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.add(ImmutableList.java:787); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.add(ImmutableList.java:748); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableCollection$Builder.addAll(ImmutableCollection.java:456); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.ImmutableList$Builder.addAll(ImmutableList.java:847); 	at org.broadinstitute.hellbender.rel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7633:2783,down,down,2783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633,1,['down'],['down']
Availability,"eld of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:5698,Error,ErrorProbabilities,5698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['Error'],['ErrorProbabilities']
Availability,eliminate misguided use of LinkedLists in the downsampling package,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1493:46,down,downsampling,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1493,1,['down'],['downsampling']
Availability,"ellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:805); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:789). the deletion that is causing the error is 141 base pairs, and I noticed the length of the contig Funcotator is trying to retrieve (895) is equal to the UTR length + deletion length + 1, 753 + 141 + 1. When I looked at the source code around where the error occurs, I see where the length of the retrieved interval is defined (line 738): . > final SimpleInterval transcriptInterval = new SimpleInterval(; > transcriptMapIdAndMetadata.mapKey,; > transcriptMapIdAndMetadata.fivePrimeUtrStart,; > transcriptMapIdAndMetadata.fivePrimeUtrEnd + extraBases; > );. and the logic for how large that extraBases should be (line 1566):. >final int numExtraTrailingBases = variant.getReference().length() < defaultNumTrail ingBasesForUtrAnnotationSequenceConstruction ? defaultNumTrailingBasesForUtrAnnotationSequenceConst ruction : variant.getReference().length() + 1;. I believe line 1566 is the source of the problem; there is no check that UTR-end + deletion length extends past the end of the transcript. #### Steps to reproduce. download funcotator_dataSources.v1.6.20190124s from Broad FTP server. run funcotator using:. `Funcotator -R /tmp/GRCh38.fa -V broken.vcf -O broken.out.vcf --data-sources-path funcotator_dataSources.v1.6.20190124s/ --output-file-format VCF --ref-version hg38`. on a vcf with a single variant:. >chr17 7241460 . ACTGCAAAAGATACAAGATGCAAGAAAGTCACAGAGGTCAAAAATGCCCTCAAAAGAACAGCTGCTAGGTGGAGCCTCCTCCCGCAGAGACTGCACTCCCACCCACAGGAAGCAAGCCTGAGTCTTGGATCAGGTTCCCAC A . #### Expected behavior; Funcotator should not attempt to retrieve a sequence that extends past the end of a transcript. #### Actual behavior; Funcotator crashes because it attempts to retrieve sequence past the end of a transcript",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6345:3026,down,download,3026,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6345,1,['down'],['download']
Availability,"emaining; 11:34:35.285 INFO PSKmerUtils - 86.4% complete - 3043.0 million kmers at 75.7 million kmers/min, 6.35 min remaining; 11:34:56.716 INFO PSKmerUtils - 87.4% complete - 3078.4 million kmers at 75.9 million kmers/min, 5.87 min remaining; 11:35:22.166 INFO PSKmerUtils - 88.4% complete - 3113.7 million kmers at 76.0 million kmers/min, 5.40 min remaining; 11:35:47.066 INFO PSKmerUtils - 89.4% complete - 3149.1 million kmers at 76.1 million kmers/min, 4.92 min remaining; 11:36:09.985 INFO PSKmerUtils - 90.4% complete - 3184.4 million kmers at 76.2 million kmers/min, 4.45 min remaining; 11:36:35.287 INFO PSKmerUtils - 91.4% complete - 3220.8 million kmers at 76.3 million kmers/min, 3.97 min remaining; 11:36:57.953 INFO PSKmerUtils - 92.4% complete - 3256.1 million kmers at 76.5 million kmers/min, 3.50 min remaining; 11:37:19.588 INFO PSKmerUtils - 93.4% complete - 3291.3 million kmers at 76.7 million kmers/min, 3.03 min remaining; 11:37:41.529 INFO PSKmerUtils - 94.4% complete - 3326.5 million kmers at 76.8 million kmers/min, 2.57 min remaining; 11:38:03.288 INFO PSKmerUtils - 95.4% complete - 3361.8 million kmers at 77.0 million kmers/min, 2.10 min remaining; 11:38:24.891 INFO PSKmerUtils - 96.4% complete - 3397.0 million kmers at 77.2 million kmers/min, 1.64 min remaining; 11:38:46.744 INFO PSKmerUtils - 97.4% complete - 3432.3 million kmers at 77.3 million kmers/min, 1.18 min remaining; 11:39:08.040 INFO PSKmerUtils - 98.4% complete - 3467.5 million kmers at 77.5 million kmers/min, 0.73 min remaining; 11:39:29.389 INFO PSKmerUtils - 99.4% complete - 3502.7 million kmers at 77.7 million kmers/min, 0.27 min remaining; 11:39:42.728 INFO PathSeqBuildKmers - Theoretical Bloom filter false positive probability: 1.0794569119941353E-4; 11:40:30.038 INFO PathSeqBuildKmers - Shutting down engine; [February 15, 2023 11:40:30 AM CST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqBuildKmers done. Elapsed time: 50.66 minutes.; Runtime.totalMemory()=58112606208; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8204:11656,down,down,11656,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204,1,['down'],['down']
Availability,embly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 danglin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:16345,Recover,Recovered,16345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"emoryStore started with capacity 2004.6 MB; 18/01/09 18:30:55 INFO spark.SparkEnv: Registering OutputCommitCoordinator; 18/01/09 18:30:55 INFO util.log: Logging initialized @25356ms; 18/01/09 18:30:55 INFO server.Server: jetty-9.3.z-SNAPSHOT; 18/01/09 18:30:55 INFO server.Server: Started @25495ms; 18/01/09 18:30:55 INFO server.AbstractConnector: Started ServerConnector@283ab206{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:30:55 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@418f0534{/jobs,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@134a8ead{/jobs/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54247647{/jobs/job,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5463f035{/jobs/job/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:7557,AVAIL,AVAILABLE,7557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"emove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnnotation -G StandardHCAnnotation \; -ERC GVCF \; --verbosity INFO \; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:5880,echo,echo,5880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['echo'],['echo']
Availability,"en the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7247:1431,error,error,1431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247,1,['error'],['error']
Availability,en/pool_sequence_nov2016/data/gVCF/pl_GV_0036_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_CW_0037_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_DL_0038_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_KS_0039_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_OF_0040_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_WR_0041_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_RI_0042_chr26.raw.g.vcf -nt 10 --max_genotype_count 1024 -L chr26 --dbsnp /usr/users/geibel/chicken/chickenrefgen/ENSEMBL_20170106/Gallus_gallus.updated.vcf -o /usr/users/geibel/chicken/pool_sequence_nov2016/data/rawVCF/IndandPool_chr26.raw.vcf ; ```. The user actually includes a shell script in the test data bundle called `JointGenotyping_chr26.sh`. ---; ### The error shows:; ```; ##### ERROR --; ##### ERROR stack trace ; java.lang.IllegalArgumentException: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; 	at org.broadinstitute.gatk.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:319); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:461); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:164); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2946:9899,error,error,9899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"ence allele is too long (220) at position chr2_KI270894v1_alt:204859; skipping that record. Set --reference_window_stop >= 220 ; INFO 21:38:54,237 LeftAlignAndTrimVariants - Reference allele is too long (262) at position chr2_KI270894v1_alt:207863; skipping that record. Set --reference_window_stop >= 262 ; 0 variants were aligned; INFO 21:38:54,554 ProgressMeter - done 3.31246907E8 31.8 m 5.0 s 99.7% 31.8 m 5.0 s ; INFO 21:38:54,554 ProgressMeter - Total runtime 1905.29 secs, 31.75 min, 0.53 hours ; ------------------------------------------------------------------------------------------; Done. There were 4 WARN messages, the first 4 are repeated below.; WARN 17:39:57,688 IndexDictionaryUtils - Track variant doesn't have a sequence dictionary built in, skipping dictionary validation ; WARN 18:13:42,039 SimpleTimer - Clock drift of -1,503,348,737,016,211,299 - -1,503,346,772,578,127,937 = 1,964,438,083,362 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 20:14:18,043 SimpleTimer - Clock drift of -1,503,355,916,564,964,097 - -1,503,348,737,015,111,124 = 7,179,549,852,973 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 21:10:35,064 SimpleTimer - Clock drift of -1,503,359,203,412,549,926 - -1,503,355,916,564,817,209 = 3,286,847,732,717 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; ------------------------------------------------------------------------------------------; WMCF9-CB5:Mutect2 shlee$ ; ```. ### Notice the following line from above. > 0 variants were aligned. Also, it would be great if the tool, which appears to keep track of the lengths of reference alleles that are too long, could give me the **maximum length** reference allele so that I can go back and set the `--reference_window_stop` argument appropriately in a second round so that I can left-align _all_ of my variants. . ### MD5 and loo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487:7677,checkpoint,checkpoint,7677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487,1,['checkpoint'],['checkpoint']
Availability,"encode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.422 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.433 INFO ProgressMeter - Starting traversal; 18:53:59.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 18:54:01.952 INFO IndexFeatureFile - Shutting down engine; [March 8, 2021 at 6:54:01 PM CET] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 min",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7134:1998,error,errors,1998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134,1,['error'],['errors']
Availability,"encode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.679 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.684 INFO ProgressMeter - Starting traversal; 14:34:51.684 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:34:51.694 INFO IndexFeatureFile - Shutting down engine; [August 2, 2021 at 2:34:51 PM CEST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=113246208; java.lang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7385:1761,error,errors,1761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385,1,['error'],['errors']
Availability,"engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource; .java:64); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.; getFivePrimeUtrSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:744); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createUtrFuncotation(GencodeFuncotationFactory.java:1568); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnSingleTranscript(GencodeFuncotationFactory.java:983); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:805); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:789). the deletion that is causing the error is 141 base pairs, and I noticed the length of the contig Funcotator is trying to retrieve (895) is equal to the UTR length + deletion length + 1, 753 + 141 + 1. When I looked at the source code around where the error occurs, I see where the length of the retrieved interval is defined (line 738): . > final SimpleInterval transcriptInterval = new SimpleInterval(; > transcriptMapIdAndMetadata.mapKey,; > transcriptMapIdAndMetadata.fivePrimeUtrStart,; > transcriptMapIdAndMetadata.fivePrimeUtrEnd + extraBases; > );. and the logic for how large that extraBases should be (line 1566):. >final int numExtraTrailingBases = variant.getReference().length() < defaultNumTrail ingBasesForUtrAnnotationSequenceConstruction ? defaultNumTrailingBasesForUtrAnnotationSequenceConst ruction : variant.getReference().length() + 1;. I believe line 1566 is the source of the problem; there is no check that UTR-end + deletion length extends past the end of the transcript. #### Steps to reproduce. download funcotator_dataSources.v1.6.20190124s from Broad FTP server. run funco",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6345:2038,error,error,2038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6345,1,['error'],['error']
Availability,"engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.str",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:4147,down,down,4147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,1,['down'],['down']
Availability,"ens: 20; 10:34:24.822 INFO Mutect2 - Requester pays: disabled; 10:34:24.823 INFO Mutect2 - Initializing engine; 10:34:25.945 INFO FeatureManager - Using codec BEDCodec to read file file:///scratch/alcalan/nextflow_work/e9/a28e7174a34d0d29fe9a0d8a506d46/test_err.bed; 10:34:25.960 INFO IntervalArgumentCollection - Processing 153 bp from intervals; 10:34:25.987 INFO Mutect2 - Done initializing engine; 10:34:26.188 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/alcalan/.conda/mutect2-cd161e2f51ff2240ce6390abc942bbdd/share/gatk4-4.1.5.0-1/gatk-package-4.1.5.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:34:26.190 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/alcalan/.conda/mutect2-cd161e2f51ff2240ce6390abc942bbdd/share/gatk4-4.1.5.0-1/gatk-package-4.1.5.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 10:34:26.264 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 10:34:26.267 INFO IntelPairHmm - Available threads: 8; 10:34:26.267 INFO IntelPairHmm - Requested threads: 4; 10:34:26.267 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 10:34:26.375 INFO ProgressMeter - Starting traversal; 10:34:26.375 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 10:34:26.950 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 10:34:26.950 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 10:34:26.950 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.03 sec; 10:34:26.951 INFO Mutect2 - Shutting down engine; [March 23, 2020 10:34:26 AM CET] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=1214251008; java.lang.StringIndexOutOfBoundsException: String index out of range: -1; 	at java.lang.String.substring(String.java:1927); 	at org.broadinstitute.hellbender.tools.walkers.annotator.TandemRepe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6516:4205,Avail,Available,4205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6516,1,['Avail'],['Available']
Availability,"ent with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. We can either change the yml location or modify the readme.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4741:1234,down,download,1234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741,2,['down'],['download']
Availability,"entation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console...; INFO 10:47:55,875 GenomeAnalysisEngine - Deflater: IntelDeflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Inflater: IntelInflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Strictness is SILENT ; INFO 10:47:56,246 GenomeAnalysisEngine - Downsampling Settings: No downsampling ; INFO 10:47:56,255 SAMDataSource$SAMReaders - Initializing SAMRecords in serial ; INFO 10:47:56,333 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.07 ; ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A USER ERROR has occurred (version 3.8-0-ge9d806836): ; ##### ERROR; ##### ERROR This means that one or more arguments or inputs in your command are incorrect.; ##### ERROR The error message below tells you what is the problem.; ##### ERROR; ##### ERROR If the problem is an invalid argument, please check the online documentation guide; ##### ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.; ##### ERROR; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.; ##### ERROR; ##### ERROR MESSAGE: Input files reads and reference have incompatible contigs. Please see https://software.broadinstitute.org/gatk/documentation/article?id=63for more information. Error details: No overlapping contigs found.; ##### ERROR reads contigs = [LmjF04_01_20050601_V5.2, LmjF05_01_20050601_V5.2, LmjF24_01_20050601_V5.2, LmjF01_01_20050601_V5.2, LmjF03_01_20050601_V5.2, LmjF13_01_20050601_V5.2, LmjF14_01_20050601_V5.2, LmjF19_01_20050601_V5.2, LmjF21_01_20050601_V5.2, LmjF23_01_20050601_V5.2, LmjF10_01_20050601_V5.2, LmjF11_01_20050601_V5.2, LmjF15_01_20050601_V5.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6798:2975,ERROR,ERROR,2975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6798,3,['ERROR'],['ERROR']
Availability,"ential additional information: /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr.; Could not retrieve content: Could not read from /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr: /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr; [2020-07-14 05:09:46,38] [info] WorkflowManagerActor WorkflowActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674 is in a terminal state: WorkflowFailedState; [2020-07-14 05:09:51,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2020-07-14 05:09:55,28] [info] Workflow polling stopped; [2020-07-14 05:09:55,30] [info] 0 workflows released by cromid-ca5c695; [2020-07-14 05:09:55,30] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2020-07-14 05:09:55,30] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2020-07-14 05:09:55,30] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2020-07-14 05:09:55,31] [info] JobExecutionTokenDispenser stopped; [2020-07-14 05:09:55,31] [info] Aborting all running workflows.; [2020-07-14 05:09:55,31] [info] WorkflowStoreActor stopped; [2020-07-14 05:09:55,31] [info] WorkflowLogCopyRouter stopped; [2020-07-14 05:09:55,31] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor All workflows finished; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor stopped; [2020-07-14 05:09:55,53] [info] Connection pools shut down; [2020-07-14 05:09:55,53] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] SubWorkflowStoreActor stopped; [2020-07-14 05:09:55,54] [info] JobStoreActor stopped; [2020-07-14 05:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:8014,down,down,8014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,3,['down'],['down']
Availability,ents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:5201,ERROR,ERROR,5201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:16119,down,download,16119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['down'],['download']
Availability,"eption); ------------. Hello,. When running CollectGcBiasMetrics on a moderately sized sam file (~500Mb), picard gives ArrayIndexOutOfBoundsException and ""Exception counting mismatches for read ..."". The SCAN\_WINDOW\_SIZE=1000. When it's set to default value 100, the error message is slightly different but ArrayIndexOutOfBoundsException persists. I have also experimented with different window sizes, all values >1000 give same error at the same read on chrX (details below). The reference fasta file is taken from UCSC: [https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz](https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz). Any feedback leading to resolving the issue is greatly appreciated. a) Picard version:. 2.21.6-SNAPSHOT. b) Command script:. java -jar picard.jar CollectGcBiasMetrics \\ ; ; I=sorted.sam \\ ; ; O=gc\_bias\_metrics.txt \\ ; ; CHART=gc\_bias\_metrics.pdf \\ ; ; S=summary\_metrics.txt \\ ; ; R=hg19.fa \\ ; ; SCAN\_WINDOW\_SIZE=1000. c) Error log:. MINIMUM\_GENOME\_FRACTION=1.0E-5 IS\_BISULFITE\_SEQUENCED=false METRIC\_ACCUMULATION\_LEVEL=\[ALL\_READS\] ALSO\_IGNORE\_DUPLICATES=false ASSUME\_SORTED=true STOP\_AFTER=0 VERBOSITY=INFO QUIET=false VALIDATION\_STRINGENCY=STRICT COMPRESSION\_LEVEL=5 MAX\_RECORDS\_IN\_RAM=500000 CREATE\_INDEX=false CREATE\_MD5\_FILE=false GA4GH\_CLIENT\_SECRETS=client\_secrets.json USE\_JDK\_DEFLATER=false USE\_JDK\_INFLATER=false ; ; \[Tue Jan 07 16:48:19 PST 2020\] Executing as [akoch@hpc5-0-3.local](mailto:akoch@hpc5-0-3.local) on Linux 2.6.32-431.11.2.el6.x86\_64 amd64; OpenJDK 64-Bit Server VM 1.8.0\_181-b13; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.21.6-SNAPSHOT ; ; INFO 2020-01-07 16:51:24 SinglePassSamProgram Processed 1,000,000 records. Elapsed time: 00:00:33s. Time for last 1,000,000: 27s. Last read position: chr5:92,832,908 ; ; INFO 2020-01-07 16:51:53 SinglePassSamProgram Processed 2,000,000 records. Elapsed time: 00:01:01s. Time for last 1,0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6372:1280,Error,Error,1280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6372,1,['Error'],['Error']
Availability,eption; at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:98); at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:49); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.fillCache(PushToPullIterator.java:72); at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.advanceToNextElement(PushToPullIterator.java:58); at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.<init>(PushToPullIterator.java:37); at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:21); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.he,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6851:5667,down,downsampling,5667,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6851,1,['down'],['downsampling']
Availability,"er - 1 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 58 total reads filtered; 03:56:46.621 INFO ProgressMeter - 13:115070262 0.0 4029 200614.1; 03:56:46.621 INFO ProgressMeter - Traversal complete. Processed 4029 total regions in 0.0 minutes.; 03:56:46.646 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0011332; 03:56:46.646 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0031919; 03:56:46.647 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.01 sec; 03:56:46.647 INFO HaplotypeCaller - Shutting down engine; [January 6, 2023 3:56:46 AM GMT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=395837440; ```. Since OrientationBiasReadCounts replaced OxoGReadCounts in GATK 4.1.1.0, we tested this version as well. It delievered the expected results, with variants reporting F1R2/F2R1:; ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT sample; 13 32911888 . A G 177.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.086;DP=21;ExcessHet=3.0103;FS=1.719;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=8.46;ReadPosRankSum=0.475;SOR=0.368 GT:AD:DP:F1R2:F2R1:GQ:PL; 0/1:13,8:21:6,6:7,2:99:185,0,339; 13 32913055 . A G 402.03 . AC=2;AF=1.00;AN=2;DP=15;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=26.80;SOR=1.112 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1:0,15:15:0,12:0,2:45:416,45,0; 13 32915005 . G C 378.02 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:F1R2:F2R1:GQ:PL 1/1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:7010,down,down,7010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['down'],['down']
Availability,"er - Loading libgkl_pairhmm_omp.so from jar:file:/home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:20.423 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 15:47:20.423 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:20.423 INFO IntelPairHmm - Available threads: 40; 15:47:20.423 INFO IntelPairHmm - Requested threads: 4; 15:47:20.423 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:22.213 INFO ProgressMeter - Starting traversal; 15:47:22.213 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:22.231 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 15:47:22.231 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 15:47:22.239 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 15:47:22.240 INFO HaplotypeCaller - Shutting down engine; [May 13, 2020 at 3:47:22 p.m. EDT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.38 minutes.; Runtime.totalMemory()=3212836864; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:290); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:984); 	at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6604:6897,down,down,6897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6604,1,['down'],['down']
Availability,"er - Total runtime 1905.29 secs, 31.75 min, 0.53 hours ; ------------------------------------------------------------------------------------------; Done. There were 4 WARN messages, the first 4 are repeated below.; WARN 17:39:57,688 IndexDictionaryUtils - Track variant doesn't have a sequence dictionary built in, skipping dictionary validation ; WARN 18:13:42,039 SimpleTimer - Clock drift of -1,503,348,737,016,211,299 - -1,503,346,772,578,127,937 = 1,964,438,083,362 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 20:14:18,043 SimpleTimer - Clock drift of -1,503,355,916,564,964,097 - -1,503,348,737,015,111,124 = 7,179,549,852,973 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 21:10:35,064 SimpleTimer - Clock drift of -1,503,359,203,412,549,926 - -1,503,355,916,564,817,209 = 3,286,847,732,717 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; ------------------------------------------------------------------------------------------; WMCF9-CB5:Mutect2 shlee$ ; ```. ### Notice the following line from above. > 0 variants were aligned. Also, it would be great if the tool, which appears to keep track of the lengths of reference alleles that are too long, could give me the **maximum length** reference allele so that I can go back and set the `--reference_window_stop` argument appropriately in a second round so that I can left-align _all_ of my variants. . ### MD5 and looking into the files, we see input and output are different and in fact the tool did change allele representations:; ```; WMCF9-CB5:Mutect2 shlee$ gzcat zeta_af-only-gnomad_Hg19toGRCh38.vcf.gz | grep -v '##' > zeta_headless.txt; WMCF9-CB5:Mutect2 shlee$ md5 zeta_headless.txt ; MD5 (zeta_headless.txt) = 6d93f1ea32c99ae5020881fa8265bdc1. WMCF9-CB5:Mutect2 shlee$ gzcat eta_af-only-gnomad_Hg19toGRCh38.vcf.gz | grep -v '##' > eta_headless.txt; WMCF9-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487:8125,checkpoint,checkpoint,8125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487,1,['checkpoint'],['checkpoint']
Availability,"er addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCF. ### Affected version(s); 4.1.7; 4.1.8; 4.1.9. ### Description ; Starting with GATK4.1.7, the AF annotation in the changed from '0' to '.'. This change is cause downstream issues with our processing pipeline. #### Steps to reproduce; CMD using 4.1.6:; gatk GenotypeGVCFs --variant proband_mother_duo.HC.g.vcf.gz -R hs38DH.fa --dbsnp dbsnp151_common.hg38.vcf.gz -O proband_mother_duo_GATK4.1.6.HC.vcf.gz. Looking at one of the sites causing this downstream issue:; CMD; gzcat proband_mother_duo_GATK4.1.6.HC.vcf.gz | grep 83598622; OUTPUT:; chr4 83598622 . AT ATT,A 1337.45 . AC=1,1;AF=0.250,0.250;AN=4;BaseQRankSum=1.26;ClippingRankSum=0.074;DP=145;ExcessHet=4.7712;FS=5.235;GQ_MEAN=625.00;LikelihoodRankSum=1.34;MLEAC=1,1;MLEAF=0.250,0.250;MQ=60.00;MQ0=0;MQRankSum=0.00;NCC=0;NCount=0;QD=10.06;ReadPosRankSum=1.56;SOR=0.375 GT:AD:AF:DP:F1R2:F2R1:GQ:PL 0/2:33,0,35:0.515,0:68:12,15,0:21,18,0:99:713,812,1542,0,731,625 0/1:32,33,0:0.493,0.00:67:9,23,0:19,8,0:99:640,0,588,728,747,1569. CMD using 4.1.7:; gatk GenotypeGVCFs --variant proband_mother_duo.HC.g.vcf.gz -R hs38DH.fa --dbsnp dbsnp151_common.hg38.vcf.gz -O proband_mother_duo_GATK4.1.7.HC.vcf.gz. Looking at one of the sites causing this downstream issue:; CMD; gzcat proband_mother_duo_GATK4.1.7.HC.vcf.gz | grep 83598622; OUTPUT:; chr4 83598622 . AT ATT,A 1337.45 . AC=1,1;AF=0.250,0.250;AN=4;BaseQRankSum=1.26;ClippingRankSum=0.074;DP=145;ExcessHet=4.7712;FS=5.235;GQ_MEAN=625.00;LikelihoodR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6938:1676,down,downstream,1676,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6938,1,['down'],['downstream']
Availability,"er approach to select best haplotypes that can handle complex graph we might well not need to prune low supported hap early as they seemly they won't be selected if the are not amongst the best haplotypes. . B.1 Now that still would produce a considerable number of unlikely haplotypes that would cause a CPU burden. That can be changed by imposing another kinds of limit, For example we include all haplotypes with scores (likelihoods) that are Q0 - Q40 or we include haplotypes until the sum of their likelihoods is larger than the 99.99% probability mass. . B.2 This could provide a downstream solution to the problem caused by ranging heads recovery (explained above in A.2). B.3 If pruning is to be maintained, it makes more sense to do it at the very end after all dangling ends hav been recovered and the edges supports are finalized. Of course I assuming here that dangling end recovery does the sensible think of updating those supports are the graphs is modified. C. The use of Smith-Waterman in dangling end recovery does not seem totally optimal or even needed. . C.1 Recovering tails quite often this finish with the same sequence as the reference path because in fact they are supposed to end like that by construction (reads are trimmed by AR coordinates). For example, this can be cause because due to the k-mer size there is not enough based after variation for the paths to merge back. In this case you can simply merge the last vertices of the tail and the reference, faster and potentially more accurate. . C.2 Similarly dangling heads, at least part of the sequence of those dangling heads are clearly threadable back into the graph without the need of SW. For example look at the AA…AAAAAGA sequence in the picture below. . C.3 PairHMM runs in effect are performing SW kind of computations and so it is totally possible to use its partial result to find good alignment of dangling ends back to other parts of the graph without the need of running a separate SW thus saving time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/264:2035,recover,recovery,2035,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/264,2,"['Recover', 'recover']","['Recovering', 'recovery']"
Availability,"er.ContextHandler: Started o.s.j.s.ServletContextHandler@134a8ead{/jobs/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54247647{/jobs/job,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5463f035{/jobs/job/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:8214,AVAIL,AVAILABLE,8214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,er.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). ### Error log 2.; [2021年12月20日 下午08时36分52秒] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 7.54 minutes.; Runtime.totalMemory()=4957667328; htsjdk.samtools.util.RuntimeIOException: java.util.zip.DataFormatException: invalid code lengths set; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:161); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132); 	at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:84); 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:300); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7614:5209,avail,available,5209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7614,1,['avail'],['available']
Availability,"er.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. Driver stacktrace:; 17/10/11 14:19:38 INFO spark.ExecutorAllocationManager: Existing executor 2 has been removed (new total is 0); 17/10/11 14:19:38 INFO scheduler.DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:203, took 19.909238 s; 17/10/11 14:19:38 INFO ui.SparkUI: Stopped Spark web UI at http://10.131.101.159:4040; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Stopped; 17/10/11 14:19:38 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/10/11 14:19:38 INFO storage.MemoryStore: MemoryStore cleared; 17/10/11 14:19:38 INFO storage.BlockManager: BlockManager stopped; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 17/10/11 14:19:38 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/10/11 14:19:38 INFO spark.SparkContext: Successfully stopped SparkContext; 14:19:38.600 INFO PrintReadsSpark - Shutting down engine; [October 11, 2017 2:19:38 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.48 minutes.; Runtime.totalMemory()=986185728; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:31105,down,down,31105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['down'],['down']
Availability,er.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(Forwa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:10058,ERROR,ERROR,10058,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,er.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(Forwa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:8800,ERROR,ERROR,8800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"er@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@642ec6{/stages/stage/kill,null,AVAILABLE,@Spark}; 10:33:07.389 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3fe5ad73{/metrics/json,null,AVAILABLE,@Spark}; 10:33:07.397 INFO SortSamSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 10:33:07.450 INFO GoogleHadoopFileSystemBase - GHFS version: 1.9.4-hadoop3; 10:33:08.183 INFO MemoryStore - Block broadcast_0 stored as values in memory (estimated size 268.7 KiB, free 1076.2 GiB); 10:33:08.581 INFO MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:47292,AVAIL,AVAILABLE,47292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,erMutectCalls done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2252865536; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:10567,Error,ErrorProbabilities,10567,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Error'],['ErrorProbabilities']
Availability,er] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildAction,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:6596,ERROR,ERROR,6596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"erates on a whole region. And a post hoc check in HC would be simple enough for SNVs, but what happens when the ambiguous site is part of a larger deletion?. Needs advice on what the behavior / solution should be by @akiezun @vruano . This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/discussion/4858/reference-bases-with-ambiguity-codes-in-dbsnp/p1) . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/829#issuecomment-85093784). In general, don't know how HC behaves with ambiguous reference bases at all.... I would not be surprised if it just crashes or outputs garbage. Perhaps this should be part of a larger effort to make sure HC, Combine- and GenotypeGVCFs are robust on ambiguous calls. To start, currently GATK/Picard handles bases as uppercase single `byte' representation of the corresponding character. Since we are investing (a mostly wasting) 8 bits already, we could change into a bit mask representation that would allow for quick comparison of ambiguous and non-ambigous base call using bit-wise operations. NO_CALL = 0, A = 1, C = 2, G = 4, T/U = 8, N = 15, etc... . Handling ambiguous reference base calls... IMO the easiest and clearest is to disambiguate using a standard alphabetical priority, A, C, G or T whichever is the first compatible base is the reference. Then we just generate non-ambigous output accordingly to this choice. . We can provide separate tools to re-ambiguate the output or reselect the reference allele as the population major allele, so making the user very aware of this. For example he/she should have an decision-making input as to how we are supposed to handle het calls where both alleles are compatible with the reference ambiguous call; I don't think is totally correct to think of these as *hom*-ref calls but if that is what the user wants... Handling ambiguous calls in the reads... I presume that these have low quality and thus are ignored, and if not we should",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2914:1469,mask,mask,1469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2914,1,['mask'],['mask']
Availability,"erencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.MultiVariantWalker.traverse(MultiVariantWalker.java:118); 	at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.traverse(MultiVariantWalkerGroupedOnStart.java:163); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar. I was planning on Combining 25 samples at a time (per library), out of 250 (Not sure if this was even the right move). But the issue comes up for only certain libraries. All the samples (250) were processed exactly the same way. Not sure what is causing it to throw out this error for some only.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913:11165,error,error,11165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913,1,['error'],['error']
Availability,"erencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Expected behavior. Runs to completed and writes out model file. #### Actual behavior; _Tell us what happens instead_. The following error occurs....; ```; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.Abstr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:16734,error,error,16734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['error'],['error']
Availability,erlapping(SparkSharder.java:128); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.shard(SparkSharder.java:101); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.getVariants(VariantWalkerSpark.java:129); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.runTool(VariantWalkerSpark.java:160); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.runPipeline(VariantWalkerSpark.java:57); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dfac787d-19aa-4296-8078-c033cd9f440d] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:12767,ERROR,ERROR,12767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,2,['ERROR'],['ERROR']
Availability,"error messages about ""tranches"" when I'm running VariantRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7225:0,error,error,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7225,1,['error'],['error']
Availability,error: unmappable character for encoding ASCII,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4434:0,error,error,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434,1,['error'],['error']
Availability,"error=TRUE and ran them through PathSeq. I generated an identical six datasets using Rsubread with simulate.sequencing.error=FALSE. . #### Expected behavior; For the six datasets with simulate.sequencing.error=TRUE, I would expect a small number of reads to be filtered for each step. For the six datasets with simulate.sequencing.error=FALSE, I would expect similar results but with even fewer reads to be filtered for the low-quality or low complexity read filter. #### Actual behavior; For the six datasets with simulate.sequencing.error=TRUE, 8,496 - 18,103 reads were filtered by the low complexity or low quality filter (the Salmonella datasets were on the lower end and the Fusobacterium datasets were on the higher end), 115 - 311 reads were filtered by the host k-mer filter and 886 - 1822 reads were filtered by the duplicate read filter. . The number of reads filtered by the low complexity or low quality filter seemed high to me so I repeated the analysis with simulate.sequencing.error=FALSE. For these six datasets, all 100,000 reads are filtered by the low-quality or low complexity read filter. . #### Steps to reproduce; I wrote the workflow using snakemake and conda. In theory, you should be able to reproduce the error using `snakemake --use-conda`; Snakefile; ```; from os.path import join; import pandas as pd. ATCC25586_CDS_URL = ""ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/007/325/GCF_000007325.1_ASM732v1/GCF_000007325.1_ASM732v1_cds_from_genomic.fna.gz""; SL1344_CDS_URL = ""ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/855/GCF_000210855.2_ASM21085v2/GCF_000210855.2_ASM21085v2_cds_from_genomic.fna.gz""; LT2_CDS_URL = ""ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/006/945/GCF_000006945.2_ASM694v2/GCF_000006945.2_ASM694v2_cds_from_genomic.fna.gz"". CDS_FA = join(""data"", ""{patient}_cds_from_genomic.fa""); SL1344_CDS_FA = CDS_FA.format(patient=""SL1344""); ATCC25586_CDS_FA = CDS_FA.format(patient=""ATCC25586""); LT2_CDS_FA = CDS_FA.format(patient=""LT2""); FQ1_PREFIX =",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6705:1600,error,error,1600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6705,1,['error'],['error']
Availability,"erties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:291); ```. This issue was discovered while trying to add spanning deletion genotyping support to HaplotypeCaller for https://github.com/broadinstitute/gatk/issues/2960 and resolution seems to be necessary to supp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4716:1211,Error,Error,1211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716,1,['Error'],['Error']
Availability,"ervletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@642ec6{/stages/stage/kill,null,AVAILABLE,@Spark}; 10:33:07.389 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3fe5ad73{/metrics/json,null,AVAILABLE,@Spark}; 10:33:07.397 INFO SortSamSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 10:33:07.450 INFO GoogleHadoopFileSystemBa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:47046,AVAIL,AVAILABLE,47046,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"ervletContextHandler@462f8fe9{/stages/stage,null,AVAILABLE,@Spark}; 10:33:07.358 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@b2e1df3{/stages/stage/json,null,AVAILABLE,@Spark}; 10:33:07.359 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6cf3b3d7{/stages/pool,null,AVAILABLE,@Spark}; 10:33:07.360 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:46555,AVAIL,AVAILABLE,46555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"es may indicate which ones are a better choice amongst the ""loosers"" we throw that info away.; ### Proposed solution. Simply do a quick and dirty AF estimation and choose the alleles with the larger frequencies. This estimate should use all the genotype likelihoods rather than just the top genotype giving a nominal score for all the alleles that would allow us to sort them all and make a better and less arbitrary selection. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260486156). I believe this was done by @vruano and @SHuang-Broad already -- right guys? Can we close this? . ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260684553). My understanding of the current state is that there are several possible places with alt allele reduction in HC, in order:; 1. The fix I put in, to prevent the calculator from becoming too slow or blow up, so downstream steps won't even include these alleles in their likelihood calculations;; 2. The fix Valentine put in, which happens after the read likelihoods are calculated (and optionally down-sampled). The relevant code is in `HaplotypCallerGenotypingEngine.java` around lines 267-279. This is the state in GATK3, in GATK4 the second possibility is not ported yet. Regarding alt allele reduction in AF calculator, has [this](https://github.com/broadinstitute/gatk/pull/1918) been ported back to GATK3?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260688221). By ""possible place"" I mean they don't always remove alt alleles, just when certain conditions are met, and are independent. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260781482). I don't think https://github.com/broadinstitute/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad com",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2958:2541,down,downstream,2541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958,1,['down'],['downstream']
Availability,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:10710,ERROR,ERROR,10710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,2,['ERROR'],['ERROR']
Availability,"eset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.O",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:3013,failure,failure,3013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['failure'],['failure']
Availability,"essMeter - chr1:145584694 70.1 174825000 2494654.9; 20:50:40.338 INFO ProgressMeter - chr1:146077591 70.2 175095000 2492576.7; ...; 21:17:56.047 INFO ProgressMeter - chr6:31828873 97.5 222011000 2276839.7; 21:18:06.108 INFO ProgressMeter - chr6:31829365 97.7 222402000 2276934.0; 21:18:16.447 INFO ProgressMeter - chr6:31829459 97.8 222828000 2277277.9; 21:18:26.537 INFO ProgressMeter - chr6:31871533 98.0 223264000 2277819.0; 21:18:36.548 INFO ProgressMeter - chr6:32031575 98.2 223609000 2277462.0; 21:18:46.550 INFO ProgressMeter - chr6:33694905 98.4 223890000 2276459.3; 21:18:56.882 INFO ProgressMeter - chr6:42652593 98.5 224124000 2274855.2; 21:19:06.922 INFO ProgressMeter - chr6:46150821 98.7 224356000 2273348.8; 21:19:16.925 INFO ProgressMeter - chr6:56607518 98.9 224691000 2272903.7; 21:19:26.951 INFO ProgressMeter - chr6:72182472 99.0 224990000 2272087.7; 21:19:36.956 INFO ProgressMeter - chr6:73519441 99.2 225334000 2271736.1; 21:19:44.136 INFO SplitNCigarReads - Shutting down engine; [September 14, 2023 9:19:44 PM BST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 99.33 minutes.; Runtime.totalMemory()=5453119488; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:53); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); at org.broadinstitute.hellbender.cmdline.CommandLine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522:8675,down,down,8675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522,1,['down'],['down']
Availability,events.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:5034,ERROR,ERROR,5034,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,events.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:5207,ERROR,ERROR,5207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,execute_with_retry() error handling improvements [VS-159],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7480:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7480,1,['error'],['error']
Availability,"extHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409732fb{/executors/threadDump/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e99e2cb{/static,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@478967eb{/,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f2b39a{/api,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18c880ea{/jobs/job/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6afbe6a1{/stages/stage/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:56 INFO ui.SparkUI: Bound SparkUI to 0.0.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:9405,AVAIL,AVAILABLE,9405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"f these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:1938,recover,recovered,1938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['recover'],['recovered']
Availability,"f.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs (small: test.cf.gz and initial: m2.vcf.gz), bamout and ""hs_err_pid.logs"" to `gs://iseq/kt/strange-bug/` ; I hope you can access them. ; Best,; Kasia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7247:2024,error,error,2024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247,2,"['error', 'toler']","['error', 'tolerance']"
Availability,"fOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:61",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:14054,failure,failure,14054,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['failure'],['failure']
Availability,"fected version(s); - [ ] Latest public release version GATK 4.1.9.0 with data version funcotator_dataSources.v1.7.20200521g. ### Description . #### Steps to reproduce. I'm trying to run GATK Funcotator using the funcotator_dataSources.v1.7.20200521g data download. The command line that I'm using is:; ```; gatk Funcotator \; --variant cohort.vcf.gz \; --reference GRCh38.d1.vd1/GRCh38.d1.vd1.fa \; --ref-version hg38 \; --data-sources-path funcotator_dataSources.v1.7.20200521g \; --output cohort.funcotator.vcf.gz \; --output-file-format VCF; ```. If I run that command line without the `gnomad_*.tar.gz`'s expanded, it works fine and annotates my `cohort.vcf.gz` into `cohort.funcotator.vcf.gz`. . Following the directions at [Funcotator Information and Tutorial - 1.1.2.2.1: enabling gnomAD](https://gatk.broadinstitute.org/hc/en-us/articles/360035889931-Funcotator-Information-and-Tutorial#1.1.2.2.1), if I expand both `gnomAD_exome.tar.gz` and `gnomAD_genome.tar.gz`, funcotator dies at startup with a `400 Bad Request` error. This also happens if I expand either one of the `gnomad_*.tar.gz` files individually. . #### Expected behavior; Funcotator annotates my VCF and includes gnomAD annotations in the output VCF. . #### Actual behavior. Crash with 400 Bad Request:. ```; Using GATK jar /opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar Funcotator --variant cohort.vcf.gz --reference GRCh38.d1.vd1/GRCh38.d1.vd1.fa --ref-version hg38 --data-sources-path funcotator_dataSources.v1.7.20200521g --output cohort.funcotator.vcf.gz --output-file-format VCF; 14:24:33.589 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compress",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:1096,error,error,1096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['error'],['error']
Availability,"ferenceMaker.apply(FastaAlternateReferenceMaker.java:141); 	at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. By narrowing down on where this happens I find it happens here:. ```; chrom	16798	.	TAGC	*	41.94	.	AC=1;AF=1.00;AN=1;BaseQRankSum=-5.240e-01;DP=29;FS=3.663;MQ=36.43;MQRankSum=-1.282e+00;QD=1.40;ReadPosRankSum=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16799	.	A	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=3.663;QD=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16800	.	G	*	3727.44	.	AC=1;AF=1.00;AN=1;BaseQRankSum=0.00;DP=29;FS=2.256;MQ=42.17;MQRankSum=1.88;QD=21.42;ReadPosRankSum=-6.100e-02;SOR=0.920	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16801	.	C	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=3.663;QD=0.00;SOR=0.446	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16802	.	A	*	0	LowQual	AC=1;AF=1.00;AN=1;DP=29;FS=4.509;QD=0.00;SOR=0.378	GT:AD:DP:GQ:PL	1:0,6:29:99:810,0. chrom	16804	.	CAGA	.	379.83	.	AN=1;BaseQRankSum=0.00;DP=48;FS=0.000;MQ=42.50;MQRankSum=-6.740e-01;QD=29.22;ReadPosRankSum=0.524;SOR=0.836	GT:AD:DP:PL	0:48:48:0; ```. The problem is at position 16800. I have gotten this error again in similar positions where a deletion has happened and the bases after it are called in contradictory ways or with low quality and I was wondering how I could fix it for all my samples.; Could BaseQRankSum have anything to do with it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7433:2903,error,error,2903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433,1,['error'],['error']
Availability,"figured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=645922816; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:2487,down,down,2487,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,1,['down'],['down']
Availability,"file:///paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200315.HC.g.vcf.gz; 12:01:38.184 INFO FeatureManager - Using codec VCFCodec to read file file:///paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-006.HC.g.vcf.gz; 12:01:38.311 INFO FeatureManager - Using codec VCFCodec to read file file:///paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-007.HC.g.vcf.gz; 12:01:38.417 INFO FeatureManager - Using codec VCFCodec to read file file:///paedwy/disk1/yangyxt/wes/backup_gvcfs/all_wes_samples.g.vcf; 12:01:49.097 INFO CombineGVCFs - Done initializing engine; 12:01:49.113 INFO ProgressMeter - Starting traversal; 12:01:49.113 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 12:01:49.492 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chrM:63 the annotation MLEAC=[2, 0] was not a numerical value and was ignored; 12:01:49.505 INFO CombineGVCFs - Shutting down engine; [August 24, 2020 12:01:49 PM HKT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=6277824512; java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.annotator.alle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766:5353,down,down,5353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766,1,['down'],['down']
Availability,"files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!`. C345.TCGA-A3-3373-11A-01D-1421-08.5.bam and C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; were successfully downloaded, but since these TCGA files use DRS URI, they were copied to two separate cromwell folders. /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; /cromwell_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7487:1416,Down,Download,1416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487,1,['Down'],['Download']
Availability,"finally here. closing the main feature requests made in #2703 . Need to wait for #3752 and #3770 . Will generate a separate dummy PR about how to run the whole prototyping holistic SV interpretation tool. Some small patches need to be applied after this PR, basically for dumpster diving into ; * ambiguous events; * events where assembly contigs clearly couldn't tell a complete picture. but all parts of the diving suit are all available after this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805:430,avail,available,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805,1,['avail'],['available']
Availability,"fix #4648 . The problems are in clipping the alignments when de-overlapping the alignments, particularly computing the new ref and read spans. Also adds a check on arguments used for constructing alignments, which helped finding errors in test data (now corrected)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4670:229,error,errors,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4670,1,['error'],['errors']
Availability,fix bug causing error finding ref base for imprecise deletions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3671:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3671,1,['error'],['error']
Availability,fix initial 0 downloads bug,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7794:14,down,downloads,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7794,1,['down'],['downloads']
Availability,fix the logj4 error that always happens,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/216:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/216,1,['error'],['error']
Availability,fix typo in AFCalculator error message,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2153:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2153,1,['error'],['error']
Availability,fixed an error where the read mate key in SplitNCigarReads was not sufficiently strict about readnames causing cigar errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6909:9,error,error,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6909,2,['error'],"['error', 'errors']"
Availability,"fixed contamination calculation, added error bars to output",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3385:39,error,error,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3385,1,['error'],['error']
Availability,fixed error in RealignmentEngine due to inclusive vs exclusive ends,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6404:6,error,error,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6404,1,['error'],['error']
Availability,fixed out-of-bounds error in CountNs annotation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6355:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6355,1,['error'],['error']
Availability,fixes #1140 (extracts file names to name constants to avoid errors and indicate where same outputs are expected). for @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1152:60,error,errors,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1152,1,['error'],['errors']
Availability,fixes #1234 ; i checked that the original error goes away. @droazen can you have a quick look?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1242:42,error,error,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1242,1,['error'],['error']
Availability,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1450:299,error,errors,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450,3,"['error', 'redundant']","['errors', 'redundant']"
Availability,fixes #754. updating spark along side the dataflow jump; also updating other dependencies as well. changing GatkTestPipeline to downgrade a naming error to a warning; replacing calls to setName; replacing calls to setCoder with calls to withCoder when possible. hooking up the validation stringency for local files; fixes #745. disabling failing test and opening #774 to reenable it,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/775:128,down,downgrade,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/775,2,"['down', 'error']","['downgrade', 'error']"
Availability,"fixes a flaw where i was looking at the 1st read, and the event could've been upstream or downstream. now i always choose the upstream read so that the event will be downstream.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3229:90,down,downstream,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3229,2,['down'],['downstream']
Availability,fixes https://github.com/broadinstitute/gatk/issues/1284. adds these lines output of running `./gatk-lauch` with no arguments. ```; gatk-launch --help #Print the list of available tools. gatk-launch Tool -help #Print help on a particular tool; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1351:170,avail,available,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1351,1,['avail'],['available']
Availability,fixes https://github.com/broadinstitute/gatk/issues/1702. Speed up in writing - from 3.1 minutes down to 2.4 minutes in MarkDuplicatesSpark; @lbergelson can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1736:97,down,down,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1736,1,['down'],['down']
Availability,fixes the error in computing contig span in BwaMemAlignment -> AlignmentRegion,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2552:10,error,error,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2552,1,['error'],['error']
Availability,"fixes the issue where CommandLineExceptions produced no error message. added a public getUsage() method to CommandLineProgram; added a catch for these in instanceMain(), where the CommandLineProgram is in scope for printing the usage message; added a protected accessor getCommandLineParser to CommandLineProgram which guards against having an uninitialized CommandLineParser; moved the ""A USER ERROR HAS OCCURRED"" text out of the actual user exception and into the pretty printing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2340:56,error,error,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2340,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,fixing test failures when AVX is supported but the library doesn't exist,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1577:12,failure,failures,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1577,1,['failure'],['failures']
Availability,fixing the spark failures introduced by #2389,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618:17,failure,failures,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618,1,['failure'],['failures']
Availability,flater: IntelInflater; 15:05:33.887 INFO PrintReads - GCS max retries/reopens: 20; 15:05:33.887 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:05:33.887 INFO PrintReads - Initializing engine; 15:05:39.011 INFO PrintReads - Done initializing engine; 15:05:39.298 INFO ProgressMeter - Starting traversal; 15:05:39.299 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 15:05:49.302 INFO ProgressMeter - chr1:12666181 0.2 578000 3467306.5; 15:05:59.302 INFO ProgressMeter - chr1:21255922 0.3 1287000 3860613.9; 15:06:09.320 INFO ProgressMeter - chr1:36027022 0.5 2121000 4239032.7; 15:06:19.323 INFO ProgressMeter - chr1:52397728 0.7 3017000 4522786.3; 15:06:29.323 INFO ProgressMeter - chr1:86811190 0.8 4064000 4874460.3; 15:06:39.479 INFO ProgressMeter - chr1:111761145 1.0 5079000 5063808.6; ...; ```. Adding in `-L` causes the command to error despite the presence of the index within the same folder.; ```; -bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; > -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -O HG00190_cram.bam \; > -L chr17; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -O HG00190_cram.bam -L chr17; 14:59:15.760 INFO Nat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:8715,error,error,8715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['error'],['error']
Availability,"flater; 12:37:00.531 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:37:00.531 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:37:00.531 INFO GenotypeGVCFs - Initializing engine; 12:37:02.095 INFO FeatureManager - Using codec VCFCodec to read file file:///home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf; 12:37:03.426 INFO GenotypeGVCFs - Done initializing engine; 12:37:03.683 INFO ProgressMeter - Starting traversal; 12:37:03.683 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 12:37:13.942 INFO ProgressMeter - chr1:1034498 0.2 14000 81887.3; 12:37:24.351 INFO ProgressMeter - chr1:1322991 0.3 41000 119030.3; 12:37:34.716 INFO ProgressMeter - chr1:1926324 0.5 83000 160474.3; 12:37:44.726 INFO ProgressMeter - chr1:3786982 0.7 124000 181273.3; 12:37:45.921 INFO GenotypeGVCFs - Shutting down engine; [July 12, 2018 12:37:45 PM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.76 minutes.; Runtime.totalMemory()=5942280192; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 8.038983630564185E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:255); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:210); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:266); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:222); at org.broadinstitute.hellbender.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5009:3238,down,down,3238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009,1,['down'],['down']
Availability,flow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-1425124017/P0000480.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1072891920/P0000481.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-724059439/P0000992.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:29354,error,error-rate,29354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['error'],['error-rate']
Availability,"fo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2020-07-14 05:09:55,30] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2020-07-14 05:09:55,31] [info] JobExecutionTokenDispenser stopped; [2020-07-14 05:09:55,31] [info] Aborting all running workflows.; [2020-07-14 05:09:55,31] [info] WorkflowStoreActor stopped; [2020-07-14 05:09:55,31] [info] WorkflowLogCopyRouter stopped; [2020-07-14 05:09:55,31] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor All workflows finished; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor stopped; [2020-07-14 05:09:55,53] [info] Connection pools shut down; [2020-07-14 05:09:55,53] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] SubWorkflowStoreActor stopped; [2020-07-14 05:09:55,54] [info] JobStoreActor stopped; [2020-07-14 05:09:55,53] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2020-07-14 05:09:55,54] [info] CallCacheWriteActor stopped; [2020-07-14 05:09:55,54] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] IoProxy stopped; [2020-07-14 05:09:55,54] [info] DockerHashActor stopped; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=1 idleQueues.size=1 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] WriteMetadataActor Shutting down: 0 queued messag",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:8545,down,down,8545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,10,['down'],['down']
Availability,"for manipulating interval lists; LiftOverIntervalList Lifts over an interval list between genome builds. --------------------------------------------------------------------------------------; QC: Tools for Diagnostics and Quality Control; AnalyzeCovariates Tool to analyze and evaluate base recalibration tables for BQSR; CalculateHsMetrics Produces Hybrid Selection-specific metrics for a SAM/BAM file; CollectAlignmentSummaryMetrics Produces from a SAM/BAM/CRAM file containing summary alignment metrics; CollectBaseDistributionByCycle Produces metrics about nucleotide distribution per cycle in a SAM/BAM/CRAM file; CollectGcBiasMetrics Produces metrics about GC bias in the reads in the provided SAM/BAM file; CollectInsertSizeMetrics Produces metrics for insert size distribution for a SAM/BAM/CRAM file; CollectJumpingLibraryMetrics Produces jumping library metrics for the provided SAM/BAMs; CollectMultipleMetrics A ""meta-metrics"" calculating program that produces multiple metrics for the provided SAM/BAM file; CollectOxoGMetrics Produces metrics quantifying the CpCG -> CpCA error rate from the provided SAM/BAM file; CollectQualityYieldMetrics Produces metrics that quantify the quality and yield of sequence data from the provided SAM/BAM/CRAM file; CollectRnaSeqMetrics Produces RNA alignment metrics for a SAM/BAM file; CollectRrbsMetrics Produces metrics about bisulfite conversion for RRBS data; CollectSequencingArtifactMetrics Produces metrics to quantify single-base sequencing artifacts from a SAM/BAM file; CollectTargetedPcrMetrics Produces Targeted PCR-related metrics given the provided SAM/BAM; CollectWgsMetrics Produces metrics related to whole genome sequencing for a SAM/BAM file; MeanQualityByCycle Produces metrics for mean quality by cycle for a SAM/BAM/CRAM file; QualityScoreDistribution Produces metrics for quality score distributions for a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; SAM/BAM/CRAM: T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1669:2472,error,error,2472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669,1,['error'],['error']
Availability,"for multiple alignment fine tuning for ; * sending back some contigs back to the 2 alignment logic tree, as handled by #3752 , and ; * capturing more incomplete picture assemblies, and ; * sending the assembly contig seemingly has a complete picture down the multiple alignment logic tree, which is being finalized. To be merged after #3752 is reviewed. Code for hooking it up exists and is minimal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3770:250,down,down,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3770,1,['down'],['down']
Availability,"form(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droazen @lbergelson @LeeTL1220 @ldgauthier @yfarjoun This is just one example of how using recently developed ML frameworks could make our lives orders of magnitude easier. The sooner we can develop a strategy to leverage these in Java land, the better!. I'd expect that roughly the same amount of code would be needed to specify this model using Stan. Interfaces for Stan exist for many other languages, so it might be relatively easy to come up with some Java bindings. However, one downside is that Stan is not built on top of a computational graph (e.g., theano/tensorflow), so we don't get GPU/distributed computing for free. I don't think this is a deal breaker, but it's something we should consider. ---. @samuelklee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302429919). It should be said that I consider this a major blocker for the CNV team. I don't think it makes sense to rebuild the somatic pipeline to include the new coverage model until we move to this ADVI framework or something like it. I do think @mbabadi should continue adding features (such as common CNV calling) to his non-ADVI germline implementation, so that we can be in a position to start calling on gnomAD or other large cohorts, but that we should eventually move the germline tool over to this framework as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2984:3655,down,downside,3655,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984,1,['down'],['downside']
Availability,"fun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/23766",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:16211,ERROR,ERROR,16211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['ERROR'],['ERROR']
Availability,"g BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.221 INFO BlockManagerMasterEndpoint - Registering block manager 172.20.19.130:43279 with 1076.2 GiB RAM, BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.225 INFO BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.226 INFO BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.345 INFO ContextHandler - Stopped o.s.j.s.ServletContextHandler@7074da1d{/,null,STOPPED,@Spark}; 10:33:07.347 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6556471b{/jobs,null,AVAILABLE,@Spark}; 10:33:07.349 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7cdb05aa{/jobs/json,null,AVAILABLE,@Spark}; 10:33:07.351 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5cb76070{/jobs/job,null,AVAILABLE,@Spark}; 10:33:07.352 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@443ac5b8{/jobs/job/json,null,AVAILABLE,@Spark}; 10:33:07.354 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@753e4eb5{/stages,null,AVAILABLE,@Spark}; 10:33:07.355 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@63318b56{/stages/json,null,AVAILABLE,@Spark}; 10:33:07.357 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@462f8fe9{/stages/stage,null,AVAILABLE,@Spark}; 10:33:07.358 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@b2e1df3{/stages/stage/json,null,AVAILABLE,@Spark}; 10:33:07.359 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6cf3b3d7{/stages/pool,null,AVAILABLE,@Spark}; 10:33:07.360 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:45240,AVAIL,AVAILABLE,45240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"g Report . ### Affected tool(s) or class(es); PathSeq. ### Affected version(s); - 4.1.6.0. ### Description . I wanted to better understand the PathSeq pipeline (and in particular, the Host Filter step) so I simulated RNA-seq reads from three microbial genomes of interest (Salmonella eneterica subsp. enterica serovar Typhimurium str. SL1344, Salmonella eneterica subsp. enterica serovar Typhimurium str. LT2, Fusobacterium nucleatum subsp. nucleatum ATCC 25586). I generate six datasets with 100,000 unpaired reads of length 75 bp (2 datasets from each genome) using Rsubread with simulate.sequencing.error=TRUE and ran them through PathSeq. I generated an identical six datasets using Rsubread with simulate.sequencing.error=FALSE. . #### Expected behavior; For the six datasets with simulate.sequencing.error=TRUE, I would expect a small number of reads to be filtered for each step. For the six datasets with simulate.sequencing.error=FALSE, I would expect similar results but with even fewer reads to be filtered for the low-quality or low complexity read filter. #### Actual behavior; For the six datasets with simulate.sequencing.error=TRUE, 8,496 - 18,103 reads were filtered by the low complexity or low quality filter (the Salmonella datasets were on the lower end and the Fusobacterium datasets were on the higher end), 115 - 311 reads were filtered by the host k-mer filter and 886 - 1822 reads were filtered by the duplicate read filter. . The number of reads filtered by the low complexity or low quality filter seemed high to me so I repeated the analysis with simulate.sequencing.error=FALSE. For these six datasets, all 100,000 reads are filtered by the low-quality or low complexity read filter. . #### Steps to reproduce; I wrote the workflow using snakemake and conda. In theory, you should be able to reproduce the error using `snakemake --use-conda`; Snakefile; ```; from os.path import join; import pandas as pd. ATCC25586_CDS_URL = ""ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6705:937,error,error,937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6705,1,['error'],['error']
Availability,"g traversal; 16:56:31.263 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 16:56:42.883 INFO ProgressMeter - 1:28433062 0.2 7000 36147.7; 16:56:53.532 INFO ProgressMeter - 1:94237401 0.4 13000 35026.3; 16:57:08.438 INFO ProgressMeter - 1:192716273 0.6 21000 33893.7; 16:57:19.009 INFO ProgressMeter - 2:24820506 0.8 31000 38957.0; 16:57:29.031 INFO ProgressMeter - 2:94856959 1.0 44000 45700.0; 16:57:39.223 INFO ProgressMeter - 2:136329636 1.1 59000 52089.5; 16:57:49.667 INFO ProgressMeter - 2:233747942 1.3 65000 49742.4; 16:58:01.608 INFO ProgressMeter - 3:57654674 1.5 71000 47152.6; 16:58:12.449 INFO ProgressMeter - 3:179974096 1.7 84000 49809.3; 16:58:23.282 INFO ProgressMeter - 4:82276408 1.9 98000 52491.1; 16:58:33.462 INFO ProgressMeter - 5:20304602 2.0 106000 52046.3; 16:58:44.217 INFO ProgressMeter - 5:141241407 2.2 114000 51446.4; 16:58:54.298 INFO ProgressMeter - 6:28447738 2.4 122000 51176.3; 16:59:03.028 INFO CollectReadCounts - Shutting down engine; [October 6, 2020 at 4:59:03 PM EDT] org.broadinstitute.hellbender.tools.copynumber.CollectReadCounts done. Elapsed time: 2.55 minutes.; Runtime.totalMemory()=981467136; java.lang.ArrayIndexOutOfBoundsException; at java.base/java.util.zip.CRC32.update(CRC32.java:76); at htsjdk.samtools.cram.io.CRC32InputStream.read(CRC32InputStream.java:54); at htsjdk.samtools.cram.io.InputStreamUtils.readFully(InputStreamUtils.java:75); at htsjdk.samtools.cram.structure.block.Block.read(Block.java:283); at htsjdk.samtools.cram.structure.SliceBlocks.<init>(SliceBlocks.java:75); at htsjdk.samtools.cram.structure.Slice.<init>(Slice.java:155); at htsjdk.samtools.cram.structure.Container.<init>(Container.java:154); at htsjdk.samtools.cram.build.CramSpanContainerIterator$Boundary.next(CramSpanContainerIterator.java:97); at htsjdk.samtools.cram.build.CramSpanContainerIterator.next(CramSpanContainerIterator.java:57); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:97); at htsjdk.samtools.C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6865:4066,down,down,4066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6865,1,['down'],['down']
Availability,"g.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/Users/shlee/Documents/branches/hellbender-protected/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 3.807 secs; WMCF9-CB5:hellbender-protected shlee$ ; ```. ---. @cmnbroad commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1048#issuecomment-302219179). The doc processing code tries to walk up the class hierarchy of all dependent types, and there are classes in the cachemanager package that have inner classes that derive from RuntimeException, which it can't resolve. Apparently we've never run it on such code before. Anyway, the fix needs to be in Barclay. ---. @sooheelee commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2988:3811,FAILURE,FAILURE,3811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2988,1,['FAILURE'],['FAILURE']
Availability,"g.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). My command looks like this:; ~/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk --java-options ""-Xmx4g"" Mutect2 -R /Users/loeblabm11/bioinformatics/reference/human/hg19/hg19.fa -I 20171027_BN31_python.dcs.filt.no_overlap.bam -tumor BN31 -O 20171027_BN31_python.dcs.MuTect2.vcf -bamout 20171027_BN31_python.dcs.MuTect2.bam --max-reads-per-alignment-start 0 --max-population-af 1 --disable-tool-default-read-filters. and I'm running on MacOSX Sierra (10.12.6). java -version returns ; java version ""1.8.0_151""; Java(TM) SE Runtime Environment (build 1.8.0_151-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode). The input file is position-sorted output from Duplex Sequencing pipelines, and has been through IndelRealigner (from GATK 3.7), and ClipOverlappingReads from FgBio. It has been indexed. . I can put together a full bug report, complete with my input file, if necessary; I just want an idea of what might be happening. . Brendan. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11790/error-in-mutect2/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:7579,error,error-in-,7579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['error'],['error-in-']
Availability,g.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:282); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:993); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036:1533,down,downsampling,1533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036,1,['down'],['downsampling']
Availability,"g/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df · broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)). . ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1603) ; at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; at java.base/java.u",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7504:1753,error,error,1753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504,1,['error'],['error']
Availability,gCNV does not fail if the ELBO becomes NaN and just finishes the maximum number of allowed iterations. This check would save some time and lead to less cryptic errors downstream,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6186:160,error,errors,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6186,2,"['down', 'error']","['downstream', 'errors']"
Availability,gCNV nan errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4824:9,error,errors,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824,1,['error'],['errors']
Availability,"gatk --java-options ""-Xmx4g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" BaseRecalibrator -I /mnt/fq2bam/sample1.markdup.sorted.bam \; -R /mnt/fq2bam/inputs/reference/files/Homo_sapiens_assembly38.fasta \; --known-sites /mnt/fq2bam/inputs/resources/files/Homo_sapiens_assembly38.known_indels.vcf.gz \; --known-sites /mnt/fq2bam/inputs/resources/files/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \; --known-sites /mnt/fq2bam/inputs/resources/files/Homo_sapiens_assembly38.dbsnp138.vcf.gz \; -L chr1 \; -DF MappingQualityNotZeroReadFilter \; -DF MappedReadFilter \; -O /mnt/fq2bam/sample1_BQSR001.recal_data.table. I got the following error. java.lang.IllegalStateException: No cigar elements left after removing leading and trailing deletions.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:138); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:143); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.consolidateCigar(BaseRecalibrationEngine.java:293); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:118); at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:189); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:100); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8523:635,error,error,635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8523,1,['error'],['error']
Availability,gatk Funcotator error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8913:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8913,1,['error'],['error']
Availability,gatk GenotypeGVCFs USER ERROR: The list of input alleles must contain <NON_REF> as an allele but that is not the case,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7147:24,ERROR,ERROR,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7147,1,['ERROR'],['ERROR']
Availability,gatk GenotypeGVCFs error：A USER ERROR has occurred: Bad input: Presence of '-RAW_MQ' annotation is detected.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,gatk GetSampleName on cram files gives error but actually succeeds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8059:39,error,error,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8059,1,['error'],['error']
Availability,gatk PostprocessGermlineCNVCalls error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7266:33,error,error,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7266,1,['error'],['error']
Availability,gatk haplotypecaller return error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7499:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499,1,['error'],['error']
Availability,"gatk silently sets TMP_DIR globally readable/writable for all users. This is problematic for admins trying to maintain a secure multi-user environment. It would be better (imho) if gatk tests if TMP_DIR is writeable and errors out when it is not instead of just globally making it writeable by all users. src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java. for (final File f : TMP_DIR) {; // Intentionally not checking the return values, because it may be that the program does not; // need a tmp_dir. If this fails, the problem will be discovered downstream.; if (!f.exists()) f.mkdirs();; f.setReadable(true, false);; f.setWritable(true, false);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4513:220,error,errors,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4513,2,"['down', 'error']","['downstream', 'errors']"
Availability,gatk-launch wasn't exiting with the same exit code that GATK did; this causes problems for testing infrastructure that expects non-zero exit codes from failures. fixes #1599,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1604:152,failure,failures,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1604,1,['failure'],['failures']
Availability,"gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The docker installed in my sever doesn't not work....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8432:1323,FAILURE,FAILURE,1323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432,2,"['FAILURE', 'down']","['FAILURE', 'downloaded']"
Availability,gatk4.3.0.0 Combines gvcfs were interrupted without errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8381:52,error,errors,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8381,1,['error'],['errors']
Availability,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:27807,error,error,27807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['error'],['error']
Availability,"geArchive (Thread[Daemon worker Thread 2,5,main]) completed. Took 0.058 secs.; :compileJava (Thread[Daemon worker Thread 2,5,main]) started.; :compileJava; Executing task ':compileJava' (up-to-date check took 0.044 secs) due to:; No history is available.; All input files are considered out-of-date for incremental task ':compileJava'.; Compiling with JDK Java compiler API.; /home/axverdier/Tools/GATK4/git/gatk/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SuspectedTransLocDetector.java:13: warning: [unchecked] unchecked conversion; import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;; ^; required: List<String>; found: List; error: warnings found and -Werror specified; 1 error; 1 warning; :compileJava FAILED; :compileJava (Thread[Daemon worker Thread 2,5,main]) completed. Took 4.116 secs. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileJava'.; > Compilation failed; see the compiler error output for details. * Try:; Run with --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':compileJava'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:5303,error,error,5303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,1,['error'],['error']
Availability,"generated VCF records; WARNING: No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 06:51:26.212 INFO GenotypeGVCFs - Done initializing engine; 06:51:26.257 INFO ProgressMeter - Starting traversal; 06:51:26.257 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 06:51:26.278 INFO GenotypeGVCFs - Shutting down engine; [January 27, 2019 6:51:26 AM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.12 minutes.; Runtime.totalMemory()=1972371456; java.lang.IllegalStateException: There are no sources based on those query parameters; at com.intel.genomicsdb.reader.GenomicsDBFeatureIterator.<init>(GenomicsDBFeatureIterator.java:131); at com.intel.genomicsdb.reader.GenomicsDBFeatureReader.query(GenomicsDBFeatureReader.java:144); at org.broadinstitute.hellbender.engine.FeatureDataSource.refillQueryCache(FeatureDataSource.java:534); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:503); at org.broadinstitute.hellbender.engine.FeatureDataSource.query(FeatureDataSource.java:469); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$2(VariantLocusWalker.java:144); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:180",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5865:8106,down,down,8106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865,1,['down'],['down']
Availability,"genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport - Importing batch 1 with 1 samples; terminate called after throwing an instance of 'FileBasedVidMapperException'; what(): FileBasedVidMapperException : type_index_iter != VidMapper::m_typename_string_to_type_index.end() && ""Unhandled field type""; [login1:01909] *** Process received signal ***; [login1:01909] Signal: Aborted (6); [login1:01909] Signal code: (-6); [login1:01909] [ 0] /lib64/libpthread.so.0[0x30bfa0f7e0]; [login1:01909] [ 1] /lib64/libc.so.6(gsignal+0x35)[0x30bf6325e5]; [login1:01909] [ 2] /lib64/libc.so.6(abort+0x175)[0x30bf633dc5]; [login1:01909] [ 3] /apps/GCC/6.3.0/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x15d)[0x7f507f7018ed]; [login1:01909] [ 4] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8a6)[0x7f507f6ff8a6]; [login1:01909] [ 5] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8f1)[0x7f507f6ff8f1]; [login1:01909] [ 6] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8eb08)[0x7f507f6ffb08]; [login1:01909] [ 7] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x151df1)[0x7f507fb56df1]; [login1:01909] [ 8] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1434d9)[0x7f507fb484d9]; [login1:01909] [ 9] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1489f9)[0x7f507fb4d9f9]; [login1:01909] [10] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12c78e)[0x7f507fb3178e]; [login1:01909] [11] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12d7b4)[0x7f507fb327b4]; [login1:01909] [12] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x11f80d)[0x7f507fb2480d]; [login1:01909] [13] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(Java_com_intel_genomicsdb_GenomicsDBImporter_jniSetupGenomicsDBLoader+0x98)[0x7f507fb99c78]; [login1:01909] [14] [0x7f50f1015814]; [login1:01909] *** End of error message ***. Thanks in advance!. Cristina.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514:5154,error,error,5154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514,1,['error'],['error']
Availability,"ges popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a63",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:1020,ERROR,ERROR,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,1,['ERROR'],['ERROR']
Availability,"getCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:779); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); at org.broadinstitute.hellbender.Main.main(Main.java:221); ```; This error occurs fairly soon after launching the job, after the progress meter shows the tool iterating over chromosome 1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154:3183,error,error,3183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154,1,['error'],['error']
Availability,"gging.log4j.core.impl.Log4jContextFactory specified in jar:file:/Users/mac/Desktop/GenomeAnalysisTK-3.8-0-ge9d806836%204/GenomeAnalysisTK.jar!/META-INF/log4j-provider.properties; ERROR StatusLogger Log4j2 could not find a logging implementation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console...; INFO 10:47:55,875 GenomeAnalysisEngine - Deflater: IntelDeflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Inflater: IntelInflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Strictness is SILENT ; INFO 10:47:56,246 GenomeAnalysisEngine - Downsampling Settings: No downsampling ; INFO 10:47:56,255 SAMDataSource$SAMReaders - Initializing SAMRecords in serial ; INFO 10:47:56,333 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.07 ; ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A USER ERROR has occurred (version 3.8-0-ge9d806836): ; ##### ERROR; ##### ERROR This means that one or more arguments or inputs in your command are incorrect.; ##### ERROR The error message below tells you what is the problem.; ##### ERROR; ##### ERROR If the problem is an invalid argument, please check the online documentation guide; ##### ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.; ##### ERROR; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.; ##### ERROR; ##### ERROR MESSAGE: Input files reads and reference have incompatible contigs. Please see https://software.broadinstitute.org/gatk/documentation/article?id=63for more information. Error details: No overlapping contigs found.; ##### ERROR reads contigs = [LmjF04_01_20050601_V5.2, LmjF05_01_20050601_V5.2, LmjF24_01_20050601_V5.2, LmjF01_01_2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6798:2802,ERROR,ERROR,2802,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6798,2,['ERROR'],['ERROR']
Availability,gine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:136); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:423); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:134); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ``` ; Based on the stacktrace I suspect an invalid value has snuck its way into `genotypeIndexMapsByPloidy[]` from `getIndexesOfRelevantAlleles` and that is causing the error. I suspect it will be necessary to look at the combined input in question to evaluate what is happening.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357:7530,error,error,7530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357,1,['error'],['error']
Availability,"git clone https://github.com/lindenb/jbwa. and make, error message is:. ksw.c:29:23: fatal error: emmintrin.h: No such file or directory; # include <emmintrin.h>. ^; compilation terminated.; make[1]: **\* [ksw.o] Error 1; make[1]: Leaving directory `/home/dlspark/gatk/lib/jbwa/bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb'; make: **\* [bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb/libbwa.a] Error 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1870:53,error,error,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1870,4,"['Error', 'error']","['Error', 'error']"
Availability,google-cloud-java: switch to a snapshot that retries on 403 errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3766:60,error,errors,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3766,1,['error'],['errors']
Availability,gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLaunche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4177,ERROR,ERROR,4177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:5539,ERROR,ERROR,5539,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"gressMeter - Z:9213604 123.3 49233000 399397.5; 02:04:54.256 INFO ProgressMeter - Z:9213609 123.4 49270000 399145.4; 02:05:04.520 INFO ProgressMeter - Z:9213615 123.6 49305000 398876.1; 02:05:14.836 INFO ProgressMeter - Z:9213617 123.8 49333000 398548.3; 02:05:24.952 INFO ProgressMeter - Z:9213623 124.0 49360000 398224.1; 02:05:35.007 INFO ProgressMeter - Z:9213627 124.1 49387000 397903.9; 02:05:45.376 INFO ProgressMeter - Z:9213630 124.3 49413000 397559.8; 02:05:55.709 INFO ProgressMeter - Z:9213632 124.5 49439000 397218.6; 02:06:05.774 INFO ProgressMeter - Z:9213633 124.6 49464000 396884.6; 02:06:15.808 INFO ProgressMeter - Z:9213636 124.8 49489000 396553.0; 02:06:25.926 INFO ProgressMeter - Z:9213644 125.0 49515000 396226.0; 02:06:36.016 INFO ProgressMeter - Z:9213645 125.1 49540000 395893.3; 02:06:46.370 INFO ProgressMeter - Z:9213647 125.3 49565000 395547.6; 02:06:56.432 INFO ProgressMeter - Z:9213649 125.5 49589000 395210.2; 02:07:03.641 INFO SplitNCigarReads - Shutting down engine; [February 21, 2021 at 2:07:03 AM PST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 125.62 minutes.; Runtime.totalMemory()=5721030656; htsjdk.samtools.util.RuntimeIOException: Stale file handle; at htsjdk.samtools.util.BinaryCodec.close(BinaryCodec.java:628); at htsjdk.samtools.util.BlockCompressedOutputStream.close(BlockCompressedOutputStream.java:344); at htsjdk.samtools.util.BlockCompressedOutputStream.close(BlockCompressedOutputStream.java:331); at htsjdk.samtools.util.BinaryCodec.close(BinaryCodec.java:624); at htsjdk.samtools.BAMFileWriter.finish(BAMFileWriter.java:155); at htsjdk.samtools.SAMFileWriterImpl.close(SAMFileWriterImpl.java:220); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyClose(AsyncSAMFileWriter.java:38); at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:89); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.close(SAMFileGATKReadWriter.java:26); at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7091:114171,down,down,114171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091,1,['down'],['down']
Availability,"gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 14:59:15.872 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 14:59:15.873 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:59:15.873 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:59:15.873 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:59:15.873 INFO PrintReads - Deflater: IntelDeflater; 14:59:15.873 INFO PrintReads - Inflater: IntelInflater; 14:59:15.873 INFO PrintReads - GCS max retries/reopens: 20; 14:59:15.873 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:59:15.873 INFO PrintReads - Initializing engine; 14:59:21.404 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:59:21.421 INFO PrintReads - Shutting down engine; [October 5, 2017 2:59:22 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.11 minutes.; Runtime.totalMemory()=2129133568; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```. Still fails with `-readIndex` specified (.cram.crai OR .crai):; ```; -bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; > -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:11943,down,down,11943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['down'],['down']
Availability,"gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:08.250 INFO PrintReads - Deflater: IntelDeflater; 15:00:08.250 INFO PrintReads - Inflater: IntelInflater; 15:00:08.250 INFO PrintReads - GCS max retries/reopens: 20; 15:00:08.250 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:00:08.250 INFO PrintReads - Initializing engine; 15:00:13.258 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 15:00:13.275 INFO PrintReads - Shutting down engine; [October 5, 2017 3:00:14 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=2233466880; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```; ```; -bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; > -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -readIndex gs://shlee-dev/1kg/exome_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:19507,down,down,19507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['down'],['down']
Availability,"gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:28.284 INFO PrintReads - Deflater: IntelDeflater; 15:00:28.284 INFO PrintReads - Inflater: IntelInflater; 15:00:28.285 INFO PrintReads - GCS max retries/reopens: 20; 15:00:28.285 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:00:28.285 INFO PrintReads - Initializing engine; 15:00:33.117 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 15:00:33.134 INFO PrintReads - Shutting down engine; [October 5, 2017 3:00:34 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=2255486976; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; -bash-4.1$ ; ```. ## Confirm all files present in bucket; ```; WMCF9-CB5:newCNV shlee$ gsutil ls gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN*; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.bam.bas; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:23666,down,down,23666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['down'],['down']
Availability,"gy/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>3.8-2-SNAPSHOT</version>; 19 >>>>>>> 0450e2531ee021e28bd7c5e92b5ba736d530d9af; 20 <packaging>pom</packaging>; 21 <name>GATK ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:1928,ERROR,ERROR,1928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,1,['ERROR'],['ERROR']
Availability,"h a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authorization"",; ""locationType"" : ""header"",; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram."",; ""reason"" : ""required""; } ],; ""message"" : ""Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.""; }; ```. ### Desired; Something like ""Unable to read gs://joel-cram/SAM24339124.cram due to permissions. Have you enabled Google Cloud Application Default Credentials by running 'gcloud auth application-default login'? See [this forum post] for details.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5468:1941,error,errors,1941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468,1,['error'],['errors']
Availability,"hard filtering error , gate-4.1.8.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6789:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6789,1,['error'],['error']
Availability,hardcode in less partitions so that we can test where things are slowing down/breaking in the VAT pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8780:73,down,down,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8780,1,['down'],['down']
Availability,hardening standard quickstart pipeline against errors when a location is specified,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8047:47,error,errors,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8047,1,['error'],['errors']
Availability,"hare/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:39:56.077 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:39:56.139 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:39:56.139 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:39:56.139 INFO IntelPairHmm - Available threads: 36; 09:39:56.139 INFO IntelPairHmm - Requested threads: 4; 09:39:56.139 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:39:56.146 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 09:39:56.146 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 09:39:56.146 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 09:39:56.148 INFO Mutect2 - Shutting down engine; [July 3, 2020 9:39:56 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2233991168; htsjdk.samtools.util.RuntimeIOException: File not found: mutect2/concatenated_ACC5611A1_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 	at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:451); 	at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:415); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.createVCFWriter(GATKVariantContextUtils.java:121); 	at org.broadinstitute.hellbender.engine.GATKTool.createVCFWriter(GATKTool.java:887); 	at org.broadinstitute.hellbender.engine.GATKTool.createVCFWriter(GATKTool.java:841); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:262); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1047); 	at org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695:4244,down,down,4244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695,1,['down'],['down']
Availability,"hat have very clear read support as seen in IGV. I have used the –bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf  . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO  Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads:    (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275   .    T    G    .    .     AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53     GT:AD:AF:DP:F1R2:F2R1:SB   0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO  Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:1610,recover,recover-all-dangling-branches,1610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['recover'],['recover-all-dangling-branches']
Availability,he - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:8141,Recover,Recovered,8141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"he Hexaploid Spartina maritima.](http://www.ncbi.nlm.nih.gov/pubmed/26530424); - [Non-Random Distribution of 5S rDNA Sites and Its Association with 45S rDNA in Plant Chromosomes.](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0035139). ---. @vruano commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217501618). I think you can use -XL to exclude intervals. . ---. @SHuang-Broad commented on [Thu Oct 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-256727048). Should the same fix for HC be ported to GenotypeGVCFs? @vdauwera @vruano ?. ---. @vdauwera commented on [Fri Oct 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-257037491). That sounds like a good idea, @SHuang-Broad . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260456840). @SHuang-Broad @vruano Is this (making the HC allele culling available to GenotypeGVCFs too) still on your radar(s)?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260687763). @vdauwera yes it is on mine. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:8999,avail,available,8999,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['avail'],['available']
Availability,he miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:8068,Recover,Recovered,8068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"he.spark.rdd.RDD.treeAggregate(RDD.scala:1104); at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438); at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:39); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:159); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:201); at org.broadinstitute.hellbender.Main.main(Main.java:287); 18/03/09 09:22:08 INFO ShutdownHookManager: Shutdown hook called; 18/03/09 09:22:08 INFO ShutdownHookManager: Deleting directory /tmp/qtestard/spark-af28085b-4317-4d0f-85dd-a966b1d26d9d; [command.txt](https://github.com/broadinstitute/gatk/files/1796441/command.txt). ```. I got the same one when I try to execute it outside of nextflow. I also tried to run it with --conf spark.executor.heartbeatInterval=120, but it seems it is useless, i'm not sure it is the good syntax for a local execution of spark. Here is attached, the complete log file. [command.txt](https://github.com/broadinstitute/gatk/files/1796471/command.txt). Can you help me ?. Also posted on the [GATK forum](https://gatkforums.broadinstitute.org/gatk/discussion/11587/gatk4-baserecalibratorspark-executor-heartbeat-timed-out-after-x-ms#latest). Best regards.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:6898,heartbeat,heartbeatInterval,6898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,2,['heartbeat'],"['heartbeat-timed-out-after-x-ms', 'heartbeatInterval']"
Availability,"he.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_202408111100502620487673658411251_0021.; org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130); at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41); at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862); at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714); at org.apache.spark.util.Utils$$anon$2.write(Utils.scala:160); at com.esotericsoftware.kryo.io.Output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:8506,ERROR,ERROR,8506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['ERROR'],['ERROR']
Availability,"he.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in stage 25.0 (TID 43224, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:6236,failure,failure,6236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['failure'],['failure']
Availability,"hello; I can find ""Difference between QUAL and GQ annotations in germline variant calling"" on GATK web,but can't find how to calculate it. ; And I can find on **https://www.biostars.org/p/174075/** QUAL **is related to** the amount of data available (=depth of coverage at the site) (because we are more confident when we have more observations to rely on), the quality of the mapping of the reads and alignment of the bases (because if we are not sure the bases observed really belong there, they do not contribute much to our confidence), and the quality of the base calls (because if they look like machine errors, they also do not contribute much to our confidence),so the answer is right?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6846:240,avail,available,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6846,2,"['avail', 'error']","['available', 'errors']"
Availability,"hi team ,; i am facing issue in running DetermineGermlineContigPloidy. my cmd : /home/ec2-user/data/gatk_cnv/gatk-4.1.4.0/gatk --java-options ""-Xmx8G"" DetermineGermlineContigPloidy --contig-ploidy-priors ploidy_model/interval_list.tsv -L preprocessed_intervals.interval_list --input sample.counts_ESI_17.hdf5 --input sample.counts_ESI_17.hdf5 --output esi_ploidy --output-prefix esi_cnvploidy --verbosity DEBUG. ERROR: ; Using GATK jar /home/ec2-user/data/gatk_cnv/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /home/ec2-user/data/gatk_cnv/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar DetermineGermlineContigPloidy --contig-ploidy-priors ploidy_model/interval_list.tsv -L preprocessed_intervals.interval_list --input sample.counts_ESI_17.hdf5 --input sample.counts_ESI_17.hdf5 --output esi_ploidy --output-prefix esi_cnvploidy --verbosity DEBUG; 08:48:45.706 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ec2-user/data/gatk_cnv/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 08:48:45.729 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/libgkl_compression2796572893882405738.so; Oct 17, 2019 8:48:45 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 08:48:45.917 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 08:48:45.918 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.1.4.0; 08:48:45.918 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:48:45.918 INFO DetermineGermlineContigPloidy - Executing as ec2-user@ip-172-31-4-142.us-east-2.compute.internal on Linux v4.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:412,ERROR,ERROR,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['ERROR'],['ERROR']
Availability,"hi, I downloaded the hg38 database from the following URL two months ago, which contains some known sites of vcf files such as snp and indel;; https://gatk.broadinstitute.org/hc/en-us/articles/360035890811；GATK resource bundle; But now I found that the contents of these vcf files cannot be viewed. I think the vcf files should be viewed under normal circumstances; now I want to restart download the data; but the official website was updated two days ago, can you give me a website to download the data?; thanks. the error for view is:; 1000G_phase1.snps.high_confidence.hg38.vcf may be a binary file;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6988:6,down,downloaded,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6988,4,"['down', 'error']","['download', 'downloaded', 'error']"
Availability,hopeful this will fix transient errors in MisencodedBaseQualityReadTransformerTest (see #311),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/313:32,error,errors,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/313,1,['error'],['errors']
Availability,"how/67601310; ```. B. Low support chain pruning might not be longer needed. Now we have a newer approach to select best haplotypes that can handle complex graph we might well not need to prune low supported hap early as they seemly they won't be selected if the are not amongst the best haplotypes. . B.1 Now that still would produce a considerable number of unlikely haplotypes that would cause a CPU burden. That can be changed by imposing another kinds of limit, For example we include all haplotypes with scores (likelihoods) that are Q0 - Q40 or we include haplotypes until the sum of their likelihoods is larger than the 99.99% probability mass. . B.2 This could provide a downstream solution to the problem caused by ranging heads recovery (explained above in A.2). B.3 If pruning is to be maintained, it makes more sense to do it at the very end after all dangling ends hav been recovered and the edges supports are finalized. Of course I assuming here that dangling end recovery does the sensible think of updating those supports are the graphs is modified. C. The use of Smith-Waterman in dangling end recovery does not seem totally optimal or even needed. . C.1 Recovering tails quite often this finish with the same sequence as the reference path because in fact they are supposed to end like that by construction (reads are trimmed by AR coordinates). For example, this can be cause because due to the k-mer size there is not enough based after variation for the paths to merge back. In this case you can simply merge the last vertices of the tail and the reference, faster and potentially more accurate. . C.2 Similarly dangling heads, at least part of the sequence of those dangling heads are clearly threadable back into the graph without the need of SW. For example look at the AA…AAAAAGA sequence in the picture below. . C.3 PairHMM runs in effect are performing SW kind of computations and so it is totally possible to use its partial result to find good alignment of dangling ends",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/264:1902,recover,recovery,1902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/264,1,['recover'],['recovery']
Availability,"hreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 6, xx.xx.xx.16, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.23, executor 5, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO BlockMa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:27319,Error,Error,27319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,htsjdk downstream tests are failing since we've merged the repos,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3234:7,down,downstream,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3234,1,['down'],['downstream']
Availability,htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:372); 	... 12 more; Caused by: htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132); 	at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:84); 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:300); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7114:5625,avail,available,5625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114,1,['avail'],['available']
Availability,"https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/xsv/LocatableXsvFuncotationFactory.java#L245-L247. Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.; Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. Modify the variable ‘supportedFieldNames’ with volatile to tackle the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376:382,reliab,reliably,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376,1,['reliab'],['reliably']
Availability,"https://github.com/broadinstitute/gatk/blob/c6daf7dd02b866907fbfebad150baeb540c35bce/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java#L701. I'm running into a recurrent issue in JointGermlineCNVSegmentation, running after PostprocessGermlineCNVCalls in a gCNV pipeline. A number of batches are being merged in parallel - some of those succeed, some fail. It's not clear just yet if this is a deterministic failure, I'll re-run a few times and see if I can answer that. . ```text; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrX:6383391 [VC SAMPLE_ID.segments.vcf.gz @ chrX:6383391-17732942 Q3076.53 of type=NO_VARIATION alleles=[N*] attr={END=17732942} GT=GT:CN:NP:QA:QS:QSE:QSS	0:1:581:1:3077:4:20 filters=. ... Caused by: java.lang.IllegalStateException: Encountered genotype with ploidy 1 but 2 alleles.; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); 	at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); 	at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); ```. The VCF row in question is . ```text; chrX	6383391	CNV_chrX_6383391_17732942	N	.	3076.53	.	END=17732942	GT:CN:NP:QA:QS:QSE:QSS	0:1:581:1:3077:4:20; ```. The characterisation of this row as `type=NO_VARIATION alleles=[N*]` seems... partially correct? There is no variation at this locus, but I'm not sure why alleles is `N*`. In this situation, as I read it, the first clause should be satisfied: 1 allele, and allele is no-call. Instead the variant process is dying in the else side of the condition. Could you clarify if I'm interpreting this correctly?. Relevant versioning:; ```; 13:18:38.320 INFO JointGermlineCNVSegmentation - ------------------------------------------------------------; 13:18:38.321 INFO JointGermlineCNVSegmentation - The Genome Analy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834:452,failure,failure,452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834,1,['failure'],['failure']
Availability,i get this when adding `--version` to the commandline. ```; A USER ERROR has occurred: Invalid command line: Argument secondsBetweenProgressUpdates has a bad value: --version. Problem constructing Double from the string '--version'.; ```. what does that mean? seems bogus,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1293:67,ERROR,ERROR,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1293,1,['ERROR'],['ERROR']
Availability,"i.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ER",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:12373,ERROR,ERROR,12373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"ian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. #### Expected behavior; Program runs :). #### Actual behavior; Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6053:1840,error,error,1840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053,2,"['Error', 'error']","['Error', 'error']"
Availability,"iates - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:13:29.982 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:13:29.983 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:13:29.983 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:13:29.983 INFO AnalyzeCovariates - Deflater: IntelDeflater; 22:13:29.983 INFO AnalyzeCovariates - Inflater: IntelInflater; 22:13:29.983 INFO AnalyzeCovariates - GCS max retries/reopens: 20; 22:13:29.983 INFO AnalyzeCovariates - Requester pays: disabled; 22:13:29.984 INFO AnalyzeCovariates - Initializing engine; 22:13:29.984 INFO AnalyzeCovariates - Done initializing engine; 22:13:30.002 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates13996065741193890473.csv'; 22:13:30.002 INFO AnalyzeCovariates - Generating plots file './sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf'; 22:13:30.518 INFO AnalyzeCovariates - Shutting down engine; [August 7, 2023 at 10:13:30 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=113246208; org.broadinstitute.hellbender.utils.R.RScriptExecutorException:; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10708586791705723928';source('/tmp/BQSR.12372590345390592260.R'); /tmp/AnalyzeCovariates13996065741193890473.csv /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; Stdout:; Stderr:; Attaching package: ‘gplots’. The following object is masked from ‘package:stats’:. lowess. Error in names(x) <- value :; 'names' attribute [6] must be the same length as the vector [1]; Calls: source ... finishTable -> .gsa.assignGATKTableToEnvironment -> colnames<-; In addition: Warning messages:; 1: In type.convert.default(d[, i]) :; 'as.is' shoul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8456:2824,down,down,2824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456,1,['down'],['down']
Availability,"ideal for all users, since you'll lose detail in the lower copy ratios, but I hope this is a good start! . --- ; ```; WriteModeledSegmentsPlot = function(sample_name, allelic_counts_file, ; denoised_copy_ratios_file, modeled_segments_file, ; contig_names, contig_lengths, output_dir, output_prefix) {; modeled_segments_df = ReadTSV(modeled_segments_file); max_log2_ratio = max(modeled_segments_df$LOG2_COPY_RATIO_POSTERIOR_10, ; modeled_segments_df$LOG2_COPY_RATIO_POSTERIOR_50,; modeled_segments_df$LOG2_COPY_RATIO_POSTERIOR_90); max_copy_ratio = (2^max_log2_ratio) + 1; ; num_plots = ifelse(all(file.exists(c(denoised_copy_ratios_file, allelic_counts_file))), 2, 1); png(output_file, 12, 3.5 * num_plots, units=""in"", type=""cairo"", res=300, bg=""white""); par(mfrow=c(num_plots, 1), cex=0.75, las=1); ; if (file.exists(denoised_copy_ratios_file) && denoised_copy_ratios_file != ""null"") {; denoised_copy_ratios_df = ReadTSV(denoised_copy_ratios_file); ; #transform to linear copy ratio; denoised_copy_ratios_df[[""COPY_RATIO""]] = 2^denoised_copy_ratios_df[[""LOG2_COPY_RATIO""]]; ; #determine copy-ratio midpoints; denoised_copy_ratios_df[[""MIDDLE""]] = round((denoised_copy_ratios_df[[""START""]] + denoised_copy_ratios_df[[""END""]]) / 2); ; SetUpPlot(sample_name, ""denoised copy ratio"", 0, max_copy_ratio, ""contig"", contig_names, contig_starts, contig_ends, TRUE); PlotCopyRatiosWithModeledSegments(denoised_copy_ratios_df, modeled_segments_df, contig_names, contig_starts); }; ; if (file.exists(allelic_counts_file) && allelic_counts_file != ""null"") {; allelic_counts_df = ReadTSV(allelic_counts_file); ; SetUpPlot(sample_name, ""alternate-allele fraction"", 0, 1.0, ""contig"", contig_names, contig_starts, contig_ends, TRUE); PlotAlternateAlleleFractionsWithModeledSegments(allelic_counts_df, modeled_segments_df, contig_names, contig_starts); }; ; dev.off(); ; #check for created file and quit with error code if not found; if (!file.exists(output_file)) {; quit(save=""no"", status=1, runLast=FALSE); }; }```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6391:2833,error,error,2833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6391,1,['error'],['error']
Availability,"iedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:4739,failure,failure,4739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['failure'],['failure']
Availability,"if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://user-images.githubusercontent.com/10062863/210841121-15a4d357-dbfa-47e2-808b-08cdeb6d42be.png"">. The rendering in gatkdoc (the variable name appears in the text, but it should be interpolated):; <img width=""1055"" alt=""Screen Shot 2023-01-05 at 12 12 46 PM"" src=""https://user-images.githubusercontent.com/10062863/210840538-9a42bf02-b968-4ac9-9591-90512e87ab50.png"">. Note that QuickDocumentation within IntelliJ seems to render them correctly. Additionally, I noticed that some {@link} targets are not rendering correctly in gatkdoc, i.e., these links in `ScoreVariantAnnotations`:; ```; * {@link VariantRecalibrator} workflow. Using a previously trained model produced by {@link TrainVariantAnnotationsModel},; ```; work in javadoc, but not gatkDoc, even though the target in this case IS included in the set of objects available to the gatkDoc process. The gatkdoc process is not translating these (and apparently its replacing them with the text). But generating the anchor tags will require translation because the javadoc output files are organized hierarchically whereas the gatkdoc files are flat. The links in html file generated by javadoc have anchor tags with proper hrefs, whereas the html generated by gatkDoc has only the plain text (no anchors - the links are removed and replaced with the text by gatkdoc). Even if we embedded the javadoc in with the gatkdoc, we still would have to translate some links for gatkdoc targets at doc generation time, because when a user views the gatkdoc for `ScoreVariants`, we want the link to `VariantRecalibrator` to go to the `VariantRecalibrator` page from gatkdoc, not the page from javadoc. Also, for any entities for which there is no gatkdoc (i.e., a method reference), the gatkdoc pa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8146:2822,avail,available,2822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146,1,['avail'],['available']
Availability,"ikelihoodMap is null; 17:04:17.128 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.130 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.132 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.670 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.671 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:18.317 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 9.92553E-4; 17:04:18.318 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.099975122; 17:04:18.318 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.19 sec; 17:04:18.319 INFO HaplotypeCaller - Shutting down engine; [November 12, 2019 5:04:18 PM CST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2102919168; java.lang.IllegalArgumentException: Unexpected base in allele bases 'TACTAGGCGGGACAAGTATCCAAGCGCAAATCTACGCCGACTGAACACGCACCATAATTCAACTTTTACAGTCTCAAAACTGACGTATTCCTCTAACATTTACCACAGGTTCTATCTCGATGCCATGCACAAAACAATCTCTGCACCCTTCTCTAGTACATTTGTGCAGCACTACTTTGTGCTCGGACTACTTTTGAC*TTCGCCCATGAATACTGTGTTGTGCGGCCTTGACGCCATGGGGCATAGAGAAGTCATAAAGAAGTGAAATTTTGTTTTGGGTGTAACTATTGTTCTTTTGTTAGTATATTGATGACGAAATTGTTTACAAGGTAAGTGGGGATTAGAAAAGGATGAGAAGGAACCCTGTAAAAGGTAAAGCACACACAAAAAACATATTGTAAGTTACACTACAATGACTATGTACAACGCAAACCTGTCTATATTAGCAACCGCTAATATTACTAT'; 	at htsjdk.variant.variantcontext.Allele.<init>(Allele.java:170); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.<init>(Haplotype.java:40); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.<init>(Haplotype.java:49); 	at org.broadinstitute.hellbender.uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6260:6122,down,down,6122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6260,1,['down'],['down']
Availability,ildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:11645,ERROR,ERROR,11645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,ildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:10387,ERROR,ERROR,10387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,ildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:14871,ERROR,ERROR,14871,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:29252,failure,failure,29252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['failure'],['failure']
Availability,"ill be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 17:08:13.200 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 17:08:13.206 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 17:08:13.227 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 17:08:13.228 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 17:08:13.260 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 17:08:13.260 INFO IntelPairHmm - Available threads: 1; 17:08:13.260 INFO IntelPairHmm - Requested threads: 4; 17:08:13.261 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 17:08:13.261 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 17:08:13.346 INFO ProgressMeter - Starting traversal; 17:08:13.346 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 17:08:17.401 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes. 17:08:43.866 INFO ProgressMeter - chr1:1053465 0.5 3780 7431.7. ...Many lines in between and then... 19:11:09.189 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.190328316; 19:11:09.189 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 398.5135636; 19:11:09.190 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 258.73 sec; 19:11:09.190 INFO HaplotypeCaller - Shutting down engine; [August 27, 2020 7:11:09 PM CDT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 122.97 minutes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6783:3608,avail,available,3608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6783,1,['avail'],['available']
Availability,ilterMutectCalls - HTSJDK Version: 2.20.1; 22:09:03.376 INFO FilterMutectCalls - Picard Version: 2.20.5; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:09:03.376 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:09:03.377 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:09:03.377 INFO FilterMutectCalls - Deflater: IntelDeflater; 22:09:03.377 INFO FilterMutectCalls - Inflater: IntelInflater; 22:09:03.377 INFO FilterMutectCalls - GCS max retries/reopens: 20; 22:09:03.377 INFO FilterMutectCalls - Requester pays: disabled; 22:09:03.378 INFO FilterMutectCalls - Initializing engine; 22:09:04.031 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz; 22:09:04.071 INFO FilterMutectCalls - Shutting down engine; [2021年2月28日 下午10时09分04秒] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.52 minutes.; Runtime.totalMemory()=1552416768; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /data/nws/WES/GenomicsDBImport/200924_A00679_0401_AHKGKKDSXY/Set13-3_L3_379X79.somatic.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:375); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:327); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:277); 	at org.broadinstitute.hellbender.engine.VariantWalker.initializeDrivingVariants(VariantWalker.java:58); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:706); 	at org.broadinstit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7114:2721,down,down,2721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114,1,['down'],['down']
Availability,improve error message during build when ToolProvider is unavailable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4532:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4532,1,['error'],['error']
Availability,improved an error message for missing sequence dictionary. Closes #1305.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2653:12,error,error,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2653,1,['error'],['error']
Availability,improving error message in GATKSparkTool.writeReads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2336:10,error,error,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2336,1,['error'],['error']
Availability,"improving gatk, one error message at a time",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3812:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3812,1,['error'],['error']
Availability,"in CountVariants -V means 'variant'. in IndexFeatureFile it seems to mean 'verbosity', see message below:. ```; ./gatk-launch IndexFeatureFile -V /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf; ```. ``````; A USER ERROR has occurred: Invalid command line: Argument VERBOSITY has a bad value: /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf. '/xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf' is not a valid value for LogLevel. Possible values: {ERROR, WARNING, INFO, DEBUG}```. ``````",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1306:240,ERROR,ERROR,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1306,2,['ERROR'],['ERROR']
Availability,"in conjunction with a locally modified version of gatk, to communicate with aws. Since we had the code that allows for communication with aws anyway, we decided to share it and maybe it can be part of the gatk toolkit in the future. # How does it work?; The user is able to provide an additional parameter '--s3', adding the nio-spi-for-s3-2.0.0-dev-all.jar file to the java classpath. File locations starting with 's3://' are then able to be provided, resulting of reading/writing of these files to aws. When using this option, however, the aws credentials have to be set correctly, for which you can find more information [here](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html). Currently, I haven't implemented it for --spark due to a lack of need/inexperience with spark. # Current Issues; We found some issues for which we do not know any solution. If this tool was to be implemented in GATK in the future, these have to be resolved eventually. ## Doesn't work for picard-based tools; First, 'aws-java-nio-spi-for-s3' doesn't seem work for (most) picard tools, since most of them utilise the java.io.File package, which is limited to local filesystem files, as opposed to java.nio.Path (we think).; ## Issues reading genome reference files from AWS; Secondy, most tools that require a reference genome (i.e. BaseRecalibrator, HaplotypeCaller..) do not seem function when provided with a reference genome file stored on AWS. The error we receive can be found underneath and is much less clear. We believe that the issue lies in the interaction between the caching of the indexed reference file and 'aws-java-nio-spi-for-s3', since we tested in a custom java script that the package 'htsjdk' works like intended when the reference genome is read from AWS.; Notably, some tools do not have this issue, such as the the vqsr tools (VariantRecalibrator and applyVQSR).; ![image](https://github.com/broadinstitute/gatk/assets/149685151/24d16941-b40c-4a5a-b8e6-0dc7415c6b1b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8672:1747,error,error,1747,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672,1,['error'],['error']
Availability,"inaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Exception in thread ""main"" org.broadinstitute.hellbender.exceptions.UserException: 'FixVcfHead' is not a valid command.; Did you mean this?; FixVcfHeader; 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:341); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:172); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:192); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. I expect something without the stack trace and the scary ""Exception"" message. For example:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Did you mean this?; FixVcfHeader; ```. The same happens with unknown commands. The code that should be changed for that is the following, where the `setupConfigAndExtractProgram` call should be also inside the try block:. https://github.com/broadinstitute/gatk/blob/8ac2f102b303f343c4787ad4e3359335641c5121/src/main/java/org/broadinstitute/hellbender/Main.java#L190-L212",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4256:1408,Avail,Available,1408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4256,1,['Avail'],['Available']
Availability,"ine.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:149); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:190); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:107); at org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest.testVCFModeIsConcordantWithGATK3_8Results(HaplotypeCallerSparkIntegrationTest.java:143); Caused by:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 26, localhost, executor driver): java.util.ConcurrentModificationException; at java.util.ArrayList.sort(ArrayList.java:1464); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); at org.broadinstitute.hellbender.tools.Haplo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:4005,failure,failure,4005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['failure'],['failure']
Availability,"info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 21:02:17.065 INFO GenotypeGVCFs - Done initializing engine; 21:02:17.110 INFO ProgressMeter - Starting traversal; 21:02:17.111 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:02:45.390 INFO ProgressMeter - Chr1:369351 0.5 1000 2121.8; 21:03:16.132 INFO ProgressMeter - Chr1:505230 1.0 2000 2033.2; 21:03:29.421 INFO ProgressMeter - Chr1:575285 1.2 3000 2489.3; ... (continued for more than 1000 lines); 21:49:51.317 INFO ProgressMeter - Chr1:3713346 47.6 242000 5087.2; 21:50:06.596 INFO ProgressMeter - Chr1:3718941 47.8 244000 5102.0; [TileDB::ReadState] Error: Cannot read tile from file; Memory map error.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ReadState] Error: Cannot read tile from file; Memory map error; Using GATK jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /mnt/d/Share/SYJ/liulan/ref/genome.fa -V gendb:///mnt/d/Share/SYJ/liulan/DBI -O /mnt/d/Share/SYJ/liulan/sortbam/combDBI.vcf.gz --tmp-dir /mnt/d/Share/SYJ/liulan/NOHUP/tmp; ```. I have tried to change -Xmx to 20G and 100G, etc. But all processes stop running at Variants Processed 244000.; Could you tell me what the problem is?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8302:4584,error,error,4584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302,3,"['Error', 'error']","['Error', 'error']"
Availability,"ing GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-b12; 10:49:12.232 INFO GenomicsDBImport - Start Date/Time: June 18, 2021 10:49:11 AM KST; 10:49:12.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7324:1134,Error,Error,1134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324,1,['Error'],['Error']
Availability,"ing a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2 – GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2) for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is: . gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processi ng\_faci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7479:1010,error,error,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479,1,['error'],['error']
Availability,"ing down engine; [September 19, 2021 6:47:11 PM CST] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinxi_pkuhpc/lus; tre1/ljx/reference_genomes/funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7474:1052,error,error,1052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474,2,['error'],['error']
Availability,"ing each allele"">; ##FORMAT=<ID=F2R1,Number=R,Type=Integer,Description=""Count of reads in F2R1 pair orientation supporting each allele"">; ##FORMAT=<ID=FT,Number=1,Type=String,Description=""Genotype-level filter"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MBQ,Number=A,Type=Integer,Description=""median base quality"">; ##FORMAT=<ID=MFRL,Number=R,Type=Integer,Description=""median fragment length"">; ##FORMAT=<ID=MMQ,Number=A,Type=Integer,Description=""median mapping quality"">; ##FORMAT=<ID=MPOS,Number=A,Type=Integer,Description=""median distance from end of read"">; ##FORMAT=<ID=OBAM,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact modes."">; ##FORMAT=<ID=OBAMRC,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact mode complements."">; ##FORMAT=<ID=OBF,Number=A,Type=Float,Description=""Fraction of alt reads indicating orientation bias error (taking into account artifact mode complement)."">; ##FORMAT=<ID=OBP,Number=A,Type=Float,Description=""Orientation bias p value for the given REF/ALT artifact or its complement."">; ##FORMAT=<ID=OBQ,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for a given REF/ALT error."">; ##FORMAT=<ID=OBQRC,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for the complement of a given REF/ALT error."">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5158:2950,error,error,2950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5158,1,['error'],['error']
Availability,"ing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.679 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.684 INFO ProgressMeter - Starting traversal; 14:34:51.684 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:34:51.694 INFO IndexFeatureFile - Shutting down engine; [August 2, 2021 at 2:34:51 PM CEST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=113246208; java.lang.IllegalArgumentException: Unexpected value: Ensembl_canonical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1391); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:197); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.<init>(GencodeGtfTranscriptFeature.java:19); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.create(GencodeGtfTranscriptFeature.java:23); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$2.create(GencodeGtfFeature.java:768); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.create(GencodeGtfFeature.java:327); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(Ab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7385:2450,down,down,2450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385,1,['down'],['down']
Availability,"ingPythonExecutorIntegrationTest.java:34); ```. Error messages in another test case:; ```; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0xE2) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x80) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x99) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; ```. This test is skipped without any apparent reason:; ```; Running Test: Test method loadIndex(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > loadIndex FAILED; java.lang.UnsatisfiedLinkError: 'boolean org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createReferenceIndex(java.lang.String, java.lang.String, java.lang.String)'; at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createReferenceIndex(Native Method); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:227); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:196); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:1617,error,error,1617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['error'],['error']
Availability,installing git-lfs and downloading large files on travis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/907:23,down,downloading,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/907,1,['down'],['downloading']
Availability,instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:2430,ERROR,ERROR,2430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['ERROR'],['ERROR']
Availability,"intGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:1800,echo,echo,1800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['echo'],['echo']
Availability,"intReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. *****************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:3554,ERROR,ERROR,3554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['ERROR'],['ERROR']
Availability,"internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScope(FieldSerializerGenericsUtil.java:116); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.newCachedFieldOfGenericType(FieldSerializerGenericsUtil.java:225); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.newCachedField(FieldSerializer.java:368); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.createCachedFields(FieldSerializer.java:331); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:261); 	at com.esotericsoftware.kryo.serial",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:18184,down,down,18184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['down'],['down']
Availability,"intervals that had >1000x coverage.; 19:50:09.183 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 12444440 mapped template names.; 19:50:59.411 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 19:57:19.267 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 62077256 kmers.; 20:03:43.231 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 42811284 unique template names for assembly.; 21:11:03.427 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 21:11:24.841 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 6799 variants.; 21:11:24.854 INFO StructuralVariationDiscoveryPipelineSpark - INV: 253; 21:11:24.855 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 4002; 21:11:24.855 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1179; 21:11:24.855 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1365; 21:11:25.045 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 9:11:25 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 84.05 minutes.; Runtime.totalMemory()=15439757312; ```. Master with `minEvidenceCount` set to 7:. ```; 15:51:52.187 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 15:52:34.778 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 185595 intervals.; 15:52:34.923 INFO StructuralVariationDiscoveryPipelineSpark - Killed 434 intervals that were near reference gaps.; 15:53:05.883 INFO StructuralVariationDiscoveryPipelineSpark - Killed 188 intervals that had >1000x coverage.; 15:54:13.400 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 29691982 mapped template names.; 15:54:45.921 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 16:05:36.053 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 191872350 kmers.; ```; Which I killed after 108 minute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684:5162,down,down,5162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684,1,['down'],['down']
Availability,"invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist.; 	at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:8161,ERROR,ERROR,8161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['ERROR'],['ERROR']
Availability,"io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx20G -jar /gatk/gatk-package-4.1.5.0-local.jar Mutect2 --max-mnp-distance 0 --input /mnt/data/input/gs/file.bam.cram --reference /mnt/data/input/gs/gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta --germline-resource /mnt/data/input/gs/ukbb_v2/projects/jamesp/data/mutect2/bravo-dbsnp-all-f5.exome.chr.sorted.reheader.vcf.gz --intervals /mnt/data/input/gs/gcp-public-data--broad-references/hg38/v0/exome_calling_regions.v1.interval_list --output /mnt/data/output/gs/file.bam.cram.unfiltered.vcf.gz; ```. 2. GenomicsDB creation for PON step:; ```; Using GATK jar /gatk/gatk-package-4.1.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx40G -jar /gatk/gatk-package-4.1.5.0-local.jar GenomicsDBImport --reference /mnt/data/input/gs/gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta --intervals chr1 --intervals chr2 --intervals chr3 --intervals chr4 --intervals chr5 --intervals chr6 --intervals chr7 --intervals chr8 --intervals chr9 --intervals chr10 --intervals chr11 --intervals chr12 --intervals chr13 --intervals chr14 --intervals chr15 --intervals chr16 --intervals chr17 --intervals chr18 --intervals chr19 --intervals chr20 --intervals chr21 --intervals chr22 --intervals chrX --genomicsdb-workspace-path pon_db {etc}. ***********************************************************************. A USER ERROR has occurred: Bad input: GenomicsDBImport does not support GVCFs with MNPs. MNP found at chr1:1914706 in VCF /mnt/data/input/gs/file.bam.cram.unfiltered.vcf.gz; ```. #### Expected behavior; I would expect no MNPs if Mutect2 is run in `--max-mnp-distance 0` mode. #### Actual behavior; There are apparently MNPs, so it seems that `--max-mnp-distance 0` has no effect?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473:2007,ERROR,ERROR,2007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473,1,['ERROR'],['ERROR']
Availability,ion 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs; AccessDownload=none; AccessUpload=none; DownloadTransfers=basic; UploadTransfers=basic. Client IP addresses:; xx.xx.xx.xx; xx.xx.xx.xx; xx.xx.xx.xx; portage$ ls -latr; total 188; -rw-r--r-- 1 portage portage 428 Apr 20 22:05 codecov.yml; -rwxr-xr-x 1 portage portage 5741 Apr 20 22:05 build_docker.sh; -rw-r--r-- 1 portage portage 32161 Apr 20 22:05 build.gradle; -rw-r--r-- 1 portage portage 37502 Apr 20 22:05 README.md; -rw-r--r-- 1 portage portage 1502 Apr 20 22:05 LICENSE.TXT; -rw-r--r-- 1 portage portage 1555 Apr 20 22:05 Dockerfile; -rw-r--r-- 1 portage portage 1128 Apr 20 22:05 AUTHORS; -rw-r--r-- 1 portage portage 8237 Apr 20 22:05 .travis.yml; -rw-r--r-- 1 portage portage 395 Apr 20 22:05 .gitignore; -rw-r--r-- 1 portage portage 128 Apr 20 22:05 .gitattributes; -rw-r--r-- 1 portage portage 142 Apr 20 22:05 .dockerignore; drwxr-xr-x 2 portage portage 4096 Apr 20 22:05 resources_for_CI; drwxr-xr-x 2 portage portage 4096 Apr 20 22:05 hooks; -rwxr-xr-x 1 portage portage 5242 Apr 20 22:05 gradlew; drwxr-xr-x 3 portage port,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:16388,Down,DownloadTransfers,16388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Down'],['DownloadTransfers']
Availability,ion [4.1.9.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File opening error; path=/storage/home/data/gendb/chr13/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/RAW_MQandDP.tdb; errno=122(Disk quota exceede; d); [TileDB::WriteState] Error: Cannot write segment to file.; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File opening error; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__book_keeping.tdb.gz; errno=122(Disk quota e; xceeded); [TileDB::utils] Error: (write_to_file_after_compression) Could not write compressed bytes to internal buffer; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__book_keeping.tdb.gz; ; errno=122(Disk quota exceeded); [TileDB::BookKeeping] Error: Cannot finalize book-keeping; Failure to write to file /storage/home/data/gendb/chr13/chr13$32310639$32310731/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__book_keeping.tdb.gz.; [TileDB::FileSystem] Error: (create_file) Failed to create file; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__tiledb_fragment.tdb; errno=122(Disk quota exceeded); [TileDB::utils] Error: (create_fragment_file) Failed to create fragment file; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087; errno=122(Disk quota exceeded); 11:23:52.390 erro NativeGenomicsDB - pid=57964 tid=57984 VariantStorageManagerException exception : Error while finalizing TileDB array chr13$32310639$32310731; TileDB error message : [TileDB::WriteState] Error: Cannot write segment to file; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File opening error; path=/s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6950:1102,Error,Error,1102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6950,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"ion workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:1482,error,errors,1482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,1,['error'],['errors']
Availability,"ion$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:396); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:173); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 17/08/22 18:54:43 INFO ShutdownHookManager: Shutdown hook called; 17/08/22 18:54:43 INFO ShutdownHookManager: Deleting directory /cromwell_root/tmp.5EEmH0/root/spark-4900f4dd-59fb-4a6d-96c8-8b99edb608ab; ```; I am using the docker image `broadinstitute/gatk:4.beta.3`. Here's the command line:; ```; java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx8g -jar /root/gatk.jar SparkGenomeReadCounts \; --input /cromwell_root/broad-dsde-methods/mehrtash/cromwell-executions/CRAMCollectCoverage/29948994-7457-43b9-b6e3-5d188906595d/call-CramToBam/shard-0/8007540135.bam \; --reference /cromwell_root/broad-dsde-methods/sv/reference/GRCh38/Homo_sapiens_assembly38.fasta \; --binsize 1000 \; --keepXYMT true \; --disableToolDefaultReadFilters false \; --disableSequenceDictionaryValidation true \; $(if [ true = true ]; then echo "" --disableReadFilter NotDuplicateReadFilter ""; else echo """"; fi) \; --outputFile 8007540135.coverage.tsv \; --verbosity DEBUG; ```. No NIO, no GCS -- everything is localized on the VM HDD at this stage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3492:6150,echo,echo,6150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3492,2,['echo'],['echo']
Availability,ion.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:10442,ERROR,ERROR,10442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,ion.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:9184,ERROR,ERROR,9184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,ion.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:10257,ERROR,ERROR,10257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,ion.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:8999,ERROR,ERROR,8999,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"ion: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:34658,ERROR,ERROR,34658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['ERROR'],['ERROR']
Availability,ionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:8594,ERROR,ERROR,8594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"ionStream.readObject(KryoSerializer.scala:246); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:4657,ERROR,ERROR,4657,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['ERROR'],['ERROR']
Availability,"ion_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -I gs://fc-8268e82b-ed61-4e04-a8c9-a95a05c0952e/bda6f5ba-8928-45bf-a6b0-9fe67d8dd9a4/PreProcessingForVariantDiscovery_GATK4/cccdda67-56e1-4363-aa6c-46ce53ef8afd/call-GatherBamFiles/attempt-2/Abrams_cell.bam -tumor Abrams_1 --germline-resource gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/canid_wgs_ref.1.0.no_samples.vcf.gz -pon gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/b92f3c35-5813-455b-94dc-3de3b54f5f98/Mutect2_Panel/c9f21d8a-384e-4d17-a6f8-79a502698827/call-MergeVCFs/1-Mutect2_PON_2019-07-25T22-08-49.vcf -L gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/f2138b33-3918-4f8a-9b87-1823a0084ac3/Mutect2/c4844164-ecad-4878-9e5d-cd134a7fb40d/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf --f1r2-tar-gz f1r2.tar.gz --af-of-alleles-not-in-resource 0.0007 --downsampling-stride 20 --max-reads-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6`; ```. The germline resource is a VCF of approximately 80 million SNPs and indels (including multi allelic sites) called from a large number of canine WGS. It is formatted as a VCF with no sample information:; ```; chr1 240 . TG T 464.40 PASS AC=4;AF=0.011;AN=332;BaseQRankSum=0.674;ClippingRankSum=0;DP=14798;ExcessHet=0.0026;FS=5.63;InbreedingCoeff=-0.005;MLEAC=14;MLEAF=0.017;MQ=7.49;MQRankSum=-0.967;QD=22.11;ReadPosRankSum=0.967;SOR=3.18; ```. The VCF for variants for contamination is a subset of this VCF, with only biallelic SNPs with AF between 0.01 and 0.2. Initially, it was formatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:3915,down,downsampling-stride,3915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['down'],['downsampling-stride']
Availability,"ipts are:. java -Xmx300g -jar $GATK Mutect2 \; --dont-use-soft-clipped-bases true \; --tmp-dir $cw/$i/tmp \; --input $DNAbam/ADAR16-DNA-2_NKD180600323/ADAR16-DNA-2_NKD180600323.best.uniq.pair.sort.markdup.bam \; --input $RNAbam/$i/$i.merge.markdup.reheader.bam \; --reference $genome\; --output $cw/$i/$i.dna.rna.vcf \; --normal-sample ADAR16-DNA-2_NKD180600323 \; --tumor-sample $i \; -bamout $cw/$i/$i.support.bam. and tail of error log are:. 12:05:06.287 INFO ProgressMeter - scaffold23905:111448 948.1 636040 670.9; 12:05:30.519 INFO ProgressMeter - scaffold23905:133852 948.5 636120 670.7; 12:05:57.277 INFO ProgressMeter - scaffold23905:147186 949.0 636170 670.4; 12:24:34.669 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 31261.455155273; 12:24:34.670 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 14618.28 sec; INFO	2019-04-13 12:45:11	SortingCollection	Creating merging iterator from 2 files; 13:30:49.708 INFO Mutect2 - Shutting down engine; [April 13, 2019 1:30:49 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 1,035.35 minutes.; Runtime.totalMemory()=238653800448; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421); 	at java.lang.StringBuilder.append(StringBuilder.java:136); 	at htsjdk.samtools.SAMTextHeaderCodec.advanceLine(SAMTextHeaderCodec.java:142); 	at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:97); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.loadDictionary(ReferenceSequenceFileFactory.java:235); 	at htsjdk.samtools.reference.AbstractFastaSequenceFile.(AbstractFastaSequenceFile.java:68); 	at htsjdk.samtools.reference.Abstract",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900:1030,down,down,1030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900,1,['down'],['down']
Availability,"irty AF estimation and choose the alleles with the larger frequencies. This estimate should use all the genotype likelihoods rather than just the top genotype giving a nominal score for all the alleles that would allow us to sort them all and make a better and less arbitrary selection. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260486156). I believe this was done by @vruano and @SHuang-Broad already -- right guys? Can we close this? . ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260684553). My understanding of the current state is that there are several possible places with alt allele reduction in HC, in order:; 1. The fix I put in, to prevent the calculator from becoming too slow or blow up, so downstream steps won't even include these alleles in their likelihood calculations;; 2. The fix Valentine put in, which happens after the read likelihoods are calculated (and optionally down-sampled). The relevant code is in `HaplotypCallerGenotypingEngine.java` around lines 267-279. This is the state in GATK3, in GATK4 the second possibility is not ported yet. Regarding alt allele reduction in AF calculator, has [this](https://github.com/broadinstitute/gatk/pull/1918) been ported back to GATK3?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260688221). By ""possible place"" I mean they don't always remove alt alleles, just when certain conditions are met, and are independent. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260781482). I don't think https://github.com/broadinstitute/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad commented on [Mon Dec 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-268016064). Looking more closely, isn't th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2958:2727,down,down-sampled,2727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958,1,['down'],['down-sampled']
Availability,"is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; MarkDuplicates. ### Affected version(s); - [ ] Latest public release version [version?]; 4.6.0.0 GATK and Picard 3.2.0; - [ ] Latest master branch as of [date of test?]; 3 Jul 2023. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; I'm trying to use gatk for finding snps in exome capture project. I get an error when trying to use MarkDuplicates - I tried using it from picard and from gatk. The screen output is:; ```; picard MarkDuplicates I=WA02_i5-537_i7-98_S11819_L004.bam O=test.dup.bam M=marked_dup_metrics.txt; INFO 2024-07-03 15:25:31 MarkDuplicates. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; **********; https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** MarkDuplicates -I WA02_i5-537_i7-98_S11819_L004.bam -O test.dup.bam -M marked_dup_metrics.txt; **********. 15:25:31.262 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.so; [Wed Jul 03 15:25:31 CEST 2024] MarkDuplicates INPUT=[WA02_i5-537_i7-98_S11819_L004.bam] OUTPUT=test.dup.bam METRICS_FILE=marked_dup_metrics.txt MAX_SEQUENCES_FOR_DISK_READ_ENDS_MA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8904:1638,error,error,1638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8904,1,['error'],['error']
Availability,"is for the number of bias covariates _and_ how to take these numbers and project an approximate memory usage. 2. It would appear that GermlineCNVCaller will, by default, attempt to use all CPU cores available on the machine. From the WDL I see that setting environment variables `MKL_NUM_THREADS` and `OMP_NUM_THREADS` seems to control the parallelism? It would be nice if `GermlineCNVCaller` took a `--threads` and then set these before spawning the python process. 3. Runtime? This would be really nice to have some guidelines around as I get wildly varying results depending on how I'm running. My experimentation is with a) 20 45X WGS samples, b) bin size = 500bp, c) running on a 96-core general purpose machine at AWS with 384GB of memory. My first attempt a) scattered the genome into 48 shards of approximately 115k bins each, representing ~50mb of genome and b) ran 24 jobs concurrently but failed to set the environment variables to control parallelism. In that attempt the first wave of jobs were still running after 24 hours and getting close to finishing up the initial de-noising epoch, with 3/24 having failed due to memory allocation failures. My second attempt, now running, scattered the genome into 150 shards, and is running 12 jobs at a time with 8 cores each and the environment variables set. On the second attempt it looks like the jobs will finish the first denoising epoch in < 1 hour each. That's far faster than the 6x reduction in runtime you might expect if a) runtime is linear in the number of bins and b) runtime is proportional to 1/cpus used. Without doing a lot more experiments it's hard to tell whether the better runtime is due to less fighting over resources (I can imagine 24 jobs each running 96 threads could degrade performance) or because runtime is super-linear vs. number of bins. I'm not asking for total precision, but the current docs are not really enough for anyone outside the GATK team to get the CNV caller up and running in an efficient manner.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166:1982,failure,failures,1982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166,1,['failure'],['failures']
Availability,"isTK-3.8-0-ge9d806836%204/GenomeAnalysisTK.jar!/META-INF/log4j-provider.properties; ERROR StatusLogger Log4j2 could not find a logging implementation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console...; INFO 10:47:55,875 GenomeAnalysisEngine - Deflater: IntelDeflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Inflater: IntelInflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Strictness is SILENT ; INFO 10:47:56,246 GenomeAnalysisEngine - Downsampling Settings: No downsampling ; INFO 10:47:56,255 SAMDataSource$SAMReaders - Initializing SAMRecords in serial ; INFO 10:47:56,333 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.07 ; ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A USER ERROR has occurred (version 3.8-0-ge9d806836): ; ##### ERROR; ##### ERROR This means that one or more arguments or inputs in your command are incorrect.; ##### ERROR The error message below tells you what is the problem.; ##### ERROR; ##### ERROR If the problem is an invalid argument, please check the online documentation guide; ##### ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.; ##### ERROR; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.; ##### ERROR; ##### ERROR MESSAGE: Input files reads and reference have incompatible contigs. Please see https://software.broadinstitute.org/gatk/documentation/article?id=63for more information. Error details: No overlapping contigs found.; ##### ERROR reads contigs = [LmjF04_01_20050601_V5.2, LmjF05_01_20050601_V5.2, LmjF24_01_20050601_V5.2, LmjF01_01_20050601_V5.2, LmjF03_01_20050601_V5.2, LmjF13_01_20050601_V5.2, LmjF14_01_20050601_V5.2, LmjF19",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6798:2907,ERROR,ERROR,2907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6798,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"isableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN N",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:2846,ERROR,ERROR,2846,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['ERROR'],['ERROR']
Availability,"ithub.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolve all these, and that will definitely happen in GATK 4. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-281073466). Ah, interesting, thanks Steve. Do you have any sense of when you might be able to look further into this? This is not to pressure you, just to estimate the roadmap/timeline. An order of magnitude (weeks, months, more) would be fine. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-287838044). I'm going to move this issue to GATK; feel free to close it there if it is redundant with existing issues describing the same problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:10544,down,down,10544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,2,"['down', 'redundant']","['down', 'redundant']"
Availability,"itute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1098); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:4327,error,error,4327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['error'],['error']
Availability,itute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: bioinfo: bioinfo: unknown error; 	at java.net.InetAddress.getLocalHost(InetAddress.java:1505); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: bioinfo: unknown error; 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); 	at java.net.InetAddress.getLocalHost(InetAddress.java:1500); 	... 21 more; . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23594/java-related-error-encountered-while-running-gatk-pathseqpipelinespark/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:4719,error,error,4719,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,2,['error'],"['error', 'error-encountered-while-running-gatk-pathseqpipelinespark']"
Availability,itute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 2019-10-29T18:18:04.001194549Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 2019-10-29T18:18:04.001367357Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 2019-10-29T18:18:04.001518160Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 2019-10-29T18:18:04.001673083Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-29T18:18:04.001846904Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-29T18:18:04.002024760Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-29T18:18:04.002140012Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-29T18:18:04.002232542Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-29T18:18:04.002242727Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-29T18:18:04.002292461Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 2019-10-29T18:18:04.002301667Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 2019-10-29T18:18:04.002307019Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-29T18:18:04.002311722Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 2019-10-29T18:18:04.002316449Z 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 2019-10-29T18:18:04.002321526Z 	at org.broadinstitute.hellbender.tools.walkers.mutect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6237:1243,Error,ErrorProbabilities,1243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6237,1,['Error'],['ErrorProbabilities']
Availability,"j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n. log4j.appender.file=org.apache.log4j.FileAppender; log4j.appender.file.file=/tmp/logs/spark/log4j-block_manager-output.txt; log4j.appender.file.layout=org.apache.log4j.PatternLayout; log4j.appender.file.layout.ConversionPattern=%d{ABSOLUTE} %p %c{1}: %m%n. # Settings to quiet third party logs that are too verbose; log4j.logger.org.eclipse.jetty=WARN; log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR; log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN; log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN; #log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO; #log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO. # added to suppres LiveListenerBus ; log4j.logger.org.apache.spark.scheduler.LiveListenerBus=OFF. #log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE,file; log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=WARN,file; log4j.additivity.org.apache.spark.storage.ShuffleBlockFetcherIterator=false; #log4j.logger.org.apache.spark.network.shuffle.OneForOneBlockFetcher=TRACE,file; #log4j.additivity.org.apache.spark.network.shuffle.OneForOneBlockFetcher=false; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:2854,ERROR,ERROR,2854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,1,['ERROR'],['ERROR']
Availability,"java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /opt/conda/envs/789546e2/share/gatk4-4.0.1.1-0/gatk-package-4.0.1.1-local.jar GenotypeGVCFs -ploidy 1 -R references/359488/genome_fasta.fasta --annotate-with-num-discovered-alleles true --annotations-to-exclude InbreedingCoeff -V gendb://typing/gatk_gvcfs/full_genome/359488/bwa/genomics_db -O typing/gatk_gvcfs/full_genome/359488/bwa/all_samples.vcf; ```; In between the last ProgressMeter and the Shutting down of the engine, I see the java process still running with top. Do you know what could be causing the problem ? Could it be related to -ERC BP_RESOLUTION ? I used to use -ERC GVCF before but I would rather keep the information of the coverage for post filtering, and I am not sure how to use --GVCFGQBands to match my criteria for coverage filtering. Thanks a lot for your help !. Edit: sorry with the latest version of gatk I get a new message error :; ```; 08:22:54.446 INFO ProgressMeter - NC_016854.1:20000 0.2 20000 87450.8; 08:23:04.942 INFO ProgressMeter - NC_016854.1:58000 0.4 58000 143694.8; 08:25:25.155 INFO ProgressMeter - NC_016854.1:82000 2.7 82000 29921.4; 08:25:35.161 INFO ProgressMeter - NC_016854.1:100000 2.9 100000 34396.6; 08:28:02.395 INFO ProgressMeter - NC_016854.1:102000 5.4 102000 19025.7; 08:28:13.248 INFO ProgressMeter - NC_016854.1:140000 5.5 140000 25261.3; 08:28:24.027 INFO ProgressMeter - NC_016854.1:175000 5.7 175000 30585.2; 08:46:13.574 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),29.232685148998623,Cpu time(s),29.09919726900138; [February 28, 2018 8:46:13 AM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 23.59 minutes.; Runtime.totalMemory()=5588910080; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.tool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467:2138,error,error,2138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467,1,['error'],['error']
Availability,"java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1098); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306); ```; #### Steps to reproduce; When the command is run with original (haplotypecaller output, left aligned and trimmed, with a number of variants), the program crashes and prematurely terminate the output. . The problem can be isolated to one variant with the bam file. . ```; gatk VariantAnnotator -I ../test.bam -V test.vcf -O test_2.vcf --reference ~/refs/hg19/ucsc.hg19.fasta --enable-all-annotations true -jdk-deflater true -jdk-inflater true; ```. test.vcf is a haplotypecaller + leftalignandtrimvariant vcf file with one single variant: ; ```; chr8 145743102 . C A 37.32 . AC=2;AF=1.00;AN=2;DP=2;ExcessHet=0.0000;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;QD=18.66;SOR=0.693 GT:AD:DP:GQ:PL 1/1:0,2:2:6:49,6,0; ```. test.bam is a hg19-aligned, duplicate-marked bam file (372kb, containing only reads associated with the site, can be sent privately if necessary) . troubleshooting steps done: ; - the program does not crash if the bam file is not provided; - the program does not crash if --enable-all-annotations true is not given. #### Expected behavior; error/warning message or no annotation generated for a variant. #### Actual behavior; crashed with outputs truncated (where original, large number of variant in a file was analyzed)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8800:8500,error,error,8500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8800,1,['error'],['error']
Availability,java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 2019-10-29T18:18:04.001018863Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 2019-10-29T18:18:04.001194549Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 2019-10-29T18:18:04.001367357Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 2019-10-29T18:18:04.001518160Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 2019-10-29T18:18:04.001673083Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-29T18:18:04.001846904Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-29T18:18:04.002024760Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-29T18:18:04.002140012Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-29T18:18:04.002232542Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-29T18:18:04.002242727Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-29T18:18:04.002292461Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 2019-10-29T18:18:04.002301667Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 2019-10-29T18:18:04.002307019Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-29T18:18:04.002311722Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 2019-10-29T18:18:04.002316449Z 	at java.util.stream.Refer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6237:1099,error,errorProbability,1099,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6237,1,['error'],['errorProbability']
Availability,java:119); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:374); at org.broadinstitute.hellbender.tools.spark.pipelines.SortSamSpark.runTool(SortSamSpark.java:114); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:546); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130); at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41); at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862); at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714); at org.apache.spark.util.Utils$$anon$2.write(Utils.scala:160); at com.esotericsoftware.kryo.io.Output.flush(Output.java:185); at com.esotericsoftware.kryo.io.Output.close(Output.java:196); at org.apache.sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:27367,failure,failure,27367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['failure'],['failure']
Availability,jenkins builds failing with out of of space errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3067:44,error,errors,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3067,1,['error'],['errors']
Availability,"joun is willing to help.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally painful to get; that dropped allele. I'd suggest making a unit test with some fake data.; You'll need two positions: one upstream with a deletion to generate the *; and one for the SNP. I think the dropped allele was a 1bp deletion at the; same position t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2959:2959,ping,ping,2959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959,1,['ping'],['ping']
Availability,"k manager 172.17.0.2:33999 with 4.1 GB RAM, BlockManagerId(driver, 172.17.0.2, 33999); 17/08/22 18:54:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.0.2, 33999); 17/08/22 18:54:43 INFO SparkUI: Stopped Spark web UI at http://172.17.0.2:4040; 17/08/22 18:54:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/08/22 18:54:43 INFO MemoryStore: MemoryStore cleared; 17/08/22 18:54:43 INFO BlockManager: BlockManager stopped; 17/08/22 18:54:43 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/08/22 18:54:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/08/22 18:54:43 INFO SparkContext: Successfully stopped SparkContext; [August 22, 2017 6:54:43 PM UTC] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=299892736; ***********************************************************************. A USER ERROR has occurred: null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:396); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:173); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3492:4208,ERROR,ERROR,4208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3492,1,['ERROR'],['ERROR']
Availability,"k.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.Spark",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:12794,failure,failure,12794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['failure'],['failure']
Availability,"k.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:38987,ERROR,ERROR,38987,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['ERROR'],['ERROR']
Availability,"k.storage.RandomBlockReplicationPolicy for block replication policy; 10:33:07.214 INFO BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.221 INFO BlockManagerMasterEndpoint - Registering block manager 172.20.19.130:43279 with 1076.2 GiB RAM, BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.225 INFO BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.226 INFO BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.345 INFO ContextHandler - Stopped o.s.j.s.ServletContextHandler@7074da1d{/,null,STOPPED,@Spark}; 10:33:07.347 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6556471b{/jobs,null,AVAILABLE,@Spark}; 10:33:07.349 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7cdb05aa{/jobs/json,null,AVAILABLE,@Spark}; 10:33:07.351 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5cb76070{/jobs/job,null,AVAILABLE,@Spark}; 10:33:07.352 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@443ac5b8{/jobs/job/json,null,AVAILABLE,@Spark}; 10:33:07.354 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@753e4eb5{/stages,null,AVAILABLE,@Spark}; 10:33:07.355 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@63318b56{/stages/json,null,AVAILABLE,@Spark}; 10:33:07.357 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@462f8fe9{/stages/stage,null,AVAILABLE,@Spark}; 10:33:07.358 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@b2e1df3{/stages/stage/json,null,AVAILABLE,@Spark}; 10:33:07.359 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6cf3b3d7{/stages/pool,null,AVAILABLE,@Spark}; 10:33:07.360 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:45119,AVAIL,AVAILABLE,45119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"k\_bundle/hg19\_v0\_Homo\_sapiens\_assembly19.fasta \\ ; ; \--variant normal1.vcf \\ ; ; \--variant normal2.vcf \\ ; ; \--variant normal3.vcf \\ ; ; \--variant normal4.vcf \\ ; ; \--variant normal5.vcf \\ ; ; \--variant normal6.vcf \\ ; ; \--variant normal7.vcf \\ ; ; \--variant normal8.vcf \\ ; ; \--variant normal9.vcf \\ ; ; \--variant normal10.vcf \\ ; ; \--variant normal11.vcf \\ ; ; \--variant normal12.vcf \\ ; ; \--variant normal13.vcf \\ ; ; \--variant normal14.vcf \\ ; ; \--variant normal15.vcf \\ ; ; \--variant normal16.vcf \\ ; ; \--variant normal17.vcf \\ ; ; .... ; ; \--variant normal80.vcf \\ ; ; \--genomicsdb-workspace-path pon\_db \\ ; ; \--tmp-dir /tmp1 \\ ; ; \-L /gatk\_bundle/hglft\_genome\_3bc14\_d6f440.bed \\ ; ; \--sequence-dictionary /gatk\_bundle/hg19\_v0\_Homo\_sapiens\_assembly19.dict \\ ; ; \--reader-threads 15 \\ ; ; \--java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true'; ```. Here For interval list, I have downloaded the hg38 target interval from GATK resource bundle and converted into hg19 format using UCSC liftover utility. GenomicsDBImport is not reporting any error related to command but also not reporting any results. Here are the details from GenomicsDBImport log file:. ```; 17:16:16.069 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jan 12, 2021 5:16:16 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 17:16:16.329 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 17:16:16.329 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 17:16:16.329 INFO GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 17:16:16.330 INFO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037:1906,down,downloaded,1906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037,1,['down'],['downloaded']
Availability,"ke this comes up because of an underlying assumption that the data is diploid. It would help out users if we could make some sort of workaround possible with this tool. This request was created from a contribution made by Samantha Zarate on May 28, 2021 14:24 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078069451-VariantEval-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078069451-VariantEval-IndexOutOfBoundsException). \--. a) GATK version used: v4.1.9 ; ; b) Exact command used:. /gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar \\ ; ; VariantEval \\ ; ; \-R /$PATH\_TO\_REFERENCE/chm13/t2t-chm13.20200921.withGRCh38chrY.chrEBV.chrYKI270740v1r.fasta \\ ; ; \--eval /$PATH\_TO\_VCF/1kgp.chrX.recalibrated.snp\_indel.pass.vcf.gz \\ ; ; \--pedigree /$PATH\_TO\_PED/1kgp\_trios.ped \\ ; ; \-no-ev -no-st -ST Family \\ ; ; \-EV MendelianViolationEvaluator \\ ; ; \-O 1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table. c) Entire error log:. 19:35:29.408 INFO VariantEval - ------------------------------------------------------------ 19:35:29.408 INFO VariantEval - The Genome Analysis Toolkit (GATK) v4.1.9.0 19:35:29.409 INFO VariantEval - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) 19:35:29.409 INFO VariantEval - Executing as root@0b79b5044551 on Linux v5.4.104+ amd64 19:35:29.409 INFO VariantEval - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 19:35:29.409 INFO VariantEval - Start Date/Time: May 27, 2021 7:35:29 PM UTC 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.410 INFO VariantEval - HTSJDK Version: 2.23.0 19:35:29.410 INFO VariantEval - Picard Version: 2.23.3 19:35:29.410 INFO VariantEval - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 19:35:29.410 INFO VariantEval - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7304:1099,error,error,1099,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304,1,['error'],['error']
Availability,"ker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 2; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Cancelling stage 1; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) failed in 10.702 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:29208,failure,failure,29208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['failure'],['failure']
Availability,"l behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); Funcotator; gatk Funcotator --variant test.somatic.vcf --reference ucsc.hg19.fasta --ref-version hg19 --data-sources-path funcotator_dataSources.v1.7.20200521s --output test.maf --output-file-format MAF; ### Affected version(s); gatk4.1.8.1 (installed using conda). ### Description ; I want to use Funcotator to annotate the VCF file given by Illumina TruSight Oncology 500 pipeline. But when I run the command above, it throws out an error, seems something related with malformat. I check my VCF file and think it should be OK. So I wonder if you can kindly tell me how to fix this bug?; The ERROR is:; `Using GATK jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator --variant /home/shiyang/Project/BGB900_101/TSO_result/TSO_somatic_vcf/112-0005-0031-B1_L1.UP12.tmb.tsv.tso.somatic.vcf --reference /storage01/ref_genome/hg19/bwa/ucsc.hg19.fasta --ref-version hg19 --data-sources-path /home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s --output /home/shiyang/Project/BGB900_101/TSO_result/test.maf --output-file-format MAF; 15:41:48.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 19, 2020 3:41:49 PM shaded.cloud_nio.com.google.auth.oau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:1811,ERROR,ERROR,1811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['ERROR'],['ERROR']
Availability,"l by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 481.33 minutes.; Runtime.totalMemory()=47982837760; java.lang.NegativeArraySizeException: -896617256; at org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(VectorLoglessPairHMM.java:131); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:272); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:197); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:790); at org.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:2149,down,down,2149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['down'],['down']
Availability,"l or two that use `PythonScriptExecutor` to call into a Python machine-learning library, and do an assessment of maintainability, etc. `PythonScriptExecutor` will come with an attached set of conditions for its use, intended to address the most serious issues raised by the engine and support teams with having Python code in the GATK. We should document these conditions in the docs for `PythonScriptExecutor` when it's implemented:. 1. All tools that use `PythonScriptExecutor` must have a Java-based front-end, with standard GATK (barclay-based) arguments. We put a lot of development effort into our arg parser and into striving for user-interface consistency across tools, and cannot afford to duplicate this effort in Python. Geraldine (CC'd) and the rest of the support team can back me up on this one!. 2. An honest effort should be made to minimize the amount of code written in Python -- as much of each tool's work as possible should be done in Java. In particular, reading/writing final inputs and outputs should happen in Java. This is important for a number of reasons, including the engine team's goal of ensuring universal GCS support, consistent Google authentication handling, etc. Again, we really don't want to have to duplicate that work in Python, or for the tools that call into Python to be inconsistent with the rest of the toolkit. 3. All dependencies (Python and native) of Python libraries used will be clearly documented, and included in the default GATK docker image. I don't think I need to explain why this one is important :) . 4. Before we go any further down this path, we prototype one or two tools using `PythonScriptExecutor`, and do a fair assessment of maintainability and other concerns of the engine/support teams, such as whether it will even be possible to package all dependencies without conflicts. 5. Engine team will continue to search for Java-based solutions while this evaluation is ongoing, but this proposal at least unblocks the CNV team for now.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501:1771,down,down,1771,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501,1,['down'],['down']
Availability,"l, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next qu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:2019,recover,recovered,2019,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,2,"['error', 'recover']","['error', 'recovered']"
Availability,l.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.base/java.lang.Thread.run(Thread.java:834); [TileDB::FileSystem] Error: hdfs: Error getting hdfs connection; [TileDB::StorageManagerConfig] Error: Error getting hdfs connection: Connection refused.; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest > testWriteToAndQueryFromGCS FAILED; java.io.IOException: GenomicsDB JNI Error: VCFAdapterException : Could not copy contents of VCF header filename gs://hellbender-test-logs/staging/703469fc-52fe-441d-b6e0-8092a114fe2c/vcfheader.vcf to temporary file /tmp/TileDBVoWFeM; at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:176); at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:80); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.getGenomicsDBFeatureReader(GenomicsDBImportIntegrationTest.java:926); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:550); at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:7805,Error,Error,7805,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,1,['Error'],['Error']
Availability,"l.tasks.compile.CleaningJavaCompilerSupport.execute(CleaningJavaCompilerSupport.java:35); at org.gradle.api.internal.tasks.compile.CleaningJavaCompilerSupport.execute(CleaningJavaCompilerSupport.java:25); at org.gradle.api.tasks.compile.JavaCompile.performCompilation(JavaCompile.java:189); at org.gradle.api.tasks.compile.JavaCompile.compile(JavaCompile.java:170); at org.gradle.api.tasks.compile.JavaCompile.compile(JavaCompile.java:113); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75); at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$IncrementalTaskAction.doExecute(DefaultTaskClassInfoStore.java:158); at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:129); at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:118); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:80); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:61); ... 68 more. BUILD FAILED. Total time: 6.066 secs; Stopped 0 compiler daemon(s).; Received result Failure[value=org.gradle.initialization.ReportedException: org.gradle.internal.exceptions.LocationAwareException: Execution failed for task ':compileJava'.] from daemon DaemonInfo{pid=32687, address=[a73e45df-d609-43d0-9385-508a26a328d4 port:39221, addresses:[/0:0:0:0:0:0:0:1, /127.0.0.1]], state=Idle, lastBusy=1516787326803, context=DefaultDaemonContext[uid=7e8a7a6d-190b-445f-9873-f0329477e561,javaHome=/usr/lib/jvm/java-8-oracle,daemonRegistryDir=/home/axverdier/.gradle/daemon,pid=32687,idleTimeout=10800000,daemonOpts=-XX:MaxPermSize=256m,-XX:+HeapDumpOnOutOfMemoryError,-Xmx1024m,-Dfile.encoding=US-ASCII,-Duser.country=US,-Duser.language=en,-Duser.variant]} (build should be done).; ```; Any idea ? Am I missing something ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:14474,Failure,Failure,14474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,1,['Failure'],['Failure']
Availability,l.writeReads(GATKSparkTool.java:259); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:39); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 17/10/11 14:19:38 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 17/10/11 14:19:38 INFO util.ShutdownHookManager: Shutdown hook called; 17/10/11 14:19:38 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; 17/10/11 14:19:38 INFO util.ShutdownHookManager: Deleting directory /tmp/hdfs/spark-8c88439f-dcb0-48b2-86f3-fc82cef4c438,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:38371,down,down,38371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,2,['down'],['down']
Availability,"l/native/libgkl_utils.so. 14:38:44.103 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation. java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!. at htsjdk.samtools.SAMFileHeader.addProgramRecord(SAMFileHeader.java:202). at htsjdk.samtools.SAMTextHeaderCodec.parsePGLine(SAMTextHeaderCodec.java:158). at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:107). at htsjdk.samtools.SAMFileHeader.clone(SAMFileHeader.java:398). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriterFromFactory(ReadUtils.java:1215). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriter(ReadUtils.java:1163). at org.broadinstitute.hellbender.utils.haplotype.SAMFileDestination.(SAMFileDestination.java:35). at org.broadinstitute.hellbender.utils.haplotype.HaplotypeBAMWriter.(HaplotypeBAMWriter.java:74). at org.broadinstitute.hellbender.tools.walkers.realignmentfilter.FilterAlignmentArtifacts.onTraversalStart(FilterAlignmentArtifacts.java:194). at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046). at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139). at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191). at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163). at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206). at org.broadinstitute.hellbender.Main.main(Main.java:292). Process exited with an error: 1 (Exit value: 1)[gbottu@r910bits 7953]$ vi stdout.txt. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24591/troubles-with-bam-files-generated-by-mutect2-and-filteralignmentartifacts/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6287:3564,error,error,3564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6287,1,['error'],['error']
Availability,"lPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 2022 2:14:09 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=850644992; java.lang.ArrayIndexOutOfBoundsException: -1; at java.util.ArrayList.elementData(ArrayList.java:422); at java.util.ArrayList.get(ArrayList.java:435); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$27(SomaticGenotypingEngine.java:376); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.stream.SliceOps$1$1.accept(SliceOps.java:204); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:4627,down,down,4627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['down'],['down']
Availability,"lVariationDiscoveryPipelineSpark. ### Affected version(s); GATK 4.1.0.0. ### Description . Running SV program generates a Java exception...; java.lang.IllegalArgumentException: provided start is negative: -1. #### Steps to reproduce; ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv//$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 80G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:1048,heartbeat,heartbeatInterval,1048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['heartbeat'],['heartbeatInterval']
Availability,"lable to help in this process @jonn-smith. Just to give you an idea of the evolution of tutorials, here are some example tutorials that focus on somatic CNV calling. - Alpha tutorial that Lee wrote:; <https://gatkforums.broadinstitute.org/gatk/discussion/7387/description-and-examples-of-the-steps-in-the-acnv-case-workflow>; - A demo-like tutorial that Sam Lee planned and developed (also Mehrtash was involved so don't be shy about asking around for help) and I encased in writing:; <https://gatkforums.broadinstitute.org/gatk/discussion/9143/how-to-call-somatic-copy-number-variants-using-gatk4-cnv/p1>; - A tutorial to highlight the factors in PoN creation currently used in workshops that I planned, developed and wrote: [GATK4_SomaticCNV_worksheet.pdf](https://github.com/broadinstitute/gatk/files/1435691/GATK4_SomaticCNV_worksheet.pdf). Depending on how much responsibility you want to take (I think it nice for you to post a tutorial on the forum under your name for posterity), you can choose to get my review only or have me help in brainstorming, organizing and/or formatting the content. I have a template available for copy-pasting with formatting elements at <https://gatkforums.broadinstitute.org/dsde/discussion/9140/how-to-tutorial-template-a-la-soo-hee-for-copy-pasting#latest>. Notice this document is only visible to DSDE members and similarly, you can draft a tutorial in private first then move it to a public forum. Here are some steps I go through in developing a tutorial. You may choose to skip certain elements, e.g. example data. I have to say that having example data really helps the users.; - Find small test data to illustrate important features of the tool; ensure data is publically sharable; - Write out commands that include recommended parameters; - Explain the important parameters and the impact of changing them; - Illustrate with results and screenshots; - Get reviewed early by others and incorporate changes; - A section of links to related or helpful docs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3774:1128,avail,available,1128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3774,1,['avail'],['available']
Availability,"lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:2231,ERROR,ERROR,2231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,laria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it repr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:4327,error,error,4327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,3,"['Error', 'error']","['Error', 'error']"
Availability,"later false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:3029,ERROR,ERROR,3029,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['ERROR'],['ERROR']
Availability,"later: IntelDeflater; 21:09:59.074 INFO CombineGVCFs - Inflater: IntelInflater; 21:09:59.074 INFO CombineGVCFs - GCS max retries/reopens: 20; 21:09:59.074 INFO CombineGVCFs - Requester pays: disabled; 21:09:59.074 INFO CombineGVCFs - Initializing engine; 21:09:59.773 INFO FeatureManager - Using codec VCFCodec to read file file:///data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/02_Haplotype/Y2003/Y2003.gatk.vcf; 21:09:59.982 INFO FeatureManager - Using codec VCFCodec to read file file:///data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/02_Haplotype/Y2010/Y2010.gatk.vcf; 21:10:00.200 INFO FeatureManager - Using codec VCFCodec to read file file:///data/cotton/QingyingMeng/Gbarbadese_5Sample/3-79/02_Haplotype/Y2013/Y2013.gatk.vcf; 21:10:41.729 INFO CombineGVCFs - Done initializing engine; 21:10:41.789 INFO ProgressMeter - Starting traversal; 21:10:41.790 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:10:42.082 INFO CombineGVCFs - Shutting down engine; [March 6, 2022 9:10:42 PM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.73 minutes.; Runtime.totalMemory()=4515692544; java.lang.IllegalStateException: Key END found in VariantContext field INFO at Gbar_A01:24359 but this key isn't defined in the VCFHeader. We require all VCFs to have complete VCF headers by default.; at htsjdk.variant.vcf.VCFEncoder.fieldIsMissingFromHeaderError(VCFEncoder.java:213); at htsjdk.variant.vcf.VCFEncoder.write(VCFEncoder.java:146); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:250); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:408); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:217); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:162); at org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart.apply(MultiVariantWalkerGroupedOnStart.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7708:3637,down,down,3637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7708,1,['down'],['down']
Availability,launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:8615,ERROR,ERROR,8615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:9873,ERROR,ERROR,9873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"ld happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 14 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-102235192). Recording test case while sanitizing: . The files are located here: . ```; gsa1:/humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles; ```. The command I ran:. ```; java -jar /humgen/gsa-hpprojects/GATK/private_unstable_builds/GenomeAnalysisTK_latest_unstable.jar \; -T GenotypeGVCFs \; -R /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/45S_Jacobsen_rearranged.fa \; -V /humgen/gsa-scr1/schandra/GenotypeGVCFs_LargePloidyAndLargeAlleles/Input_ploidy.list \; -o Sheila.GenotypeGVCFs.vcf; ```. Which produces:. ```; ##### ERROR MESSAGE: the combination of ploidy (19) and number of alleles (21) results in a very large number of genotypes (> 2147483647). You need to limit ploidy or the number of alternative alleles to analyze this locus; ```. ---. @chandrans commented on [Wed Jan 20 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-173357206). @davidbenjamin Hi David. Have you had a chance to look at this?. ---. @davidbenjamin commented on [Sat Jan 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-174245406). @chandrans My other bug turned into a very long undertaking, but seems to be nearing completion. It might even be done already, pending Laura's confirmation that the output vcfs are what we want. Then I will move on to this one. ---. @chandrans commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-175192516). Ah wonderful. Thanks David. ---. @davidbenjamin commented on [Tue Apr 12 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:3940,ERROR,ERROR,3940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['ERROR'],['ERROR']
Availability,ldevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:5376,ERROR,ERROR,5376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,ldevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:11285,ERROR,ERROR,11285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,ldevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:10027,ERROR,ERROR,10027,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"le hdf5/grexome0280.hdf5 (378 / 387); 10:58:46.452 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0078.hdf5 (379 / 387); 10:58:46.743 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0186.hdf5 (380 / 387); 10:58:47.081 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0284.hdf5 (381 / 387); 10:58:47.421 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0179.hdf5 (382 / 387); 10:58:47.659 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0221.hdf5 (383 / 387); 10:58:47.889 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0358.hdf5 (384 / 387); 10:58:48.140 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0144.hdf5 (385 / 387); 10:58:48.423 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0231.hdf5 (386 / 387); 10:58:48.650 INFO GermlineCNVCaller - Aggregating read-count file hdf5/grexome0263.hdf5 (387 / 387); 11:04:58.313 INFO GermlineCNVCaller - Shutting down engine; [July 25, 2018 11:04:58 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 8.56 minutes.; Runtime.totalMemory()=10962337792; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/tintest/cohort_denoising_calling.4390748645603329412.py --ploidy_calls_path=DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls --output_calls_path=GermlineCNVCaller/GermlineCNVCaller-calls --output_tracking_path=GermlineCNVCaller/GermlineCNVCaller-tracking --modeling_interval_list=/tmp/tintest/intervals7440933759308039041.tsv --output_model_path=GermlineCNVCaller/GermlineCNVCaller-model --enable_explicit_gc_bias_modeling=False --read_count_tsv_files /tmp/tintest/sample-04881922240505697119.tsv /tmp/tintest/sample-17677568501630201512.tsv /tmp/tintest/sample-26804327235005483714.tsv /tmp/tintest/sample-37617330639944470775.tsv /tmp/tintest/sample-46355851108368390478.tsv /tmp/tin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:42550,down,down,42550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['down'],['down']
Availability,"le.java:405); at htsjdk.samtools.seekablestream.SeekableFileStream.read(SeekableFileStream.java:85); at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244); at java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:284); at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:343); at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:133); at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:582); at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:571); at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:536); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:479); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:469); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:207); at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:252); at htsjdk.tribble.readers.TabixReader.readLine(TabixReader.java:215); at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:434); at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); ... 29 more; Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar CombineGVCFs -R ./test/test.fna -V ./gvcf_all.list -L NC_038255.2 -O ./NC_038255.2.merged.g.vcf.gz; total 2.3G; '""-rw-rw-rw- 1 root root  3.6K Dec 13 23:32 GATKConfig.EXAMPLE.properties""; drwxr-xr-x 2 root root  4.0K Mar 13 06:26 GCF_000004515.6_Glycine_max_v4.0; '""-rw-r--r-- 1 root root  1.6G Mar 13 06:47 NC_038255.2.merged.g.vcf.gz""; '""-rw-r--r-- 1 root",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735:26177,avail,available,26177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735,1,['avail'],['available']
Availability,"le:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:18:43.549 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:18:43.599 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002b5f92e39fab, pid=85482, tid=0x00002b5f56e60ae8; #; # JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12); # Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops); # Derivative: IcedTea 3.6.0; # Distribution: Custom build (Tue Nov 21 11:22:36 GMT 2017); # Problematic frame:; # C [libgomp.so.1+0x7fab] omp_get_max_threads+0xb; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /beegfs/work/iiipe01/Exome-Test/work/1e/fc972c6b14c8006857230849630a49/hs_err_pid85482.log; #; # If you would like to submit a bug report, please include; # instructions on how to reproduce the bug and visit:; # http://icedtea.classpath.org/bugzilla; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ``` . Here's the `hs_err` file:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002b5f92e39fab, pid=85482, tid=0x00002b5f56e60ae8; #; # JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12); # Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops); # Derivative: IcedTea 3.6.0; # Distribution: Custom build (Tue Nov 21 11:22:36 GMT 2017); # Problematic frame:; # C [libgomp.so.1+0x7fab] omp_get_max_threads+0xb; #; # Failed to write core dump. Core dumps have been disabled. To ena",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:4447,error,error,4447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['error'],['error']
Availability,"leMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in stage 25.0 (TID 43224, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:5568,ERROR,ERROR,5568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['ERROR'],['ERROR']
Availability,"ler - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Version: 2.14.3; 22:25:20.635 INFO HaplotypeCaller - Picard Version: 2.17.2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:25:20.636 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:25:20.636 INFO HaplotypeCaller - Inflater: IntelInflater; 22:25:20.636 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:25:20.636 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:25:20.636 INFO HaplotypeCaller - Initializing engine; 22:25:21.061 INFO HaplotypeCaller - Done initializing engine; 22:25:21.069 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:25:21.069 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:25:21.070 INFO HaplotypeCaller - Shutting down engine; [October 29, 2018 10:25:21 PM EDT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1506344960; USAGE: HaplotypeCaller [arguments]. Call germline SNPs and indels via local re-assembly of haplotypes; Version:4.0.3.0. ***********************************************************************. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can only be used in single sample mode currently. Use the sample_name argument to run on a single sample out of a multi-sample BAM file. ***********************************************************************. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5372:3361,down,down,3361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:24573,error,error,24573,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['error'],['error']
Availability,"libratorEngine - Evaluating full set of 3660 variants...; 15:51:15.373 INFO VariantRecalibrator - Building FS x ReadPosRankSum plot...; 15:51:15.374 INFO VariantRecalibratorEngine - Evaluating full set of 3660 variants...; 15:51:16.493 INFO VariantRecalibratorEngine - Evaluating full set of 3660 variants...; 15:51:16.722 INFO VariantRecalibrator - Building MQRankSum x ReadPosRankSum plot...; 15:51:16.722 INFO VariantRecalibratorEngine - Evaluating full set of 3600 variants...; 15:51:17.819 INFO VariantRecalibratorEngine - Evaluating full set of 3600 variants...; 15:51:18.045 INFO VariantRecalibrator - Executing: Rscript /gss1/home/ldl20190322/a_haoxiaoshuai/a_project/WGS_Z/e_vqsr_plot/Ztem.gatk.snp.plots.R; 15:51:38.589 INFO VariantRecalibrator - Executing: Rscript (resource)org/broadinstitute/hellbender/tools/walkers/vqsr/plot_Tranches.R /gss1/home/ldl20190322/a_haoxiaoshuai/a_project/WGS_Z/e_vqsr_plot/Ztem.gatk.snp.tranches 2.15; 15:51:39.227 INFO VariantRecalibrator - Shutting down engine; [November 9, 2020 3:51:39 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator done. Elapsed time: 9.43 minutes.; Runtime.totalMemory()=2029518848; Exception in thread ""Thread-1"" htsjdk.samtools.util.RuntimeIOException: java.nio.file.NoSuchFileException: /gss1/home/ldl20190322/a_haoxiaoshuai/JavaTmpDir/Rlib.7359270660945146060; 	at htsjdk.samtools.util.IOUtil.recursiveDelete(IOUtil.java:1346); 	at org.broadinstitute.hellbender.utils.io.IOUtils.deleteRecursively(IOUtils.java:1061); 	at org.broadinstitute.hellbender.utils.io.DeleteRecursivelyOnExitPathHook.runHooks(DeleteRecursivelyOnExitPathHook.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.nio.file.NoSuchFileException: /gss1/home/ldl20190322/a_haoxiaoshuai/JavaTmpDir/Rlib.7359270660945146060; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOEx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6948:2757,down,down,2757,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6948,1,['down'],['down']
Availability,licable; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; [E::bwa_idx_load_from_disk] fail to locate the index files; 15:47:34.944 ERROR Executor:95 - Exception in task 5.0 in stage 0.0 (TID 5); org.broadinstitute.hellbender.exceptions.GATKException: Cannot run BWA-MEM; at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$null$1(BwaSparkEngine.java:113); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:4943,ERROR,ERROR,4943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,1,['ERROR'],['ERROR']
Availability,"lizing engine; 14:10:28.532 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/groups/OE0540/internal/users/gleixner/cropseq_uli/rep_ex/test3/output.g.vcf; 14:10:28.566 INFO GenotypeGVCFs - Done initializing engine; 14:10:28.620 INFO ProgressMeter - Starting traversal; 14:10:28.622 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:10:29.048 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr19:55910646 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 14:10:29.118 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes; 14:10:29.175 INFO ProgressMeter - unmapped 0.0 37 4021.7; 14:10:29.175 INFO ProgressMeter - Traversal complete. Processed 37 total variants in 0.0 minutes.; 14:10:29.236 INFO GenotypeGVCFs - Shutting down engine; [October 29, 2023 at 2:10:29 PM CET] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=285212672; ```; results in a vcf file that still has called genotypes:. ```; $ bcftools view output.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB | tail; ##source=HaplotypeCaller; ##bcftools_viewVersion=1.16+htslib-1.16; ##bcftools_viewCommand=view -c1 output.vcf; Date=Sun Oct 29 14:09:42 2023; ##bcftools_annotateVersion=1.16+htslib-1.16; ##bcftools_annotateCommand=annotate -x INFO,FORMAT/SB; Date=Sun Oct 29 14:09:42 2023; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT CGAAGAGGTAGGTGCGAG-1; chr19 55910646 . AC A 352.6 . . GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:6,9:15:99:0|1:55910646_AC_A:360,0,225:55910646; chr19 55910648 . AAATCCCCC A 352.6 . . GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:6,9:15:99:0|1:55910646_AC_A:360,0,225:55910646; chr19 55910653 . CCCCAT *,C 227.84 . . GT:AD:DP:GQ:PGT:PID:PL:PS 1|2:0,9,6:15:99:1|0:55910646_AC_A:630,252,225,378,0,360:55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8569:3904,down,down,3904,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8569,1,['down'],['down']
Availability,"lizing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY mode: Not an HDF5 file; at org.broadinstitute.hdf5.HDF5File.open(HDF5File.java:490); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:82); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:66); at org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts.doWork(DenoiseReadCounts.java:188); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7258:4572,down,down,4572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258,1,['down'],['down']
Availability,"ll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in st",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:5392,ERROR,ERROR,5392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['ERROR'],['ERROR']
Availability,"ll/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #5556 because some users were running matched normals with much lower depth than their cases. This meant that many normal sites fell below the default threshold of 30 counts and were thus not pulled from the case, even though the latter had much higher depth. It's conceivable that there will be some use cases for which we might want to relax this and allow a non-zero case threshold; for example,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652:1471,down,down,1471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652,1,['down'],['down']
Availability,"ller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. When `--min-base-quality-score 12` is used, it is as if HaplotypeCaller invents some reference allele reads and then uses them to genotype the variant as heterozygous. I have seen issues like this all over the genome. Interestingly, if I run the same code on my own Ubuntu laptop, I get an error instead:; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method lambda$identity$d67512bf$1(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/utils/read/GATKRead; at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:276); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6045:2290,error,error,2290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045,1,['error'],['error']
Availability,"lls - HTSJDK Version: 2.23.0; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Picard Version: 2.22.8; 12:49:08.688 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:49:08.688 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:49:08.688 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:49:08.688 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 12:49:08.688 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Initializing engine; 12:49:12.598 INFO PostprocessGermlineCNVCalls - Done initializing engine; 12:49:15.678 INFO PostprocessGermlineCNVCalls - Shutting down engine; [October 29, 2020 12:49:15 PM MSK] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=2457862144; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:60); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:297); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1047); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924:9239,down,down,9239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924,1,['down'],['down']
Availability,"lly any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. After sourcing the tab-completion script, some tools shown cannot be run. Maybe they exist somewhere in an experimental dev version but are not bundled for public release?. ### Affected version(s); - [x ] Latest public release version [4.1.7.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. After trying to tab complete the DepthOfCoverage, I saw a few tools not listed in the documentation. I tried running them and sure enough, there were errors:. `A USER ERROR has occurred: '*' is not a valid command.`; (* is one of the tools listed below). #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. ```; cd gatk-4.1.7.0; source gatk-completion.sh; ./gatk Depth<tab>; #>DepthOfCoverage DepthPerAlleleBySample DepthPerSampleHC; ./gatk DepthPerSampleHC -h; ...; ***********************************************************************; A USER ERROR has occurred: 'DepthPerSampleHC' is not a valid command.; ***********************************************************************; ./gatk DepthPerAlleleBySample -h; ...; ***********************************************************************; A USER ERROR has occurred: 'DepthPerAlleleBySample' is not a valid command.; ***********************************************************************; ./gatk DepthOfCoverage -h; **BETA FEATURE - WORK IN PROGRESS**; USAGE: DepthOfCov",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6615:1844,ERROR,ERROR,1844,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6615,1,['ERROR'],['ERROR']
Availability,"loci, an exception is raised on FilterMutectCalls. This issue was reported https://github.com/broadinstitute/gatk/issues/6237, and I have bumped GATK to several versions, including latest, but the issue persists. This exception is not raised when the `--alleles` flag is excluded. ### Affected tool(s) or class(es); Mutect2, FilterMutectCalls. ### Affected version(s); - [ ] 4.2.0.0; - [ ] 4.1.9.0; - [ ] 4.1.4.1; - [ ] _not_ 4.1.5.0 (but unfiltered vcf is not correct due to missing info fields so we are not using this version). ### Description ; here is the stack trace; ```; 20:29:45.346 INFO FilterMutectCalls - Done initializing engine; 20:29:45.399 INFO ProgressMeter - Starting traversal; 20:29:45.399 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:29:45.400 INFO FilterMutectCalls - Starting pass 0 through the variants; 20:29:45.580 INFO FilterMutectCalls - Finished pass 0 through the variants; 20:29:45.630 INFO FilterMutectCalls - Shutting down engine; [May 25, 2021 8:29:45 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=462946304; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7276:1295,down,down,1295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276,1,['down'],['down']
Availability,log4j Error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3651:6,Error,Error,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3651,1,['Error'],['Error']
Availability,log4j error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:6,error,error,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,1,['error'],['error']
Availability,"ls methods (i.e., remove remaining BucketUtils.isHadoopURL overloads(File, Path, String), etc.); - [ ] Update RevertSAMSpark String/File assumptions and intervconversions; - [ ] Add ReferenceFileSource(`GATKPath`) constructor, remove remaining CachingIndexedFastaSequenceFile/overloads; - [ ] Update tools in the pathseq package (PathSeqBwaSpark, PathSeqScoreSpark) that do directory manipulation. [Edit] Somewhat tangentially, PathSeqBwaSpark currently rejects read inputs specified through `--inputs` and uses separate args to allow the user to identify inputs as paired or unpaired. Once this is using `GATKPathSpecifier` this could be changed to use ""--inputs"" annotated with tags instead. Might be a problem for WDL gen though (which doesn't support tags).; - [ ] Test utilities (createTempFile/Dir, etc. that return GATKPath); - [ ] Add a `toHadoopPath` method to `GATKPath` that returns a `org.apache.hadoop.fs.Path`.; - [ ] Change tools that generate multiple output files using a stem (SplitReads, etc) to use the `resolve` methods listed above once they're available.; - [ ] All usages of `PrintStream` should be replaced with `OutputStreamWriter` (code that requires printf-style formatting can use `write` with `String.format` instead of the `printf` methods). `PrintStream` doesn't propagate IOExceptions and instead requires calls to `checkError`, but almost all usages of `PrintStream` don't call it.; - [ ] Update `org.broadinstitute.hellbender.utils.report` (`GATKReport` and friends) to eliminate `File` references and `PrintStream` usages.; - [ ] Update `org.broadinstitute.hellbender.utils.recalibration` (`RecalUtils` and friends) to eliminate `File` references and `PrintStream` usages.; - [ ] Fix cases where we have a tool with a `File` that needs to be accessible to R code (determine if the code can handle non-local file paths). i.e.`VariantRecalibrator` TRANCHES_FILE.; - [ ] Fix cases where we have a tool with a `File` that needs to be accessible to Python (determine if ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6610:1579,avail,available,1579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6610,1,['avail'],['available']
Availability,"ls. ~197/200 jobs finished, but several gave an odd error. I'm probably not giving you enough information to debug, but maybe this is enough to ask questions. the command is below:. ```. java8 -Xmx48g -Xms48g -Xss2m \; -jar GenomeAnalysisTK4.jar GenotypeGVCFs \; -R REF.fasta \; --variant gendb://<genomicsdb_path> \; -O OUTPUT.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 --max-alternate-alleles 12 \; -L <Repeated ~40 times for small contigs>. ```; ; The error is the following:. ```. 21:58:51.873 WARN  MinimalGenotypingEngine - Attempting to genotype more than 50 alleles. Site will be skipped at location QNVO02001146.1:1343; --; 21:59:01.308 INFO  ProgressMeter -  QNVO02001146.1:1679            425.0               1264000           2974.3; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),6.825391631999999,Cpu time(s),6.825079531999995; #; # A fatal error has been detected by the Java Runtime Environment:; #; #  SIGSEGV (0xb) at pc=0x00007fcca9a2ea19, pid=36873, tid=140574431450880; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode linux-amd64 ); # Problematic frame:; # C  [libtiledbgenomicsdb3086049122144672414.so+0x3e3a19]  ArraySchema::tile_num(void const*) const+0x79; #; # Core dump written. Default location: /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-10-06_16-46-33/Job734/core or core.36873; #; # An error report file with more information is saved as:; # /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-10-06_16-46-33/Job734/hs_err_pid36873.log; #; # If you would like to submit a bug report, please visit:; #   http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. Do you have an debugging suggestions based on this? Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910:1693,error,error,1693,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910,1,['error'],['error']
Availability,ls; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:20128,Recover,Recovered,20128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"lse --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:46:42.813 INFO BaseRecalibrator - Executing as mpmachado@lx-bioinfo02 on Linux v2.6.32-696.23.1.el6.x86_64 amd64; 15:46:42.814 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:2429,error,error,2429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['error'],['error']
Availability,ltScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3658,ERROR,ERROR,3658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,ltiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce; The error occurs when running a command: ; ```; gatk Mutect2 -R /home/genome/gatk.hg38/Homo_sapiens_assembly38.fasta -L panel_collapsed.bed -I bam/tumour_recalibrated.bam -I bam/normal_recalibrated.bam -tumor tumour -normal normal -germline-resource /home/genome/gatk.hg38/af-only-gnomad.hg38.vcf.gz -pon /home/genome/pon/PON_B1.vcf --genotype-pon-sites --f1r2-tar-gz results/learnOrientation/tumour_lo.tar.gz -O results/Mutect2/tumour.s.vcf.gz -bamout bam/tumour.mutect2.bam --disable-read-filter MateOnSameContigOrNoMappedMateReadFilter --af-of-alleles-not-in-resource 0.000001; ```. #### Expected behavior; Mutect2 producing outputs. #### Actual behavior; Full log: ; [Mutect2_error.txt](https://github.com/broadinstitute/gatk/files/8772744/Mutect2_error.txt). ---. I would be grateful if you could help me to investigate the cause of this error. I couldn't find any clues when googling it and tried `picard ValidateSamFile` but it returns no errors or warnings. Many thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7872:2274,error,error,2274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872,3,['error'],"['error', 'errors']"
Availability,luateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7298:7570,Error,ErrorProbabilities,7570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298,1,['Error'],['ErrorProbabilities']
Availability,"ly(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:2719,down,down,2719,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['down'],['down']
Availability,ly(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook called; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8/httpd-7370bdc2-eb41-46d1-9512-2f387b972cac; 16/11/16 23:25:11 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; + /spark-1.6.2-bin-hadoop2.6//sbin/stop-master.sh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:25840,down,down,25840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['down'],['down']
Availability,"m.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. `gatk JointGermlineCNVSegmentation --reference hs37d5.fa --variant index.vcf.gz --variant father.vcf.gz --variant mother.vcf.gz --model-call-intervals gcnv_preprocess_intervals.Agilent_SureSelect_Human_All_Exon_V6.interval_list --pedigree family.ped --output out.vcf.gz`. The input VCF lines look as follows:. ```; ## index; Y 2654827 CNV_Y_2654827_24461230 N . 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS .:0:220:94:3077:472:1358; ## father; Y 2654827 CNV_Y_2654827_24461230 N . 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS 0:1:220:58:3077:105:376; ## mother; Y 2654827 CNV_Y_2654827_24461230 N <DEL> 3076.53 . END=24461230 GT:CN:NP:QA:QS:QSE:QSS 1:0:220:29:3077:357:640; ```. The call looks like an artifact in the BAM alignments. However, the contig ploidy for the mother looks ... interesting. ```; ## index (sex assigned at birth: female); CONTIG PLOIDY PLOIDY_GQ; X 2 123.51003746478007; Y 0 9.176757618621913; ## father (sex assigned at birth: male); CONTIG PLOIDY PLOIDY_GQ; X 1 123.5100374633715; Y 1 17.498503426830368; ## mother (sex assigned at birth: female); CONTIG PLOIDY PLOIDY_GQ; X 2 123.51003745758246; Y 1 0.09888866060944837; ```. The sample of the mother has a slightly increased fraction of chrY reads when compared to other female samples but is far below the fraction of chrY reads that male samples have that were sequenced with the same kit. There is an increase in the variance of alternate allele balance for het. sites in this sample as well. I assume that this sample has been contaminated with male DNA. #### Expected behavior; I would like to be able to deactivate the hard error on the command line and replace it with a warning in the output logs. #### Actual behavior; There is a hard crash that cannot be circumvented.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8164:4707,error,error,4707,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164,1,['error'],['error']
Availability,mReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignment,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:22347,Recover,Recovered,22347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,make task robust to reads and index being in different locations.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6900:10,robust,robust,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6900,1,['robust'],['robust']
Availability,making constants BQSR_TABLE_LONG_NAME and BQSR_TABLE_SHORT_NAME in StandardArgumentDefinitions; fixing outdated references to -BQSR -> -bqsr in documentation and error messages; fixes #1631,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1633:162,error,error,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1633,1,['error'],['error']
Availability,manage_sv_pipeline checks version from gatk-spark.jar and compares it; to the current git hash (to ensure the correct version is run). Newer; gatk versions had a slightly different file name format and caused; errors parsing the hash. This updates the hash check and produces; more comprehensible error messages when it fails. Resolves: #3593,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3595:210,error,errors,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3595,2,['error'],"['error', 'errors']"
Availability,"mand line inside a Nextflow module, it gives a generic error for most of the samples (sometimes all of them, sometimes some of them). ; It looks like a random issue, because if I run the same code outside of Nextflow, it works perfectly on every sample. . I would really appreciate if someone may give me some hints on why this is occurring and, eventually, how to fix it. ## Bug Report. ### Affected tool(s) or class(es); AnalyzeSaturationMutagenesis . ### Affected version(s); gatk4-4.3.0.0. ### Description ; Here it follows the output from Nextflow that appears on screen:. ```; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:36:03.397 INFO AnalyzeSaturationMutagenesis - ------------------------------------------------------------; 09:36:03.398 INFO AnalyzeSaturationMutagenesis - The Genome Analysis Toolkit (GATK) v4.3.0.0; 09:36:03.398 INFO AnalyzeSaturationMutagenesis - For suppor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8357:1239,error,error,1239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357,1,['error'],['error']
Availability,"many methods in MathUtils are redundant with ApacheCommons/Math or JDK. We should remove those methods.; For example, `MathUtils.lnGamma` should be deleted in favor of `Gamma.logGamma' unless there's a severe speed penalty.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/400:30,redundant,redundant,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/400,1,['redundant'],['redundant']
Availability,maybe this will lower failure rates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6087:22,failure,failure,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6087,1,['failure'],['failure']
Availability,"mbda$calculateQuantileBackgroundResponsibilities$10(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.utils.MathUtils.applyToArray(MathUtils.java:1035); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.calculateQuantileBackgroundResponsibilities(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:165); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:325); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); > 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); > 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); > 	at org.broadinstitute.hellbender.Main.main(Main.java:289). For those files experiencing the error, it disappears when disabling the mitochondria mode for `FilterMutectCalls`. I wonder how this problem could be solved so that all VCFs can be filtering in a consistent way, enabling the mitochondria mode. I am happy to share the VCF and reference sequence used it that helps reproducing/solving the issue. Thank you,; Eugenio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8455:2407,error,error,2407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8455,1,['error'],['error']
Availability,"mdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306). real 481m24.418s; user 581m54.752s; sys 2m49.965s. ```. This run did not complete successfully - the Exception caused it to fail prematurely. . Previously I had seen HaplotypeCaller run out of memory and fail in almost as much time, so I think this and the OOM error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:4543,failure,failure,4543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['failure'],['failure']
Availability,me in java Smith-Waterman : 4695.36 sec ; ; 23:44:30.027 INFO HaplotypeCaller - Shutting down engine ; ; \[2021年11月1日 下午11时44分30秒\] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 569.97 minutes. ; ; Runtime.totalMemory()=742916096 ; ; htsjdk.samtools.SAMFormatException: Did not inflate expected amount ; ; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147) ; ; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331) ; ; at java.io.DataInputStream.read(DataInputStream.java:149) ; ; at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:421) ; ; at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:394) ; ; at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380) ; ; at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:282) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:866) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:1005) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:840) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:834) ; ; at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:802) ; ; at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReade,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:8846,avail,available,8846,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['avail'],['available']
Availability,"me.log.txt ; ; Using GATK jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-packa ; ; ge-4.2.5.0-local.jar HaplotypeCaller -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.54",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7741:4133,Redundant,Redundant,4133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741,1,['Redundant'],['Redundant']
Availability,"meAnalysisEngine - Inflater: IntelInflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Strictness is SILENT ; INFO 10:47:56,246 GenomeAnalysisEngine - Downsampling Settings: No downsampling ; INFO 10:47:56,255 SAMDataSource$SAMReaders - Initializing SAMRecords in serial ; INFO 10:47:56,333 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.07 ; ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A USER ERROR has occurred (version 3.8-0-ge9d806836): ; ##### ERROR; ##### ERROR This means that one or more arguments or inputs in your command are incorrect.; ##### ERROR The error message below tells you what is the problem.; ##### ERROR; ##### ERROR If the problem is an invalid argument, please check the online documentation guide; ##### ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.; ##### ERROR; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.; ##### ERROR; ##### ERROR MESSAGE: Input files reads and reference have incompatible contigs. Please see https://software.broadinstitute.org/gatk/documentation/article?id=63for more information. Error details: No overlapping contigs found.; ##### ERROR reads contigs = [LmjF04_01_20050601_V5.2, LmjF05_01_20050601_V5.2, LmjF24_01_20050601_V5.2, LmjF01_01_20050601_V5.2, LmjF03_01_20050601_V5.2, LmjF13_01_20050601_V5.2, LmjF14_01_20050601_V5.2, LmjF19_01_20050601_V5.2, LmjF21_01_20050601_V5.2, LmjF23_01_20050601_V5.2, LmjF10_01_20050601_V5.2, LmjF11_01_20050601_V5.2, LmjF15_01_20050601_V5.2, LmjF18_01_20050601_V5.2, LmjF02_01_20050601_V5.2, LmjF25_01_20050601_V5.2, LmjF27_01_20050601_V5.2, LmjF28_01_20050601_V5.2, LmjF29_01_20050601_V5.2, LmjF30_01_20050601_V5.2, LmjF31",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6798:3189,ERROR,ERROR,3189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6798,3,['ERROR'],['ERROR']
Availability,"mentations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand and @jsotobroad (sorry guys, I wasn't sure how to track your contributions while fixing up commits!) I also added tests for both GC/no-GC pair workflows.; -@MartonKN should review to gain familiarity with the WDL. Note that this WDL has already been through many revisions from @meganshand, @jsotobroad, and @LeeTL1220, so hopefully there shouldn't be too much for you to find serious fault with. Note that I punted on adding MultidimensionalKernelSegmenterUnitTest and ModelSegmentsIntegrationTest. Filed #3916. Closes #2858. (FINALLY!); Closes #3825.; Closes #3661.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:2385,fault,fault,2385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['fault'],['fault']
Availability,mention --help in error message,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1341:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1341,1,['error'],['error']
Availability,"minates-without-an-active-exception). \--. JointGenotyping fails in ImportGvcfs with the c++ error ""terminate called without an active exception"", which occurs when a thread goes out of scope without calling join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:1392,error,error,1392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['error'],['error']
Availability,"mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/m421cdb2b133a2578e9a2670dfbb5d33e.so /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/mod.cpp -lpython3.6m; ERROR (theano.gof.cmodule): [Errno 12] Cannot allocate memory; Traceback (most recent call last):; File ""/tmp/tintest/cohort_denoising_calling.4390748645603329412.py"", line 143, in <module>; shared_workspace, initial_params_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/tasks/task_cohort_denoising_calling.py"", line 140, in __init__; denoising_model = DenoisingModel(denoising_config, shared_workspace, initial_param_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; instance.__init__(*args, **kwargs); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 851, in __init__; observed=shared_workspace.n_st); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; return model.Var(name, dist, data, total_size); File ""/home/tintest/min",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:63149,ERROR,ERROR,63149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['ERROR'],['ERROR']
Availability,"mmandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /root/gatk.jar FilterMutectCalls -V gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeVCFs/Abrams_cell-unfiltered.vcf -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -O Abrams_cell-filtered.vcf --contamination-table /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/contamination.table --tumor-segmentation /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/segments.table --ob-priors /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-LearnReadOrientationModel/artifact-priors.tar.gz -stats /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeStats/merged.stats --filtering-stats filtering.stats --min-median-read-position 10; ```. Both of these tests were run on an interval that included a single chromosome (approximately 24Mb). Thank you for your help!. Best,; Kate. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24345/m2-error-with-canine-germline-resource-and-variants-for-contamination-files/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:10319,error,error-with-canine-germline-resource-and-variants-for-contamination-files,10319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['error'],['error-with-canine-germline-resource-and-variants-for-contamination-files']
Availability,modify build.gradle to give a reasonable error if R isn't found,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/351:41,error,error,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/351,1,['error'],['error']
Availability,"mon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:12187,ERROR,ERROR,12187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,monCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:13087,ERROR,ERROR,13087,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"mory()=3391094784; java.lang.NullPointerException; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:231); at org.broadinstitute.hellbender.tools.funcotator.vcfOutput.VcfOutputRenderer.close(VcfOutputRenderer.java:137); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:883); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:970); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. #### Steps to reproduce. Does not work; ```; gatk Funcotator --variant input.vcf.gz --reference /path/to/human_g1k_v37_decoy.fasta --ref-version hg19 --data-sources-path /path/to/funcotator_dataSources.v1.6.20190124s/ --output output.vcf.gz --output-file-format VCF; ```. Works; ```; gatk Funcotator --variant input.vcf.gz --reference /path/to/human_g1k_v37_decoy.fasta --ref-version hg19 --data-sources-path /path/to/funcotator_dataSources.v1.6.20190124s/ --output output.vcf --output-file-format VCF; ```. (note that the `--output` parameter is different). #### Expected behavior; It should either give an error/warning saying outputting compressed VCF output is not supported, or output a compressed VCF like other GATK tools. #### Actual behavior; See examples above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5683:2755,error,error,2755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5683,1,['error'],['error']
Availability,moving some test utilities from src/test to src/main/ in order to make them available to hellbender-dataflow. removed a method that was only used by dataflow,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/992:76,avail,available,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/992,1,['avail'],['available']
Availability,moving test utilities to make them available to hellbender-dataflow,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/992:35,avail,available,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/992,1,['avail'],['available']
Availability,"mp_0003_of_10/cohort-model/ --calls-shard-path /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//cohort_calls//temp_0001_of_10/cohort-calls/ --calls-shard-path /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//cohort_calls//temp_0002_of_10/cohort-calls/ --calls-shard-path /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//cohort_calls//temp_0003_of_10/cohort-calls/ --allosomal-contig chrX --allosomal-contig chrY --contig-ploidy-calls /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//cohort_ploidy//cohort-calls/ --sample-index 16 --output-genotyped-intervals /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//vcfs//sample.intervals.vcf --output-genotyped-segments /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//vcfs//sample.segments.vcf --sequence-dictionary /home/lmbs02/bio/databases/referenses/hg19_37/ucsc/hg19.dict --output-denoised-copy-ratios /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//vcfs//sample.copy_ratios.tsv `. In the same time, if you use the first 4 or the third and 4 at the same time, an error pops up.; `12:49:08.552 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/lmbs02/bio/biosoft/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 29, 2020 12:49:08 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:49:08.687 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 12:49:08.687 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:49:08.687 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:49:08.687 INFO PostprocessGermlineCNVCalls - Executing as lmbs02@Lmbs01 on Linux v5.4.0-48-generic amd64; 12:49:08.687 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924:6906,error,error,6906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924,1,['error'],['error']
Availability,"mport - Importing batch 5 with 50 samples. 01:50:11.697 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 02:11:57.531 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 02:20:02.171 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 02:35:13.654 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 03:04:30.490 INFO  GenomicsDBImport - Importing batch 5 with 50 samples. 03:05:06.171 INFO  GenomicsDBImport - Done importing batch 5/7. 03:05:14.150 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 03:08:31.080 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 03:23:52.054 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 03:30:37.049 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 03:43:46.119 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 04:09:27.761 INFO  GenomicsDBImport - Importing batch 6 with 50 samples. 04:10:10.953 INFO  GenomicsDBImport - Done importing batch 6/7. 04:10:18.233 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 04:13:55.022 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 04:27:28.342 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 04:33:32.781 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 04:44:09.752 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 05:04:33.112 INFO  GenomicsDBImport - Importing batch 7 with 45 samples. 05:05:02.272 INFO  GenomicsDBImport - Done importing batch 7/7. 05:05:02.299 INFO  GenomicsDBImport - Import of all batches to GenomicsDB completed!. 05:05:02.300 INFO  GenomicsDBImport - Shutting down engine. \[October 19, 2022 5:05:02 AM GMT\] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 506.11 minutes. Runtime.totalMemory()=8653373440. pure virtual method called. terminate called without an active exception<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/297458'>Zendesk ticket #297458</a>)<br> gz#297458</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:21965,down,down,21965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['down'],['down']
Availability,"mtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/projects/gentaumix/Ressources/grch38\_BWA\_2/GCA\_000001405.15\_GRCh38\_no\_alt\_plus\_hs38d1\_analysis\_set.fa -V gendb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dbsnp138.vcf --sequence-dictionary /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dict -L /shared/projects/gentaumix/Ressources/interval\_genomicsdbi/temp\_6/interval.interval\_list -G StandardAnnotation -G AS\_StandardAnnotation --merge-input-intervals ; ; 14:17:35.171 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 14:17:35.232 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Sep 10, 2021 2:17:35 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:17:35.492 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.492 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 14:17:35.492 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:17:35.493 INFO GenotypeGVCFs - Executing as quentin67100@cpu-node-9 on Linux v3.10.0-1160.6.1.el7.x86\_64 amd64 ; ; 14:17:35.493 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7465:2646,Redundant,Redundant,2646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465,1,['Redundant'],['Redundant']
Availability,"my pipeline(from fastq ) is 👍 ; . 1. FastqToSam . 2. ConvertHeaderlessHadoopBamShardToBam. 3. BwaAndMarkDuplicatesPipelineSpark; ; ...... but in the step 3 （BwaAndMarkDuplicatesPipelineSpark），the pipeline crash; ```; Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, com1, executor 1): # **java.lang.IllegalArgumentException: Reference name for '1853452901' not found in sequence dictionary.; at htsjdk.samtools.SAMRecord.resolveNameFromIndex(SAMRecord.java:569); at htsjdk.samtools.SAMRecord.setMateReferenceIndex(SAMRecord.java:506); at htsjdk.samtools.BAMRecord.<init>(BAMRecord.java:94); at htsjdk.samtools.DefaultSAMRecordFactory.createBAMRecord(DefaultSAMRecordFactory.java:42); at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:210); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.<init>(BAMFileReader.java:963); at htsjdk.samtools.BAMFileReader.getIterator(BAMFileReader.java:491)**; at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:182); at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:211); at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:180); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:179); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:134); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4179:286,failure,failure,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4179,2,['failure'],['failure']
Availability,"n FilterVariantTranches step with error:. htsjdk.tribble.TribbleException: The provided reference alleles do not appear to represent the same position, C\* vs. T\*. The command line  is  ; ; gatk FilterVariantTranches -I ${R1%%\_\*}-recal.bam -V ${R1%%\_\*}-annotated.vcf -R /mnt/d/GenLab/WES/reference/hg19.fasta --create-output-variant-index true --resource /mnt/d/GenLab/WES/db/00-All.vcf.gz --resource /mnt/d/GenLab/WES/db/00-common\_all.vcf.gz --resource /mnt/d/GenLab/WES/reference/1000G\_phase1.indels.hg19.sites.vcf --resource /mnt/d/GenLab/WES/reference/Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf --snp-tranche 99.9 --snp-tranche 99.95 --indel-tranche 99.0 --indel-tranche 99.4 -O ${R1%%\_\*}-filtered.vcf --tmp-dir /mnt/d/GenLab/WES/output/tmp --java-options ""-Xmx24G"". On 4.1.4.0 no problems whatsoever, on 4.1.8.0 not working at all. Double-confirmed by 2 seperate conda envs. The reference file is unchanged during whole running processes, obviously. Full error log: ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx24G -jar /mnt/d/GenLab/WES/software/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar FilterVariantTranches -I D1394-recal.bam -V D1394-annotated.vcf -R /mnt/d/GenLab/WES/reference/hg19.fasta --create-output-variant-index true --resource /mnt/d/GenLab/WES/db/00-All.vcf.gz --resource /mnt/d/GenLab/WES/db/00-common\_all.vcf.gz --resource /mnt/d/GenLab/WES/reference/1000G\_phase1.indels.hg19.sites.vcf --resource /mnt/d/GenLab/WES/reference/Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf --snp-tranche 99.9 --snp-tranche 99.95 --indel-tranche 99.0 --indel-tranche 99.4 -O D1394-filtered.vcf --tmp-dir /mnt/d/GenLab/WES/output/tmp ; ; 14:50:12.699 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/mnt/d/GenLab/WES/software/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/nativ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701:1627,error,error,1627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701,1,['error'],['error']
Availability,"n Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:4000,ERROR,ERROR,4000,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['ERROR'],['ERROR']
Availability,"n the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to “0” (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are tested. **contaminationOppositeDepth = Math.max(oppositeDepth - errorDepth, 0);**; **contamination = contaminationOppositeDepth / totalDepthWeightedByOppositeFrequency**. Solution proposed:; Currently, when errorDepth is greater than oppositeDepth, the output contamination is reported as **’0’ contamination**; it seems to us that this should instead be interpreted as **“unable to calculate contamination”** because of the high error rate in these pileups. Improvements for current version:; In addition, we suggest calculating all contamination values over all strategies/iterations and outputting the highest contamination value (by MAF and/or strategy, as appropriate), rather than exiting after the first MAF iteration where stdError exit condition is met. ### Description what needs to be added or modified; Please see the code change attached, compare it to 4.2.0.0. The code ou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7177:1811,error,errorDepth,1811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177,1,['error'],['errorDepth']
Availability,"n$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:4965,ERROR,ERROR,4965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['ERROR'],['ERROR']
Availability,"n$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikeliho",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:9732,failure,failure,9732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['failure'],['failure']
Availability,"n, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154:1203,down,down,1203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154,1,['down'],['down']
Availability,"n-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397:1202,error,error,1202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397,1,['error'],['error']
Availability,"n/gatk-4.1.9.0/gatk --java-options -Xms24g VariantRecalibrator -V temp/vartiant_germline/sites.only.vcf.gz -O temp/vartiant_germline/recaliberation.indel.vcf --tranches-file temp/vartiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz --use-allele-specific-annotations`. #### Error Message; ```; Using GATK jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms24g -jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_su",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6963:1684,Error,Error,1684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6963,1,['Error'],['Error']
Availability,"n: chr3:117,527,190; INFO	2022-05-06 12:14:45	SortVcf	wrote 800,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:134,613,380; INFO	2022-05-06 12:14:45	SortVcf	wrote 825,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:153,780,108; INFO	2022-05-06 12:14:45	SortVcf	wrote 850,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:173,329,831; INFO	2022-05-06 12:14:46	SortVcf	wrote 875,000 records. Elapsed time: 00:00:03s. Time for last 25,000: 0s. Last read position: chr3:192,133,262; [Fri May 06 12:14:46 EDT 2022] picard.vcf.SortVcf done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=2855272448; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp. java.lang.ArrayIndexOutOfBoundsException: 16799; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:102); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:92); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); 	at picard.vcf.SortVcf.writeSortedOutput(SortVcf.java:183); 	at picard.vcf.SortVcf.doWork(SortVcf.java:101); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. #### Expected output. There's almost certainly some format issue with my VCF, but ideally GATK would have a better error message than ArrayIndexOutOfBoundsException.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7838:3163,error,error,3163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7838,1,['error'],['error']
Availability,"n; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:38553,Error,Error,38553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,nReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.toolin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:7275,ERROR,ERROR,7275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,nReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:10923,ERROR,ERROR,10923,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,nReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:9665,ERROR,ERROR,9665,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"nScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python -c import gatktool. Stdout: ; Stderr: Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'gatktool'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:198); when I checked gatktool python package, it is installed in the python packages by conda. after activate gatk4 , I checked with pip install gatktool, and it says the package already installed. Anyone experienced this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397:3570,error,error,3570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397,1,['error'],['error']
Availability,"n_rnaseq/gatk_output/CDL-164-04P/CDL-164-04P-1_0_249250621_genomicsdb; 10:24:57.553 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 10:24:57.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:10769,error,error,10769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['error'],['error']
Availability,"na \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.addGivenAlleles(AssemblyBasedCallerUtils.java:350); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:291); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6037:1691,error,error,1691,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037,1,['error'],['error']
Availability,"nager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS184_1.raw_variants.g.vcf; 11:30:53.894 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS201_1.raw_variants.g.vcf; 11:30:54.000 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS209_1.raw_variants.g.vcf; 11:34:25.030 INFO CombineGVCFs - Done initializing engine; 11:34:25.154 INFO ProgressMeter - Starting traversal; 11:34:25.155 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:34:25.473 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location DS235882:44 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 11:34:25.944 INFO CombineGVCFs - Shutting down engine; [October 26, 2020 11:34:25 AM EDT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 3.59 minutes.; Runtime.totalMemory()=3738173440; java.lang.NumberFormatException: empty String; 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842); 	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); 	at java.lang.Double.parseDouble(Double.java:538); 	at htsjdk.variant.vcf.VCFUtils.parseVcfDouble(VCFUtils.java:262); 	at htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:808); 	at htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:121); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.ensureSampleNameMap(LazyGenotypesContext.java:180); 	at htsjdk.variant.variantcontext.GenotypesContext.getSampleNames(GenotypesContext.java:646); 	at htsjdk.variant.variantcontext.VariantContex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913:7146,down,down,7146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913,1,['down'],['down']
Availability,"nal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. 21/04/13 07:32:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurren",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:11280,failure,failure,11280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['failure'],['failure']
Availability,nal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.r,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:5485,ERROR,ERROR,5485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['ERROR'],['ERROR']
Availability,"nce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:9194,ERROR,ERROR,9194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['ERROR'],['ERROR']
Availability,"ncher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```. It does not matter if I produce the pre-adapter metrics with the latest Picard jar v2.9.2. I get the same error. . I'm using a M2 callset from GATK3. Even so, I don't think I should get the above error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030:3981,error,error,3981,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030,2,['error'],['error']
Availability,nd:; `./gatk BaseRecalibratorSpark --tmp-dir /dev/shm/gatktmp/ -I /home/data/WGS/F002/F002.sort.bam -O 1.grp --known-sites /home/data/ref/dbsnp_138.hg19.vcf --known-sites /home/data/ref/1000G_phase1.indels.hg19.sites.vcf --known-sites /home/data/ref/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf -R /home/data/ref/ucsc.hg19.fasta -- --spark-runner SPARK --spark-master local[8] --driver-memory 100G`. Here is the log:. > 19:23:59.384 INFO FeatureManager - Using codec VCFCodec to read file file:///dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf; 19:23:59.411 INFO BaseRecalibrationEngine - The covariates being used here: ; 19:23:59.411 INFO BaseRecalibrationEngine - 	ReadGroupCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	QualityScoreCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	ContextCovariate; 19:23:59.412 INFO BaseRecalibrationEngine - 	CycleCovariate; 18/10/17 19:23:59 ERROR Executor: Exception in task 517.0 in stage 0.0 (TID 517); org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316:1261,ERROR,ERROR,1261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316,1,['ERROR'],['ERROR']
Availability,"ndLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). This request was created from a contribution made by Domniki Manousi on March 07, 2022 12:01 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4556136866843-FindBreakpointEvidenceSpark-sudden-shutdown](https://gatk.broadinstitute.org/hc/en-us/community/posts/4556136866843-FindBreakpointEvidenceSpark-sudden-shutdown). \--. Hi, I am trying to run the tool FindBreakpointEvidenceSpark. I have successfully  produced the required kmers and the tool seems to run for several minutes until it finally stops without producing output. I have read in past issues that memory usage might be a problem and have tried to accomodate for it using the -Xmx option. . a) GATK version used: gatk4: 4.2.0.0 through singularity (/cvmfs/singularity.galaxyproject.org/all/gatk4:4.2.0.0--0) . b) Exact command used: . singularity exec /cvmfs/singularity.galaxyproject.org/all/gatk4:4.2.0.0--0 gatk --java-options ""-Xmx75g -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true"" FindBreakpointEvidenceSpark \\ ; ;    -R /mnt/SCRATCH/domniman/references/ssa\_selected/Simon\_Final2021\_Ssa\_selected.fa -I /mnt/SCRATCH/domniman/2014G\_NO\_Males\_1169\_D03\_RG.bam \\ ; ;    --aligner-index-image /mnt/SCRATCH/domniman/references/ssa\_selected/Simon\_Final2021\_Ssa\_selected.fa.img \\ ; ;    --kmers-to-ignore /mnt/users/domniman/ag\_fish/kmers\_to\_ignore.txt -O /mnt/SCRATCH/domniman/assembly.sam \\ ; ;    --tmp-dir /mnt/SCRATCH/domniman/tmp -L ssa03. Entire error log: ; ; Due to length of the complete log (37.671 lines) I attach it as a separate link: [https://www.dropbox.com/s/n7q5dco4z5t3moz/gatk%20error%20log%20.txt?dl=0](https://www.dropbox.com/s/n7q5dco4z5t3moz/gatk%20error%20log%20.txt?dl=0). Best,. Domniki<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/275546'>Zendesk ticket #275546</a>)<br> gz#275546</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7710:2825,error,error,2825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7710,1,['error'],['error']
Availability,nder.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:349); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:90); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.io.IOException: Stream closed; at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170); at java.io.BufferedInputStream.read(BufferedInputStream.java:336); at java.io.DataInputStream.read(DataInputStream.java:149); at java.io.DataInputStream.read(DataInputStream.java:149); at org.disq_bio.disq.impl.file.HadoopFileSystemWrapper$SeekableHadoopStream.read(HadoopFileSystemWrapper.java:232); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:133); at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:21); ... 22 more; 19/04/28 10:11:25 INFO ShutdownHookManager: Shutdown hook called. Could you please help me to resolve this issue. Thanks In Advance; Fazulur Rehaman. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23954/stream-closed-error-with-gatk-4-1-1-0/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5919:4122,error,error-with-gatk-,4122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919,1,['error'],['error-with-gatk-']
Availability,ndexedFeatureReader$WFIterator.readNextRecord(TribbleIndexedFeatureReader.java:365); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.next(TribbleIndexedFeatureReader.java:346); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.next(TribbleIndexedFeatureReader.java:307); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. I ran `java -jar gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar SelectVariants -V gnomADaccuracyTest.noMQinSNPVQSR.SynDip.vcf.gz -O testNoIndex.vcf.gz`. Data is at `/humgen/gsa-hpprojects/dev/gauthier/reblockGVCF` If I remember to pull down the index everything works swimmingly. I'd love for this to either work without an index or fail early with an appropriate message about the index being missing.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224:2831,down,down,2831,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224,1,['down'],['down']
Availability,"ndler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409732fb{/executors/threadDump/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e99e2cb{/static,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextH",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:8875,AVAIL,AVAILABLE,8875,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"ndler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@642ec6{/stages/stage/kill,null,AVAILABLE,@Spark}; 10:33:07.389 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3fe5ad73{/metrics/json,null,AVAILABLE,@Spark}; 10:33:07.397 INFO SortSamSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 10:33:07.450 INFO GoogleHadoopFileSystemBase - GHFS version: 1.9.4-hadoop3; 10:33:08.183 INFO MemoryStore - Block broadcast_0 stored as values in memory (estimated size 268.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:47179,AVAIL,AVAILABLE,47179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"ne.CommandLineProgram.runTool(CommandLineProgram.java:147); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:166); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:209); at org.broadinstitute.hellbender.Main.main(Main.java:306); Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms200G -Xmx200G -XX:ParallelGCThreads=2 -jar /gatk/gatk-package-4.5.0.0-local.jar BaseRecalibrator -I VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam -O VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.baseRecal.bam -R GRCh38.primary_assembly.genome.fa --known-sites 1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --tmp-dir /tmp --disable-bam-index-caching true. Work dir:; /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/71/ac26344f0e095f7fe77cbb45a334db. Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`. -- Check '.nextflow.log' file for details. ```. I tried to run it like this:; ```; gatk --java-options ""-Xms200G -Xmx200G -XX:ParallelGCThreads=2"" \; BaseRecalibrator \; -I $input_bam \; -O ""${file(input_bam).baseName}.baseRecal.bam"" \; -R $reference \; --known-sites $kg_snp \; --known-sites $kg_indel \; --tmp-dir /tmp \; --disable-bam-index-caching true; ```. but I still get the memory error. I have more memory to use, but it seems very inefficient if I need to go up to 1TB? Why can I not make this run? And is there any alternative when I want to do the MarkDup, SplitCigar, BaseRecal ? . Hope you can help, ; BR, ; Mette",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8726:5601,error,error,5601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8726,1,['error'],['error']
Availability,"ne; 22:13:30.002 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates13996065741193890473.csv'; 22:13:30.002 INFO AnalyzeCovariates - Generating plots file './sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf'; 22:13:30.518 INFO AnalyzeCovariates - Shutting down engine; [August 7, 2023 at 10:13:30 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=113246208; org.broadinstitute.hellbender.utils.R.RScriptExecutorException:; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10708586791705723928';source('/tmp/BQSR.12372590345390592260.R'); /tmp/AnalyzeCovariates13996065741193890473.csv /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_before_recal_data.table /attach/data/vinit/human_exome/test/./sample_analysis/SRR25308851/SRR25308851_recalibration_plots.pdf; Stdout:; Stderr:; Attaching package: ‘gplots’. The following object is masked from ‘package:stats’:. lowess. Error in names(x) <- value :; 'names' attribute [6] must be the same length as the vector [1]; Calls: source ... finishTable -> .gsa.assignGATKTableToEnvironment -> colnames<-; In addition: Warning messages:; 1: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 2: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 3: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 4: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; 5: In type.convert.default(d[, i]) :; 'as.is' should be specified by the caller; using TRUE; Execution halted. at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:18); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.execut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8456:3534,mask,masked,3534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8456,1,['mask'],['masked']
Availability,"neGVCFs \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -R ""$ref_gen""/ucsc.hg19.fasta \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200272.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200273.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200274.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200313.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200314.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200315.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-006.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-007.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/backup_gvcfs/all_wes_samples.g.vcf \; -O /paedwy/disk1/yangyxt/wes/backup_gvcfs/all_wes_samples_plus_${sample_batch}.g.vcf.gz && echo ""Combine_gvcfs done"". Error Log:; ```; 12:01:36.798 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:01:36.824 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 24, 2020 12:01:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:01:37.108 INFO CombineGVCFs - ------------------------------------------------------------; 12:01:37.108 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:01:37.108 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:01:37.108 INFO CombineGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 12:01:37.108 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 12:01:37.108 INFO CombineGVCFs - Start Date/Time: August 24, 202",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766:1570,Redundant,Redundant,1570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766,1,['Redundant'],['Redundant']
Availability,nerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildCon,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3108,ERROR,ERROR,3108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"new Germline CNV wdl test are too slow, causing travis failures",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4064:55,failure,failures,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4064,1,['failure'],['failures']
Availability,new dangling head recovery leads to array index out of bounds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7085:18,recover,recovery,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085,1,['recover'],['recovery']
Availability,"nflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:3675,ERROR,ERROR,3675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['ERROR'],['ERROR']
Availability,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:4547,down,down,4547,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,3,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"ng GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:1078,Redundant,Redundant,1078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['Redundant'],['Redundant']
Availability,"ng for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:1552,ERROR,ERROR,1552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,3,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,ngGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:11056,Recover,Recovered,11056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"ngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.679 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.684 INFO ProgressMeter - Starting traversal; 14:34:51.684 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:34:51.694 INFO IndexFeatureFile - Shutting down engine; [August 2, 2021 at 2:34:51 PM CEST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=113246208; java.lang.IllegalArgumentException: Unexpected value: Ensembl_canonical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1391); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:197); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7385:2071,error,errors,2071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385,1,['error'],['errors']
Availability,"ngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.422 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.433 INFO ProgressMeter - Starting traversal; 18:53:59.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 18:54:01.952 INFO IndexFeatureFile - Shutting down engine; [March 8, 2021 at 6:54:01 PM CET] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=473956352; java.lang.IllegalArgumentException: Unexpected value: MANE_Plus_Clinical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1388); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(Genc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7134:2308,error,errors,2308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134,1,['error'],['errors']
Availability,"nnel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused by: java.io.IOException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.throwIfFalseEOF(NetHttpResponse.java:202); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:171); at java.io.FilterInputStream.read(FilterInputStream.java:107); at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:638); ... 13 more. Subsequent reruns on 4.0.4.0 yielded only 1 failure out of 3 walkers x 59 samples; no failures were observed on 4.0.9.0 or 4.0.11.0. So the problem is not unique to 4.0.12.0, but the rate of failure is much higher. Additional reruns on 4.0.12.0 suggest that the original failures were not intermittent; one run showed 6/59 samples failing, with many of those having more than one of the 3 walkers fail. Additional reruns with a branch of 4.0.12.0 that reverted to the htsjdk version used in 4.0.11.0 still showed a high rate of failure. There were at least a couple of instances where the same BAM appeared to fail in roughly the same spot as in the 4.0.12.0 runs, and other instances where the same BAM failed in roughly the same spot, only in two different walkers. However, the set of BAMs that failed was not consistent across all runs. Leaving to @droazen to delegate. The FC CNV featured WDL doesn't use NIO yet, but I'm surprised this hasn't cropped up in other WDLs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631:8062,failure,failure,8062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631,5,['failure'],"['failure', 'failures']"
Availability,"node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:500); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(PostprocessGermlineCNVCalls.java:436); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.traverse(PostprocessGermlineCNVCalls.java:297); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:892); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Let me know what data files we need to look into to figure out the cause of error so I can make them available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:14257,error,error,14257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,2,"['avail', 'error']","['available', 'error']"
Availability,"not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7315:1878,ERROR,ERROR,1878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"nps.high\_confidence.b37.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file. 19:13:26.973 INFO ASEReadCounter - Done initializing engine. 19:13:26.977 INFO ProgressMeter - Starting traversal. 19:13:26.977 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute. 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835092. 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835132. 19:13:27.118 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:835133. ... . ... ... 19:13:28.229 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:29617944. 19:13:28.230 WARN ASEReadCounter - Ignoring site: variant is not het at postion: 1:29618025. 19:13:28.231 INFO ASEReadCounter - 0 read(s) filtered by: ValidAlignmentStartReadFilter. 0 read(s) filtered by: ValidAlignmentEndReadFilter. 0 read(s) filtered by: HasReadGroupReadFilter. 0 read(s) filtered by: MatchingBasesAndQualsReadFilter. 0 read(s) filtered by: SeqIsStoredReadFilter. 51 read(s) filtered by: NotDuplicateReadFilter. 63 read(s) filtered by: NotSecondaryAlignmentReadFilter. 3 read(s) filtered by: MappedReadFilter. 117 total reads filtered. 19:13:28.231 INFO ProgressMeter - 1:29618022 0.0 110019 5264067.0. 19:13:28.231 INFO ProgressMeter - Traversal complete. Processed 110019 total loci in 0.0 minutes. 19:13:28.233 INFO ASEReadCounter - Shutting down engine. \[June 14, 2021 7:13:28 PM UTC\] org.broadinstitute.hellbender.tools.walkers.rnaseq.ASEReadCounter done. Elapsed time: 0.04 minutes. Runtime.totalMemory()=2303197184. output.txt:. contig position variantID refAllele altAllele refCount altCount totalCount lowMAPQDepth lowBaseQDepth rawDepth otherBases improperPairs. Thanks for your help!. Chunyang<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/165377'>Zendesk ticket #165377</a>)<br>gz#165377</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7327:5880,down,down,5880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327,1,['down'],['down']
Availability,"nputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist.; 	at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.Com",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:8672,ERROR,ERROR,8672,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['ERROR'],['ERROR']
Availability,"nquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK CalibrateDragstrModel. ### Affected version(s); - [x] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When running CalibrateDragstrModel in parallel mode, the supplied reference isn't detected correctly causing the following error stack trace:. ```bash; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx72g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CalibrateDragstrModel --input input.cram --output input.txt --reference hg38.fa --str-table-path hg38.zip --threads 12 --intervals fasta_bed.bed --tmp-dir .; 10:24:21.117 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:21.289 INFO CalibrateDragstrModel - ------------------------------------------------------------; 10:24:21.289 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:24:21.289 INFO CalibrateDragstrModel -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:1509,error,error,1509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['error'],['error']
Availability,nsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:4081,ERROR,ERROR,4081,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['ERROR'],['ERROR']
Availability,"nsferService' on port 44190.; 18/01/09 18:31:06 INFO netty.NettyBlockTransferService: Server created on 192.168.1.4:44190; 18/01/09 18:31:06 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/01/09 18:31:06 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.4, 44190, None); 18/01/09 18:31:06 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.1.4:44190 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.1.4, 44190, None); 18/01/09 18:31:06 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.4, 44190, None); 18/01/09 18:31:06 INFO storage.BlockManager: external shuffle service port = 7337; 18/01/09 18:31:06 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.4, 44190, None); 18/01/09 18:31:06 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60c8909a{/metrics/json,null,AVAILABLE,@Spark}; 18/01/09 18:31:06 INFO scheduler.EventLoggingListener: Logging events to hdfs://tele-1:8020/user/spark/spark2ApplicationHistory/application_1515493209401_0001; 18/01/09 18:31:09 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000002 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.cont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:15758,AVAIL,AVAILABLE,15758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"nsider if this documentation is adequate from the perspective of a technically competent new user reading the documentation that has not seen examples outside of what is shown in the docs. Actually, to be honest, most of the examples out in the wild stick to only the most basic of options, so for more advanced uses it really is pretty unclear what such arguments should be. . This is not a generic please write better documentation (though that's always appreciated). This is particularly about 1) The tool usage format documentation 2) the particular mis-specification for interval formats (maybe some interpretive element of the page drops the ':' and '-', but viewing the source doesn't show any sign of them). For resolving one, I think moving a bit closer to the specifications in Linux man pages would help, but those are far from perfect themselves. . Here, even way back in 2013, Geraldine gives the correct version of the format. ; https://gatkforums.broadinstitute.org/gatk/discussion/3395/interval-file-errors. > Hi Kristine,; > ; > Make sure your intervals list is named with either extension .bed or .list as appropriate; it cannot end in .txt. The program gets confused, thinks header lines are intervals and doesn't parse the file correctly. For the record, the simplest format for intervals (which I prefer, personally) is the \<chr\>:\<start\>-\<stop\> format, which doesn't require a sequence dictionary.; > ; > The intervals list specifies which regions of the genome the analysis will be run on. I can't comment on how it's used in MuTect, but in GATK it's typically used to restrict analysis to exome capture targets, or to particular regions of interest. And confirms shortly after in the same thread that this is referring to the GATK formats described (.list and .intervals): . > Oh, if you have the intervals in that format the extension needs to be .interval_list or .list, not bed. You'll need to change the starting zeroes to ones. Sorry, the formatting requirements are",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6639:5045,error,errors,5045,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6639,1,['error'],['errors']
Availability,"ntBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:4776,ERROR,ERROR,4776,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['ERROR'],['ERROR']
Availability,"ntVariantsSpark - Initializing engine; 19:43:09.993 INFO PrintVariantsSpark - Done initializing engine; 17/11/15 19:43:11 INFO org.spark_project.jetty.util.log: Logging initialized @4976ms; 17/11/15 19:43:11 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 17/11/15 19:43:11 INFO org.spark_project.jetty.server.Server: Started @5092ms; 17/11/15 19:43:11 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@5917b44d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/11/15 19:43:12 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 17/11/15 19:43:13 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at gatk-test-8875b999-b609-4a3f-86ea-973b929fe662-m/10.240.0.18:8032; 17/11/15 19:43:17 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1510774921124_0001; 17/11/15 19:43:28 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 17/11/15 19:43:35 ERROR org.apache.spark.scheduler.TaskResultGetter: Exception while getting task result; com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; Serialization trace:; genotypes (org.seqdoop.hadoop_bam.VariantContextWithHeader); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsof",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:4540,ERROR,ERROR,4540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['ERROR'],['ERROR']
Availability,"ntamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(Filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:5730,Error,ErrorProbabilities,5730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['Error'],['ErrorProbabilities']
Availability,"ntextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:8609,AVAIL,AVAILABLE,8609,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"ntextHandler: Started o.s.j.s.ServletContextHandler@54247647{/jobs/job,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5463f035{/jobs/job/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:8350,AVAIL,AVAILABLE,8350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"ntextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409732fb{/executors/threadDump/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e99e2cb{/static,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@478967eb{/,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f2b39a{/api,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18c880ea{/jobs/job/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6afbe6a1{/stages/stage/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:56 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.4:4040; 18/01/09 18:30:56 INFO spark.SparkContext: Added JAR file:/opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar at spark://192.168.1.4:38793/jars/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar with timestamp 1515493856032; 18/01/09 18:30:56 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 18/01/09 18:30:57 INFO client.RMProxy: Connecting to ResourceManager at tele-1/192.168.1.4:8032; 18/01/09 18:30:57 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers; 18/01/09 18:30:58 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (18432 MB per container); 18/01/09 18:30:58 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 18/01/09 18:30:58 INFO yarn.Client: Setting up containe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:10328,AVAIL,AVAILABLE,10328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:21407,failure,failure,21407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['failure'],['failure']
Availability,"nts -V INPUT_VCF -R REF -O OUTPUT_VCF. Then, GATK outputs the following:. ```; 18:30:57.468 INFO CNNScoreVariants - Initializing engine; 18:30:57.985 INFO FeatureManager - Using codec VCFCodec to read file ...; 18:30:58.183 INFO IntervalArgumentCollection - Processing 48129895 bp from intervals; 18:30:58.188 INFO CNNScoreVariants - Done initializing engine; 18:31:00.188 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:.../1d_cnn_mix_train_full_bn.2560984151625233621.json and weights:.../1d_cnn_mix_train_full_bn.2397909300265264152.hd5; 18:31:19.873 INFO ProgressMeter - Starting traversal; 18:31:19.874 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 18:31:50.095 INFO CNNScoreVariants - Shutting down engine; [2018/04/24 18:31:50 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.88 minutes.; Runtime.totalMemory()=2141716480; ***********************************************************************; A USER ERROR has occurred: A timeout ocurred waiting for output from the remote Python command.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A timeout ocurred waiting for output from the remote Python command.; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4696:1327,ERROR,ERROR,1327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696,1,['ERROR'],['ERROR']
Availability,nts.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(Gradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:5379,ERROR,ERROR,5379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"nvs/gatk/lib/python3.6/site-packages/theano/gof/link.py"", line 325, in raise_with_op; reraise(exc_type, exc_value, exc_trace); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/six.py"", line 692, in reraise; raise value.with_traceback(tb); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 978, in p; self, node); File ""theano/scan_module/scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform (/home/shlee/.theano/compiledir_Linux-4.13--gcp-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/scan_perform/mod.cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:11467,error,error,11467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,1,['error'],['error']
Availability,"nzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.addGivenAlleles(AssemblyBasedCallerUtils.java:350); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:291); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6037:1249,echo,echo,1249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037,1,['echo'],['echo']
Availability,"o be special and left untouched by BQSR. Currently, there is no easy way to convert base qualities to two. The only instances I am aware of is (i) for SamToFastq, which then unaligns the reads and (ii) MergeBamAlignment, which isn't necessarily a part of everyone's workflow. Also, MergeBamAlignment's `CLIP_ADAPTERS` softclips XT tagged sequence, which then becomes fair game for our assembly-based callers. MarkIlluminaAdapters uses aligned reads to mark those with 3' adapter sequence with the XT tag. The XT tag values note the start of the 3' adapter sequence in the read. During MergeBamAlignment, one must especially request that this XT tag is retained in the merged output. Because our assembly-based callers throw out CIGAR strings from the aligner when reassembling reads, so as to use soft-clipped sequence that may contain true variants we wish to resolve, adapter sequence can be incorporated into the graph. This is not an issue for libraries with low levels of adapter read through and for germline calling as we prune nodes in the graph that have less than two reads supporting it. . However, for somatic cases and for libraries where there is considerable adapter read through, the current solution is to hard-clip adapter sequences out of reads or to toss these reads altogether so as not to increase the extent of spurious calls. The issue with hard-clipping is that our reads become malformed due to a mismatch in CIGAR string and sequence length. These the GATK engine filters. So the solution is to either correct the CIGAR strings or to go back and re-align the clipped reads or again to toss the reads. It would be great not to have to throw out reads that include some adapter sequence in somatic workflows that call down to the lowest allele fraction variants. It seems this would simply be a matter of a tool or feature that replaces adapter sequence marked with the XT tag with base qualities of 2 and special handling by our callers of sequence with base quality of two.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3540:2018,down,down,2018,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3540,1,['down'],['down']
Availability,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option ‘-rpath’; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742:2965,error,error,2965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742,5,['error'],['error']
Availability,"o read file file:///data/users/zhanglei/species/Medicago/result/SRR340097.HC.g.vcf.gz; 18:08:15.044 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340098.HC.g.vcf.gz; 18:08:15.098 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340099.HC.g.vcf.gz; 18:08:15.142 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340100.HC.g.vcf.gz; 18:08:15.183 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340101.HC.g.vcf.gz; 18:08:15.217 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340102.HC.g.vcf.gz; 18:08:15.250 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340103.HC.g.vcf.gz; 18:08:15.277 INFO CombineGVCFs - Shutting down engine; [May 18, 2019 6:08:15 PM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2372403200; java.lang.IllegalArgumentException: Feature inputs must be unique: /data/users/zhanglei/species/Medicago/result/SRR340103.HC.g.vcf.gz; 	at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$initializeDrivingVariants$0(MultiVariantWalker.java:60); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); 	at org.broadinstitute.hellbender.engine.MultiVariantWalker.initializeDrivingVariants(MultiVariantWalker.java:56); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:47); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:558); 	at org.broadinstitute.hellbender.engine.MultiVariantWalker.onStartup(MultiVariantWalker.java:48); 	at org.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947:3988,down,down,3988,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947,1,['down'],['down']
Availability,"o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409732fb{/executors/threadDump/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e99e2cb{/static,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@478967eb{/,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f2b39a{/api,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18c880ea{/jobs/job/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6afbe6a1{/stages/stage/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:56 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.4:4040; 18/01/09 18:30:56 INFO spark.SparkContext: Added JAR file:/opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar at spark://192.168.1.4:38793/jars/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:9690,AVAIL,AVAILABLE,9690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"o.s.j.s.ServletContextHandler@753e4eb5{/stages,null,AVAILABLE,@Spark}; 10:33:07.355 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@63318b56{/stages/json,null,AVAILABLE,@Spark}; 10:33:07.357 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@462f8fe9{/stages/stage,null,AVAILABLE,@Spark}; 10:33:07.358 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@b2e1df3{/stages/stage/json,null,AVAILABLE,@Spark}; 10:33:07.359 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6cf3b3d7{/stages/pool,null,AVAILABLE,@Spark}; 10:33:07.360 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:46312,AVAIL,AVAILABLE,46312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"oadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 284.6 KB); 16/08/24 14:06:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56998 (size: 23.2 KB, free: 10.4 GB); 16/08/24 14:06:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:105; 16/08/24 14:06:10 INFO FileInputFormat: Total input paths to process : 1; 16/08/24 14:06:21 INFO SparkUI: Stopped Spark web UI at http://10.200.98.30:4040; 16/08/24 14:06:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/08/24 14:06:21 INFO MemoryStore: MemoryStore cleared; 16/08/24 14:06:21 INFO BlockManager: BlockManager stopped; 16/08/24 14:06:21 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/08/24 14:06:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/08/24 14:06:21 INFO SparkContext: Successfully stopped SparkContext; 14:06:21.109 INFO SparkGenomeReadCounts - Shutting down engine; [August 24, 2016 2:06:21 PM EDT] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=3192389632; java.lang.IndexOutOfBoundsException; at java.nio.ByteBuffer.wrap(ByteBuffer.java:375); at htsjdk.samtools.BAMRecord.getCigar(BAMRecord.java:246); at org.seqdoop.hadoop_bam.BAMSplitGuesser.guessNextBAMRecordStart(BAMSplitGuesser.java:189); at org.seqdoop.hadoop_bam.BAMInputFormat.addProbabilisticSplits(BAMInputFormat.java:244); at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:159); at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:253); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:120); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2113:1743,down,down,1743,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113,1,['down'],['down']
Availability,oadinstitute/hellbender/utils/recalibration/covariates/ContextCovariate.java line 191 -->. ```; while (bases[currentNPenalty] != 'N') {; final int baseIndex = BaseUtils.simpleBaseToBaseIndex(bases[currentNPenalty]);; currentKey |= (baseIndex << offset);; offset -= 2;; currentNPenalty--;; }; ```. The current while loop allows the array index to become negative and walk right off the edge of the read. So a proposed fix is as follows (assuming it does not break the covariate logic) -->. ```; while (currentNPenalty > 0 && bases[currentNPenalty] != 'N') {; final int baseIndex = BaseUtils.simpleBaseToBaseIndex(bases[currentNPenalty]);; currentKey |= (baseIndex << offset);; offset -= 2;; currentNPenalty--;; }; ```. Minimal Command (test.bam attached - added txt extension just so site would let me attach it) -->. ```; gatk-launch BaseRecalibrator -I test.bam -O test.table -R GATK_Bundle_Build38/Homo_sapiens_assembly38.fasta --knownSites GATK_Bundle_Build38/dbsnp_146.hg38.vcf.gz; ```. Error message --> . ```; java.lang.ArrayIndexOutOfBoundsException: -1; 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ContextCovariate.contextWith(ContextCovariate.java:191); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ContextCovariate.recordValues(ContextCovariate.java:68); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.StandardCovariateList.recordAllValuesInStorage(StandardCovariateList.java:133); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:546); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:527); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:136); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:180); 	at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:96); 	at java.util.strea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4005:1673,Error,Error,1673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4005,1,['Error'],['Error']
Availability,"object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:4082,down,down,4082,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['down'],['down']
Availability,"oblem using BaseRecalibratorSpark. The tool fails soon after starting. The same error appears with the same bam file on different machines. Additionally, vanilla BaseRecalibrator works just fine on these bams (so I don't think the issue is with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:1160,error,error,1160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['error'],['error']
Availability,occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2933,ERROR,ERROR,2933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:3341,error,error,3341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,4,['error'],"['error', 'errors']"
Availability,"of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:35.527 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:11:35.528 INFO CombineGVCFs - Executing as jpac1984@p0002.ten.osc.edu on Linux v3.10.0-1160.21.1.el7.x86_64 amd64; 20:11:35.529 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 20:11:35.529 INFO CombineGVCFs - Start Date/Time: June 13, 2021 8:11:34 PM GMT; 20:11:35.529 INFO CombineGVCFs - --",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7311:1457,error,error,1457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311,1,['error'],['error']
Availability,"ok well this was a draft, but Miguel got here first, so.....; do we want to just shut this all down and skip Validate VDS?. https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/91bc9190-d623-4ce6-8184-b20e5cb622e5. ok I hate this pr---I dont think it makes sense to have a VDS validation script that only produces a VDS if the VDS matches the VCF--that makes it very hard to debug. https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/c27c9bbe-6a01-4639-bdf9-14b00d5dc252",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8343:95,down,down,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8343,1,['down'],['down']
Availability,"ol behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. After sourcing the tab-completion script, some tools shown cannot be run. Maybe they exist somewhere in an experimental dev version but are not bundled for public release?. ### Affected version(s); - [x ] Latest public release version [4.1.7.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. After trying to tab complete the DepthOfCoverage, I saw a few tools not listed in the documentation. I tried running them and sure enough, there were errors:. `A USER ERROR has occurred: '*' is not a valid command.`; (* is one of the tools listed below). #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. ```; cd gatk-4.1.7.0; source gatk-completion.sh; ./gatk Depth<tab>; #>DepthOfCoverage DepthPerAlleleBySample DepthPerSampleHC; ./gatk DepthPerSampleHC -h; ...; ***********************************************************************; A USER ERROR has occurred: 'DepthPerSampleHC' is not a valid command.; ***********************************************************************; ./gatk DepthPerAlleleBySample -h; ...; ***********************************************************************; A USER ERROR has occurred: 'DepthPerAlleleBySample' is not a valid command.; ***********************************************************************; ./gatk DepthOfCoverage -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6615:1827,error,errors,1827,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6615,1,['error'],['errors']
Availability,"ol(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	... 5 more. To bring about success I added 4 lines of code : . ```; ln -vs ${ref_fasta} ; ; ln -vs ${ref_fasta_fai} ;; ln -vs ${ref_fasta_dict} ;; FASTA_NAME=`basename ${ref_fasta} `; . ```. Then, instead of --reference ${ref_fasta} in calling gatk-protected, I put --reference $FASTA_NAME and the ""null"" exception went away and the program run successfully. ---. @eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-264264203). per @droazen : @achevali @LeeTL1220 . ---. @LeeTL1220 commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265342816). @eddiebroad Before this gets assigned, what version of gatk-protected are you using?; Assuming that this is a version we built (despite the name ""eddie.jar""): @achevali , can you figure out how you are reporting the error. @droazen are you sure this is not in the engine?. ---. @eddiebroad commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gatk-protected/issues/806#issuecomment-265464665). @LeeTL1220 . The original JAVA JAR where I first observed the ""null"" message I presume was based off commit 3a2bb0d. At the time the project was initiated I think it was the latest commit. The original JAR where the ""null"" message was observed was gatk-protected-all-3a2bb0d-SNAPSHOT-spark_standalone.jar . Because of the ""3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2922:3459,error,error,3459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922,1,['error'],['error']
Availability,"ol(s) or class(es); TransferReadTags. ### Affected version(s); - [X] Latest public release version [gatk-4.3.0.0]. ### Description ; When traversing the reads in both aligned (target) and unaligned (the one with the desired tag) BAMs an error is thrown complaining about a read `found in the aligned bam is not found in the unmapped bam`. However the reads exists. It looks like the `traverse` function that uses the lexicographic order difference between both query names will find a _negative_ `diff` and assume that the read in the aligned BAM is missing in the uBAM. However, with Illumina read headers it seems almost guaranteed that this is going to be an issue since the y-coord (the last colon-separated field in the header) often has numbers with different number of digits. The lexicographical comparison will fail to adjust when comparing two read names where the length of the read in the target BAM is larger than the length of the read in the uBAM. . This is the `traverse` function that throws the error:; https://github.com/broadinstitute/gatk/blob/2b0a558fdb9fdf654e796d5d69a092e26345583b/src/main/java/org/broadinstitute/hellbender/tools/walkers/qc/TransferReadTags.java#L109-L145 . #### Steps to reproduce; Run `TransferReadTags` with an Illumina sequenced aligned BAM. I can provide dummy files if needed, but should be easy to reproduce. The following example should help illustrate the issue:. ```sh; $ /data/reddylab/software/gatk/gatk-4.3.0.0/gatk TransferReadTags \; --output /data/reddylab/Alex/tmp/TEST_BAM.with_umis.bam \; --read-tags RX \; --unmapped-sam /data/reddylab/Alex/tmp/TEST_BAM.umi.nsorted.ubam \; --input /data/reddylab/Alex/tmp/TEST_BAM.nsorted.bam; ```. Produces the following output:; ```; Using GATK jar /gpfs/fs1/data/reddylab/software/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8147:1043,error,error,1043,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8147,1,['error'],['error']
Availability,"oleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4j.appender.console=org.apache.log4j.ConsoleAppender; log4j.appender.console.target=System.out; log4j.appender.console.layout=org.apache.log4j.PatternLayout; log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:1406,ERROR,ERROR,1406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,1,['ERROR'],['ERROR']
Availability,"olean readHasStarted = false;; boolean addedHardClips = false;. while (!cigarStack.empty()) {; final CigarElement cigarElement = cigarStack.pop();. if (!readHasStarted &&; cigarElement.getOperator() != CigarOperator.DELETION &&; cigarElement.getOperator() != CigarOperator.SKIPPED_REGION &&; cigarElement.getOperator() != CigarOperator.HARD_CLIP) {; readHasStarted = true;; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.HARD_CLIP) {; totalHardClip += cigarElement.getLength();; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.DELETION) {; totalHardClip += cigarElement.getLength();; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.SKIPPED_REGION) {; totalHardClip += cigarElement.getLength();; }. if (readHasStarted) {; if (i == 1) {; if (!addedHardClips) {; if (totalHardClip > 0) {; inverseCigarStack.push(new CigarElement(totalHardClip, CigarOperator.HARD_CLIP));; }; addedHardClips = true;; }; inverseCigarStack.push(cigarElement);; } else {; if (!addedHardClips) {; if (totalHardClip > 0) {; cleanCigar.add(new CigarElement(totalHardClip, CigarOperator.HARD_CLIP));; }; addedHardClips = true;; }; cleanCigar.add(cigarElement);; }; }; }; // first pass (i=1) is from end to start of the cigar elements; if (i == 1) {; shiftFromEnd = shift;; cigarStack = inverseCigarStack;; }; // second pass (i=2) is from start to end with the end already cleaned; else {; shiftFromStart = shift;; }; }; }; `. Notice that the variable _shift_ is initialized, but never assigned to again for the duration of the loop. Thus _shiftFromStart_ and _shiftFromEnd_ are always set to zero upon completion of the loop. These values are used by the _applyHardClipBases_ function, which is called in a number of places to hard clip bases from a read, but because of this error, they will always be zeroed out. The function containing the error is in the file ""src/main/java/org/broadinstitute/hellbender/utils/clipping/ClippingOp.java"" line 523",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6130:2226,error,error,2226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6130,2,['error'],['error']
Availability,"omics/dataflow/readers/bam/ReadBAMTransform.getReadsFromBAMFilesSharded(Lcom/google/cloud/dataflow/sdk/Pipeline;Lcom/google/cloud/genomics/utils/GenomicsFactory$OfflineAuth;Ljava/lang/Iterable;Lcom/google/cloud/genomics/dataflow/readers/bam/ReaderOptions;Ljava/util/List;)Lcom/google/cloud/dataflow/sdk/values/PCollection; @25: invokevirtual; Reason:; Type 'com/google/cloud/dataflow/sdk/transforms/Create' (current frame, stack[2]) is not assignable to 'com/google/cloud/dataflow/sdk/transforms/PTransform'; Current Frame:; bci: @25; flags: { }; locals: { 'com/google/cloud/dataflow/sdk/Pipeline', 'com/google/cloud/genomics/utils/GenomicsFactory$OfflineAuth', 'java/lang/Iterable', 'com/google/cloud/genomics/dataflow/readers/bam/ReaderOptions', 'java/util/List', 'com/google/cloud/genomics/dataflow/readers/bam/ReadBAMTransform' }; stack: { 'com/google/cloud/dataflow/sdk/values/TupleTag', 'com/google/cloud/dataflow/sdk/Pipeline', 'com/google/cloud/dataflow/sdk/transforms/Create' }; Bytecode:; 0x0000000: bb00 0159 2db7 0002 3a05 1905 2bb6 0003; 0x0000010: b200 042a 1904 b800 05b6 0006 c000 07b8; 0x0000020: 0008 b600 09b8 000a b200 0b2a 2cb8 0005; 0x0000030: b600 06c0 0007 120c b800 0db6 0009 b600; 0x0000040: 0e3a 0619 0519 06b6 000f b0 . at org.broadinstitute.hellbender.engine.dataflow.datasources.ReadsDataflowSource.getReadPCollection(ReadsDataflowSource.java:130); at org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BadTypeRepro.ingestReadsAndGrabHeader(BadTypeRepro.java:100); at org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BadTypeRepro.setupPipeline(BadTypeRepro.java:74); ```. This is the same error I saw when upgrading to google-cloud-dataflow-java-sdk-all:0.4.150710 (#754), so perhaps this is caused by a version mismatch somewhere. I wrote a small bug-reproducing class, [BadTypeRepro](https://github.com/broadinstitute/hellbender/blob/jp_badtype_repro/src/main/java/org/broadinstitute/hellbender/dev/tools/walkers/bqsr/BadTypeRepro.java), in its eponymous branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/791:2253,error,error,2253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/791,1,['error'],['error']
Availability,"omicsDBImport - Initializing engine; 01:07:02.331 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 01:07:02.702 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 01:07:02.868 INFO IntervalArgumentCollection - Processing 135534747 bp from intervals; 01:07:02.869 INFO GenomicsDBImport - Done initializing engine; 01:07:02.870 INFO GenomicsDBImport - Callset Map JSON file will be re-written to /paedwy/disk1/yangyxt/wes/healthy_bams_for_CNV/using_v6_probe/genomicdbimport_chr10/callset.json; 01:07:02.870 INFO GenomicsDBImport - Incrementally importing to workspace - /paedwy/disk1/yangyxt/wes/healthy_bams_for_CNV/using_v6_probe/genomicdbimport_chr10; 01:07:02.871 INFO ProgressMeter - Starting traversal; 01:07:02.871 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 01:07:03.006 INFO GenomicsDBImport - Shutting down engine; [August 29, 2020 at 1:07:03 AM HKT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2147483648; org.genomicsdb.exception.GenomicsDBException: Duplicate sample name found: A130489. Sample was originally in /paedwy/disk1/yangyxt/wes/batch11_13/gvcfs/A130489.HC.g.vcf.gz; at org.genomicsdb.importer.extensions.CallSetMapExtensions.checkDuplicateCallsetsForIncrementalImport(CallSetMapExtensions.java:270); at org.genomicsdb.importer.extensions.CallSetMapExtensions.mergeCallsetsForIncrementalImport(CallSetMapExtensions.java:241); at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:252); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:745); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1049); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:13161,down,down,13161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['down'],['down']
Availability,"ommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:3467,error,error,3467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['error'],['error']
Availability,"ommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; GET https://storage.googleapis.com/storage/v1/b/fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/o?maxResults=1&prefix=5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list.idx/&projection=full&userProject; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""User project specified in the request is invalid."",; ""reason"" : ""invalid""; } ],; ""message"" : ""User project specified in the request is invalid.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7716:4416,error,errors,4416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716,1,['error'],['errors']
Availability,"on emitted from ""AD"" field in vcf is much lower than the real depth. I think I have disabled downsampling by set ""--max-reads-per-alignment-start"" to 0. The command line I used is as follow: . gatk  Mutect2  -R  reference.fa  -I  tumor.bam  --panel-of-normals  pon.vcf.gz  -L  target.bed  -O  sample.snvIndels.vcf  --callable-depth  30  --f1r2-tar-gz  sample.f1r2.tar.gz  --min-base-quality-score  25  --max-reads-per-alignment-start  0  --minimum-allele-fraction  0.002  --dont-use-soft-clipped-bases  --force-active  --mitochondria-mode  --enable-all-annotations . For example, a mutated point information in vcf called by GATK4.2.0.0-Mutect2 is: . 1 24868045 . A G . . AC=1;AF=0.500;AN=2;AS\_MQ=60.00;AS\_SB\_TABLE=51,50|46,23;AS\_UNIQ\_ALT\_READ\_COUNT=69;BaseQRankSum=0.561;ClippingRankSum=-1.473;DP=179;ECNT=2;FS=13.849;LikelihoodRankSum=-0.392;MBQ=37,37;MFRL=236,239;MMQ=60,60;MPOS=44;MQ=60.00;MQ0=0;MQRankSum=0.000;NCC=0;NCount=0;OCM=0;PON;POPAF=7.30;REF\_BASES=GCTCAGCAGAACAGACCCAGA;ReadPosRankSum=1.335;SOR=1.545;Samples=HD786\_4-1;TLOD=230.09 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:**101,69**:0.408:170:54,30:45,39:51,50,46,23. But the information of the same point in vcf called by GATK4.1.9.0-Mutect2 **using the same command** is: . 1 24868045 . A G . . AC=1;AF=0.500;AN=2;AS\_MQ=60.00;AS\_SB\_TABLE=299,290|183,155;AS\_UNIQ\_ALT\_READ\_COUNT=328;BaseQRankSum=1.715;ClippingRankSum=-0.613;DP=934;ECNT=2;FS=1.802;LikelihoodRankSum=0.052;MBQ=20,20;MFRL=153,145;MMQ=60,60;MPOS=39;MQ=60.00;MQ0=0;MQRankSum=0.000;NCC=0;NCount=0;OCM=0;PON;POPAF=7.30;REF\_BASES=GCTCAGCAGAACAGACCCAGA;ReadPosRankSum=3.636;SOR=0.837;Samples=HD786\_4-1;TLOD=779.29 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:**589,338**:0.371:927:290,167:293,171:299,290,183,155. This ""580+338"" is exactly the true depth. Is there any other downsampling or filter in GATK4.2.0.0-Mutect2 but not in GATK4.1.9.0-Mutect2?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/159302'>Zendesk ticket #159302</a>)<br>gz#159302</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7285:2598,down,downsampling,2598,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7285,1,['down'],['downsampling']
Availability,"on to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) Entire program log:. Using GATK jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar. Running:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR. 15:30:47.303 info Nativ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7966:1985,ERROR,ERROR,1985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966,1,['ERROR'],['ERROR']
Availability,on.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:13446,ERROR,ERROR,13446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"on.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:12546,ERROR,ERROR,12546,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"one importing batch 3/6; 00:53:26.360 INFO GenomicsDBImport - Starting batch input file preload; 00:54:29.501 INFO GenomicsDBImport - Finished batch preload; 00:54:29.502 INFO GenomicsDBImport - Importing batch 4 with 50 samples; 06:31:35.151 INFO ProgressMeter - C1:10577213 1386.8 4 0.0; 06:31:35.152 INFO GenomicsDBImport - Done importing batch 4/6; 06:31:35.152 INFO GenomicsDBImport - Starting batch input file preload; 06:32:34.398 INFO GenomicsDBImport - Finished batch preload; 06:32:34.404 INFO GenomicsDBImport - Importing batch 5 with 50 samples; 11:49:07.726 INFO ProgressMeter - C1:10577213 1704.4 5 0.0; 11:49:07.727 INFO GenomicsDBImport - Done importing batch 5/6; 11:49:07.727 INFO GenomicsDBImport - Starting batch input file preload; 11:49:48.117 INFO GenomicsDBImport - Finished batch preload; 11:49:48.117 INFO GenomicsDBImport - Importing batch 6 with 45 samples; 16:32:47.060 INFO ProgressMeter - C1:10577213 1988.0 6 0.0; 16:32:47.060 INFO GenomicsDBImport - Done importing batch 6/6; 16:32:47.061 INFO ProgressMeter - C1:10577213 1988.0 6 0.0; 16:32:47.062 INFO ProgressMeter - Traversal complete. Processed 6 total batches in 1988.0 minutes.; 16:32:47.062 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 16:32:47.062 INFO GenomicsDBImport - Shutting down engine; [February 29, 2020 4:32:47 PM PST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 1,988.13 minutes.; Runtime.totalMemory()=57266405376; ```; And the SLURM log: ; ```; Name : combine_gvcfs.count=0004; User : sdturner; Partition : med2; Nodes : c6-74; Cores : 21; GPUs : 0; State : COMPLETED; Submit : 2020-02-28T07:24:31; Start : 2020-02-28T07:24:31; End : 2020-02-29T16:32:58; Reserved walltime : 14-00:00:00; Used walltime : 1-09:08:27; Used CPU time : 1-10:10:37; % User (Computation): 89.40%; % System (I/O) : 10.60%; Mem reserved : 80G/node; Max Mem used : 62.43G (c6-74); Max Disk Write : 0.00 (c6-74); Max Disk Read : 307.20K (c6-74); ```; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487:6471,down,down,6471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487,1,['down'],['down']
Availability,"one iterations:. 0) Initialize allele frequencies to the mean of the Dirichlet heterozygosity prior; i.e. ~1 for ref, ~1/1000 for each alt, plus any allele counts from the resources. Genotype priors come from the multinomial distribution (one genotype is a draw of 2 alleles) of these allele frequencies.; 1) (E step) genotype posteriors are the product of genotype likelihoods with the priors from step 0). Pseudocounts are the sum of expected posterior allele counts.; 2) (M step) MLE allele frequencies are the mode of the Dirichlet parameterized by the sum of the original step 0) prior+resources pseudocounts with the E step pseudocounts from step 1). Hmmm that does sound a lot like what the code is doing now. I suppose it's defensible after all. ---. @ldgauthier commented on [Thu May 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-220347447). So it sounds to me like the action item here is to fix the Dirichlet heterozygosity prior. I like the idea of adding one count for the ref and 1/1000 for each alt (rather than, for example, 1000 for ref and one for alt) so the heterozygosity prior does something in the absence of external resource counts, but doesn't overwhelms them if they are present. @davidbenjamin Can you think of a more rigorous justification for the scaling of counts between sample genotype allele counts and the heterozygosity?. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-260474993). Is this still alive? To be continued in GATK3 or 4?. ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-260646009). You can move to GATK4. ---. @davidbenjamin commented on [Sun Nov 20 2016](https://github.com/broadinstitute/gatk-protected/issues/792#issuecomment-261761138). @vdauwera @ldgauthier the new qual score model does exactly this. IMO we should simply have this CLI use `AlleleFrequencyCalculator`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2918:6253,alive,alive,6253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918,1,['alive'],['alive']
Availability,"ool will have the exact same functionality as `CollectAllelicCounts`, to the point where I can re-use the integration tests. However, the integration tests fail. When I dig deeper in `CollectAllelicCountsSpark`, I see that only 8 RDDs (correct amount: 11) are being passed to processAlignments... Consider the following code:. ```; @Override; protected void processAlignments(JavaRDD<LocusWalkerContext> rdd, JavaSparkContext ctx) {; final String sampleName = SampleNameUtils.readSampleName(getHeaderForReads());; final SampleMetadata sampleMetadata = new SimpleSampleMetadata(sampleName);; final Broadcast<SampleMetadata> sampleMetadataBroadcast = ctx.broadcast(sampleMetadata);. final AllelicCountCollector finalAllelicCountCollector =; rdd.mapPartitions(distributedCount(sampleMetadataBroadcast.getValue(), minimumBaseQuality)); .reduce((a1, a2) -> combineAllelicCountCollectors(a1, a2, sampleMetadataBroadcast.getValue()));; final List<LocusWalkerContext> tmp = rdd.collect();; ....snip....; ```. In this case `tmp` will have a size of 8. However, the integration test would indicate a size of 11 is correct, since 11 intervals are being passed in. Note that `emitEmptyLoci()` returns `true`, so 11 is the correct number as seen in `CollectAllelicCountsSparkIntegrationTest` . . Additionally, in (at least) one result, the counts are wrong. `CollectAllelicCounts` (non-spark) passes the integration test. I have tried a couple of tests to gather more information:. - Is `emitEmptyLoci()` causing an issue? ; Does not appear to be causing the issue. I say this because when set to `false`, I get (essentially) the same error.; - The code uses `mapPartition` and not `map`, does this cause the issue? Why are you doing this?; This does not cause the issue. I refactored the code to use `map` and got the exact same issue. I use `mapPartition` in order to instantiate only one instance of `AllelicCountCollector` per partition, instead of per locus. Assigning to @tomwhite by request of @droazen ...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3823:1731,error,error,1731,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3823,1,['error'],['error']
Availability,"or ""terminate called without an active exception"", which occurs when a thread goes out of scope without calling join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:1427,error,errors,1427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['error'],['errors']
Availability,"or.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.Contai",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:34543,Error,Error,34543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,orEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292). I realize this is a open source project. But I've made copy of the failing VCF available at:; /dsde/working/fleharty/tmp/buggy.vcf; /dsde/working/fleharty/tmp/buggy.vcf.idx. #### Steps to reproduce; gatk VariantAnnotator -V buggy.vcf --resource:gnomad af-only-gnomad.raw.sites.vcf -E gnomad.AF --resource-allele-concordance -O gnomad_annotated.vcf; #### Expected behavior; Should work. #### Actual behavior; Throws exception,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689:3710,avail,available,3710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689,1,['avail'],['available']
Availability,orEvidenceFilter.java:27); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:11335,Error,ErrorProbabilities,11335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Error'],['ErrorProbabilities']
Availability,"ords; WARNING: No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 23:16:27.971 INFO GenotypeGVCFs - Done initializing engine; 23:16:28.168 INFO ProgressMeter - Starting traversal; 23:16:28.169 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; WARNING: No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 23:20:28.785 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.11700176899999999,Cpu time(s),0.11079876499999991; [January 7, 2020 11:20:29 PM EST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.71 minutes.; Runtime.totalMemory()=2446327808; java.lang.ArrayIndexOutOfBoundsException: 5; 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.generatePL(ReferenceConfidenceVariantContextMerger.java:652); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:543); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:130); 	at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:310); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:136); 	at java.util.stream.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357:4708,down,down,4708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357,1,['down'],['down']
Availability,"ored as bytes in memory (estimated size 23.6 KB, free 360.5 MB); 19/04/08 19:03:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 23.6 KB, free: 365.8 MB); 19/04/08 19:03:27 INFO SparkContext: Created broadcast 4 from newAPIHadoopFile at PathSplitSource.java:96; 19/04/08 19:03:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.9 MB, free 323.6 MB); 19/04/08 19:03:28 INFO SparkUI: Stopped Spark web UI at http://ip-xx.xx.xx.xx.ec2.internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:17342,down,down,17342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,2,['down'],['down']
Availability,"org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.bfi --filter-bwa-image /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathseq_host.fa.img --microbe-bwa-image /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/pathogen_ref/pathseq_microbe.fa.img --microbe-fasta /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/pathogen_ref/pathseq_microbe.fa --taxonomy-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/pathogen_ref/pathseq_taxonomy.db --min-clipped-read-length 50 --scores-output /data/shenlab/abd/TCGA_microbiome/WXS_colorectal_all/out/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.50.scores.txt --output /data/shenlab/abd/TCGA_microbiome/WXS_colorectal_all/out/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.50.pathseq.bam; /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file; `",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:7103,error,error,7103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['error'],['error']
Availability,"org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:1138,ERROR,ERROR,1138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,1,['ERROR'],['ERROR']
Availability,"org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:16709,down,down,16709,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,2,['down'],['down']
Availability,"org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.Stre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:4798,failure,failure,4798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['failure'],['failure']
Availability,"ormatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:5585,error,errorProbability,5585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['error'],['errorProbability']
Availability,"ormatter - Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14 ; INFO 17:39:56,368 HelpFormatter - Program Args: -T LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V zeta_af-only-gnomad_Hg19toGRCh38.vcf.gz -o eta_af-only-gnomad_Hg19toGRCh38.vcf.gz ; INFO 17:39:56,373 HelpFormatter - Executing as shlee@WMCF9-CB5 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14. ; INFO 17:39:56,374 HelpFormatter - Date/Time: 2017/08/22 17:39:56 ; INFO 17:39:56,374 HelpFormatter - -------------------------------------------------------------------------------- ; INFO 17:39:56,374 HelpFormatter - -------------------------------------------------------------------------------- ; INFO 17:39:56,385 GenomeAnalysisEngine - Strictness is SILENT ; INFO 17:39:57,377 GenomeAnalysisEngine - Downsampling Settings: Method: BY_SAMPLE, Target Coverage: 1000 ; WARN 17:39:57,688 IndexDictionaryUtils - Track variant doesn't have a sequence dictionary built in, skipping dictionary validation ; INFO 17:39:58,497 GenomeAnalysisEngine - Preparing for traversal ; INFO 17:39:58,502 GenomeAnalysisEngine - Done preparing for traversal ; INFO 17:39:58,503 ProgressMeter - [INITIALIZATION COMPLETE; STARTING PROCESSING] ; INFO 17:39:58,503 ProgressMeter - | processed | time | per 1M | | total | remaining ; INFO 17:39:58,503 ProgressMeter - Location | sites | elapsed | sites | completed | runtime | runtime ; INFO 17:39:58,692 LeftAlignAndTrimVariants - Reference allele is too long (245) at position chr1:10146; skipping that record. Set --reference_window_stop >= 245 ; INFO 17:39:58,697 LeftAlignAndTrimVariants - Reference allele is too long (225) at position chr1:10178; skipping that record. Set --reference_window_stop >= 225 ; INFO 17:39:58,700 LeftAlignAndTrimVariants - Reference allele is too long (221) at position chr1:10213; skipping that record. Set --reference_window_stop >= 221 ; INFO 17:39:58,700 LeftAlignAndTrimVariants - Reference allele is",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487:3012,Down,Downsampling,3012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487,1,['Down'],['Downsampling']
Availability,ortGVCFs/shard-5/inputs/1422537242/000006KQ0748.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0757.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0775.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0784.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0793.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ1479.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/00000. ...(all the shards fail in the same way). (this is stderr.background for one shard; all 10 shards log the same error). lee04110@ln0005 \[/scratch.global/lee04110/batch\] % cat /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.background . INFO:    Using cached SIF image. INFO:    Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:.     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\] --reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:13934,error,error,13934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['error'],['error']
Availability,"orted to GenotypeGVCFs? @vdauwera @vruano ?. ---. @vdauwera commented on [Fri Oct 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-257037491). That sounds like a good idea, @SHuang-Broad . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260456840). @SHuang-Broad @vruano Is this (making the HC allele culling available to GenotypeGVCFs too) still on your radar(s)?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260687763). @vdauwera yes it is on mine. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:9595,ping,ping,9595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['ping'],['ping']
Availability,"ory - Creating default GencodeFuncotation on transcript ENST00000377837.5 for problem variant: chr1:6412417-6412418(AT* -> A); 12:22:46.360 INFO ProgressMeter - chr1:7546157 0.2 3000 14771.0; 12:22:56.769 INFO ProgressMeter - chr1:15441040 0.4 6000 15932.0; 12:23:08.615 INFO ProgressMeter - chr1:24268222 0.6 10000 17420.6; 12:23:18.806 INFO ProgressMeter - chr1:32245507 0.7 13000 17475.9; 12:23:29.190 INFO ProgressMeter - chr1:40884137 0.9 16000 17449.2; 12:23:40.060 INFO ProgressMeter - chr1:49383659 1.1 19000 17302.6; 12:23:52.381 INFO ProgressMeter - chr1:59695584 1.3 23000 17645.3; 12:24:03.424 INFO ProgressMeter - chr1:68377682 1.5 27000 18151.3; 12:24:13.590 INFO ProgressMeter - chr1:76887474 1.7 31000 18709.1; 12:24:25.294 INFO ProgressMeter - chr1:85925943 1.9 36000 19438.3; 12:24:35.871 INFO ProgressMeter - chr1:94465524 2.0 40000 19721.1; 12:24:46.052 INFO ProgressMeter - chr1:102403847 2.2 44000 20018.4; 12:24:57.807 INFO ProgressMeter - chr1:111078655 2.4 49000 20468.7; 12:25:09.216 INFO ProgressMeter - chr1:120476481 2.6 53000 20510.4; 12:25:20.032 INFO ProgressMeter - chr1:148975263 2.8 57000 20620.0; 12:25:23.276 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000420382.1 for variant: chr1:151765708-151765710(ATT* -> A): Variant overlaps transcript but is not completely contained within it. Funcotator cannot currently handle this case. Transcript: ENST00000420382.1 Variant: [VC Unknown @ chr1:151765708-151765710 Q. of type=INDEL alleles=[ATT*, A] attr={AS_FilterStatus=weak_evidence, AS_SB_TABLE=[0, 0|0, 0], DP=1, ECNT=1, GERMQ=1, MBQ=[0, 30], MFRL=[0, 325], MMQ=[60, 60], MPOS=29, POPAF=0.839, RPA=[15, 13], RU=T, STR=true, STRQ=93, TLOD=3.31} GT=GT:AD:AF:DP:F1R2:F2R1:FAD:SB 0/1:0,1:0.667:1:0,1:0,0:0,1:0,0,0,1 filters=germline,weak_evidence; 12:25:23.277 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000420382.1 for problem variant: chr1:151765708-151765710(ATT* -> A).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8844:1948,ERROR,ERROR,1948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8844,1,['ERROR'],['ERROR']
Availability,"ory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:47207,failure,failure,47207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['failure'],['failure']
Availability,"ot a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.pathExists(SparkUtils.java:100); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.setHadoopBAMConfigurationProperties(ReadsSparkSource.java:241); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:203); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [da63aa3c-e3bc-4893-9f40-42921719a343] entered state [ERROR] while waiting for [DONE].; ```. to reproduce this error, . ```bash; cd /Users/shuang/GATK/gatk. CLUSTER_NAME=""svdev-caller""; MASTER_NODE=""hdfs://svdev-caller-m:8020""; PROJECT_DIR=""user/shuang/NA12878_PCR-_30X"". ./gatk-launch FindBreakpointEvidenceSpark \; -R ""$MASTER_NODE""/reference/Homo_sapiens_assembly38.fasta \; -I ""$MASTER_NODE""/data/smallCram.cram \; -O ""$MASTER_NODE""/""$PROJECT_DIR""/fastq \; --exclusionIntervals gs://sv-data-dsde-dev/reference/GRCh38.kill.intervals \; --kmersToIgnore gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.dups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --clust",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:4200,ERROR,ERROR,4200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['ERROR'],['ERROR']
Availability,"ot sure if this is a bug or something wrong with my bam files. Any help on solving/debugging would be welcomed. Running mutect2 returns >100,000 warnings of more than two reads with the same name found. The bams were processing following best practices. The header of the log file: . Mutect2 -R resources/hg38/genome/d ; 975 efault/genome.fa -L resources/hg38/a.interval_list -I recal/RBL3_diagnostic.bam -I recal/RBL3_germline.bam -I recal/RBL3_diagnostic.bam -I recal/RBL3_relapse1.bam -I recal/RBL ; 976 3_relapse2.bam -I recal/RBL3_PDX.bam -normal RBL3_germline_hg38 --germline-resource resources/hg38/gnomad/af-only-gnomad.hg38.vcf.gz --panel-of-normals resources/hg38/pon/1000 ; 977 g_pon.hg38.vcf.gz --f1r2-tar-gz results/RBL3/f1r2.tar.gz --read-filter NotSupplementaryAlignmentReadFilter --read-filter NotSecondaryAlignmentReadFilter --native-pair-hmm-thre ; 978 ads 20 -O results/RBL3/unfiltered.vcf ; 979 17:07:51.270 WARN GATKReadFilterPluginDescriptor - Redundant enabled filter (NotSecondaryAlignmentReadFilter) is enabled for this tool by default ; 980 17:07:51.313 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rds/project/rds-cyiwgCzJok8/WES_snakemake/.snakemake/conda/773770bb2edb9f4c58fb17b5017e1f ; 981 be_/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so ; 982 17:07:51.633 INFO Mutect2 - ------------------------------------------------------------ ; 983 17:07:51.635 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.5.0.0 ; 984 17:07:51.635 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/ ; 985 17:07:51.635 INFO Mutect2 - Executing as cjs236@cpu-r-25 on Linux v4.18.0-553.16.1.el8_10.x86_64 amd64 ; 986 17:07:51.635 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v17.0.11-internal+0-adhoc..src ; 987 17:07:51.635 INFO Mutect2 - Start Date/Time: August 28, 2024 at 5:07:51 PM BST ; 988 17:07:51.635 INFO Mutect2 - ----------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8966:967,Redundant,Redundant,967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8966,1,['Redundant'],['Redundant']
Availability,"otator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:55:32.063 INFO Funcotator - Deflater: IntelDeflater; 02:55:32.063 INFO Funcotator - Inflater: IntelInflater; 02:55:32.063 INFO Funcotator - GCS max retries/reopens: 20; 02:55:32.063 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 02:55:32.063 WARN Funcotator - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 02:55:32.063 INFO Funcotator - Initializing engine; 02:55:32.318 INFO FeatureManager - Using codec VCFCodec to read file file:///export2/liuhw/wes_test/Mutect2_filter/K001137N_somatic_filtered.vcf.gz; 02:55:32.459 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 02:55:32.466 INFO Funcotator - Shutting down engine; [July 12, 2024 2:55:32 AM EDT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2148532224; ***********************************************************************. A USER ERROR has occurred: Bad input: ERROR in config file: file:///./software/gatk_Funcotator/funcotator_dataSources.v1.8.hg38.20230908s/gnomAD_exome/hg38/gnomAD_exome.config - src_file does not exist: /./software/gatk_Funcotator/funcotator_dataSources.v1.8.hg38.20230908s/gnomAD_exome/hg38/gs:/broad-public-datasets/funcotator/gnomAD_2.1_VCF_INFO_AF_Only/hg38/gnomad.exomes.r2.1.sites.liftoverToHg38.INFO_ANNOTATIONS_FIXED.vcf.gz. ***********************************************************************; ```; How to solved it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8913:3372,down,down,3372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8913,3,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"ots file '/researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/AnalyzeCovariates.pdf'; 23:15:31.932 INFO AnalyzeCovariates - Shutting down engine; [January 19, 2020 11:15:31 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2161115136; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.2074992987327687075';source('/tmp/BQSR.6874121927957307421.R'); /tmp/AnalyzeCovariates6611620304443967041.csv /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/IN-PM01004_rmd.recal.bam.recalTable /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/AnalyzeCovariates.pdf; Stdout: WARNING: ignoring environment value of R_HOME. Stderr: During startup - Warning messages:; 1: Setting LC_CTYPE failed, using ""C"" ; 2: Setting LC_COLLATE failed, using ""C"" ; 3: Setting LC_TIME failed, using ""C"" ; 4: Setting LC_MESSAGES failed, using ""C"" ; 5: Setting LC_MONETARY failed, using ""C"" ; 6: Setting LC_PAPER failed, using ""C"" ; 7: Setting LC_MEASUREMENT failed, using ""C"" ; Error in readRDS(pfile) : ; cannot read workspace version 3 written by R 3.6.0; need R 3.5.0 or newer; Calls: source ... library -> find.package -> lapply -> FUN -> readRDS; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generatePlots(RecalUtils.java:360); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.generatePlots(AnalyzeCovariates.java:329); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.Analy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6393:4045,Error,Error,4045,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6393,1,['Error'],['Error']
Availability,"otypeCaller - Inflater: IntelInflater; 02:07:51.774 INFO HaplotypeCaller - GCS max retries/reopens: 20; 02:07:51.775 INFO HaplotypeCaller - Requester pays: disabled; 02:07:51.775 INFO HaplotypeCaller - Initializing engine; 02:07:52.246 INFO HaplotypeCaller - Done initializing engine; 02:07:52.303 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 02:07:52.312 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 02:07:52.314 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 02:07:52.355 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 02:07:52.355 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 02:07:52.356 INFO IntelPairHmm - Available threads: 104; 02:07:52.356 INFO IntelPairHmm - Requested threads: 4; 02:07:52.356 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 02:07:52.408 INFO ProgressMeter - Starting traversal; 02:07:52.408 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 02:07:53.316 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 02:07:53.598 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.49244E-4; 02:07:53.598 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.007888748000000001; 02:07:53.598 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 02:07:53.598 INFO HaplotypeCaller - Shutting down engine; [28 November 2019 at 2:07:53 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=8220835840; java.lang.Null",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:3478,Avail,Available,3478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['Avail'],['Available']
Availability,"ount the fact that targets on different chromosomes are ""infinitely"" separated. Agilent/Ice targets are very unevenly spaced and the distribution of target spacing has a heavy tail, making the matter worse. The regularizer could still be used with certain modifications. Let us define the ""target coverage noise"" for sample s as:. u_{st} = \sum_{\mu} W_{t \mu} z_{s \mu} + m_t. (1) [implementation] First of all, the target coverage noise must be regularized on each contig separately. It doesn't make sense to stack up all targets and take once giant FFT of u_{st}. This can be fixed in the current implementation with little effort. (2) [formal development + implementation] Within each contig, u_{st} must be mapped from target space to genomic position space e.g. via kernel density estimation. It is crucial to take into account the uncertainty in density estimation in the penalty function. For example, if the pre-image of a genomic position $x$ lies at the middle of a certain target $t$, the estimated value is much more reliable than the case where it lies between two largely separated targets. The penalty must be weighted according to the certainty of estimation. (3) [formal development + implementation] once step 1 and 2 are done, the iterative solver code must be updated accordingly. ---. @mbabadi commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246015485). @samuelklee @davidbenjamin @asmirnov239 Let's have a joint meeting at some point to discuss the problem. It is (probably) not too hard to figure out, and it will make our model really shine!. ---. @mbabadi commented on [Tue Sep 13 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246822768). I have some notes written on this which I can discuss tomorrow in the CNV meeting. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-249735634). Here's a nice demonstration @asmir",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2892:1293,reliab,reliable,1293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2892,1,['reliab'],['reliable']
Availability,"oups in a SAM/BAM/CRAM file with a single new read group; ApplyBQSR Applies the BQSR table to the input SAM/BAM/CRAM; BaseRecalibrator Generates recalibration table for BQSR; BuildBamIndex Generates a BAM index (.bai) file; CalculateReadGroupChecksum Creates a hash code based on the read groups (RG) in the SAM/BAM/CRAM header; CleanSam Cleans the provided SAM/BAM/CRAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads; ClipReads Clip reads in a SAM/BAM/CRAM file; CompareBaseQualities Compares base qualities of two input SAM/BAM/CRAM files; CompareSAMs Compares two input SAM/BAM/CRAM files; CountBases Count bases in a SAM/BAM/CRAM file; CountReads Count reads in a SAM/BAM/CRAM file; DownsampleSam Down-sample a SAM/BAM file to retain a random subset of the reads; EstimateLibraryComplexity Estimates library complexity from the sequence of read pairs; ExampleReadWalkerWithReference Print reads with reference context; ExampleReadWalkerWithVariants Print reads with overlapping variants; FastqToSam Converts a fastq file to an unaligned SAM/BAM file; FilterReads Creates a new SAM/BAM/CRAM file by including or excluding aligned reads; FixMateInformation Ensure that all mate-pair information is in sync between each read and its mate pair; FixMisencodedBaseQualityReads Fix Illumina base quality scores in a SAM/BAM/CRAM file; FlagStat A reimplementation of the 'samtools flagstat' subcommand; GatherBQSRReports Gathers scattered BQSR recalibration reports into a single file; GatherBamFiles Concatenates one or more BAM files together as efficiently as possible; LeftAlignIndels Left-aligns indels from reads in a SAM/BAM/CRAM file; MarkDuplicates Examines aligned records in the supplied SAM/BAM/CRAM file to locate duplicate molecules.; MergeBamAlignment Merges alignment data from a SAM/BAM with data in an unmapped SAM/BAM/CRAM file; MergeSamFiles Merges multiple SAM/BAM files into one file; PrintReads Print reads in the SAM/BAM/CRAM file; Reor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1669:4270,Down,DownsampleSam,4270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669,2,['Down'],"['Down-sample', 'DownsampleSam']"
Availability,"ource('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:2578,Error,Error,2578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,1,['Error'],['Error']
Availability,outputting better error messages when command line parsing fails,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/173:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/173,1,['error'],['error']
Availability,overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [a85f28df-e6b8-4f64-bafb-c0f195dcd4d5] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:14760,ERROR,ERROR,14760,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,2,['ERROR'],['ERROR']
Availability,"p [often](https://gatk.broadinstitute.org/hc/en-us/community/posts/4566282375835-Mutect2-AF-does-not-match-AD-and-DP) in the GATK forum and it feels like having clear documentation around this would be helpful. . My impression is that Mutect2 might be using an AD ""1-read-per-allele"" prior and incorporating that into its reported AF. From the [article on informative reads](https://gatk.broadinstitute.org/hc/en-us/articles/360035532252-Allele-Depth-AD-is-lower-than-expected), once you're at the sample level (FORMAT field), both DP and AD appear to include only informative alleles. It is tempting to think that AF would be computed from them directly (e.g., `AD_alt / DP`, which is equivalent to `AD_alt/[AD_alt+AD_ref]` in the biallelic case since only informative reads are retained). However, as noted in those linked forum posts, Mutect2 (in my case, version 4.2.5.0) does not produce AF values that can be computed from the AD values in that way. Rather, the AF value appears to incorporate a prior. . I investigated this across a range of allele depths in real calls. Here are some examples. The format is:; |AlleleDepthRef,AlleleDepthAlt | DP | AF[provided by Mutect2] | AF[if I calculate it myself]|; | ------- | ------- | ----- | ------- |; | 0,1|1|0.667|1.000 |; | 23,4|27|0.170|0.148 |; | 39,125|164|0.758|0.762 | . The intuition here is that there is a huge discrepancy between the Mutect2 AF and the AF I calculate when AD (or DP) is small (first row), and the error gets smaller as DP increases. The formula that Mutect2 seems to use to compute AF is:. ```py; # Formula that Mutect2 seems to use to calculate AF of the alternative allele in a biallelic scenario; Mutect2_AF = (ADalt+1) / (ADalt+1 + ADref+1). # Which is equivalent to:; Mutect2_AF = (ADalt + 1) / (DP + 2); ```. 1. Is my inference about a prior weight being added by Mutect2 prior to computing AF accurate?; 2. If so, is it intended behavior?; 3. If so, can the VCF header field be a bit more informative about this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8080:2145,error,error,2145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8080,1,['error'],['error']
Availability,"p from intervals; 16:51:51.068 INFO HaplotypeCaller - Done initializing engine; 16:51:51.075 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 16:51:51.293 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 16:51:51.509 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 16:51:51.762 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/vlad/bcbio/anaconda/share/gatk4-4.0b5-0/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 16:51:51.764 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/vlad/bcbio/anaconda/share/gatk4-4.0b5-0/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 16:51:51.795 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 16:51:51.796 INFO IntelPairHmm - Available threads: 32; 16:51:51.796 INFO IntelPairHmm - Requested threads: 4; 16:51:51.796 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 16:51:51.815 INFO ProgressMeter - Starting traversal; 16:51:51.815 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 16:51:51.881 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 16:51:51.881 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 16:51:51.881 INFO HaplotypeCaller - Shutting down engine; [16 November 2017 4:51:51 PM] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1640497152; java.lang.IllegalArgumentException: contig must be non-null and not equal to *, and start must be >= 1; at org.broadinstitute.hellbender.utils.read.SAMRecordToGATKReadAdapter.setPosition(SAMRecordToGATKReadAdapter.java:92); at org.broadinstitute.hellbender.utils.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3845:8813,Avail,Available,8813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3845,1,['Avail'],['Available']
Availability,"p(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:380); 	... 14 more; Caused by: java.io.FileNotFoundException: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy); 	at java.io.RandomAccessFile.open0(Native Method); 	at java.io.RandomAccessFile.open(RandomAccessFile.java:316); 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:243); 	at htsjdk.samtools.seekablestream.SeekableFileStream.<init>(SeekableFileStream.java:47); 	at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:111); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:94); 	... 17 more; ```. Which doesn't happen when running it without the intervals file. I am using the latest version of GATK4 available in conda.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7059:3062,avail,available,3062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059,1,['avail'],['available']
Availability,"pPartitions$1$$anonfun$apply$23.apply(RDD.scala:796); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; And I notice `gatk/src/main/java/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.java` doesn't exist, and there is no class file `gatk/build/classes/main/org/broadinstitute/hellbender/utils/bwa/BwaMemAligner.class` either. Is that causing this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186:4156,error,error,4156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186,1,['error'],['error']
Availability,packaging gatk-launch in our jar so that it's available to downstream projects; some build.gradle refactoring,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1772:46,avail,available,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1772,2,"['avail', 'down']","['available', 'downstream']"
Availability,"park - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.REFERENCE_FASTA : null; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:19:00.371 INFO BaseRecalibratorSpark - Deflater IntelDeflater; 17:19:00.372 INFO BaseRecalibratorSpark - Inflater IntelInflater; 17:19:00.372 INFO BaseRecalibratorSpark - Initializing engine; 17:19:00.372 INFO BaseRecalibratorSpark - Done initializing engine; 17:19:00.872 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java clas; 17:22:09.153 INFO BaseRecalibratorSpark - Shutting down engine; [May 17, 2017 5:22:09 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 3.15 min; Runtime.totalMemory()=15504244736; java.lang.ArrayIndexOutOfBoundsException: 1073741865; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:195); at org.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236)atorg.apache.spark.broadcast.TorrentBroadcast; anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:236); at org.apache.spark.util.Utils$.tryWithSafeFinally(Uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2732:3280,down,down,3280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732,1,['down'],['down']
Availability,"park web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!** ; **20/03/05 09:28:58 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:1342177280+33554432** ; **20/03/05 09:28:58 INFO MemoryStore: MemoryStore cleared** ; **20/03/05 09:28:58 INFO BlockManager: BlockManager stopped** ; **20/03/05 09:28:58 INFO BlockManagerMaster: BlockManagerMaster stopped** ; **20/03/05 09:28:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!** ; **20/03/05 09:28:58 INFO SparkContext: Successfully stopped SparkContext** ; **09:28:58.889 INFO PathSeqPipelineSpark - Shutting down engine** ; **[2020년 3월 5일 (목) 오전 9시 28분 58초] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.25 minutes.** ; **Runtime.totalMemory()=19560660992** ; **org.apache.spark.SparkException: Job aborted due to stage failure: Task 34 in stage 0.0 failed 1 times, most recent failure: Lost task 34.0 in stage 0.0 (TID 34, localhost, executor driver): com.esotericsoftware.kryo.KryoException: Buffer underflow.** ; **at com.esotericsoftware.kryo.io.Input.require(Input.java:199)** ; **at com.esotericsoftware.kryo.io.Input.readLong(Input.java:686)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet.<init>(LongHopscotchSet.java:83)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:527)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:519)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:712)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LargeLongHopscotchSet.<init>(LargeLongHopscotchSet.java:55)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LargeLongHopscotchSet$Serializer.read(LargeLongHopscotchSet.java:172)** ; **at org.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:42461,failure,failure,42461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['failure'],['failure']
Availability,pe ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin; > /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin (No such file or directory). * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; * ; BUILD FAILED in 761ms; ====================================. How can I build GATK4? . Thanks a lot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:2452,FAILURE,FAILURE,2452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['FAILURE'],['FAILURE']
Availability,"peline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291). My GATK version is :GATK4.1.2.0; My command is:; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta \; -I M1.bam \; --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz \; --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz \; --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \; --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; -O M1_recal.table; Then I run the ValidateSamFile to check the BAM file,this is the command : ; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" ValidateSamFile -I M1.bam. And the result is: No errors found; I also tried to use the BAM file before I merge them to run BaseRecalibrator and ValidateSamFile, but I got the same result.Can anybody tell me how solve this problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5968:12139,error,errors,12139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968,1,['error'],['errors']
Availability,"pertiesAreValid(DataSourceUtils.java:841); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.getAndValidateDataSourcesFromPaths(DataSourceUtils.java:216); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.onTraversalStart(Funcotator.java:776); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1047); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: shaded.cloud_nio.com.google.api.client.http.HttpResponseException: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Bad Request""; }; 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1097); 	at shaded.cloud_nio.com.google.auth.oauth2.UserCredentials.refreshAccessToken(UserCredentials.java:197); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at shaded.cloud_nio.com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:159); 	at shaded.cloud_nio.com.google.cloud.http.CensusHttpModule$CensusHttpRequestInitializer.initialize(CensusHttpModule.java:109); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientReques",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:9758,error,error,9758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['error'],['error']
Availability,"pling Settings: No downsampling ; INFO 10:47:56,255 SAMDataSource$SAMReaders - Initializing SAMRecords in serial ; INFO 10:47:56,333 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.07 ; ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A USER ERROR has occurred (version 3.8-0-ge9d806836): ; ##### ERROR; ##### ERROR This means that one or more arguments or inputs in your command are incorrect.; ##### ERROR The error message below tells you what is the problem.; ##### ERROR; ##### ERROR If the problem is an invalid argument, please check the online documentation guide; ##### ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.; ##### ERROR; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.; ##### ERROR; ##### ERROR MESSAGE: Input files reads and reference have incompatible contigs. Please see https://software.broadinstitute.org/gatk/documentation/article?id=63for more information. Error details: No overlapping contigs found.; ##### ERROR reads contigs = [LmjF04_01_20050601_V5.2, LmjF05_01_20050601_V5.2, LmjF24_01_20050601_V5.2, LmjF01_01_20050601_V5.2, LmjF03_01_20050601_V5.2, LmjF13_01_20050601_V5.2, LmjF14_01_20050601_V5.2, LmjF19_01_20050601_V5.2, LmjF21_01_20050601_V5.2, LmjF23_01_20050601_V5.2, LmjF10_01_20050601_V5.2, LmjF11_01_20050601_V5.2, LmjF15_01_20050601_V5.2, LmjF18_01_20050601_V5.2, LmjF02_01_20050601_V5.2, LmjF25_01_20050601_V5.2, LmjF27_01_20050601_V5.2, LmjF28_01_20050601_V5.2, LmjF29_01_20050601_V5.2, LmjF30_01_20050601_V5.2, LmjF31_01_20050601_V5.2, LmjF32_01_20050601_V5.3, LmjF33_01_20050601_V5.2, LmjF34_01_20050601_V5.2, LmjF35_01_20050601_V5.2, LmjF36_01_20050601_V5.2, LmjF07_01_2005",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6798:3366,ERROR,ERROR,3366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6798,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,porter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:7145,ERROR,ERROR,7145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,porter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:7997,ERROR,ERROR,7997,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"porter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:1729,ERROR,ERROR,1729,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"posterior mode. Getting a few hundred MCMC samples is probably more expensive but roughly comparable. These numbers are manageable but get expensive when we relearn the model at every iteration of segment merging. In my opinion it makes sense to come back to this issue after we have a new segmentation strategy. We'll see how pressing it is then. ---. @samuelklee commented on [Wed Jun 08 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-224786950). To clarify, I think this is primarily an issue for WGS, where we have ~1.5 million hets. From the logs in /dsde/working/lichtens/wgs/out_case_chip_wgs/acnv/*out it looks like finding the MLE takes ~10 minutes (which is roughly consistent with your estimate), but 200 MCMC iterations takes ~1 hr. Naive profiling of the AlleleFractionModeller tests suggests that around ~60% CPU is going toward log gammas, so if we can improve on this I think it might be an easy win. But we should perhaps profile more carefully. However, I agree that changing our segmentation is more pressing! Note that oversegmentation (typically 1000+ segments) hurts us by both by increasing the number of MAF parameters and by increasing the number of similar-segment merge iterations required to smooth things (looks like the WGS samples hit the limit of 25 merge iterations = ~25 hrs). Turning off refitting between iterations helps, perhaps at the cost of smoothness of the final result, but you're still looking at 2+ hours for the initial and final fit. Just to note, other possibilities for cutting down the runtime include trimming down the number of hets for WGS, changing similar-segment merging so that we can locally refit only the MAF for the newly created segment, ditching MCMC, etc. @LeeTL1220 can you make the plots for your WGS runs so we can see what these hets look like?. ---. @davidbenjamin commented on [Thu Jun 09 2016](https://github.com/broadinstitute/gatk-protected/issues/542#issuecomment-225074377). Ahhhhh, I get it!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2860:2634,down,down,2634,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860,2,['down'],['down']
Availability,"ppable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5934:2177,error,error,2177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934,1,['error'],['error']
Availability,"pping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droazen @lbergelson @LeeTL1220 @ldgauthier @yfarjoun This is just one example of how using recently developed M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2984:2242,down,down,2242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984,1,['down'],['down']
Availability,"practice?; - The error message when there is no supporting code does not tell what the problem is; whether the extension of the file (due to the the 1-to-1 name to type quasi-restriction above) or a more complex formatting issue in the file (e.g. required header missing, version not supported ... blah blah). ; - All codecs are tried out even when most won't ever apply. Even if the performance impact should in practice be minimal still may cause several file IO open operations as several Codec do actually peek into the file (e.g. BCF and VCF codecs). ; - Codec developers have to make sure their new codec does not collides with others; it would be better if codec development can be totally independent.; - General file extensions such as .tab , .tsv cannot be used by codecs due to possible collisions constraining users to name their files the way GATK needs them to; ""I don't like people telling what file names a have to use... I'm already placing the correct argument name before the file name. What else you need!"". Proposal:. An annotation to tell what codes to try out, the first one that canDecode returns true is used otherwise a configurable error message saying what the problem could be:. <pre>; @Codecs(BEDCodec.class); FeatureInput&lt;BEDFeature&gt; features;; </pre>. <pre>; @Codecs(value = BEDCodec.class, failureMessage = ""The file provided must be a BED formatted file with extension .bed""); FeatureInput&lt;BEDFeature&gt; features;; </pre> . <pre>; @Codecs(BCFCodec.class, VCFCodec.class); FeatureInput&lt;VariantContext&gt; variants;; </pre>. <pre>; // force = true, means that canDecode won't be called and instead we try to read the content directly,; // the codec's code is responsible to throw an appropriate UserException.BadInput indicating formatting issues; this should be the case already anyway.; @Codecs(value = TargetCodec.class, force = true); FeatureInput&lt;Target&gt; target;; </pre>. If the annotation is not present it can default to the current behavior.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184:2208,error,error,2208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184,2,"['error', 'failure']","['error', 'failureMessage']"
Availability,"preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,18\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:8:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,18\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:7:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:33,19\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:1:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, memory. \[2022-10-18 15:38:43,39\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:4:1\]: set -euo pipefail. rm -rf genomicsdb . .... (and so on). singularity exec --containall docker://us.gcr.io/broad-gatk/gatk@sha256:21c3cb43b7d11891ed4b63cc7274f20187f00387cfaa0433b3e7991b5be34dbe \\.   echo ""successfully pulled us.gcr.io/broad-gatk/gatk@sha256:21c3cb43b7d11891ed4b63cc7274f20187f00387cfaa0433b3e7991b5be34dbe!"". singularity exec --containall --bind /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-0:/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-0 docker://us.gcr.io/broad-gatk/gatk@sha256:21c3cb43b7d11891ed4b63cc7274f20187f00387cfaa0433b3e7991b5be34dbe /bin/bash /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-0/execution/script. \[2022-10-18 15:38:47,76\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:5:1\]: job id: 698507. \[2022-10-18 15:38:47,76\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:2:1\]: job id: 698509. \[2022-10-18 15:38:47,76\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:4:1\]: j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:4917,echo,echo,4917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['echo'],['echo']
Availability,prevent log4j error messages in spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2622:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622,1,['error'],['error']
Availability,"previously avx code was sometimes included if installDist had been run prior to running sparkJar, now sparkJar will always contain the native code; fixes #1576. also changing download of inteldeflator.so to only happen if it doesn't exist already",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1681:175,down,download,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1681,1,['down'],['download']
Availability,"processGermlineCNVCalls - Requester pays: disabled ; ; 11:04:20.985 INFO PostprocessGermlineCNVCalls - Initializing engine ; ; 11:04:26.627 INFO PostprocessGermlineCNVCalls - Done initializing engine ; ; 11:04:27.492 INFO ProgressMeter - Starting traversal ; ; 11:04:27.492 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute ; ; 11:04:27.493 INFO ProgressMeter - unmapped 0.0 0 NaN ; ; 11:04:27.493 INFO ProgressMeter - Traversal complete. Processed 0 total records in 0.0 minutes. ; ; 11:04:27.493 INFO PostprocessGermlineCNVCalls - Generating intervals VCF file... ; ; 11:04:27.510 INFO PostprocessGermlineCNVCalls - Writing intervals VCF file to /staging/wes/1\_sample\_20210615/CNV\_calling/genotyped-intervals-case-A210066-vs-v7cohort.vcf.gz... ; ; 11:04:27.510 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 1... ; ; 11:04:30.169 INFO PostprocessGermlineCNVCalls - Generating segments... ; ; 11:04:37.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:4533,down,down,4533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['down'],['down']
Availability,profiling of GenotypeGVCFs showed a lot of wasted time in VariantContext.toString() which can be tracked to computing an error message we never display in `AFCalculator.getLog10PNonRef`; fixing it so we only compute the message when we the error occurs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3478:121,error,error,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3478,2,['error'],['error']
Availability,pysam compile failure,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742:14,failure,failure,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742,1,['failure'],['failure']
Availability,"q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:39413,failure,failure,39413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['failure'],['failure']
Availability,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2485:4750,avail,available,4750,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485,1,['avail'],['available']
Availability,"r - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf15:16:55.375 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xhgnc_v90_38.hg38.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 15:16:57.746 INFO Funcotator - Initializing Funcotator Engine...; > 15:16:57.777 INFO Funcotator - Creating a VCF file for output: file:/home/pkus/mutect_test/filtered_variants/P1.avcf.gz; > 15:16:57.894 INFO ProgressMeter - Starting traversal; > 15:16:57.894 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 15:16:57.979 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; > 15:16:57.981 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; > 15:16:57.991 INFO Funcotator - Shutting down engine; > [July 17, 2020 3:16:57 PM CEST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.31 minutes.; > Runtime.totalMemory()=883949568; > java.lang.IllegalArgumentException: Unexpected value: lncRNA; > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1052); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:158); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:753); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.create(GencodeGtfFeature.java:320); > at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:17161,down,down,17161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['down'],['down']
Availability,"r dangling heads yet it seems to have other problems downstream when selecting or pruning haplotypes:. ```; https://www.pivotaltracker.com/story/show/67601310; ```. B. Low support chain pruning might not be longer needed. Now we have a newer approach to select best haplotypes that can handle complex graph we might well not need to prune low supported hap early as they seemly they won't be selected if the are not amongst the best haplotypes. . B.1 Now that still would produce a considerable number of unlikely haplotypes that would cause a CPU burden. That can be changed by imposing another kinds of limit, For example we include all haplotypes with scores (likelihoods) that are Q0 - Q40 or we include haplotypes until the sum of their likelihoods is larger than the 99.99% probability mass. . B.2 This could provide a downstream solution to the problem caused by ranging heads recovery (explained above in A.2). B.3 If pruning is to be maintained, it makes more sense to do it at the very end after all dangling ends hav been recovered and the edges supports are finalized. Of course I assuming here that dangling end recovery does the sensible think of updating those supports are the graphs is modified. C. The use of Smith-Waterman in dangling end recovery does not seem totally optimal or even needed. . C.1 Recovering tails quite often this finish with the same sequence as the reference path because in fact they are supposed to end like that by construction (reads are trimmed by AR coordinates). For example, this can be cause because due to the k-mer size there is not enough based after variation for the paths to merge back. In this case you can simply merge the last vertices of the tail and the reference, faster and potentially more accurate. . C.2 Similarly dangling heads, at least part of the sequence of those dangling heads are clearly threadable back into the graph without the need of SW. For example look at the AA…AAAAAGA sequence in the picture below. . C.3 PairHMM runs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/264:1810,recover,recovered,1810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/264,1,['recover'],['recovered']
Availability,"r directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.27:46181 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.27:46181 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:39 WARN TaskSetManager: Lost task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:32072,Error,Error,32072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,r id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 ERROR cluster.YarnScheduler: Lost executor 1 on com2: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:18882,ERROR,ERROR,18882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['ERROR'],['ERROR']
Availability,"r issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); FilterMutectCalls. ### Affected version(s); - gatk-4.1.9.0. ### Description ; When I was running FilterMutectCalls with one of my samples, I got an error as ""Duplicate key"". 14:50:59.201 INFO FilterMutectCalls - ------------------------------------------------------------; 14:50:59.202 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:50:59.202 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:50:59.203 INFO FilterMutectCalls - Executing as mparment@her2-w110 on Linux v5.7.7-1.el7.elrepo.x86_64 amd64; 14:50:59.203 INFO FilterMutectCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 14:50:59.203 INFO FilterMutectCalls - Start Date/Time: December 12, 2020 2:50:57 PM CET; 14:50:59.203 INFO FilterMutectCalls - ------------------------------------------------------------; 14:50:59.203 INFO FilterMutectCalls - ------------------------------------------------------------; 14:50:59.204 INFO FilterMutectCalls - HTSJDK Version: 2.23.0; 14:50:59.204 INFO FilterMutectCalls - Picard Version: 2.23.3; 14:50:59.204 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6996:1366,error,error,1366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6996,1,['error'],['error']
Availability,"r the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remotes"", repos=""https://cloud.r-project.org/""); > library(remotes); > install_version(""scales"", version=""1.2.1"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.2.1’; > quit(); $ gatk VariantRecalibrator [arguments omitted for brevity]; $; ```. #### Expected be",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8664:1250,Error,Error,1250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664,1,['Error'],['Error']
Availability,"r thread; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors; 18/01/09 18:31:26 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 18/01/09 18:31:26 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Stopped; 18/01/09 18:31:26 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/01/09 18:31:26 INFO memory.MemoryStore: MemoryStore cleared; 18/01/09 18:31:26 INFO storage.BlockManager: BlockManager stopped; 18/01/09 18:31:26 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/01/09 18:31:26 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/01/09 18:31:26 INFO spark.SparkContext: Successfully stopped SparkContext; 18:31:26.896 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [January 9, 2018 6:31:26 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 0.89 minutes.; Runtime.totalMemory()=881328128; ***********************************************************************. A USER ERROR has occurred: Input files reference and reads have incompatible contigs: No overlapping contigs found.; reference contigs = [chrM, chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chr1_gl000191_random, chr1_gl000192_random, chr4_ctg9_hap1, chr4_gl000193_random, chr4_gl000194_random, chr6_apd_hap1, chr6_cox_hap2, chr6_dbb_hap3, chr6_mann_hap4, chr6_mcf_hap5, chr6_qbl_hap6, chr6_ssto_hap7, chr7_gl000195_random, chr8_gl000196_random, chr8_gl000197_random, chr9_gl000198_random, chr9_gl000199_random, chr9_gl000200_random, chr9_gl000201_random, chr11_gl000202_random, chr17_ctg5_hap1, chr17_gl000203_rand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:31077,down,down,31077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['down'],['down']
Availability,"r-mixed-ploidy-samples/p1); ----------; I'm attempting to call variants on whole genomes for about 500 illumina paired-end samples with varying ploidy (haploid to tetraploid). I'm running a fairly standard uBam to GVCF pipeline with HaplotypeCaller passed the ploidy information (1,2,3, or 4) in -ERC GVCF mode. I then try to collect the GVCFs using GenomicsDBImport in a batch size of 50 and use GenotypeGVCFs on the combined database. My interval list that is passed to GenomicsDBImport is just each chromosome on a separate line. I'm using GATK v4.1.1.0<br />; <br />; Command:<br />; ```<br />; ${GATK_DIR}/gatk GenomicsDBImport \<br />; --java-options ""-Xmx110g -Xms110g"" \<br />; -R ${REF} \<br />; --variant ${FILE_LIST} \<br />; -L ${SCRIPT_DIR}/GATK_Style_Interval.list \<br />; --genomicsdb-workspace-path ${WORK_DIR}/GenomicsDB_20190912 \<br />; --batch-size 50 \<br />; --tmp-dir=${WORK_DIR}/<br />; ```<br />; <br />; GenomicsDBImport appears to run without error, but only shows progress for the first 6000 bp before moving onto the next batch. When I run select variants on the created database, I only get variants up to position 6716 in the first interval. When I try to run GenotypeGVCF on it, I get a strange error:<br />; htsjdk.tribble.TribbleException: Invalid block size -1570639203<br />; <br />; My first assumption is that one of the gvcf's is malformed from HaplotypeCaller failing after the first 6000 bp, but I've verified that the gvcfs have all completed and have 'validated' them with ValidateVariants using GATK v4.1.3.0. When I grep for the particular position in the sample's gvcfs I don't find anything out of the ordinary. I would use CombineGVCFs, but it fails due to trying to combine mixed ploidies. <br />; <br />; Any ideas on troubleshooting or experience with problems like this?. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24446/genomicsdbimport-not-completing-for-mixed-ploidy-samples/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275:4241,error,error,4241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275,1,['error'],['error']
Availability,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:30689,down,down,30689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,3,['down'],['down']
Availability,"r.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409732fb{/executors/threadDump/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e99e2cb{/static,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@478967eb{/,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f2b39a{/api,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18c880ea{/jobs/job/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6afbe6a1{/stages/stage/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:56 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.4:4040; 18/01/09 18:30:56 INFO spark.SparkContext: Added JAR file:/opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar at spark://192.168.1.4:38793/jars/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar with timestamp 1515493856032; 18/01/09 18:30:56 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 18/01/09 18:30:57 INFO client.RMProxy: Connecting to ResourceManager at tele-1/192.168.1.4:8032; 18/01/09 18:30:57 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers; 18/01/09 18:30:58 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (18432 MB per container); 18/01/09 18:30:58 INFO yarn.Clien",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:10191,AVAIL,AVAILABLE,10191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,r.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:8245,ERROR,ERROR,8245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,r.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:3797,ERROR,ERROR,3797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['ERROR'],['ERROR']
Availability,r.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); Caused by: org.gradle.api.internal.tasks.compile.CompilationFailedException: Compilation failed; see the compiler error output for details.; at org.gradle.api.internal.tasks.compile.JdkJavaCompiler.execute(JdkJavaCompiler.java:48); at org.gradle.api.internal.tasks.compile.JdkJavaCompiler.execute(JdkJavaCompiler.java:33); at org.gradle.api.internal.tasks.compile.NormalizingJavaCompiler.delegateAndHandleErrors(NormalizingJavaCompiler.java:104); at org.gradle.api.internal.tasks.compile.NormalizingJavaCompiler.execute(NormalizingJavaCompiler.java:53); at org.gradle.api.internal.tasks.compile.NormalizingJavaCompiler.execute(NormalizingJavaCompiler.java:38); at org.gradle.api.internal.tasks.compile.CleaningJavaCompilerSupport.execute(CleaningJavaCompilerSupport.java:35); at org.gradle.api.internal.tasks.compile.CleaningJavaCompilerSupport.execute(CleaningJavaCompilerSupport.java:25); at org.gradle.api.tasks.compile.JavaCompile.performCompilation(JavaCompile.java:189); at org.gradle.api.tasks.compile.JavaCompile.compile(JavaCompile.java:170); at org.gradle.api.tasks.compile.JavaCompil,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:12634,error,error,12634,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,1,['error'],['error']
Availability,"r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); ```. #### Steps to reproduce; These are the arguments I used (the input bam is on the file system):. ```; final String[] args = {; ""-I"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/Filtering/IGV/198489_vs_811158/sorted.mt.1.bam"",; ""-"" + M2ArgumentCollection.TUMOR_SAMPLE_SHORT_NAME, ""198489"",; ""-R"", ""/humgen/gsa-hpprojects/dev/mshand/SpecOps/Mitochondria/MitochondriaOnlyFastas/Homo_sapiens_assembly38.mt_only.fasta"",; ""-O"", outputVcf.getAbsolutePath(),; ""--max-reads-per-alignment-start"", ""0"",; ""-default-af"", ""0"",; ""--initial-tumor-lod"", ""0"",; ""--tumor-lod-to-emit"", ""0"",; ""--min-pruning"", ""10"",; ""--annotation"", ""StrandBiasBySample"",; //""--ignore-itr-artifacts"", ""true"",; };; ```. I tried it with and without `--ignore-itr-artifacts` but got the same error both times. @davidbenjamin Any idea if this is an easy fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036:5499,error,error,5499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036,1,['error'],['error']
Availability,"r.utils.clipping.ClippingOp.apply(ClippingOp.java:73); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:145); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:126); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:330); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipSoftClippedBases(ReadClipper.java:333); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:82); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:242); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:496); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:221); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:254); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:231); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); at org.broadinstitute.hellbender.Main.main(Main.java:239); ```. Attempts with different regions (e.g. neighbouring positions 86 or 88 at this chromosome, or other sets of reads, or taking one of the culprit reads alone) didn't give me the error. Can you suggest what I'm doing wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3845:11865,error,error,11865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3845,1,['error'],['error']
Availability,"r/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89"": [0] remote missing object 4ed7feb0343e9ac03135b1456b2c8d2edab1b359c4950908c4d44152c0634a89; error transferring ""eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e"": [0] remote missing object eda2517817b23238c2b28f69a1fa39e9b85b45985854f0a5d5508280e76da39e; error transferring ""5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b"": [0] remote missing object 5e69a86f301ab9ab0d507ad7659abb4ad3732382ccbeb714db497e51eb3cf87b; Failed to fetch some objects from 'file:///startdir/gatk'. ```. #### Expected behavior; can be compiled. #### Actual behavior; see above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8320:3565,error,error,3565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320,7,['error'],['error']
Availability,"r: Created local directory at /tmp/sun/blockmgr-b03058dc-763a-449c-bd05-18f3304c01ea; 18/01/09 18:30:55 INFO memory.MemoryStore: MemoryStore started with capacity 2004.6 MB; 18/01/09 18:30:55 INFO spark.SparkEnv: Registering OutputCommitCoordinator; 18/01/09 18:30:55 INFO util.log: Logging initialized @25356ms; 18/01/09 18:30:55 INFO server.Server: jetty-9.3.z-SNAPSHOT; 18/01/09 18:30:55 INFO server.Server: Started @25495ms; 18/01/09 18:30:55 INFO server.AbstractConnector: Started ServerConnector@283ab206{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:30:55 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@418f0534{/jobs,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@134a8ead{/jobs/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54247647{/jobs/job,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5463f035{/jobs/job/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:7424,AVAIL,AVAILABLE,7424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"r; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.seri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:4532,down,down,4532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['down'],['down']
Availability,"r; 09:55:48.867 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 09:55:48.867 INFO GenotypeGVCFs - Requester pays: disabled; 09:55:48.867 INFO GenotypeGVCFs - Initializing engine; 09:55:49.015 INFO FeatureManager - Using codec VCFCodec to read file file:///share/org/YZWL/yzwl_hanxt/leizhou/variant/H-4/H-4.g.vcf.gz; 09:55:49.190 INFO GenotypeGVCFs - Done initializing engine; 09:55:49.215 INFO ProgressMeter - Starting traversal; 09:55:49.216 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:55:49.310 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr1_1-157403528:5512 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 09:55:49.336 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1_1-157403528:5512 and possibly subsequent; at least 10 samples must have called genotypes; 09:55:50.064 INFO GenotypeGVCFs - Shutting down engine; [September 3, 2024 at 9:55:50 AM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1241513984; java.lang.RuntimeException: Invalid deflate block found.; at com.intel.gkl.compression.IntelInflater.inflateNative(Native Method); at com.intel.gkl.compression.IntelInflater.inflate(IntelInflater.java:176); at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:145); at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:561); at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:543); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:479); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:469); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8969:3287,down,down,3287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8969,1,['down'],['down']
Availability,"rFuncotations - Done initializing engine ; ; 02:00:35.260 INFO  ProgressMeter - Starting traversal ; ; 02:00:35.261 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute ; ; 02:00:35.262 INFO  FilterFuncotations - Starting pass 0 through the variants ; ; 02:00:35.778 ERROR FuncotationMap - Keys:  Gencode\_34\_hugoSymbol, Gencode\_34\_ncbiBuild, Gencode\_34\_chromosome, Gencode\_34\_start, Gencode\_34\_end, Gencode\_34\_variantClassification, Gencode\_34\_secondaryVariantClassification, Gencode\_34\_variantType, Gencode\_34\_refAllele, Gencode\_34\_tumorSeqAllele1, Gencode\_34\_tumorSeqAllele2, Gencode\_34\_genomeChange, Gencode\_34\_annotationTranscript, Gencode\_34\_transcriptStrand, Gencode\_34\_transcriptExon, Gencode\_34\_transcriptPos, Gencode\_34\_cDnaChange, Gencode\_34\_codonChange, Gencode\_34\_proteinChange, Gencode\_34\_gcContent, Gencode\_34\_referenceContext, Gencode\_34\_otherTranscripts, ACMGLMMLof\_LOF\_Mechanism, ACMGLMMLof\_Mode\_of\_Inheritance, ACMGLMMLof\_Notes, ACMG\_recommendation\_Disease\_Name, ClinVar\_VCF\_AF\_ESP, ClinVar\_VCF\_AF\_EXAC, ClinVar\_VCF\_AF\_TGP, ClinVar\_VCF\_ALLELEID, ClinVar\_VCF\_CLNDISDB, ClinVar\_VCF\_CLNDISDBINCL, ClinVar\_VCF\_CLNDN, ClinVar\_VCF\_CLNDNINCL, ClinVar\_VCF\_CLNHGVS, ClinVar\_VCF\_CLNREVSTAT, ClinVar\_VCF\_CLNSIG, ClinVar\_VCF\_CLNSIGCONF, ClinVar\_VCF\_CLNSIGINCL, ClinVar\_VCF\_CLNVC, ClinVar\_VCF\_CLNVCSO, ClinVar\_VCF\_CLNVI, ClinVar\_VCF\_DBVARID, ClinVar\_VCF\_GENEINFO, ClinVar\_VCF\_MC, ClinVar\_VCF\_ORIGIN, ClinVar\_VCF\_RS, ClinVar\_VCF\_SSR, ClinVar\_VCF\_ID, ClinVar\_VCF\_FILTER, LMMKnown\_LMM\_FLAGGED, LMMKnown\_ID, LMMKnown\_FILTER ; ; 02:00:35.778 ERROR FuncotationMap - Values:  , , , , , , , , , , , , , , , , , , , , , , , , , , , , false, ,  ; ; 02:00:35.793 INFO  FilterFuncotations - Shutting down engine ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7865:5142,ERROR,ERROR,5142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865,1,['ERROR'],['ERROR']
Availability,rM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:10983,Recover,Recovered,10983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"rMutectCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 11:03:49.970 INFO FilterMutectCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 11:03:49.970 INFO FilterMutectCalls - Deflater: IntelDeflater ; ; 11:03:49.971 INFO FilterMutectCalls - Inflater: IntelInflater ; ; 11:03:49.971 INFO FilterMutectCalls - GCS max retries/reopens: 20 ; ; 11:03:49.971 INFO FilterMutectCalls - Requester pays: disabled ; ; 11:03:49.971 INFO FilterMutectCalls - Initializing engine ; ; 11:03:50.504 INFO FeatureManager - Using codec VCFCodec to read file file:///home/lqh/somatic\_mutation/Mutect2/test.vcf.gz ; ; 11:03:50.696 INFO FilterMutectCalls - Done initializing engine ; ; 11:03:50.840 INFO ProgressMeter - Starting traversal ; ; 11:03:50.840 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute ; ; 11:03:50.841 INFO FilterMutectCalls - Starting pass 0 through the variants ; ; 11:03:51.014 INFO FilterMutectCalls - Shutting down engine ; ; \[June 4, 2021 11:03:51 AM CST\] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.19 minutes. ; ; Runtime.totalMemory()=625999872 ; ; java.lang.NumberFormatException: **For input string: ""167|35|14""** ; ; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ; ; at java.lang.Integer.parseInt(Integer.java:580) ; ; at java.lang.Integer.valueOf(Integer.java:766) ; ; at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:288) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.Collections$2.tryAdvance(Collections.java:4717) ; ; at java.util.Collections$2.forEachRemaining(Collections.java:4725) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7298:5592,down,down,5592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298,1,['down'],['down']
Availability,"rUn_KI270374v1, chrUn_KI270372v1, chrUn_KI270373v1, chrUn_KI270375v1, chrUn_KI270371v1, chrUn_KI270448v1, chrUn_KI270521v1, chrUn_GL000195v1, chrUn_GL000219v1, chrUn_GL000220v1, chrUn_GL000224v1, chrUn_KI270741v1, chrUn_GL000226v1, chrUn_GL000213v1, chrUn_KI270743v1, chrUn_KI270744v1, chrUn_KI270745v1, chrUn_KI270746v1, chrUn_KI270747v1, chrUn_KI270748v1, chrUn_KI270749v1, chrUn_KI270750v1, chrUn_KI270751v1, chrUn_KI270752v1, chrUn_KI270753v1, chrUn_KI270754v1, chrUn_KI270755v1, chrUn_KI270756v1, chrUn_KI270757v1, chrUn_GL000214v1, chrUn_KI270742v1, chrUn_GL000216v2, chrUn_GL000218v1, chrEBV]; features contigs = [X, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]"". The VCF that I have uses just numbers for chromosomes, whereas the reference genome uses chr1, chr2, etc. Both naming conventions are valid. This is a 4.3 VCF. I have read https://gatk.broadinstitute.org/hc/en-us/articles/360035891131-Errors-about-input-files-having-missing-or-incompatible-contigs and this seems to be the same issue, but I believe there should be a translation that happens, e.g. 1 -> chr1 or the reverse as well. #### Steps to reproduce; Ran the following command using a VCF and reference file that I have:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar LeftAlignAndTrimVariants --max-indel-length 500 --max-leading-bases 2000 --dont-trim-alleles false --verbosity DEBUG --variant <input vcf file> --output /data/<vcf_output> --reference /data/<reference file> --split-multi-allelics true. #### Expected behavior; I would expect GATK to be able to translate 1 -> chr1, 2 -> chr2, etc. since both naming conventions are valid according to the VCF spec http://samtools.github.io/hts-specs/VCFv4.3.pdf. When running the same exact command on a VCF file that uses chr1, chr2, etc. as the naming convention the command runs su",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7538:4202,Error,Errors-about-input-files-having-missing-or-incompatible-contigs,4202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538,1,['Error'],['Errors-about-input-files-having-missing-or-incompatible-contigs']
Availability,r] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:6092,ERROR,ERROR,6092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,r] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:11981,ERROR,ERROR,11981,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,r] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:10723,ERROR,ERROR,10723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,radle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.pr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:12530,ERROR,ERROR,12530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,radle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:11272,ERROR,ERROR,11272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,radle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:12715,ERROR,ERROR,12715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,radle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:11457,ERROR,ERROR,11457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"rageImpl.get(StorageImpl.java:238); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }; at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:451); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1089); at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7489:2008,error,errors,2008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489,1,['error'],['errors']
Availability,"rated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 INFO ProgressMeter - chr1:32128426 0.2 40 202.1 ; 18:15:48.416 INFO ProgressMeter - chr1:36398656 0.4 80 194.0 ; 18:15:51.025 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.012874514 ; 18:15:51.026 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 5.818477527000001; 18:15:51.026 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.35 sec ; 18:15:51.027 INFO HaplotypeCaller - Shutting down engine ; [November 24, 2022 6:15:51 PM CET] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.53 minutes.; Runtime.totalMemory()=3212836864 ; java.lang.ArrayIndexOutOfBoundsException ; at java.util.Arrays.copyOfRange(Arrays.java:3521) ; at org.broadinstitute.hellbender.tools.walkers.annotator.TandemRepeat.getNumTandemRepeatUnits(TandemRepeat.java:54) ; at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer.trim(AssemblyRegionTrimmer.java:189); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:655); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:271); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200) ; at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173) ; at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8106:6090,down,down,6090,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106,1,['down'],['down']
Availability,"rather than this:. ```; A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Dictionary reference is missing contigs found in dictionary reads.; reference contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]; ```. it would be more friendly to say `contig NC_007605 is missing from the reference`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1289:31,ERROR,ERROR,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1289,1,['ERROR'],['ERROR']
Availability,"rc/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-control-full.pcov)(org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormalsIntegrationTest) produced standard out/err: 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB. 18:03:07.612 WARN TaskSetManager:70 - Stage 182 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010a5a9401, pid=2425, tid=8963; #; # JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x1a9401]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/louisb/Workspace/gatk-protected/hs_err_pid2425.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```. [hs_err_pid2425.log.txt](https://github.com/broadinstitute/gatk-protected/files/448383/hs_err_pid2425.log.txt). @yfarjoun Is this similar to the crash you saw a while back?. ---. @yfarjoun commented on [Wed Aug 31 2016](https://github.com/broadinstitute/gatk-protected/issues/659#issuecomment-243946864). no. this is different. On Wed, Aug 31, 2016 at 3:27 PM, Louis Bergelson notifications@github.com; wrote:. > I got a segfault while running CreatePanelOfNormalsIntegrationTest.; > Subsequent runs were unable to reproduce it.; > ; > 18:03:07.573 WARN TaskSetManager:70 - Stage 181 contains a task of very large size (119 KB). The maximum recommended task size is 100 KB.; > Test: Test method testAllTargetsHDF5PoNCreationSpark[0](null, src/test/resources/org/broadinstitute/h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2883:1414,error,error,1414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2883,1,['error'],['error']
Availability,"rceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_tissue.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/cosmic_tissue/hg38/cosmic_tissue.tsv; > 15:16:43.926 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v34.annotation.REORDERED.gtf -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; > 15:16:43.926 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; > 15:16:43.937 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:43.938 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:43.939 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; > 15:16:43.946 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:44.093 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v34.pc_transcripts.fa -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode/hg38/gencode.v34.pc_transcripts.fa; > 15:16:54.854 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_fusion.tsv -> fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:14064,error,errors,14064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['error'],['errors']
Availability,"rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:11382,down,down,11382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['down'],['down']
Availability,"rdinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:13150,ERROR,ERROR,13150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"re: . ```; java.lang.AssertionError: Failed Matching VCF and MAF fields:; 	VCF (Gencode_43_variantClassification): 	RNA[0]	RNA[1]	RNA[2]	RNA[3]	RNA[4]	RNA[5]	RNA[6]	RNA[7]	RNA[8]	RNA[9]	RNA[10]; 	MAF (Variant_Classification): 	LINCRNA[0]	LINCRNA[1]	LINCRNA[2]	LINCRNA[3]	LINCRNA[4]	LINCRNA[5]	LINCRNA[6]	LINCRNA[7]	LINCRNA[8]	LINCRNA[9]	LINCRNA[10]; ----; 	VCF (Gencode_43_otherTranscripts): 	[0]	[1]	[2]	[3]	[4]	[5]	[6]	[7]	[8]	[9]	[10]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[11]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[12]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[13]	PIK3CA_ENST00000643187.1_INTRON/PIK3CA-DT_ENST00000435560.1_FIVE_PRIME_FLANK[14]	[48]	[49]	[50]	[51]	[52]	[53]	[54]	[55]	[56]	[57]	[58]	[59]	[60]	[61]	[62]	[63]	[64]	[65]	[66]	[67]	[68]	[69]	[70]	[71]	[72]	[73]	[74]	[75]	[76]	[77]	[78]	[79]	[80]	[81]	[82]	[83]	[84]	[85]	[86]	[87]	[88]	[89]	[90]	[91]	[92]	[93]	[94]	[95]	[96]	[97]	[98]	[99]	[100]	[101]	[102]	[103]; 	MAF (Other_Transcripts): 	[0]	[1]	[2]	[3]	[4]	[5]	[6]	[7]	[8]	[9]	[10]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[11]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[12]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[13]	PIK3CA_ENST00000643187.1_INTRON|PIK3CA-DT_ENST00000435560.1_FIVE_PRIME_FLANK[14]	[48]	[49]	[50]	[51]	[52]	[53]	[54]	[55]	[56]	[57]	[58]	[59]	[60]	[61]	[62]	[63]	[64]	[65]	[66]	[67]	[68]	[69]	[70]	[71]	[72]	[73]	[74]	[75]	[76]	[77]	[78]	[79]	[80]	[81]	[82]	[83]	[84]	[85]	[86]	[87]	[88]	[89]	[90]	[91]	[92]	[93]	[94]	[95]	[96]	[97]	[98]	[99]	[100]	[101]	[102]	[103]; ----; ```. Its unclear what is the most correct output rendering between the LINCRNA vs RNA for this specific transcript, its worth investigating and adding more robust gencodev43 tests to funcotator in case this is a real issue and not just a mismatch in the testing framework.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9013:2108,robust,robust,2108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9013,1,['robust'],['robust']
Availability,"re; Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar CombineGVCFs -R ./test/test.fna -V ./gvcf_all.list -L NC_038255.2 -O ./NC_038255.2.merged.g.vcf.gz; total 2.3G; '""-rw-rw-rw- 1 root root  3.6K Dec 13 23:32 GATKConfig.EXAMPLE.properties""; drwxr-xr-x 2 root root  4.0K Mar 13 06:26 GCF_000004515.6_Glycine_max_v4.0; '""-rw-r--r-- 1 root root  1.6G Mar 13 06:47 NC_038255.2.merged.g.vcf.gz""; '""-rw-r--r-- 1 root root   24K Mar 13 06:47 NC_038255.2.merged.g.vcf.gz.tbi""; '""-rw-rw-rw- 1 root root   40K Dec 13 23:32 README.md""; '""-rwxrwxrwx 1 root root   21K Dec 13 23:32 gatk""; '""-rw-rw-rw- 1 root root 1016K Dec 13 23:32 gatk-completion.sh""; '""-rw-rw-rw- 1 root root  422M Dec 13 23:32 gatk-package-4.5.0.0-local.jar""; '""-rw-rw-rw- 1 root root  320M Dec 13 23:32 gatk-package-4.5.0.0-spark.jar""; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk-spark.jar -> /gatk/gatk-package-4.5.0.0-spark.jar; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk.jar -> /gatk/gatk-package-4.5.0.0-local.jar; '""-rw-rw-rw- 1 root root  117K Dec 13 23:32 gatkPythonPackageArchive.zip""; '""-rw-rw-rw- 1 root root  4.2K Dec 13 23:32 gatkcondaenv.yml""; '""-rw-r--r-- 1 root root    53 Dec 13 23:37 gatkenv.rc""; '""-rw-r--r-- 1 root root  2.3K Mar 13 06:26 gvcf_all.list""; '""-rw-r--r-- 1 root root   866 Dec 13 23:33 run_unit_tests.sh""; drwxrwxrwx 5 root root  4.0K Dec 13 23:32 scripts; '""-rw-r--r-- 1 root root  1.3K Mar 13 06:26 wgs_jcalling_combine_gvcf_job.sh""; Filesystem      Size  Used Avail Use% Mounted on; overlay         100G   13G   88G  13% /; tmpfs            64M     0   64M   0% /dev; tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup; /dev/nvme0n1p1  100G   13G   88G  13% /etc/hosts; shm              64M     0   64M   0% /dev/shm; wgs-pipeline    1.0P     0  1.0P   0% /mnt. Thank you very much",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735:28247,Avail,Avail,28247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735,1,['Avail'],['Avail']
Availability,read downsampling (+ commandline options),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/64:5,down,downsampling,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/64,1,['down'],['downsampling']
Availability,readingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing as,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:17251,Recover,Recovered,17251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 21:02:11.780 info NativeGenomicsDB - pid=19608 tid=19609 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 21:02:17.065 INFO GenotypeGVCFs - Done initializing engine; 21:02:17.110 INFO ProgressMeter - Starting traversal; 21:02:17.111 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:02:45.390 INFO ProgressMeter - Chr1:369351 0.5 1000 2121.8; 21:03:16.132 INFO ProgressMeter - Chr1:505230 1.0 2000 2033.2; 21:03:29.421 INFO ProgressMeter - Chr1:575285 1.2 3000 2489.3; ... (continued for more than 1000 lines); 21:49:51.317 INFO ProgressMeter - Chr1:3713346 47.6 242000 5087.2; 21:50:06.596 INFO ProgressMeter - Chr1:3718941 47.8 244000 5102.0; [TileDB::ReadState] Error: Cannot read tile from file; Memory map error.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ReadState] Error: Cannot read tile from file; Memory map error; Using GATK jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /mnt/d/Share/SYJ/liulan/ref/genome.fa -V gendb:///mnt/d/Share/SYJ/liulan/DBI -O /mnt/d/Share/SYJ/liulan/sortbam/combDBI.vcf.gz --tmp-dir /mnt/d/Share/SYJ/liulan/NOHUP/tmp; ```. I have tried to change -Xmx to 20G and 100G, etc. But all processes stop running at Variants Processed 244000.; Could you tell me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8302:4348,Error,Error,4348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302,2,"['Error', 'error']","['Error', 'error']"
Availability,reduce non-deterministic WDL test failures,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4129:34,failure,failures,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4129,1,['failure'],['failures']
Availability,"reedingCoeff - InbreedingCoeff will not be calculated at position 1A:219798 and possibly subsequent; at least 10 samples must have called genotypes ; ; 14:28:47.373 INFO ProgressMeter - 1A:568402 0.4 1000 2590.3 ; ; 14:28:57.413 INFO ProgressMeter - 1A:44165059 0.6 255000 460815.6 ; ; 14:29:07.419 INFO ProgressMeter - 1A:78552884 0.7 435000 604040.8 ; ; 14:29:25.201 INFO ProgressMeter - 1A:137636565 1.0 670000 659113.6 ; ; 14:29:35.211 INFO ProgressMeter - 1A:278089494 1.2 994000 839988.2 ; ; 14:29:45.226 INFO ProgressMeter - 1A:317697103 1.4 1162000 860570.8 ; ; 14:30:01.906 INFO ProgressMeter - 1A:363225043 1.6 1347000 827260.1 ; ; 14:30:12.084 INFO ProgressMeter - 1A:441459399 1.8 1676000 932198.7 ; ; 14:30:22.093 INFO ProgressMeter - 1A:466677934 2.0 1835000 933976.9 ; ; 14:30:38.874 INFO ProgressMeter - 1A:495722203 2.2 1996000 889324.5 ; ; 14:30:48.882 INFO ProgressMeter - 1A:536558193 2.4 2320000 962176.5 ; ; 14:30:49.143 INFO GenotypeGVCFs - Shutting down engine ; ; GENOMICSDB\_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),84.09050524498751,Cpu time(s),59.479603645012425 ; ; \[July 7, 2021 2:30:49 PM IDT\] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 2.45 minutes. ; ; Runtime.totalMemory()=12867076096 ; ; java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:142) ; ; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106) ; ; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129) ; ; at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177) ; ; at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:233) ; ; at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.closeTool(GenotypeGVCFs.java:295) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1064) ; ; at org.broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7348:6115,down,down,6115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348,1,['down'],['down']
Availability,reliability,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7310:0,reliab,reliability,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7310,1,['reliab'],['reliability']
Availability,removing deprecated genomes in the cloud docker image that was causing test failures,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8891:76,failure,failures,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8891,1,['failure'],['failures']
Availability,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2770:9,redundant,redundant,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770,1,['redundant'],['redundant']
Availability,removing the non-docker unit and integration test matrix entries because; they were redundant with the docker ones,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804:84,redundant,redundant,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804,1,['redundant'],['redundant']
Availability,removing the redundant command line parse exception,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/197:13,redundant,redundant,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/197,1,['redundant'],['redundant']
Availability,removing two redundant travis matrix entries to save vms,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804:13,redundant,redundant,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804,1,['redundant'],['redundant']
Availability,removing uncessary gradle download,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/547:26,down,download,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/547,1,['down'],['download']
Availability,"rencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:994); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). I don't really know how to fix it. ValidateVariants gives no errors, and I am able to perform variant selection, e.g.:. gatk-4.0.5.1/gatk SelectVariants -R data/genome.fasta -V variants/6753_12-15-2015_first_pass_raw.vcf -select 'vc.getGenotype(""6753_12-15-2015"").getAD().1/vc.getGenotype(""6753_12-15-2015"").getDP() > 0.9 ' -output variants/6753_12-15-2015_first_pass_filtered.vcf. with no problems. Insights would be gratefully appreciated.; Thanks!; Gavin. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12223/java-lang-numberformatexception-when-trying-to-perform-variantfiltration/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4921:7469,error,errors,7469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921,1,['error'],['errors']
Availability,"rently attempting to use on a computing cluster without spark enabled. . Command line used:; gatk/gatk-4.1.0.0/gatk StructuralVariationDiscoveryPipelineSpark \; -I $CRAM \; -R $Hg38 \; --aligner-index-image reference.fasta.Hg38.img \; --kmers-to-ignore kmers_to_ignore_hg38.txt \; --contig-sam-file aligned_contigs.sam \; -O ${base}_GATK_SV_output.vcf . **Error Log**:; 19/02/01 21:28:27 INFO TaskSetManager: Starting task 700.0 in stage 5.0 (TID 4405, localhost, executor driver, partition 700, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:27 INFO Executor: Running task 700.0 in stage 5.0 (TID 4405); 19/02/01 21:28:27 INFO TaskSetManager: Finished task 668.0 in stage 5.0 (TID 4373) in 37331 ms on localhost (executor driver) (669/741); 19/02/01 21:28:27 INFO BlockManagerInfo: Removed taskresult_4373 on 10.120.16.54:34926 in memory (size: 1645.1 KB, free: 15.8 GB); 19/02/01 21:28:27 INFO NewHadoopRDD: Input split: file: /cram8/1-00004__CG0000-1789.GMKF2.cram:23488102400+33554432; 19/02/01 21:28:28 ERROR Executor: Exception in task 698.0 in stage 5.0 (TID 4403); **java.lang.IllegalArgumentException: provided start is negative: -24**; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterato",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5647:1142,ERROR,ERROR,1142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647,1,['ERROR'],['ERROR']
Availability,report errors to github comment,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6247:7,error,errors,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6247,1,['error'],['errors']
Availability,"rested. This is a richer and more flexible approach to working with reads data. It allows you to keep your genomics data in a common BAM file format on Google Cloud Storage and work with it efficiently from your computation pipelines, using standard bioinformatics tools. We have already launched our own open source implementation of this protocol, which you can use to access your reads data. Many popular tools such as samtools and htslib have been updated by the community to support htsget. Documentation is provided here. The Reads API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month by those receiving this notice, whichever comes first. ; > ; > Variants API is now replaced by htsget and Variant Transforms ; > ; > The GA4GH team also plans to extend the htsget protocol to cover variant data, and we will extend our implementation of htsget to cover this use case. ; > ; > After analyzing usage of the Variants API, we found that users primarily used it to import variant data and then export it to BigQuery. To save time and effort, we created Variant Transforms, an open source tool for directly importing VCF data into BigQuery. Variant Transforms and its documentation are published here. Variant Transforms is more scalable than the legacy Variants API, and it has a robust roadmap with a dedicated team. We also welcome collaborators on this project as it advances. ; > ; > The Variants API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month, whichever comes first. ; > ; > We are excited to move in step with the global genomics community and provide you with the latest technology for managing your genomic data. We have lots of other projects on the way, and look forward to supporting you. . Accordingly, we should remove our code that uses them. Unfortunately, this means we'll only have a single implementation of `GATKRead` which is unfortunate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166:1679,robust,robust,1679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166,1,['robust'],['robust']
Availability,return more useful error messages from RScriptExecutor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/223:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/223,1,['error'],['error']
Availability,"rg.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.Comm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:4341,ERROR,ERROR,4341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['ERROR'],['ERROR']
Availability,rg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [3bae2377-4ae0-4a9d-af6a-c94cd1fcebc1] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:7563,ERROR,ERROR,7563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,2,['ERROR'],['ERROR']
Availability,"rg.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:9444,down,down,9444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['down'],['down']
Availability,"rg.broadinstitute.hellbender.utils.python.StreamingPythonExecutorIntegrationTest > testRequirePythonEnvironment FAILED; java.lang.NullPointerException: Cannot invoke ""Object.getClass()"" because the return value of ""java.lang.RuntimeException.getCause()"" is null; at org.broadinstitute.hellbender.utils.python.StreamingPythonExecutorIntegrationTest.testRequirePythonEnvironment(StreamingPythonExecutorIntegrationTest.java:34); ```. Error messages in another test case:; ```; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0xE2) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x80) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x99) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; ```. This test is skipped without any apparent reason:; ```; Running Test: Test method loadIndex(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > loadIndex FAILED; java.lang.UnsatisfiedLinkError: 'boolean org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createReferenceIndex(java.lang.S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:1234,error,error,1234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['error'],['error']
Availability,rg.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:2233,ERROR,ERROR,2233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,rg.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.inte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:12166,ERROR,ERROR,12166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,rg.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.se,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:10908,ERROR,ERROR,10908,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,rget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3494,ERROR,ERROR,3494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"riants - Built for Spark Version: 2.4.5; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:42.996 INFO NVScoreVariants - Deflater: IntelDeflater; 08:37:42.996 INFO NVScoreVariants - Inflater: IntelInflater; 08:37:42.996 INFO NVScoreVariants - GCS max retries/reopens: 20; 08:37:42.996 INFO NVScoreVariants - Requester pays: disabled; 08:37:42.996 WARN NVScoreVariants - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: NVScoreVariants is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 08:37:42.996 INFO NVScoreVariants - Initializing engine; 08:37:43.031 INFO NVScoreVariants - Shutting down engine; [August 29, 2023 8:37:43 AM GMT] org.broadinstitute.hellbender.tools.walkers.vqsr.NVScoreVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=2123366400; java.lang.RuntimeException: A required Python package (""scorevariants"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:228); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.NVScoreVariants.onStartup(NVScoreVariants.java:108); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8501:2278,down,down,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8501,1,['down'],['down']
Availability,"riants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.663 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; 06:42:41.665 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_transcripts.fa; 06:42:46.805 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.805 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 06:42:46.807 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.951 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///dat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7090:7150,error,errors,7150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090,1,['error'],['errors']
Availability,"right now you get this which is bogus on many levels (duplicated and confusing categories, confusing tool names etc). We need to put more order into this. @vdauwera can you help come up with a better scheme of how to organize tools?; Compare to the ADAM project (much much smaller scope of course but very clean UI: https://github.com/bigdatagenomics/adam). ```; /gatk-launch --list; Running:; /Users/akiezun/IdeaProjects/gatk/build/install/gatk/bin/gatk --help; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Copy Number Analysis: Tools to analyze copy number data.; CalculateTargetCoverage Count overlapping reads target by target. --------------------------------------------------------------------------------------; Fasta: Tools for analysis and manipulation of files in fasta format; CreateSequenceDictionary Creates a dict file from reference sequence in fasta format; NormalizeFasta Normalizes lines of sequence in a fasta file to be of the same length. --------------------------------------------------------------------------------------; Intervals: Tools for processing intervals and associated overlapping records; BedToIntervalList Converts a BED file to an Picard Interval List; ExampleIntervalWalker Print intervals with optional contextual data; IntervalListTools General tool for manipulating interval lists; LiftOverIntervalList Lifts over an interval list between genome builds. --------------------------------------------------------------------------------------; QC: Tools for Diagnostics and Quality Control; AnalyzeCovariates Tool to analyze and evaluate base recalibration tables for BQSR; CalculateHsMetrics Produces Hybrid Selection-specific metrics for a SAM/BAM file; CollectAlignmentSummaryMetrics Produces from a SAM/BAM/CRAM file containing summary alignment metrics; CollectBaseDistributionByCycle Produces metrics about nucleotide distribution per cycle in a SAM/BAM/CRAM fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1669:491,Avail,Available,491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669,1,['Avail'],['Available']
Availability,"ription. **(Background)**; I've spent a lot of time working with Mutect2 in the past year (I've built a whole workflow centered around this tool). But, while I recognize that the internal reassembly feature leads to ""best-in-class"" results in terms of calling variants, for my purposes it generally just creates headaches since it makes interpreting (certain) calls and verifying (certain) base-level behaviors/expectations very difficult (even when looking at the bamout and assembly logs). Moreover, while we know our alignment process isn't perfect, we think it's appropriate for our purposes, and we would gladly accept the loss of a few calls to be able to have more control over the expected behaviors. With that, I purpose a ""--skip-assembly"" flag that would cause the Mutect2/HaplotypeCaller engine to use the original alignment information to determine the haplotypes. . All that said, I imagine this could be a niche feature request, so I've spent some time digging through the source code trying to see if there could be a quick fix that could be made available to whatever group of developers would want this. It seemed like there could be another conditional branched added here (https://github.com/broadinstitute/gatk/blob/9ff3f8b180c063a3fa67dae129b0cbd04012448e/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/ReadThreadingAssembler.java#L159) to build a `resultSet` based on a non-assembly based approach. However, I'm not certain how using the original alignment information would affect the statistics employed for genotyping the candidate haplotypes, so I'm starting to back off implementing a custom fix and hoping the experts can help (or at least explain to me why this feature is not currently possible OR if there is a way that I can access this behavior that I'm missing). Thank you for the consideration. **(TL;DR)**; Introduce a `--skip-assembly` option that would cause the Mutect2/HaplotypeCaller engine to use the original alignm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7064:1150,avail,available,1150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7064,1,['avail'],['available']
Availability,"rk Version: 2.4.5 ; ; 01:03:32.955 INFO GetPileupSummaries - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 01:03:32.955 INFO GetPileupSummaries - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SA MTOOLS : false ; ; 01:03:32.955 INFO GetPileupSummaries - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_S AMTOOLS : true ; ; 01:03:32.956 INFO GetPileupSummaries - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_T RIBBLE : false ; ; 01:03:32.956 INFO GetPileupSummaries - Deflater: IntelDeflater ; ; 01:03:32.956 INFO GetPileupSummaries - Inflater: IntelInflater ; ; 01:03:32.956 INFO GetPileupSummaries - GCS max retries/reopens: 20 ; ; 01:03:32.956 INFO GetPileupSummaries - Requester pays: disabled ; ; 01:03:32.956 INFO GetPileupSummaries - Initializing engine ; ; 01:03:33.330 INFO FeatureManager - Using codec VCFCodec to read file file:///ga tk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_commo n\_3.hg19.vcf.gz ; ; 01:03:33.395 INFO GetPileupSummaries - Shutting down engine ; ; \[September 12, 2021 1:03:33 AM GMT\] org.broadinstitute.hellbender.tools.walkers. contamination.GetPileupSummaries done. Elapsed time: 0.01 minutes. ; ; Runtime.totalMemory()=462946304 ; ; java.lang.IllegalArgumentException: Dictionary cannot have size zero ; ; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798) ; ; at org.broadinstitute.hellbender.utils.MRUCachingSAMSequenceDictionary.< init>(MRUCachingSAMSequenceDictionary.java:35) ; ; at org.broadinstitute.hellbender.utils.GenomeLocParser.<init>(GenomeLocP arser.java:78) ; ; at org.broadinstitute.hellbender.utils.GenomeLocParser.<init>(GenomeLocP arser.java:62) ; ; at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArg umentCollection.getTraversalParameters(IntervalArgumentCollection.java:180) ; ; at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArg umentCollection.getIntervals(IntervalArgumentCollection.java:111) ; ; at org.broadinstitute.hellbender.engine.GATKTool.i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7479:4684,down,down,4684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479,1,['down'],['down']
Availability,"rk.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in stage 25.0 (TID 43224, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:5744,ERROR,ERROR,5744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['ERROR'],['ERROR']
Availability,rker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:3565,ERROR,ERROR,3565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,rker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.dispatchMessages(AsyncDispatch.java:132); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch.access$000(AsyncDispatch.java:33); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.AsyncDispatch$1.run(AsyncDispatch.java:72); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 2 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:15039,ERROR,ERROR,15039,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,8,"['ERROR', 'Failure']","['ERROR', 'FailureHandlingDispatch']"
Availability,"rkflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. Another question is related to the fasta file:. I downloaded the reference data in the link of [https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37](https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37) , when I noticed that this is an old database, I have already generated GVCF files. It seems like GenotypeGVCFs does not understand the FAI index file. error informaion; ================. \[E::fai\_read\] Could not understand FAI /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai line 1 ; ; \[E::fai\_load3\] Failed to read FASTA index /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai. FAI file; ========. 1 dna:chromosome chromosome:GRCh37:1:1:249250621:1 249250621 52 60 61 ; ; 2 dna:chromosome chromosome:GRCh37:2:1:243199373:1 243199373 253404903 60 61 ; ; 3 dna:chromosome chromosome:GRCh37:3:1:198022430:1 198022430 500657651 60 61 ; ; 4 dna:chromosome chromosome:GRCh37:4:1:191154276:1 191154276 701980507 60 61 ; ; 5 dna:chromosome chromosome:GRCh37:5:1:180915260:1 180915260 896320740 60 61. If I use the latest fasta data provided by the GATK team [https://console.cloud.google.com/storage/browser/gcp-public-data--broad-references/hg19/v0;tab=objects?prefix=&forceOnObjectsSortingFiltering=false](https://console.cloud.google.com/storage/browser/gcp-public-data--broad-re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7442:6903,error,error,6903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442,1,['error'],['error']
Availability,rnal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:8430,ERROR,ERROR,8430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4852:3127,ERROR,ERROR,3127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852,2,['ERROR'],['ERROR']
Availability,roadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.buildFiltersList(Mutect2FilteringEngine.java:290); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.<init>(Mutect2FilteringEngine.java:60); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.onTraversalStart(FilterMutectCalls.java:138); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1047); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); srun: error: her2-w110: task 0: Exited with exit code 3. #### Steps to reproduce. cd /software/gatk-4.1.9.0. srun ./gatk FilterMutectCalls -R /CECI/proj/iribhm/tc_phylogeny/genome/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta -V /workdir/mparment/data/process/A2683/PTC2_unfiltered.vcf.gz --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T1_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T2_S1_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T2_S3_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T2_S5_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T3_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T4_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T1_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6996:5584,error,error,5584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6996,1,['error'],['error']
Availability,"roazen/google-cloud-java/tree/dr_all_nio_fixes. 18:58:01.494 INFO HaplotypeCaller - Initializing engine. 18:58:02.043 INFO HaplotypeCaller - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM. 18:58:02.933 INFO IntelPairHmm - Available threads: 8. 18:58:02.933 INFO IntelPairHmm - Requested threads: 4. 18:58:02.933 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. 18:58:02.986 INFO ProgressMeter - Starting traversal. 18:58:02.987 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute. 18:58:12.992 INFO ProgressMeter - chr1:6969901 0.2 23290 139712.1. 18:58:22.989 INFO ProgressMeter - chr1:13470130 0.3 44960 134866.5. 18:58:32.991 INFO ProgressMeter - chr1:21393130 0.5 71370 142721.0. INFO 18:58:37,986 HelpFormatter - ---------------------------------------------------------------------------------- . INFO 18:58:37,989 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.8-0-ge9d806836, Compiled 2017/07/28 21:26:50 . INFO 18:58:37,989 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute . INFO 18:58:37,989 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk . INFO 18:58:37,989 HelpFormatt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4788:3938,Avail,Available,3938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788,1,['Avail'],['Available']
Availability,"rocess still running with top. Do you know what could be causing the problem ? Could it be related to -ERC BP_RESOLUTION ? I used to use -ERC GVCF before but I would rather keep the information of the coverage for post filtering, and I am not sure how to use --GVCFGQBands to match my criteria for coverage filtering. Thanks a lot for your help !. Edit: sorry with the latest version of gatk I get a new message error :; ```; 08:22:54.446 INFO ProgressMeter - NC_016854.1:20000 0.2 20000 87450.8; 08:23:04.942 INFO ProgressMeter - NC_016854.1:58000 0.4 58000 143694.8; 08:25:25.155 INFO ProgressMeter - NC_016854.1:82000 2.7 82000 29921.4; 08:25:35.161 INFO ProgressMeter - NC_016854.1:100000 2.9 100000 34396.6; 08:28:02.395 INFO ProgressMeter - NC_016854.1:102000 5.4 102000 19025.7; 08:28:13.248 INFO ProgressMeter - NC_016854.1:140000 5.5 140000 25261.3; 08:28:24.027 INFO ProgressMeter - NC_016854.1:175000 5.7 175000 30585.2; 08:46:13.574 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),29.232685148998623,Cpu time(s),29.09919726900138; [February 28, 2018 8:46:13 AM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 23.59 minutes.; Runtime.totalMemory()=5588910080; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator$CombinedPoolLikelihoods.getLikelihoodOfConformation(GeneralPloidyExactAFCalculator.java:61); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.computeLofK(GeneralPloidyExactAFCalculator.java:283); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.calculateACConformationAndUpdateQueue(GeneralPloidyExactAFCalculator.java:187); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.fastCombineMultiallelicPool(GeneralPl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467:2701,down,down,2701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467,1,['down'],['down']
Availability,rocessing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dan,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:10072,Recover,Recovered,10072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"rogram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: cngb-nas-f17-1: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1631); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:924); 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1504); 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:843); 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1494); 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1626); 	... 21 more; ```. #### Steps to reproduce; On a Linux machine without _Hadoop_, run `java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5` locally. #### Expected behavior; Produce *out.pon.hdf5*. #### Actual behavior; Exit with error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:5263,error,error,5263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['error'],['error']
Availability,"rogram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: GenomicsDB JNI Error: VariantQueryProcessorException : Could not open array 1$1$188260577 at workspace: /data1/EquCab/GenomicsDB/ECA3_GenomicsDB_260/1; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed; 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:407); 	... 12 more. I'm assuming it is something in the array 1$1$188260577 files, and possibly the _book_keep.tbs.gz file, although I'm not sure how to go about trouble shooting the issue. I also recreated the database for these chromosomes (still using the same scripts as other chromosomes where variant calling was successful) to see if perhaps something went wrong during the initial database creation. I still received this error when I was trying to call variants. ; What is most confusing to me is that this issue isn't happening for every chromosome, just the first 3. Any advice to get over this hump is greatly appreciated, and let me know if there is more information you need to help trouble shoot. . Thanks, ; Caitlin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012:6402,error,error,6402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012,1,['error'],['error']
Availability,"root@ab23aa1f82e3 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.1; [July 20, 2017 2:22:06 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 3.65 minutes.; Runtime.totalMemory()=4188012544; htsjdk.samtools.SAMFormatException: Did not inflate expected amount; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:537); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:519); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:455); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:445); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:326); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:366); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:209); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316:3939,avail,available,3939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316,1,['avail'],['available']
Availability,rovider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:9688,ERROR,ERROR,9688,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"rox=|851|0|0;AS_RAW_BaseQRankSum=|||;AS_RAW_MQ=0.00|12289.00|0.00|0.00;AS_RAW_MQRankSum=|||;AS_RAW_ReadPosRankSum=|||;AS_SB_TABLE=0,0|1,4|0,0|0,0;AS_VarDP=0|26|1|0;DP=49;QUALapprox=853;RAW_GT_COUNT=0,0,1;RAW_MQandDP=144241,49;VarDP=27	GT:AD:DP:GQ:PL:SB	1/2:0,26,1,0:27:2:853,2,347,878,0,989,875,282,907,1161:0,0,1,4; ```. When merged into a GenomicsDB, they give an empty element in the allele-specific array fields such as `AS_RAW_MQ` and `AS_SB_TABLE`:. ```; chr7 2377548 . A G,*,<NON_REF> . . AS_QUALapprox=|338|0|0;AS_RAW_BaseQRankSum=|1.000,1||;AS_RAW_MQ=35536.000|25290.000||0.000;AS_RAW_MQRankSum=|-0.800,1||;AS_RAW_ReadPosRankSum=|-2.300,1||;AS_SB_TABLE=3,7|0,8||0,0;AS_VarDP=10|10|0|0;BaseQRankSum=1.006;DP=71;MQRankSum=-0.752;QUALapprox=338;RAW_GT_COUNT=0,1,0;RAW_MQandDP=72828,22;ReadPosRankSum=-2.285;VarDP=20 GT:AD:GQ:PL:SB:D; P ./.:10,10,0,0:99:338,0,387,341,416,762,341,416,762,762:3,7,0,8:20 ./.:0,0,26,0:2:853,875,1161,2,282,347,875,1161,282,1161:0,0,1,4:27; ```. Which leads to a GnarlyGenotyper error:. ```; gatk GnarlyGenotyper -R Homo_sapiens_assembly38.fasta -O gnarlied.vcf.gz --only-output-calls-starting-in-intervals --keep-all-sites -V gendb.gs://mybucket/genomicsdbs/interval_20_outof_50 -L 0020-scattered.interval_list; <...>; java.lang.IllegalStateException: Something went wrong:; 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyperEngine.finalizeGenotype(GnarlyGenotyperEngine.java:147); 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyperEngine.finalizeGenotype(GnarlyGenotyperEngine.java:78); 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyper.apply(GnarlyGenotyper.java:298); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(Re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7483:2147,error,error,2147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7483,1,['error'],['error']
Availability,"rror: Cannot finalize book-keeping; Failure to write to file /storage/home/data/gendb/chr13/chr13$32310639$32310731/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__book_keeping.tdb.gz.; [TileDB::FileSystem] Error: (create_file) Failed to create file; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__tiledb_fragment.tdb; errno=122(Disk quota exceeded); [TileDB::utils] Error: (create_fragment_file) Failed to create fragment file; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087; errno=122(Disk quota exceeded); 11:23:52.390 erro NativeGenomicsDB - pid=57964 tid=57984 VariantStorageManagerException exception : Error while finalizing TileDB array chr13$32310639$32310731; TileDB error message : [TileDB::WriteState] Error: Cannot write segment to file; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File opening error; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/RAW_MQandDP.tdb; errno=122(Disk quota exceede; d); #### Steps to reproduce; Below code ran on a cluster; ```; gatk --java-options ""-Xmx100g -Xms100g"" GenomicsDBImport \; -V sample1.g.vcf.gz -V sample2.g.vcf.gz -V sample3.g.vcf.gz -V sample4.g.vcf.gz \; -L chr13.bed \; --genomicsdb-workspace-path /storage/home/data/gendb/chr13\; --tmp-dir /storage/home/scratch/tmp; ```. #### Expected behavior; Over 1TB of scratch space available for temporary directory and around 500GB of storage space available to hold outputs of GenomicsDBImport outputs. #### Actual behavior; Above error message indicating that disk quota has exceeded. I'm not exactly sure what's going on here as I am directing the outputs of the GenomicsDBImport runs to directories with more than enough storage space and yet it seems to fail. Any help will be greatly appreciated. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6950:2674,avail,available,2674,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6950,3,"['avail', 'error']","['available', 'error']"
Availability,"rs specific to PGEN extract. ### Large scale testing; All of my testing (with the exception of some very small scale stuff early on) has been done in the [GVS_AoU_PGEN_Extract_Development Terra workspace](https://app.terra.bio/#workspaces/allofus-drc-prod-auxiliary/GVS_AoU_PGEN_Extract_Development) and using data from the GVS Delta callset (aou-genomics-curation-prod.aou_wgs_fullref_v2). My test process (for the majority of tests) has been as follows:. 1. Select a list of sample names from aou_wgs_fullref_v2.sample_info (excluding control samples and sample 3224672 because of the data issue mentioned [here](https://broadinstitute.slack.com/archives/CJRLP6ZSA/p1699026273329339)).; 2. Use that list as an input to GvsPrepareRangesCallset to create a cohort of test data in a separate BigQuery dataset (aou-genomics-curation-prod.klydon_pgen_extract_test).; 3. Run GvsExtractCallsetPgen on the newly created cohort. (I would just run GvsExtractCallsetPgenMerged, but I like using Workflow Dashboard to monitor how the job is going and dig into it if there are any failures. Workflow Dashboard doesn't seem to let you dig into individual tasks for workflows with sub-workflows, so it wouldn't allow me to look at individual shards running ExtractTask if I ran GvsExtractCallsetPgenMerged. Job Manager would be an alternative for this, but it seems to be pretty much unusable for even moderately-sized jobs.); 4. Run GvsExtractCallset on the newly created cohort, making sure to use the same parameters, including scatter count. This will generate VCF files that we can use to compare to the PGEN files created during the previous step for validation.; 5. Run GvsExtractCallsetPgenMerged with the same parameters used to run GvsExtractCallsetPgen in Step 3. This will use call-caching for the extract steps and then merge the PGEN files by chromosome. (Running it this way is maybe not the ideal way to do this, but it's what I've been doing for reasons described in the parenthetical in Step 3).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708:10454,failure,failures,10454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708,1,['failure'],['failures']
Availability,"rsion: 2.17.2; 00:17:06.848 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:17:06.849 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:17:06.849 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:17:06.850 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:17:06.850 INFO IndexFeatureFile - Deflater: IntelDeflater; 00:17:06.855 INFO IndexFeatureFile - Inflater: IntelInflater; 00:17:06.856 INFO IndexFeatureFile - GCS max retries/reopens: 20; 00:17:06.858 INFO IndexFeatureFile - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 00:17:06.859 INFO IndexFeatureFile - Initializing engine; 00:17:06.860 INFO IndexFeatureFile - Done initializing engine; 00:17:07.292 INFO FeatureManager - Using codec VCFCodec to read file file://bad.vcf; 00:17:07.310 INFO IndexFeatureFile - Shutting down engine; [January 26, 2018 12:17:07 AM GMT] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=512229376; java.lang.IllegalStateException: the progress meter has not been started yet; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:697); at org.broadinstitute.hellbender.engine.ProgressMeter.stop(ProgressMeter.java:230); at org.broadinstitute.hellbender.utils.codecs.ProgressReportingDelegatingCodec.isDone(ProgressReportingDelegatingCodec.java:104); at htsjdk.tribble.index.IndexFactory$FeatureIterator.readNextFeature(IndexFactory.java:522); at htsjdk.tribble.index.IndexFactory$FeatureIterator.<init>(IndexFactory.java:440); at htsjdk.tribble.index.IndexFactory.createDynamicIndex(IndexFactory.java:326); at org.broadinstitute.hellbender.tools.IndexFeatureFile.createAppropriateIndexInMemory(IndexFeatureFile.java:122); at org.broadinstitute.hellbender.tools.IndexFeatureFile.doWork(IndexFeatureFile.java:71); at org.broadin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4269:2676,down,down,2676,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4269,1,['down'],['down']
Availability,"rsion: 2.17.2; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:29:01.393 INFO GenomicsDBImport - Deflater: IntelDeflater; 19:29:01.393 INFO GenomicsDBImport - Inflater: IntelInflater; 19:29:01.393 INFO GenomicsDBImport - GCS max retries/reopens: 20; 19:29:01.393 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 19:29:01.393 INFO GenomicsDBImport - Initializing engine; 19:29:23.214 INFO IntervalArgumentCollection - Processing 1000000 bp from intervals; 19:29:23.216 INFO GenomicsDBImport - Done initializing engine; 19:29:23.332 INFO GenomicsDBImport - Shutting down engine; [January 10, 2018 7:29:23 PST PM] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=2804940800; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteOrCreateWorkspace(GenomicsDBImport.java:706); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:448); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124:2247,down,down,2247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124,1,['down'],['down']
Availability,"rsion: 2.24.1; 19:10:31.439 INFO CalculateContamination - Picard Version: 2.25.4; 19:10:31.439 INFO CalculateContamination - Built for Spark Version: 2.4.5; 19:10:31.439 INFO CalculateContamination - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:10:31.439 INFO CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:10:31.439 INFO CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:10:31.439 INFO CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:10:31.439 INFO CalculateContamination - Deflater: IntelDeflater; 19:10:31.439 INFO CalculateContamination - Inflater: IntelInflater; 19:10:31.439 INFO CalculateContamination - GCS max retries/reopens: 20; 19:10:31.439 INFO CalculateContamination - Requester pays: disabled; 19:10:31.439 INFO CalculateContamination - Initializing engine; 19:10:31.439 INFO CalculateContamination - Done initializing engine; 19:10:31.451 INFO CalculateContamination - Shutting down engine; [March 6, 2022 7:10:31 PM CST] org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2141192192; java.lang.IllegalArgumentException: there is no such column: contig; 	at org.broadinstitute.hellbender.utils.tsv.DataLine.columnIndex(DataLine.java:483); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:452); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:581); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:193); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:188); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.fetchNextRecord(TableReader.java:364); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.access$200(TableReader.java:99); 	at org.broadinstitute.hellbender.utils",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7707:3063,down,down,3063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707,1,['down'],['down']
Availability,"rt**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----; I am getting error while implementing docker images of GATK4 into variant calling workflow. As I connect the images into the WDL script, it first time run properly but as a streamlined input for next step the previous output is not visible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5906:2292,error,error,2292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5906,1,['error'],['error']
Availability,rter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:6606,ERROR,ERROR,6606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"run this. ```; ./gatk-launch CountReads -I fred; ```. and get this. ```; ***********************************************************************; A USER ERROR has occurred: Couldn't read file /Users/akiezun/IdeaProjects/gatk/fred. Error was: Cannot read non-existent file: /Users/akiezun/IdeaProjects/gatk/fred. ***********************************************************************; Exception in thread ""main"" java.lang.NullPointerException; at org.broadinstitute.hellbender.Main.main(Main.java:93); ```. this is bogus - I don't want any NPEs here",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1598:153,ERROR,ERROR,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1598,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:21 INFO storage.BlockManagerMaster: Removal of executor 9 requested; 18/01/09 18:31:21 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 9; 18/01/09 18:31:21 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 9 from BlockManagerMaster.; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms); 18/01/09 18:31:26 INFO server.AbstractConnector: Stopped Spark@283ab206{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:31:26 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.4:4040; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors; 18/01/09 18:31:26 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 18/01/09 18:31:26 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Stopped; 18/01/09 18:31:26 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/01/09 18:31:26 INFO memory.MemoryStore: MemoryStore cleared; 18/01/09 18:31:26 INFO storage.BlockManager: BlockManager stopped; 18/01/09 18:31:26 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/01/09 18:31:26 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/01/09 18:31:26 INFO spark.SparkContext: Successfully stopped SparkContext; 18:31:26.896 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [January 9, 2018 6:31:26 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 0.89 minutes.; Runtime.totalM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:30274,down,down,30274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['down'],['down']
Availability,"ry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:1187,mask,mask,1187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['mask'],['mask']
Availability,ry$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3308,ERROR,ERROR,3308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,ryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:5388,ERROR,ERROR,5388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['ERROR'],['ERROR']
Availability,"s - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:13:30.506 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:13:30.506 INFO FilterMutectCalls - Deflater: IntelDeflater; 17:13:30.506 INFO FilterMutectCalls - Inflater: IntelInflater; 17:13:30.506 INFO FilterMutectCalls - GCS max retries/reopens: 20; 17:13:30.506 INFO FilterMutectCalls - Requester pays: disabled; 17:13:30.506 INFO FilterMutectCalls - Initializing engine; 17:13:30.997 INFO FeatureManager - Using codec VCFCodec to read file file:///data-ddn/home/anthony/sbx/mutect2/work/ea/18e314102728d4b34b636b04f1f897/her2-crefix-unfiltered.vcf; 17:13:31.136 INFO FilterMutectCalls - Done initializing engine; 17:13:31.285 INFO ProgressMeter - Starting traversal; 17:13:31.286 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 17:13:31.286 INFO FilterMutectCalls - Starting first pass through the variants; 17:13:31.570 INFO FilterMutectCalls - Shutting down engine; [February 17, 2019 5:13:31 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=845676544; java.lang.NumberFormatException: For input string: "".""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Integer.parseInt(Integer.java:569); 	at java.lang.Integer.valueOf(Integer.java:766); 	at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:287); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Collections$2.tryAdvance(Collections.java:4717); 	at java.util.Collections$2.forEachRemaining(Collections.java:4725); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5684:2824,down,down,2824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684,1,['down'],['down']
Availability,"s must have called genotypes; 09:01:33.373 WARN DepthPerSampleHC - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:33.374 WARN StrandBiasBySample - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:36.316 INFO ProgressMeter - 1A:2054431 0.2 7310 43025.3; 09:01:46.831 INFO ProgressMeter - 1A:3580946 0.3 12960 37547.1; 09:01:56.858 INFO ProgressMeter - 1A:4888859 0.5 17840 34824.5; 09:02:07.416 INFO ProgressMeter - 1A:7184455 0.7 26090 37907.7; 09:02:17.850 INFO ProgressMeter - 1A:9469826 0.9 34580 40109.0; 09:02:28.162 INFO ProgressMeter - 1A:11632942 1.0 42480 41082.5; 09:02:38.391 INFO ProgressMeter - 1A:12861813 1.2 47220 39203.0; 09:02:48.460 INFO ProgressMeter - 1A:15373965 1.4 56590 41236.8. 09:20:43.520 INFO ProgressMeter - 1A:536836177 19.3 1951140 101147.8; 09:20:44.715 INFO HaplotypeCaller - Shutting down engine; [February 8, 2023 at 9:20:44 AM CST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 19.31 minutes.; Runtime.totalMemory()=1811939328; java.lang.ArrayIndexOutOfBoundsException: Index 32770 out of bounds for length 32770; at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:142); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:233); at org.broadinstitute.hellbender.utils.variant.writers.GVCFWriter.close(GVCFWriter.java:71); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.closeTool(HaplotypeCaller.java:277); at org.broadinstitute.hellbender.engine.GATKTool.doWork(G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192:5479,down,down,5479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192,1,['down'],['down']
Availability,"s on the versions tested. This does not occur with version 4.1.4.1 where the program manages to process and output the entire VCF. Upon further testing, it seems that the FORMAT field AF causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariants.apply(LeftAlignAndTrimVariants.java:225); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferenceP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7211:1318,error,error,1318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211,1,['error'],['error']
Availability,s specifically from the GCS access in `CloudStorageReadChannel.fetchSize()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:202); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:234); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:78); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:68); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:304); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:265); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at htsjdk.samtools.seekablestream.SeekablePathStream.<init>(SeekablePathStream.java:41); at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:101); at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:270). [2:58] ; From stdout:. [2:58] ; 15:55:58.059 INFO GenomicsDBImport - Done importing batch 19/444; 15:56:21.780 INFO GenomicsDBImport - Shutting down engine; code: 503; message: 503 Service Unavailable; reason: null; location: null; retryable: false; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253:2188,down,down,2188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253,1,['down'],['down']
Availability,s$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); 	at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.NumberFormatException: empty String; 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842); 	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); 	at java.lang.Double.parseDouble(Double.java:538); 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_RMSMappingQuality.parseRawDataString(AS_RMSMappingQuality.java:172); 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_RMSMappingQuality.finalizeRawData(AS_RMSMappingQuality.java:196); 	at org.broadinstitute.hellbender.tools.walkers.gnarlyGenotyper.GnarlyGenotyperEngine.finalizeGenotype(GnarlyGenotyperEngine.java:137); 	... 23 more; ```. GenotypeGVCFs throws a similar error but with the `AS_SB_TABLE` field.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7483:5286,error,error,5286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7483,1,['error'],['error']
Availability,"s-include-non-variant-sites](https://gatk.broadinstitute.org/hc/en-us/community/posts/6808932798363-Monomorphic-sites-after-GenotypeGVCFs-include-non-variant-sites). \--. Hello,. I am using GATKv4.2.6.1 and GATK best practices. I performed joint genotyping of a multi-sample GVCF with GenotypeGVCFs. Because I am doing a population genetic analysis I am very interested in obtaining high confidence monomorphic sites, so I included the option --include-non-variant-sites. In the output VCF, however, I find that there are 3 types of monomorphic sites, for example:. #CHROM                POS  ID   REF               ALT   QUAL     FILTER. HiC\_scaffold\_493    961    .    A                     .        .              . ; ; HiC\_scaffold\_493    962    .    ATCTCCCC    .        7.65        LowQual ; ; HiC\_scaffold\_493    963    .    T                     .        180.56    . I am not sure what the differences between those 3 types of monomorphic sites are. I tracked down those positions in the input GVCF and they look like this:. #CHROM                     POS  ID     REF                    ALT                     QUAL     FILTER. HiC\_scaffold\_493        961     .       A                        <NON\\\_REF>        .            .  ; ; HiC\_scaffold\_493        962     .       ATCTCCCC       A,<NON\\\_REF>     .            . ; ; HiC\_scaffold\_493        963     .       T                         \*,<NON\\\_REF>     .            . I assume that in the GVCF, position 962 had some evidence of the presence of an alternative allele (A) but it was so poor (QUAL < 30) that it was discarded and the position was deemed as monomorphic in the VCF (LowQual). But what about position 963? There was some evidence of a deletion (\*) as alternative allele in the GVCF but it got discarded in the VCF despite QUAL = 180.56?. Also, why does position 961 has no QUAL score at all? In fact, these are results from a small scaffold with 1,000 bp, of which 789 monomorphic sites have no QUAL score at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8030:1183,down,down,1183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8030,1,['down'],['down']
Availability,s.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(Execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:6411,ERROR,ERROR,6411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,s.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecuto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4857,ERROR,ERROR,4857,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,s.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.B,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:7631,ERROR,ERROR,7631,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@642ec6{/stages/stage/kill,null,AVAILABLE,@Spark}; 10:33:07.389 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3fe5ad73{/metrics/json,null,AVAILABLE,@Spark}; 10:33:07.397 INFO SortSamSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 10:33:07.450 INFO GoogleHadoopFileSystemBase - GHFS version: 1.9.4-hadoop3; 10:33:08.183 INFO MemoryStore - Block broadcast_0 stored as values in memory (estimated size 268.7 KiB, free 1076.2 GiB); 10:33:08.581 INFO MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 1076.2 GiB); 10:33:08.585 INFO BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.19.130:43279 (size: 41.8 KiB, free: 1076.2 GiB); 10:33:08.591 INFO SparkContext - Created broadcast 0 from newAPIHadoopFile at PathSplitSource.java:96; 10:33:09.126 INFO MemoryStore - Block broadcast_1 stored as values in memory (estimated size 268.7 KiB, free 1076.2 GiB); 10:33:09.142 INFO MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 1076.2 GiB); 10:33:09.144 INFO BlockManagerInfo - Added broadcast_1_piece0 in me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:47875,AVAIL,AVAILABLE,47875,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@642ec6{/stages/stage/kill,null,AVAILABLE,@Spark}; 10:33:07.389 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3fe5ad73{/metrics/json,null,AVAILABLE,@Spark}; 10:33:07.397 INFO SortSamSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 10:33:07.450 INFO GoogleHadoopFileSystemBase - GHFS version: 1.9.4-hadoop3; 10:33:08.183 INFO MemoryStore - Block broadcast_0 stored as values in memory (estimated size 268.7 KiB, free 1076.2 GiB); 10:33:08.581 INFO MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 1076.2 GiB); 10:33:08.585 INFO BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.19.130:43279 (size: 41.8 KiB, free: 1076.2 GiB); 10:33:08.591 INFO SparkContext - Created broadcast 0 from ne",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:47511,AVAIL,AVAILABLE,47511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"s.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:3474,ERROR,ERROR,3474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['ERROR'],['ERROR']
Availability,"s.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false. 07:33:15.362 INFO FilterMutectCalls - Deflater: IntelDeflater. 07:33:15.362 INFO FilterMutectCalls - Inflater: IntelInflater. 07:33:15.363 INFO FilterMutectCalls - GCS max retries/reopens: 20. 07:33:15.363 INFO FilterMutectCalls - Requester pays: disabled. 07:33:15.363 INFO FilterMutectCalls - Initializing engine. 07:33:16.008 INFO FeatureManager - Using codec VCFCodec to read file file:///nobackup/lnsingh/MTRNA/out/COVSUBJ_0121_1_N_HA_filtered.humanspliced.gvcf.gz. 07:33:16.053 INFO IntervalArgumentCollection - Processing 16569 bp from intervals. 07:33:16.059 INFO FilterMutectCalls - Done initializing engine. 07:33:16.157 INFO ProgressMeter - Starting traversal. 07:33:16.157 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. 07:33:16.158 INFO FilterMutectCalls - Starting pass 0 through the variants. 07:33:17.341 INFO FilterMutectCalls - Finished pass 0 through the variants. 07:33:17.404 INFO FilterMutectCalls - Shutting down engine. [September 20, 2020 7:33:17 AM PDT] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.04 minutes. Runtime.totalMemory()=1256194048. java.lang.IllegalArgumentException: alpha must be greater than 0 but got NaN. at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:727). at org.broadinstitute.hellbender.utils.param.ParamUtils.isPositive(ParamUtils.java:165). at org.broadinstitute.hellbender.tools.walkers.readorientation.BetaDistributionShape.<init>(BetaDistributionShape.java:13). at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.BinomialCluster.getFuzzyBinomial(BinomialCluster.java:43). at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.BinomialCluster.<init>(BinomialCluster.java:17). at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:184). at org.broadinstitute.hellbender.tools.walkers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6850:4281,down,down,4281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6850,1,['down'],['down']
Availability,s.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:282); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:993); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036:1392,down,downsampling,1392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036,1,['down'],['downsampling']
Availability,s; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:20203,Recover,Recovered,20203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,sBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.la,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:7877,ERROR,ERROR,7877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938:4952,ERROR,ERROR,4952,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938,6,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability,"schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:1217,error,error,1217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"scovered by feeding the tool with coverage data on autosomes + X chromosome (no Y chromosome). Since the X chr in XX samples has 2x ploidy of X in XY samples, one expects the tool to be able to make the correct inference. However, the tool genotyped all samples as XX (see the attached figure -- left: autosome+X+Y, right:autosome+X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3015:1191,robust,robustness,1191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015,1,['robust'],['robustness']
Availability,"se(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Expected behavior. Runs to completed and writes out model file. #### Actual behavior; _Tell us what happens instead_. The following error occurs....; ```; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:16960,down,down,16960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['down'],['down']
Availability,"seArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030:2436,error,errors,2436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030,2,['error'],"['error', 'errors']"
Availability,"seReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:08:45.223 INFO DenoiseReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7258:3922,Error,Error,3922,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258,1,['Error'],['Error']
Availability,"section of the gatkcondaenv.yml. removes it and install numpy-1.18.1. see relevant part of conda env create -n gatk -f gatk-4.1.4.0/gatkcondaenv.yml 2>&1 | tee log; NB full log is attached : [log.txt](https://github.com/broadinstitute/gatk/files/4091802/log.txt). ```; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... done. Downloading and Extracting Packages. keras-preprocessing- | 36 KB | ########## | 100%; astor-0.8.0 | 46 KB | ########## | 100%; setuptools-36.4.0 | 563 KB | ########## | 100%; termcolor-1.1.0 | 8 KB | ########## | 100%; protobuf-3.11.2 | 635 KB | ########## | 100%; keras-applications-1 | 33 KB | ########## | 100%; readline-6.2 | 606 KB | ########## | 100%; libgfortran-ng-7.3.0 | 1006 KB | ########## | 100%; numpy-1.13.3 | 3.1 MB | ########## | 100%; ```. numpy-1.13.3 is corectly installed . but then . ```; Collecting numpy (from biopython==1.70->-r /root/gatk-4.1.4.0/condaenv.g1uyq0ce.requirements.txt (line 1)); Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB); ```. that does . ```; Found existing installation: numpy 1.13.3; Uninstalling numpy-1.13.3:; Successfully uninstalled numpy-1.13.3; ```. this causes ```gatk DetermineGermlineContigPloidy ```; to exit with an error related to numpy.testing.decorators which is deprecated since numpy 1.15.0 see https://docs.scipy.org/doc/numpy-1.15.0/release.html. ```; Deprecations. Aliases of builtin pickle functions are deprecated, in favor of their unaliased pickle.<func> names:; numpy.loads; numpy.core.numeric.load; numpy.core.numeric.loads; numpy.ma.loads, numpy.ma.dumps; numpy.ma.load, numpy.ma.dump - these functions already failed on python 3 when called with a string.; Multidimensional indexing with anything but a tuple is deprecated. This means that the index list in ind = [slice(None), 0]; arr[ind] should be changed to a tuple, e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6396:1169,Down,Downloading,1169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6396,1,['Down'],['Downloading']
Availability,"seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM UTC; 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.220 INFO ASEReadCounter - HTSJD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7314:1303,error,error,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314,1,['error'],['error']
Availability,"segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T2_S1_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T2_S3_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T2_S5_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T3_segments.table --tumor-segmentation /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T4_segments.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T1_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T2_S1_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T2_S3_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T2_S5_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T3_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S2_PTC2_T4_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T1_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T2_S1_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T2_S3_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T2_S5_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T3_contamination.table --contamination-table /workdir/mparment/data/process/A2683/PTC2_N3_S4_PTC2_T4_contamination.table -ob-priors /workdir/mparment/data/process/A2683/PTC2_read-orientation-model.tar.gz -O /workdir/mparment/data/process/A2683/PTC2_filtered.vcf.gz. #### Actual behavior; It look like this error: https://github.com/broadinstitute/gatk/issues/3018. I followed GATK's good practice rules and I use Mutect2 in ""multisample"" mode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6996:8376,error,error,8376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6996,1,['error'],['error']
Availability,"segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:2059,down,down,2059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,1,['down'],['down']
Availability,"seq.bam""). samples = pd.DataFrame.from_dict({""patient"": [""ATCC25586"", ""SL1344"", ""LT2"", ""ATCC25586"", ""SL1344"", ""LT2""], ""sample"": [""1"", ""1"", ""1"", ""2"", ""2"", ""2""]}). localrules: simulate_RNAseq_reads, download_ATCC25586_cds_from_genomic, download_LT2_cds_from_genomic, download_SL1344_cds_from_genomic. rule all:; input:; expand(output/{patient}-{sample}/unaligned_simulated_bam.bam, zip, sample=samples[""sample""], patient=samples[""patient""]); # run this bam file through PathSeq. rule convert_FASTA_to_BAM:; input:; fq1=FQ1,; output:; output/{patient}-{sample}/unaligned_simulated_bam.bam; shell:; ""module load picard && ""; ""java -Xmx8g -XX:ParallelGCThreads=5 -jar $PICARDJARPATH/picard.jar ""; ""FastqToSam F1={input.fq1} O={output} ""; ""SM={wildcards.sample} RG={wildcards.sample} ""; ""TMP_DIR=/lscratch/$SLURM_JOBID"". rule simulate_RNAseq_reads:; conda:; ""../envs/rsubread-env.yaml""; params:; FQ1_PREFIX; input:; CDS_FA; output:; FQ1; script:; ""R/simulate_RNAseq.R"". # download the cds_from_genomic fasta file; rule download_SL1344_cds_from_genomic:; params:; url=SL1344_CDS_URL; output:; SL1344_CDS_FA; shell:; ""wget -O - {params.url} | gunzip -c > {output}"". rule download_LT2_cds_from_genomic:; params:; url=LT2_CDS_URL; output:; LT2_CDS_FA; shell:; ""wget -O - {params.url} | gunzip -c > {output}"". rule download_ATCC25586_cds_from_genomic:; params:; url=ATCC25586_CDS_URL; output:; ATCC25586_CDS_FA; shell:; ""wget -O - {params.url} | gunzip -c > {output}""; ```; rsubread-env.yaml; ```; name: rsubread; channels:; - conda-forge; - bioconda; - defaults; dependencies:; - bioconductor-rsubread; - bioconductor-biostrings; ```; simulate_RNAseq.R; ```; library(Rsubread); library(Biostrings); set.seed(strtoi(snakemake@wildcards[[""sample""]])). fasta = readDNAStringSet(snakemake@input[[1]]). expr = matrix(1, ncol=1, nrow=length(fasta)). simReads(transcript.file=snakemake@input[[1]], expression.levels=expr,; output.prefix=snakemake@params[[1]], library.size=100000, simulate.sequencing.error=TRUE); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6705:3754,down,download,3754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6705,2,"['down', 'error']","['download', 'error']"
Availability,"ser-images.githubusercontent.com/61913000/87845904-eea14f80-c8e0-11ea-90bd-235c9205f72f.png"">. (gatk) root@bc3c6aca6231:/gatk/my_data/tools# java -jar cromwell-51.jar run /gatk/my_data/seq-format-validation/validate-bam.wdl --inputs /gatk/my_data/seq-format-validation/validate-bam.inputs.json; [2020-07-14 05:09:22,78] [info] Running with database db.url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:1848,heartbeat,heartbeat,1848,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,3,"['failure', 'heartbeat']","['failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,"server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://githu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:11815,ERROR,ERROR,11815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unkn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:12907,ERROR,ERROR,12907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,setDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:10794,ERROR,ERROR,10794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,setDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:9536,ERROR,ERROR,9536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,setting RScriptExecutor to output useful messages on failure,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/237:53,failure,failure,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/237,1,['failure'],['failure']
Availability,"shard-12910/inputs/-724059439/P0000992.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:29833,error,error,29833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['error'],['error']
Availability,"should contain A element(s).; In file/stream ""2105614020_IVRN_stream"", at contig ""chr14"", position 60604048, for sample ""Einstein"", the field AF has 3 elements; expected 2; Using GATK jar /gatk/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms8g -jar /gatk/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate. </p>; </details>. #### Steps to reproduce; Run ReblockGVCF with Dragen 3.6.3 gvcf output. #### Expected behavior; Do the parse as expected. #### Actual behavior; You can see below 2 different examples that return the same error.; <br>-- First; * Dragen output; ```; chr14 60604048 . TCACACACACACA TCACACA,T,<NON_REF> 135.20 PASS DP=44;MQ=250.00;MQRankSum=1.636;ReadPosRankSum=1.540;FractionInformativeReads=0.818;R2_5P_bias=0.000 GT:AD:AF:DP:F1R2:F2R1:GQ:PL:SPL:ICNT:GP:PRI:SB:MB 1/1:1,34,1,0:0.944,0.028,0.000:36:1,20,1,0:0,14,0,0:82:140,88,0,967,86,138,935,101,985,948:255,0,220:0,29:1.3520e+02,8.5204e+01,0.0000e+00,4.5000e+02,8.5278e+01,1.3828e+02,4.5000e+02,1.3246e+02,4.5000e+02,4.5000e+02:0.00,2.00,5.00,2.00,4.00,5.00,34.77,36.77,36.77,37.77:1,0,18,17:1,0,22,13; ```; * ReblockGVCF (4.2.3.0) output; ```; chr14 60604048 . TCACACA T,<NON_REF> 135.20 . AS_QUALapprox=|140|0;AS_VarDP=1|34|0;DP=44;MQ=250.00;MQRankSum=1.636;QUALapprox=140;RAW_GT_COUNT=0,0,1;RAW_MQandDP=2750000,44;ReadPosRankSum=1.540;VarDP=35 GT:AD:AF:DP:F1R2:F2R1:GQ:ICNT:MB:PL:PRI:SB:SPL 1/1:1,34,0:0.944,0.028,0.000:36:1,20,1,0:0,14,0,0:88:0,29:1,0,22,13:140,88,0,935,101,948:0.00,2.00,5.00,2.00,4.00,5.00,34.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7589:5474,error,error,5474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589,1,['error'],['error']
Availability,"sks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 93, in __call__; self.input_path, self.denoising_model_approx, self.denoising_model); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 471, in read_mean_field_global_params; ""expected: {2}"".format(var_name, var_mu.shape, vmap.shp); AssertionError: Loaded mean for ""log_mean_bias_t"" has an unexpected shape; loaded: (11903,), expected: (11901,). at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:351); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289) ```. Can you give me some hint where this error comes from? ; Thanks in advanve; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:9679,error,error,9679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['error'],['error']
Availability,"spark complains but non-spark does not. ```; CollectQualityYieldMetricsSpark --output a.metrics --useOriginalQualities true --reference src/test/resources/Homo_sapiens_assembly19_chr1_1M.fasta --input src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectQualityYieldMetrics/collect_quality_yield_metrics.cram --disableSequenceDictionaryValidation false. org.broadinstitute.hellbender.exceptions.UserException$IncompatibleSequenceDictionaries: A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Found contigs with the same name but different lengths:; contig reference = 1 / 1000000; contig reads = 1 / 249250621.; reference contigs = [1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]; ```. Once fixed, reenable 2 tests in CollectQualityYieldMetricsSparkIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1262:475,ERROR,ERROR,475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1262,1,['ERROR'],['ERROR']
Availability,spark error due to missing cloud storage enum DURABLE_REDUCED_AVAILABILITY,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517:6,error,error,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517,1,['error'],['error']
Availability,"spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGSch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:6365,failure,failure,6365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['failure'],['failure']
Availability,"specops issue #273: https://github.com/broadinstitute/dsp-spec-ops/issues/273. - renamed `ngs_cohort_extract.py` -> `create_cohort_extract_data_table.py`; - run the script in a WDL (GvsPrepareCallset.wdl); - use a custom docker - include script for creating and pushing this docker to gcr.io; - enable running as a SA - this has been tested in Terra and works as expected. if using a dataset that requires SA access and the user does not provide a working SA key, they get this error: `User does not have bigquery.jobs.create permission in project specops-variantstore-sa-tests.`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7200:478,error,error,478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7200,1,['error'],['error']
Availability,"src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ```. FreeBSD 11.2 amd64",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5934:2851,error,error,2851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934,3,['error'],['error']
Availability,"ssignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set everything to be logged to the console; log4j.rootCategory=WARN,console; log4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:1204,ERROR,ERROR,1204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,1,['ERROR'],['ERROR']
Availability,"ssing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants — and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up someh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8208:1445,down,downstream,1445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208,1,['down'],['downstream']
Availability,"ssuecomment-260687763). @vdauwera yes it is on mine. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolve all these, and that will definitely happen in GATK 4. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-281073466). Ah, interesting, thanks Steve. Do you have any sense of when you might be able to look further into this? This is not to pressure you, just to estimate the roadmap/timeline. An order of magnitude (weeks, months, more) would be fine. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-287838044). I'm going to move this issu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:10001,avail,available,10001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['avail'],['available']
Availability,"st$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.lambda$save$bddeb71b$1(AnySamSinkM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:4995,failure,failure,4995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['failure'],['failure']
Availability,st'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:1433,ERROR,ERROR,1433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"stage 2.0 (TID 7) on xx.xx.xx.23, executor 5: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 3]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:56:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49966 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49966 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:07 WARN TaskSetManager: Lost task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:29475,Error,Error,29475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,"sted number of threads: 12; 10:24:22.008 INFO CalibrateDragstrModel - Done initializing engine; 10:24:22.008 INFO ProgressMeter - Starting traversal; 10:24:22.008 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 10:24:32.859 INFO ProgressMeter - chr1:26000000 0.2 59038 326477.4; 10:24:42.867 INFO ProgressMeter - chr1:83000000 0.3 184245 529998.1; 10:24:52.965 INFO ProgressMeter - chr1:137193529 0.5 306766 594565.4; 10:25:03.307 INFO ProgressMeter - chr1:193193529 0.7 428759 622924.6; 10:25:13.318 INFO ProgressMeter - chr2:3237107 0.9 564835 660497.0; 10:25:23.358 INFO ProgressMeter - chr2:57237107 1.0 681209 666219.1; 10:25:33.392 INFO ProgressMeter - chr2:109237107 1.2 799610 672091.8; 10:25:44.527 INFO ProgressMeter - chr2:177512416 1.4 930822 676805.6; 10:25:54.821 INFO ProgressMeter - chr2:237512416 1.5 1069999 691712.8; 10:26:04.863 INFO ProgressMeter - chr3:54999378 1.7 1197525 698570.8; 10:26:09.642 INFO CalibrateDragstrModel - Shutting down engine; [January 2, 2023 at 10:26:09 AM GMT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 1.81 minutes.; Runtime.totalMemory()=47647293440; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006); at org.broadinstitute.hellbender.utils.Utils.runInParallel(Uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:5281,down,down,5281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['down'],['down']
Availability,"stering block manager 172.20.19.130:43279 with 1076.2 GiB RAM, BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.225 INFO BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.226 INFO BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.345 INFO ContextHandler - Stopped o.s.j.s.ServletContextHandler@7074da1d{/,null,STOPPED,@Spark}; 10:33:07.347 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6556471b{/jobs,null,AVAILABLE,@Spark}; 10:33:07.349 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7cdb05aa{/jobs/json,null,AVAILABLE,@Spark}; 10:33:07.351 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5cb76070{/jobs/job,null,AVAILABLE,@Spark}; 10:33:07.352 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@443ac5b8{/jobs/job/json,null,AVAILABLE,@Spark}; 10:33:07.354 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@753e4eb5{/stages,null,AVAILABLE,@Spark}; 10:33:07.355 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@63318b56{/stages/json,null,AVAILABLE,@Spark}; 10:33:07.357 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@462f8fe9{/stages/stage,null,AVAILABLE,@Spark}; 10:33:07.358 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@b2e1df3{/stages/stage/json,null,AVAILABLE,@Spark}; 10:33:07.359 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6cf3b3d7{/stages/pool,null,AVAILABLE,@Spark}; 10:33:07.360 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:45354,AVAIL,AVAILABLE,45354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"stitute/hellbender/gcnvkernel/structs/metadata.py#L177); [gcnvkernel metadata.py SampleMetadataCollection class](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, Elemwise{mul,no_inplace}.0, Subtensor{int64::}.0, Elemwise{sub,no",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8097:1211,error,error,1211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097,1,['error'],['error']
Availability,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5644:3034,error,error,3034,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644,2,['error'],['error']
Availability,"successful run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. failing run with the bug:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/f1c952fc-7f05-4468-ae20-1c1cc5b9bf38. AC is:. Cohort builder subcohort extract in AoU and our extract workflow work with both VETS and VQSR callsets, including past callsets. (Note--I did not test in AoU, just on quickstart since the issue doesn't seem to be permission or scale related--see failure reproduced above). Full extract with past callset & VQSR; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. Subcohort extract with past callset & VQSR. Full extract with new callset & VETS; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50ef3073-f618-42ee-b207-73712a783a8a; (note this failed but only on one of the 4 runs and it's based on query cost). <img width=""1202"" alt=""Screenshot 2023-08-25 at 1 22 58 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/39468ed8-fe2b-4bf8-9326-3bfcf6dabbb1"">. Kevin is able to run latest extract on Delta (still waiting on Kevin, but otherwise the above are all set). note that there was briefly no ""score"" col but I dont _think_ we need to be backwards compatible for that as there was no release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8488:544,failure,failure,544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8488,1,['failure'],['failure']
Availability,surpressing tool output statement if the tool returned null; exiting with 1 if the tool threw an error. should fix #341 and #342 . @vruano Please review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/343:97,error,error,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/343,1,['error'],['error']
Availability,sy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:13844,ERROR,ERROR,13844,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,syncSAMFileWriter.java:38); at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:89); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.close(SAMFileGATKReadWriter.java:26); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:193); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1053); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.samtools.util.RuntimeIOException: Write error; BinaryCodec in writemode; streamed file (filename not available); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:222); at htsjdk.samtools.util.BlockCompressedOutputStream.writeGzipBlock(BlockCompressedOutputStream.java:451); at htsjdk.samtools.util.BlockCompressedOutputStream.deflateBlock(BlockCompressedOutputStream.java:415); at htsjdk.samtools.util.BlockCompressedOutputStream.write(BlockCompressedOutputStream.java:305); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:212); at htsjdk.samtools.BAMRecordCodec.encode(BAMRecordCodec.java:168); at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:134); ... 12 more; Caused by: java.io.IOException: Stale file handle; at java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method); at java.base/sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:62); at java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:115); at java.base/sun.nio,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7091:60190,error,error,60190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091,2,"['avail', 'error']","['available', 'error']"
Availability,"t (which happens to be supported by all haplotypes present) falls outside of our active region in the padding then we try to draw the variant span based on the first 4 haplotypes by the rules of haplotype expansion we end up making our trimming span `chr#:###326-###555` (note ###555 falls inside the span the 5th haplotype). When we go to trim all of our variant haplotypes (which happen to all have variant #5) they run into this code inside `Haplotype.trim()`:; ```; // note: the following returns null if the bases covering the ref interval start or end in a deletion.; final byte[] newBases = AlignmentUtils.getBasesCoveringRefInterval(newStart, newStop, getBases(), 0, getCigar());. if ( newBases == null || newBases.length == 0 ) { // we cannot meaningfully chop down the haplotype, so return null; return null;; }; ```; For all of our variant haplotypes at this site we find deletions at the end base and throw the whole haplotypes away when we try to trim it. In this particular case it meant we lost real variants in the previous 4 haplotypes as a result. I propose remedying this in one of two ways:; 1) Allow `AlignmentUtils.getBasesCoveringRefInterval()` to return partially spanning haplotypes when there are potentially 'shorter' than the reference haplotype span (this could easily cause all sorts of errors as the later code might not account for those mismatches. ; 2) Make `AlignmentUtils.getBasesCoveringRefInterval()` cheat and paste reference bases at the front or back of the haplotype to make it square with the reference offsets (we should never call or worry about deletions at the ends of haplotypes anyway) ; 3) Try to catch this edge case at the `AssemblyRegionTrimmer.trim()` stage, try to make the trimmer aware that there might be deletions overlapping its boundaries and expand them until there are no more overlaps. . This is a very unlikely case I suspect but it could cost us some sensitivity in noisy low complexity regions. @davidbenjamin what are your thoughts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7137:2159,error,errors,2159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7137,1,['error'],['errors']
Availability,t 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:15528,Recover,Recovered,15528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"t I filter snp.raw.vcf with `VariantFiltration` command below: ; `gatk-4.1.2.0/gatk --java-options ""-Djava.io.tmpdir=/tmp/"" VariantFiltration -R genome.fa -V snp_rmnan.raw.vcf --filter-expression ""QUAL < 30.0 || QD < 2.0 || FS > 60.0 || MQ < 40.0 || SOR > 4.0 || ReadPosRankSum < -8.0"" --filter-name ""my_snp_filter"" --missing-values-evaluate-as-failing true -O snp_rmnan.raw.vcf.tmp.vcf `. This command runs successfully. But when I'm using `SelectVariants` command to extact the filtered site:; `gatk-4.1.2.0/gatk --java-options ""-Djava.io.tmpdir=/tmp/"" SelectVariants -R genome.fa -V snp_rmnan.raw.vcf.tmp.vcf --exclude-filtered -O snp_rmnan.raw.vcf.filter.vcf `. I get this java ERROR below, even without the wrong line number and do not know how to deal with it......... o(╥﹏╥)o，Thank you very much!. ~~~; 11:15:52.195 INFO ProgressMeter - Chr01:15144308 19.9 541000 27161.2; 11:16:04.187 INFO ProgressMeter - Chr01:15388212 20.1 547000 27189.6; 11:16:12.515 INFO SelectVariants - Shutting down engine; [May 9, 2019 11:16:12 AM CST] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 20.37 minutes.; Runtime.totalMemory()=2814377984; java.lang.NumberFormatException: For input string: ""1,0""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:734); at htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132); at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); at htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148); at htsjdk.variant.variantcontext.GenotypesContext.iterator(GenotypesContext.java:465); at org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants.initalizeAlleleAnyploidIndicesCache(SelectVarian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5929:1003,down,down,1003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5929,1,['down'],['down']
Availability,t oAB.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at oAB.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at oAB.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:706); 	at oAB.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); 	at oAB.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at oAB.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseAABs(CommandLineProgram.java:191); 	at oAB.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at oAB.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at oAB.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at oAB.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.io.IOException: GenomicsDB JNI Error: GenomicsDBConfigException : Syntax error in JSON file /data/xxxxxx/ABchroneALL/callset.json; 	at oAB.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at oAB.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at oAB.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at oAB.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at oAB.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:176); 	at oAB.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:80); 	at oAB.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:407); 	... 12 more. #GenotypeGVCF working when only six samples are imported in GenomicsDB; gatk GenotypeGVCFs -R Reference/File_S16_uT_chromosomes.fasta -V gendb://GenomicsDB_wd -O test_chromosome_1_6_samples.vcf; Using GATK jar /data/xxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6616:11984,Error,Error,11984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6616,2,"['Error', 'error']","['Error', 'error']"
Availability,"t org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:11678,failure,failure,11678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['failure'],['failure']
Availability,"t org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:4791,down,down,4791,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['down'],['down']
Availability,"t org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5336:5091,error,error,5091,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336,1,['error'],['error']
Availability,t org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:5036,ERROR,ERROR,5036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"t org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123); Caused by: com.intel.genomicsdb.GenomicsDBException: Could not load genomicsdb native library; 	at com.intel.genomicsdb.GenomicsDBImporter.<clinit>(GenomicsDBImporter.java:72); 	... 37 more; ```. if you dig into it more you get down to the following error:; ```; /private/var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/libtiledbgenomicsdb6159269479234619546.dylib: dlopen(/private/var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/libtiledbgenomicsdb6159269479234619546.dylib, 1): ; Library not loaded: /opt/local/lib/libuuid.16.dylib; Referenced from: /private/var/folders/xt/vq7wz8955r1401mv8w0f4zf9qbfwzl/T/libtiledbgenomicsdb6159269479234619546.dylib; Reason: image not found; ```. It seems like there is a dylib included correctly in the jar, but it's looking for libuuid.16.dylib at runtime. libuuid.16.dylib needs to be statically linked into the GenomicsDB lib.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4062:3232,down,down,3232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4062,2,"['down', 'error']","['down', 'error']"
Availability,"t-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; 14:13:54.570 INFO FeatureManager - Using codec IntervalListCodec to read file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 20",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:3647,Avail,Available,3647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['Avail'],['Available']
Availability,"t.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:13875,ERROR,ERROR,13875,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['ERROR'],['ERROR']
Availability,tCollection - Processing 170805979 bp from intervals; 10:29:22.613 INFO Mutect2 - Done initializing engine; 10:29:22.622 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:29:22.624 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 10:29:22.625 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 10:29:22.625 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 10:29:22.631 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 10:29:22.660 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 10:29:22.660 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 10:29:22.660 INFO IntelPairHmm - Available threads: 40; 10:29:22.660 INFO IntelPairHmm - Requested threads: 4; 10:29:22.660 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 10:29:22.688 INFO ProgressMeter - Starting traversal; 10:29:22.688 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 10:29:32.698 INFO ProgressMeter - 6:378640 0.2 1420 8512.3; 10:29:42.723 INFO ProgressMeter - 6:1034304 0.3 3810 11410.0; 10:29:52.736 INFO ProgressMeter - 6:1705479 0.5 6270 12520.4; 10:30:02.745 INFO ProgressMeter - 6:2543064 0.7 9270 13885.2; 10:30:12.776 INFO ProgressMeter - 6:3144654 0.8 11520 13799.7; 10:30:22.791 INFO ProgressMeter - 6:3912571 1.0 14330 14305.4; 10:30:32.792 INFO ProgressMeter - 6:4678136 1.2 17120 14652.7; 10:30:42.803 INFO ProgressMeter - 6:5436632 1.3 19900 14903.6; 10:30:52.831 INFO ProgressMeter - 6:6213304 1.5 22710 15116.0; 10:31:02.837 INFO ProgressMeter - 6:7019025 1.7 25670 15379.2; 10:31:12.906 INFO ProgressMeter - 6:7571523,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7032:4189,Avail,Available,4189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7032,1,['Avail'],['Available']
Availability,"tConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.6.1-local.jar -version; ```. This request was created from a contribution made by Pryce Turner on July 29, 2022 03:44 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078378372--Could-not-determine-local-host-name-#community\_comment\_7692552841755](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078378372--",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7983:1615,failure,failure,1615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983,1,['failure'],['failure']
Availability,"tFiltration - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-b09; 12:01:06.399 INFO VariantFiltration - Start Date/Time: November 15, 2020 12:01:06 MST PM; 12:01:06.399 INFO VariantFiltration - ------------------------------------------------------------; 12:01:06.399 INFO VariantFiltration - ------------------------------------------------------------; 12:01:06.399 INFO VariantFiltration - HTSJDK Version: 2.23.0; 12:01:06.400 INFO VariantFiltration - Picard Version: 2.22.8; 12:01:06.400 INFO VariantFiltration - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:01:06.400 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:01:06.400 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:01:06.400 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:01:06.400 INFO VariantFiltration - Deflater: IntelDeflater; 12:01:06.400 INFO VariantFiltration - Inflater: IntelInflater; 12:01:06.400 INFO VariantFiltration - GCS max retries/reopens: 20; 12:01:06.400 INFO VariantFiltration - Requester pays: disabled; 12:01:06.400 INFO VariantFiltration - Initializing engine; 12:01:06.830 INFO FeatureManager - Using codec VCFCodec to read file file:///work/mtgraovac_lab/matthew/c_elegans/COOVAR/VCFS/haplotypecaller.vcf; 12:01:06.851 INFO VariantFiltration - Done initializing engine; 12:01:06.928 INFO ProgressMeter - Starting traversal; 12:01:06.928 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 12:01:07.147 INFO ProgressMeter - unmapped 0.0 926 253698.6; 12:01:07.147 INFO ProgressMeter - Traversal complete. Processed 926 total variants in 0.0 minutes.; 12:01:07.172 INFO VariantFiltration - Shutting down engine; [November 15, 2020 12:01:07 MST PM] org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=209715200. ```. However, the same record remains in the output file, despite it's `MQ=30.51`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6960:3782,down,down,3782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6960,1,['down'],['down']
Availability,"tHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@642ec6{/stages/stage/kill,null,AVAILABLE,@Spark}; 10:33:07.389 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3fe5ad73{/metrics/json,null,AVAILABLE,@Spark}; 10:33:07.397 INFO SortSamSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 10:33:07.450 INFO GoogleHadoopFileSystemBase - GHFS version: 1.9.4-hadoop3; 10:33:08.183 INFO MemoryStore - Block broadcast_0 stored as values in memory (estimated size 268.7 KiB, free 1076.2 GiB); 10:33:08.581 INFO MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 1076.2 GiB); 10:33:08.585 INFO BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:47400,AVAIL,AVAILABLE,47400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,"tPercentileThreshold 2.5 --truncatePercentileThreshold 0.1 --numberOfEigensamples auto --noQC false --dryRun false --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false; [June 23, 2017 6:54:00 PM UTC] Executing as root@b4f42b5ba157 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_91-8u91-b14-1~bpo8+1-b14; Version: 4.alpha.2-1134-ga9d9d91-SNAPSHOT; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; [June 23, 2017 6:54:09 PM UTC] org.broadinstitute.hellbender.tools.exome.CreatePanelOfNormals done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=1400373248; ***********************************************************************. A USER ERROR has occurred: Bad input: The number of zeros per count column is too large resulting in all count columns to be dropped. ***********************************************************************; Use -DSTACK_TRACE_ON_USEREXCEPTION to print the stack trace.; ```. I am running the tool with parameters that should be the standard, i.e. with QC, unlike the settings in our repo's WDL scripts ([WDL](https://github.com/broadinstitute/gatk/blob/502fd4119ebde964d24d39aafd1b7346ac5d84d5/scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl#L137), [JSON](https://github.com/broadinstitute/gatk/blob/56e6baa79b4e56ebee5fb8d2b2288373a4269fa8/scripts/cnv_cromwell_tests/somatic/cnv_somatic_panel_wes_workflow.json#L9)). The command I use is:; ```; 	command {; 	/usr/shlee/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; 		--javaOptions ""-Xmx16g"" \; 		CreatePanelOfNormals \; 	-I ${combined_coverage} \; 	-O ${basename}.pon ${additional_options} \; 	--disableSpark; 	}; ```. Where there are no additional options. ---; The coverage counts were processed with",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3163:2306,ERROR,ERROR,2306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163,1,['ERROR'],['ERROR']
Availability,"tPileupSummaries -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GetPileupSummaries is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:35:17.121 INFO GetPileupSummaries - Initializing engine; 14:35:17.456 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/data/gnomad/vcf/genomes/liftover_grch38/gnomad.b38.biallelic_only.concat.sorted.filtered.vcf.gz; 14:35:17.586 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/data/gnomad/vcf/genomes/liftover_grch38/gnomad.b38.biallelic_only.concat.sorted.filtered.vcf.gz; 16:39:08.359 INFO IntervalArgumentCollection - Processing 236373212 bp from intervals; 16:41:01.520 INFO GetPileupSummaries - Done initializing engine; 16:41:01.521 INFO ProgressMeter - Starting traversal; 16:41:01.521 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 02:44:42.116 INFO GetPileupSummaries - Shutting down engine; [April 25, 2019 2:44:42 AM UTC] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 729.42 minutes.; Runtime.totalMemory()=23243784192; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.grow(ArrayList.java:265); at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:239); at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:231); at java.util.ArrayList.add(ArrayList.java:462); at htsjdk.samtools.BinningIndexContent.getChunksOverlapping(BinningIndexContent.java:131); at htsjdk.samtools.CachingBAMFileIndex.getSpanOverlapping(CachingBAMFileIndex.java:75); at htsjdk.samtools.BAMFileReader.getFileSpan(BAMFileReader.java:935); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:952); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5918:3150,down,down,3150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918,1,['down'],['down']
Availability,"tUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:34996,failure,failure,34996,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['failure'],['failure']
Availability,"t_gatk_variantEval.txt`. I got the following error; ```; !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: VariantEval is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:37:10.767 INFO VariantEval - Initializing engine; 14:37:11.138 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/justinzhang/daiichi/SimpleExample.vcf.gz; 14:37:11.268 INFO VariantEval - Done initializing engine; 14:37:11.278 INFO VariantEval - Creating 3 combinatorial stratification states; 14:37:11.281 INFO ProgressMeter - Starting traversal; 14:37:11.282 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:37:11.454 INFO ProgressMeter - unmapped 0.0 250 87719.3; 14:37:11.454 INFO ProgressMeter - Traversal complete. Processed 250 total variants in 0.0 minutes.; 14:37:11.454 INFO VariantEval - Finalizing variant report; 14:37:11.455 INFO VariantEval - Shutting down engine; [October 11, 2019 2:37:11 PM EDT] org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=386924544; **java.lang.NullPointerException**; 	at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.getnProcessedLoci(VariantEval.java:822); 	at org.broadinstitute.hellbender.tools.walkers.varianteval.evaluators.CountVariants.finalizeEvaluation(CountVariants.java:184); 	at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.onTraversalSuccess(VariantEval.java:709); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1050); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Mai",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6212:1057,down,down,1057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6212,1,['down'],['down']
Availability,"t` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:1041,ERROR,ERROR,1041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"ta--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unmerged.bam; ```. Merge unmapped and aligned BAMs:; ```; gatk-4.2.1.0/gatk \; MergeBamAlignment \; -ALIGNED_BAM aligned.unmerged.bam \; -UNMAPPED_BAM unmapped.bam \; -OUTPUT aligned.unsorted.bam \; -SORT_ORDER ""unsorted"" \; -REFERENCE_SEQUENCE \; Homo_sapiens_assembly38.fasta; ```. This produces the error:; ```; java.lang.IllegalStateException: Aligned record iterator (NB500989:333:HKYJNAFX2:1:11101:10000:1915) is behind the unmapped reads (NB500989:333:HKYJNAFX2:1:11101:24447:1024); 	at picard.sam.AbstractAlignmentMerger.mergeAlignment(AbstractAlignmentMerger.java:557); 	at picard.sam.SamAlignmentMerger.mergeAlignment(SamAlignmentMerger.java:186); 	at picard.sam.MergeBamAlignment.doWork(MergeBamAlignment.java:368); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Despite the fact that option `-SORT_ORDER ""unsorted""` is being used and the two BAM files have the reads in the same order:; ```; $ samtools view unmapped.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7398:2960,error,error,2960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398,1,['error'],['error']
Availability,taminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:6466,Error,ErrorProbabilities,6466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['Error'],['ErrorProbabilities']
Availability,"tarted o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f96dd64{/executors/threadDump,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409732fb{/executors/threadDump/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e99e2cb{/static,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@478967eb{/,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f2b39a{/api,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18c880ea{/jobs/job/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6afbe6a1{/stages/stage/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:56 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.4:4040; 18/01/09 18:30:56 INFO spark.SparkContext: Added JAR file:/opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar at spark://192.168.1.4:38793/jars/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar with timestamp 1515493856032; 18/01/09 18:30:56 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 18/01/09 18:30:57 INFO client.RMProxy: Connecting to ResourceManager at tele-1/192.168.1.4:8032; 18/01/09 18:30:57 INFO yarn.Client: Requesti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:9936,AVAIL,AVAILABLE,9936,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"tations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:01:26.067 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 09:01:26.067 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:01:26.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:01:26.078 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:01:26.089 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:01:26.089 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:01:26.090 INFO IntelPairHmm - Available threads: 36; 09:01:26.090 INFO IntelPairHmm - Requested threads: 4; 09:01:26.090 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:01:26.121 INFO ProgressMeter - Starting traversal; 09:01:26.121 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:01:26.406 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 1A:145 and possibly subsequent; at least 10 samples must have called genotypes; 09:01:33.373 WARN DepthPerSampleHC - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:33.374 WARN StrandBiasBySample - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample SRR9851087 is not called; 09:01:36.316 INFO ProgressMeter - 1A:2054431 0.2 7310 43025.3; 09:01:46.831 INFO ProgressMeter - 1A:3580946 0.3 12960 37547.1; 09:01:56.858 INFO ProgressMeter - 1A:4888",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192:4014,Avail,Available,4014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192,1,['Avail'],['Available']
Availability,te VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it reproducible with any particular imput... it seems that one has to simulate the IO errors for example by using a nearly full storage for the output or create so,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:4581,error,error,4581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,1,['error'],['error']
Availability,te/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:1375,avail,available,1375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,1,['avail'],['available']
Availability,tect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:22274,Recover,Recovered,22274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"tect2/vcf --f1r2-tar-gz f1r2.tar.gz --native-pair-hmm-threads 4 --bam-output tumor.recalibrated.realigned.bam --add-output-sam-program-record false -bam-output. The log of the command that generated the error was :. Using GATK jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar. Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar FilterAlignmentArtifacts --variant tumor.recalibrated.filtered.vcf --input tumor.recalibrated.realigned.bam --reference /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/genome/Homo_sapiens.UCSC.hg38.fa --bwa-mem-index-image /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/bwa_index_img/Homo_sapiens.UCSC.hg38.img --output tumor.recalibrated.filtered2.vcf --bam-output tumor.recalibrated.realigned2.bam --verbosity ERROR --tmp-dir TMP --QUIET true. 14:38:44.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so. 14:38:44.103 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation. java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!. at htsjdk.samtools.SAMFileHeader.addProgramRecord(SAMFileHeader.java:202). at htsjdk.samtools.SAMTextHeaderCodec.parsePGLine(SAMTextHeaderCodec.java:158). at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:107). at htsjdk.samtools.SAMFileHeader.clone(SAMFileHeader.java:398). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriterFromFactory(ReadUtils.java:1215). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriter(ReadUtils.java:1163). at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6287:1624,ERROR,ERROR,1624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6287,1,['ERROR'],['ERROR']
Availability,ted.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL. Command exit status:; 3. Command output:; (empty). Command error:; 23:21:52.354 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:22:02.735 INFO ProgressMeter - chr6:1162012 0.2 25000 144494.8; 23:22:12.789 INFO ProgressMeter - chr6:2449556 0.3 53000 155623.0; 23:22:23.019 INFO ProgressMeter - chr6:3663394 0.5 82000 160448.7; 23:22:33.257 INFO ProgressMeter - chr6:4991347 0.7 112000 164291.1; 23:22:43.683 INFO ProgressMeter - chr6:6325045 0.9 141000 164832.0; 23:22:53.824 INFO ProgressMeter - chr6:7646289 1.0 171000 166913.4; 23:23:03.973 INFO ProgressMeter - chr6:9029926 1.2 200000 167553.3; 23:23:14.220 INFO ProgressMeter - chr6:10374988 1.4 229000 167835.2; 23:23:24.322 INFO ProgressMeter - chr6:11782077 1.5 259000 168971.8; 23:23:34.465 INFO ProgressMeter - chr6:13360174 1.7 290000 170404.5; 23:23:44.556 INFO ProgressMeter - chr6:14757971 1.9 319000 170585.2; 23:23:54.657 INFO ProgressMeter - chr6:16217652 2.0 350000 171704.7; 23:24:04.905 INFO ProgressMeter - chr6:17737681 2.2 381000 172461.9; 23:24:15.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8054:2186,error,error,2186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054,1,['error'],['error']
Availability,"telInflater; 15:46:27.296 INFO BaseRecalibrator - GCS max retries/reopens: 20; 15:46:27.296 INFO BaseRecalibrator - Requester pays: disabled; 15:46:27.297 INFO BaseRecalibrator - Initializing engine; 15:46:28.062 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/dbsnp_146.hg38.vcf; 15:46:28.075 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/1000G_phase1.snps.high_confidence.hg38.vcf; 15:46:28.127 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/Mills_and_1000G_gold_standard.indels.hg38.vcf; 15:46:28.213 INFO BaseRecalibrator - Done initializing engine; 15:46:28.216 INFO BaseRecalibrator - Shutting down engine; [2021年1月8日 下午03时46分28秒] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.52 minutes.; Runtime.totalMemory()=1488977920; ***********************************************************************. A USER ERROR has occurred: Number of read groups must be >= 1, but is 0. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Number of read groups must be >= 1, but is 0; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.<init>(BaseRecalibrationEngine.java:96); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.onTraversalStart(BaseRecalibrator.java:144); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7031:4031,ERROR,ERROR,4031,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7031,1,['ERROR'],['ERROR']
Availability,"ten to gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/callset.json; 14:26:53.640 INFO GenomicsDBImport - Complete VCF Header will be written to gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50/vcfheader.vcf; 14:26:53.640 INFO GenomicsDBImport - Importing to workspace - gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50; 14:26:56.113 INFO GenomicsDBImport - Starting batch input file preload; 14:26:57.968 INFO GenomicsDBImport - Finished batch preload; 14:26:57.968 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 15:59:12.833 INFO GenomicsDBImport - Done importing batch 5/6; 15:59:12.833 INFO GenomicsDBImport - Starting batch input file preload; 15:59:13.218 INFO GenomicsDBImport - Finished batch preload; 15:59:13.218 INFO GenomicsDBImport - Importing batch 6 with 14 samples; [TileDB::FileSystem] Error: (write_to_file) GCS: Only the last of the uploadable parts can be less than 5MB, try increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (gzip_write_buffer) Cannot write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; [TileDB::StorageBuffer] Error: (write_buffer) Cannot compress and/or write bytes path=seqr_loader/v0/genomicsdbs/interval_0_outof_50/chr1$1$61698845/__64761969-0f52-4be1-a7c5-264d6dd36465140686419941120_1643299495929/__book_keeping.tdb.gz; 16:39:59.490 INFO GenomicsDBImport - Done importing batch 6/6; 16:40:00.293 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 16:40:00.293 INFO GenomicsDBImport - Shutting down engine; [January 27, 2022 at 4:40:00 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 133.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7653:5811,Error,Error,5811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653,1,['Error'],['Error']
Availability,"ter'; ```. #### Expected behavior. ReadsPipelineSpark should be able to resolve the hdfs file path: `hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`. #### Actual behavior; The tool tries to access: `file:///user/hadoop/gatk/common/human_g1k_v37.20.21.fasta` even when the input is: `hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`. Verified that the file is accesible through hdfs:; ```; (gatk) root@2e738717b9c1:/gatk/mnt# $HADOOP_HOME/bin/hdfs dfs -ls hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta; -rw-r--r-- 3 hadoop supergroup 113008112 2020-07-29 15:54 hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta; ```; When I specify input as: `hdfs://cromwellhadooptest/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`, (i.e. without the port) I get the same error. **Stack trace for this**:; ```; ***********************************************************************; A USER ERROR has occurred: The specified fasta file (file:///user/hadoop/gatk/common/human_g1k_v37.20.21.fasta) does not exist.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: The specified fasta file (file:///user/hadoop/gatk/common/human_g1k_v37.20.21.fasta) does not exist.; at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.checkFastaPath(CachingIndexedFastaSequenceFile.java:173); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:143); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:125); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:110); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730:2440,ERROR,ERROR,2440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730,1,['ERROR'],['ERROR']
Availability,ter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32); 11:54:40.434 [ERROR] [org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:3361,ERROR,ERROR,3361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"ter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. 21/04/13 07:32:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:61",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:11337,failure,failure,11337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['failure'],['failure']
Availability,"ter: IntelDeflater; 12:01:18.571 INFO  GenotypeGVCFs - Inflater: IntelInflater; 12:01:18.571 INFO  GenotypeGVCFs - GCS max retries/reopens: 20; 12:01:18.571 INFO  GenotypeGVCFs - Requester pays: disabled; 12:01:18.571 INFO  GenotypeGVCFs - Initializing engine; 12:01:19.353 INFO  GenomicsDBLibLoader - GenomicsDB native library version : 1.4.4-ce4e1b9; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field InbreedingCoeff  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field MLEAC  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.262 INFO  NativeGenomicsDB - pid=1923139 tid=1923140 No valid combination operation found for INFO field MLEAF  - the field will NOT be part of INFO fields in the generated VCF records; 12:01:33.288 INFO  GenotypeGVCFs - Shutting down engine; [March 1, 2024 at 12:01:33 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.25 minutes.; Runtime.totalMemory()=1130364928; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.  ; content of my callset.json file:. {""callsets"": [{""sample_name"": ""ERR318225"",""row_idx"": 223,""idx_in_file"": 0,""stream_name"": ""ERR318225_stream""},{""sample_name"": ""ERR318226"",""row_idx"": 224,""idx_in_file"": 0,""stream_name"": ""ERR318226_stream""},{""sample_name"": ""ERR4133262"",""row_idx"": 225,""idx_in_file"": 0,""stream_name"": ""ERR4133262_stream""},{""sample_name"": ""ERR4133361"",""row_idx"": 226,""idx_in_file"": 0,""stream_name"": ""ERR4133361_stream""},{""sample_name"": ""ERR4133400"",""row_idx"": 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8709:4986,down,down,4986,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709,1,['down'],['down']
Availability,ter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/; portage$ git lfs pull --include src/main/resources/large; No default remote. Errors logged to /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:14431,ERROR,ERROR,14431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,ter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:6944,ERROR,ERROR,6944,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,ternal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doB,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:5888,ERROR,ERROR,5888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,ternal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:11460,ERROR,ERROR,11460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,ternal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.util.Swapper.swap(Swapper.java:38); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72); 22:05:55.981 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.982 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:10202,ERROR,ERROR,10202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,tervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.   ; ; c) Entire program log:. Using GATK jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar. Running:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF records. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field HaplotypeScore - the field will NOT be part of INFO fields in the generated VCF records. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records. 15:30:47.303 info NativeGenomicsDB - pid=135716 tid=135717 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO f,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7966:2897,ERROR,ERROR,2897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966,1,['ERROR'],['ERROR']
Availability,"test version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. Another question is related to the fasta file:. I downloaded the reference data in the link of [https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37](https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37) , when I noticed that this is an old database, I have already generated GVCF files. It seems like GenotypeGVCFs does not understand the FAI index file. error informaion; ================. \[E::fai\_read\] Could not understand FAI /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai line 1 ; ; \[E::fai\_load3\] Failed to read FASTA index /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai. FAI file; ========. 1 dna:chromosome chromosome:GRCh37:1:1:249250621:1 249250621 52 60 61 ; ; 2 dna:chromosome chromosome:GRCh37:2:1:243199373:1 243199373 253404903 60 61 ; ; 3 dna:chromosome chromosome:GRCh37:3:1:198022430:1 198022430 500657651 60 61 ; ; 4 dna:chromosome chromosome:GRCh37:4:1:191154276:1 191154276 701980507 60 61 ; ; 5 dna:chromosome chromosom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7442:6557,down,downloaded,6557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442,1,['down'],['downloaded']
Availability,tests using the commandline. Removing error codes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/76:38,error,error,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/76,1,['error'],['error']
Availability,"tf; > 15:16:43.926 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; > 15:16:43.937 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:43.938 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:43.939 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; > 15:16:43.946 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:44.093 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v34.pc_transcripts.fa -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode/hg38/gencode.v34.pc_transcripts.fa; > 15:16:54.854 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_fusion.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/cosmic_fusion/hg38/cosmic_fusion.tsv; > 15:16:54.876 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/achilles_lineage_results.import.txt -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/achilles/hg38/achilles_lineage_results.import.txt; > 15:16:54.881 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:14549,error,errors,14549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['error'],['errors']
Availability,"the current repo we are using seems to be down, changing to a different one to unblock us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3443:42,down,down,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3443,1,['down'],['down']
Availability,"the first commit is the fix, the second is a deliberate test failure so we can validate that the fix works when the tests fail",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5108:61,failure,failure,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5108,1,['failure'],['failure']
Availability,"the google genomics API has deprecated all the features we were using,; this includes the reference lookup api, and the google Read data types. removing all google genomics related dependencies; * replacing com.google.cloud.genomics:gatk-tools-java:1.1 with gov.nist.math.jama:gov.nist.math.jama:1.1.1; 	we rely on this transitive dependency, making it a direct dependency instead; * remove com.google.apis:google-api-services-genomics:v1-rev527-1.22.0; * remove com.google.cloud.genomics:google-genomics-utils:v1-0.10. * delete ReferenceAPISource and tests; * delete GoogleGenomicsReadToGATKReadAdapter and tests; * delete CigarConversionUtils and tests. * update other classes to remove references to these types; * improve an error message",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4266:729,error,error,729,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4266,1,['error'],['error']
Availability,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1560:210,failure,failure,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560,2,['failure'],['failure']
Availability,"there is one bug commented out in ValidateSamFileIntegrationTest. The issue is https://github.com/samtools/htsjdk/issues/369, the fix is in https://github.com/samtools/htsjdk/pull/368. SamFileValidator throws NPE on a CRAM file with an invalid sort order. Once that fix is available we can uncomment the test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1138:273,avail,available,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1138,1,['avail'],['available']
Availability,"there was a memory overflow error (either using -Xmx20G/100G/800G with 1TB of physical memory). The same memory error can occur in CombineGVCFs, so I select GenomicsDBImport for genome-merging. This is the code when using GenomicsDBImport, completed successfully.; ```; gatk GenomicsDBImport \; -R $path1/ref/genome.fa --java-options ""-Xmx100g -Xms80g"" \; $(for i in $(ls $path1/sortbam/2/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/4/*.g.vcf.gz); do echo ""--variant $i""; done) \; $(for i in $(ls $path1/sortbam/6/*.g.vcf.gz); do echo ""--variant $i""; done) \; --genomicsdb-workspace-path $path1/DBI \; --tmp-dir $path1/NOHUP/tmp --intervals $path1/chr.list; ```; But when I run the following **GenotypeGVCFs code**: ; ```; gatk --java-options '-Xmx800G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true' GenotypeGVCFs \; -R $path1/ref/genome.fa -V gendb://$path1/DBI \; -O $path1/sortbam/combDBI.vcf.gz --tmp-dir $path1/NOHUP/tmp. ```; **It warns**: [TileDB::ReadState] Error: Cannot read tile from file; Memory map error. ```; 21:02:06.717 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/wtc/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 29, 2023 9:02:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:02:06.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:02:06.864 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 21:02:06.864 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:06.864 INFO GenotypeGVCFs - Executing as wtc@PC10-7742 on Linux v4.4.0-19041-Microsoft amd64; 21:02:06.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 21:02:06.865 INFO GenotypeGVCFs - Start Date/Time: April 29, 2023 9:02:06 PM CST; 21:02:06.86",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8302:1073,Error,Error,1073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8302,2,"['Error', 'error']","['Error', 'error']"
Availability,there's some highly redundant code in those classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1882:20,redundant,redundant,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1882,1,['redundant'],['redundant']
Availability,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7226:818,error,error,818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226,2,['error'],"['error', 'errored']"
Availability,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1512:798,error,error,798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512,1,['error'],['error']
Availability,this includes `SeekableStream.available()`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4350:30,avail,available,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4350,1,['avail'],['available']
Availability,this is enabled for the driving variants of VariantWalker as well as any auxiliary FeatureInput. a genomicsdb workspace is referenced by putting the loader.json that was used to create the arrays as well as a query.json into a directory; this is then specified with a url of the form gendb://path/to/directory; i.e; /myfiles/mygendbfiles/loader.json; /myfiles/mygendbfiles/query.json. ```; SomeVariantWalker -V gendb:///myfiles/mygendbfiles; ```. FeatureWalker isn't yet wired to support gendb urls; performance is untested. invalid input files are likely to result in Segfaults or non-helpful errors. resolves #1647,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1975:594,error,errors,594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975,1,['error'],['errors']
Availability,this is now available on cran again,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8048:12,avail,available,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8048,1,['avail'],['available']
Availability,this should stop the timeout errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1322:29,error,errors,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1322,1,['error'],['errors']
Availability,"this used to lead to complaints when building on some machines ""error: unmappable character for encoding ASCII"". for @davidadamsphd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1011:64,error,error,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1011,1,['error'],['error']
Availability,"tion --create-output-variant-index true > combine/human_combine.log 2>&1. ---. Here the log:; [human_combine.log](https://github.com/broadinstitute/gatk/files/5165640/human_combine.log). 09:10:26.647 INFO IntervalArgumentCollection - Processing 3095677412 bp from intervals; 09:10:26.694 INFO CombineGVCFs - Done initializing engine; 09:10:26.713 INFO ProgressMeter - Starting traversal; 09:10:26.714 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:10:30.685 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location 1:13021 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 09:10:39.543 INFO ProgressMeter - 1:232994 0.2 1000 4676.9; 09:10:51.253 INFO ProgressMeter - 1:688469 0.4 2000 4890.4; 09:11:01.889 INFO ProgressMeter - 1:809005 0.6 3000 5117.3; 09:11:13.838 INFO ProgressMeter - 1:818424 0.8 5000 6366.2; 09:11:16.811 INFO CombineGVCFs - Shutting down engine; [September 3, 2020 at 9:11:16 AM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 1.20 minutes.; Runtime.totalMemory()=107374182400; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1624); 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); 	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.base/java.util.stream.ReferencePipeline.colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6790:1610,down,down,1610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6790,1,['down'],['down']
Availability,"tion ; I think there's a problem with the StrandOddsRatio (SOR) annotation and the `--map-mnp-distance` flag. I'm looking at a small region of NA24143 (one of the GIAB samples). There's a pair of SNPs in very close proximity. When called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independently. Since the first SNP is hom-var and the second is het, I would have expected the SOR value for the MNP call to closely mirror that of the het SNP. My suspicion is that what's going on here is probably that the calculation is bein",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5698:1258,error,errors,1258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698,1,['error'],['errors']
Availability,"tion.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); ```. However, when trying to run the unit tests that failed using commands like:; ```; ./gradlew test --tests VctOutputRendererUnitTest; ```; The same tests will pass. Following the stack trace, I found that several of these failures were because the FeatureManager class threw a GATKException. Per the source code in FeatureManager.java, the exception was thrown because of either an InstantiationException, IllegalAccessException, NoSuchMethodException, or an InvocationTargetException caught when trying to determine candidate codecs for reading a VCF file. The unit test files FeatureDataSourceUnitTest and FeatureManagerUnitTest pass when running the unit tests all at once, and also pass individually. The test files correctly generate under appropriate directories under src/test/resources, as far as I can tell. . Attached is a zip archive of the test results:; [test_results.zip](https://github.com/broadinstitute/gatk/files/5065501/test_results.zip). #### Steps to reproduce; ```; export TEST_TYPE=unit; ./gradlew test; ./gradlew test --tests VcfOutputRendererUnitTest; ```; The above also will give the same results for any of the other affected classes listed above. . #### Expected behavior; I expect unit tests to pas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6748:6359,failure,failures,6359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6748,1,['failure'],['failures']
Availability,tionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:95); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:7458,ERROR,ERROR,7458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,tionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:8391,ERROR,ERROR,8391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,tionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:7539,ERROR,ERROR,7539,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"tional-arguments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.2.0 ; ; b) Exact command used:. ${gatk} PostprocessGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Executing as yangyxt@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86\_64 amd64 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Serve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:1261,error,error,1261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['error'],['error']
Availability,"tionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```; ➜ cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt; ➜ cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7104:1588,avail,available,1588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104,1,['avail'],['available']
Availability,"titute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:168) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:139) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:739) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(PostprocessGermlineCNVCalls.java:485) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalSuccess(PostprocessGermlineCNVCalls.java:456) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1089) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289) ; ; Using GATK jar /home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar Po. If not an error, choose a category for your question(REQUIRED): ; ; a)How do I (......)? ; ; b) What does (......) mean? ; ; c) Why do I see (......)? ; ; d) Where do I find (......)? ; ; e) Will (......) be in future releases?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/181533'>Zendesk ticket #181533</a>)<br>gz#181533</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:7870,error,error,7870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['error'],['error']
Availability,"tive Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:13:58.943 INFO ProgressMeter - chrM:15445 38.3 63 1.6; 12:13:58.946 INFO ProgressMeter - Traversal complete. Processed 63 total regions in 38.3 minutes.; 12:13:59.105 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.7153035790000002; 12:13:59.110 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 1084.6708644550001; 12:13:59.114 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 54.84 sec; 12:13:59.118 INFO Mutect2 - Shutting down engine; [May 31, 2021 12:13:59 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 38.32 minutes.; Runtime.totalMemory()=18715508736; ```. From the log, we see that Mutect2 finished in 40 minutes. In the meanwhile the `g.vcf`, `g.vcf.idx` and `g.vcf.stats` files are generated and contain non-empty contents. However, the program keeps running for hours and still has not finished. Therefore I wonder if Mutect2 is stuck with some post-processing that is less documented. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:23971,down,down,23971,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['down'],['down']
Availability,tive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 7,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:13117,Recover,Recovered,13117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,"tkforums.broadinstitute.org/gatk/discussion/24446/genomicsdbimport-not-completing-for-mixed-ploidy-samples/p1); ----------; I'm attempting to call variants on whole genomes for about 500 illumina paired-end samples with varying ploidy (haploid to tetraploid). I'm running a fairly standard uBam to GVCF pipeline with HaplotypeCaller passed the ploidy information (1,2,3, or 4) in -ERC GVCF mode. I then try to collect the GVCFs using GenomicsDBImport in a batch size of 50 and use GenotypeGVCFs on the combined database. My interval list that is passed to GenomicsDBImport is just each chromosome on a separate line. I'm using GATK v4.1.1.0<br />; <br />; Command:<br />; ```<br />; ${GATK_DIR}/gatk GenomicsDBImport \<br />; --java-options ""-Xmx110g -Xms110g"" \<br />; -R ${REF} \<br />; --variant ${FILE_LIST} \<br />; -L ${SCRIPT_DIR}/GATK_Style_Interval.list \<br />; --genomicsdb-workspace-path ${WORK_DIR}/GenomicsDB_20190912 \<br />; --batch-size 50 \<br />; --tmp-dir=${WORK_DIR}/<br />; ```<br />; <br />; GenomicsDBImport appears to run without error, but only shows progress for the first 6000 bp before moving onto the next batch. When I run select variants on the created database, I only get variants up to position 6716 in the first interval. When I try to run GenotypeGVCF on it, I get a strange error:<br />; htsjdk.tribble.TribbleException: Invalid block size -1570639203<br />; <br />; My first assumption is that one of the gvcf's is malformed from HaplotypeCaller failing after the first 6000 bp, but I've verified that the gvcfs have all completed and have 'validated' them with ValidateVariants using GATK v4.1.3.0. When I grep for the particular position in the sample's gvcfs I don't find anything out of the ordinary. I would use CombineGVCFs, but it fails due to trying to combine mixed ploidies. <br />; <br />; Any ideas on troubleshooting or experience with problems like this?. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275:3984,error,error,3984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275,1,['error'],['error']
Availability,"tmost coordinate when the read begins with an insertion, the desired effect for the start of the interval. #### Steps to reproduce; Use an alignment with a read that begins with an insertion and a BED that specifies an interval that begins at that position. For example, alignment file:; ```; @HD VN:1.6 SO:coordinate; @SQ SN:ref LN:10; @RG ID:foo SM:bar PU:baz PL:ILLUMINA; r001 0 ref 2 40 6I4M * 0 0 AAAAAAAAAA IIIIIIIIII RG:Z:foo; ```; and ref:; ```; >ref; AAAAAAAAAA; ```. and BED file; ```; ref 0 1; ref 1 2; ref 2 3; ref 3 4; ref 4 5; ref 5 6; ref 6 7; ref 7 8; ref 8 9; ref 9 10; ```; Then run BaseRecalibrator and look at the output:; `gatk BaseRecalibrator -I aln.bam -R ref.fa --known-sites sites.bed.gz -O recal.txt`. #### Expected behavior; The output tables should be empty, since every site in our reference (bases 1-10 inclusive) should be skipped. #### Actual behavior; The output tables include the 6 inserted bases, and the cycle covariate values confirm they are the 6 leading inserted bases:; ```; ReadGroup QualityScore CovariateValue CovariateName EventType EmpiricalQuality Observations Errors; baz 40 1 Cycle M 40.0000 1 0.00; baz 40 2 Cycle M 40.0000 1 0.00; baz 40 3 Cycle M 40.0000 1 0.00; baz 40 4 Cycle M 40.0000 1 0.00; baz 40 5 Cycle M 40.0000 1 0.00; baz 40 6 Cycle M 40.0000 1 0.00; ```. ### Minimal BED examples; In the example read, the 1-based reference coordinates of the read bases are `2,2,2,2,2,2,2,3,4,5`.; I've done tests with various BED files and the bug only seems to happen when the interval begins with position 2; when position 2 lies in the middle of the interval or the interval ends with position 2, the insertion is properly skipped. This is consistent with the usage of the GetReadCoordinateForReferenceCoordinate function linked above, so I'm somewhat confident that is the reason for this behavior. A minimalistic BED to reproduce this behavior is:; ```; ref 0 1; ref 1 10; ```; which specifies 1-based inclusive intervals [1,1] and [2,10]. Like ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6385:1995,Error,Errors,1995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6385,1,['Error'],['Errors']
Availability,to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.makeGenotypeCall(GATKVa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4661:868,failure,failure,868,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661,3,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,tor$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:14026,ERROR,ERROR,14026,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"tor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineSpark - Shutting down engine; [April 23, 2018 8:42:03 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=793247744; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:17352,down,down,17352,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['down'],['down']
Availability,"tor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:2142,ERROR,ERROR,2142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['ERROR'],['ERROR']
Availability,"tor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **20/03/05 09:28:58 INFO DAGScheduler: Job 0 failed: count at PathSeqPipelineSpark.java:245, took 63.806676 s** ; **20/03/05 09:28:58 INFO SparkUI: Stopped Spark web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!** ; **20/03/05 09:28:58 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:1342177280+33554432** ; **20/03/05 09:28:58 INFO MemoryStore: MemoryStore cleared** ; **20/03/05 09:28:58 INFO BlockManager: BlockManager stopped** ; **20/03/05 09:28:58 INFO BlockManagerMaster: BlockManagerMaster stopped** ; **20/03/05 09:28:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!** ; **20/03/05 09:28:58 INFO SparkContext: Successfully stopped SparkContext** ; **09:28:58.889 INFO PathSeqPipelineSpark - Shutting down engine** ; **[2020년 3월 5일 (목) 오전 9시 28분 58초] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.25 minutes.** ; **Runtime.totalMemory()=19560660992** ; **org.apache.spark.SparkException: Job aborted due to stage failure: Task 34 in stage 0.0 failed 1 times, most recent failure: Lost task 34.0 in stage 0.0 (TID 34, localhost, executor driver): com.esotericsoftware.kryo.KryoException: Buffer underflow.** ; **at com.esotericsoftware.kryo.io.Input.require(Input.java:199)** ; **at com.esotericsoftware.kryo.io.Input.readLong(Input.java:686)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet.<init>(LongHopscotchSet.java:83)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:527)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:519)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:712)** ; **at org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:42202,down,down,42202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['down'],['down']
Availability,"tps://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L147. Casued because attribute ""oneShotLogger"" is uninitialized. See line (https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L42) and its missing initialization in the constructor method (https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L65). ### Affected version(s); - [ ] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [12/01/2023]. ### Description ; I work as support for a HPC cluster and this bug has affected one of our users, so I won't be able to provide the exact specifics. Long story short, the user reports that for a high enough value of ploidy (20-50), they start getting null pointer exception errors. Here we can see an example of how they launch the program:. ```; gatk --java-options ""-Xmx4g"" HaplotypeCaller \; -I ${bamfile} \; -R ${reference} \; -O ${outpath}/${sample_id}.ploidy_${SLURM_ARRAY_TASK_ID}.output.g.vcf.gz \; -RF MappingQualityReadFilter \; --minimum-mapping-quality 10 \; --max-alternate-alleles 10 \; --max-genotype-count 75000 \; --dont-use-soft-clipped-bases true \; -ploidy ${SLURM_ARRAY_TASK_ID} \; -ERC GVCF; ```. And this is the stack trace obtained when it fails:. ```; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.logging.OneShotLogger.warn(String)"" because ""this.oneShotLogger"" is null; at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:147); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:21",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8158:1144,error,errors,1144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158,1,['error'],['errors']
Availability,"tputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly cons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8555:1569,failure,failures,1569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555,1,['failure'],['failures']
Availability,"tputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly consumed huge amounts of memory. In the below image, there was no memory at ~11:26, and MarkDuplicatesSpark also hangs at that time. ![image](https://github.com/broadinstitute/gatk/assets/34618938/cc9ac23c-2f84-47c3-bbde-335efb325791). Below is",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8555:1809,failure,failures,1809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555,1,['failure'],['failures']
Availability,"tputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly consumed huge amounts of memory. In the below image, there was no memory at ~11:26, and MarkDuplicatesSpark also hangs at that time. ![image](https://github.com/broadinstitute/gatk/assets/34618938/cc9ac23c-2f84-47c3-bbde-335efb325791). Below is the head of the log file showing my command and tool version. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8555:2049,failure,failures,2049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555,1,['failure'],['failures']
Availability,"tputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:38:18.971 INFO BlockManagerInfo - Removed broadcast_10_piece0 on hhnode-ib-16:42186 in memory (size: 1561.7 KiB, free: 17.8 GiB); ```. I have checked the node status and found MarkDuplicatesSpark suddenly consumed huge amounts of memory. In the below image, there was no memory at ~11:26, and MarkDuplicatesSpark also hangs at that time. ![image](https://github.com/broadinstitute/gatk/assets/34618938/cc9ac23c-2f84-47c3-bbde-335efb325791). Below is the head of the log file showing my command and tool version. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/hcaoad/miniconda2/envs/gatk4/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar MarkDuplicatesSpark -I U23_FDSW210237516-1r_H52MYDSX2_L4.namesort.bam -O U23.markdup.sort.bam; 10:38:16.187 INFO NativeLibraryLoader - Loading libgkl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8555:2289,failure,failures,2289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555,1,['failure'],['failures']
Availability,"tq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unmerged.bam; ```. Merge unmapped and aligned BAMs:; ```; gatk-4.2.1.0/gatk \; MergeBamAlignment \; -ALIGNED_BAM aligned.unmerged.bam \; -UNMAPPED_BAM unmap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7398:1805,echo,echo,1805,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398,1,['echo'],['echo']
Availability,"travis is now using the gradlew wrapper, which handles this download for us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/547:60,down,download,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/547,1,['down'],['download']
Availability,"tructure a bit simpler.; -@MartonKN should review, since he wrote PreprocessIntervals and is updating the caller. Added segmentation classes and tests for ModelSegments CNV pipeline.; -I added implementations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand and @jsotobroad (sorry guys, I wasn't sure how to track your contributions while fixing up commits!) I also added tests for both GC/no-GC pair workflows.; -@MartonKN should review to gain familiarity with the WDL. Note that this WDL has already been through many revisions from @meganshand, @jsotobroad, and @LeeTL1220, so hopefully there shouldn't be too much for you t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:1345,down,downsampling,1345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,2,['down'],"['down', 'downsampling']"
Availability,"ts index) for each sample ; ##; ## Description of inputs :; ##; ## ** Runtime ** (requires Docker; to use on-premises without Docker, change gatk4_jar from String to File); ## gatk4_jar: path to the java jar file containing GATK 4 (beta.2 or later) in the specified docker; ## picard_jar: path to retrieve a Picard jar (will be replaced by a docker image in a future version); ## m2_docker, oncotator_docker: docker images to use for GATK4 Mutect2 and for Oncotator; ## preemptible_attempts: how many preemptions to tolerate before switching to a non-preemptible machine (on Google); ##; ## ** Workflow options **; ## intervals: genomic intervals (will be used for scatter); ## scatter_count: number of parallel jobs to generate when scattering over intervals; ## artifact_modes: filtering options; ## m2_extra_args, m2_extra_filtering_args: additional arguments for Mutect2 calling and filtering (optional); ## is_run_orientation_bias_filter: if true, run the orientation bias filter post-processing step; ## is_run_oncotator: if true, annotate the M2 VCFs using oncotator (to produce a TCGA MAF); ##; ## ** Primary inputs **; ## ref_fasta, ref_fasta_index, ref_dict: reference genome, index, and dictionary; ## tumor_bam, tumor_bam_index, and tumor_sample_name: BAM, index and sample name for the tumor sample (sample name used for output naming); ## normal_bam, normal_bam_index, and normal_sample_name: BAM, index and sample name for the normal sample (optional if running tumor-only); ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_index: optional panel of normals in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_index: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_index: VCF of common variants with allele frequencies fo calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## onco_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3341:986,toler,tolerate,986,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341,1,['toler'],['tolerate']
Availability,"ts1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_250to35.g.vcf.gz; 17:52:24.744 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_350to5.g.vcf.gz; 17:52:24.795 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_50to69.g.vcf.gz; 17:52:24.841 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_690to999.g.vcf.gz; 17:53:24.067 INFO CombineGVCFs - Done initializing engine; 17:53:24.121 INFO ProgressMeter - Starting traversal; 17:53:24.122 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 17:53:24.189 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location scaffold1159:34 the annotation MLEAC=[2, 0] was not a numerical value and was ignored; 17:53:31.218 INFO CombineGVCFs - Shutting down engine; [January 11, 2020 5:53:31 PM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 1.15 minutes.; Runtime.totalMemory()=2739404800; java.lang.IllegalStateException: The elements of the input Iterators are not sorted according to the comparator htsjdk.variant.variantcontext.VariantContextComparator; 	at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:107); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6368:4790,down,down,4790,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6368,1,['down'],['down']
Availability,tsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:353); 	... 14 more; Caused by: htsjdk.samtools.util.RuntimeIOException: /home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz has invalid uncompressedLength: -795051631; 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:257); 	at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:132); 	at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:84); 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); 	at java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:300); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6248:5859,avail,available,5859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6248,1,['avail'],['available']
Availability,"tter - Program Args: -T IndelRealigner -R /Users/mac/Desktop/NGS-/TriTrypDB-47_LmajorLV39c5_Genome.fasta -I /Users/mac/Desktop/NGS-/marked_duplicates42-pe.bam -targetIntervals /Users/mac/Desktop/NGS-/42-pe-realigner.intervals -o /Users/mac/Desktop/NGS-/42-pe-idelsrealigner.bam ; INFO 10:47:54,492 HelpFormatter - Executing as mac@MacBook-Air-de-mac.local on Mac OS X 10.15.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_65-b17. ; INFO 10:47:54,493 HelpFormatter - Date/Time: 2020/09/08 10:47:54 ; INFO 10:47:54,494 HelpFormatter - ---------------------------------------------------------------------------------- ; INFO 10:47:54,494 HelpFormatter - ---------------------------------------------------------------------------------- ; ERROR StatusLogger Unable to create class org.apache.logging.log4j.core.impl.Log4jContextFactory specified in jar:file:/Users/mac/Desktop/GenomeAnalysisTK-3.8-0-ge9d806836%204/GenomeAnalysisTK.jar!/META-INF/log4j-provider.properties; ERROR StatusLogger Log4j2 could not find a logging implementation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console...; INFO 10:47:55,875 GenomeAnalysisEngine - Deflater: IntelDeflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Inflater: IntelInflater ; INFO 10:47:55,876 GenomeAnalysisEngine - Strictness is SILENT ; INFO 10:47:56,246 GenomeAnalysisEngine - Downsampling Settings: No downsampling ; INFO 10:47:56,255 SAMDataSource$SAMReaders - Initializing SAMRecords in serial ; INFO 10:47:56,333 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.07 ; ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A USER ERROR has occurred (version 3.8-0-ge9d806836): ; ##### ERROR; ##### ERROR This means that one or more arguments or inputs in your command are incorrect.; ##### ERROR The error message below tells you what is the problem.; ##### ERROR; ##### ERROR If the problem is an invalid argument, please chec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6798:2017,ERROR,ERROR,2017,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6798,1,['ERROR'],['ERROR']
Availability,"tter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:288); Caused by: java.net.URISyntaxException: Illegal character in path at index 15: /media/yoshi/My Book/Aet_v4.0_ChrSeqSplit/HC.KU-2103.raw.snps.indels.g.vcf; at java.net.URI$Parser.fail(URI.java:2848); at java.net.URI$Parser.checkChars(URI.java:3021); at java.net.URI$Parser.parseHierarchical(URI.java:3105); at java.net.URI$Parser.parse(URI.java:3063); at java.net.URI.<init>(URI.java:588); at java.net.URI.create(URI.java:850); ... 19 more; '''. When I ran GenotypeGVCFs using the gvcf, it ran to completion, but threw the same ""java.lang.IllegalArgumentException"" errors at the end. I have submitted a bug report to FTP server.; The name of the uploaded archive file is YoshiM_BugReport.tar.gz.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4657:2948,error,errors,2948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657,1,['error'],['errors']
Availability,tupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:9503,ERROR,ERROR,9503,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"tureSources(FeatureManager.java:209); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:156); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:488); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:380); 	... 14 more; Caused by: java.io.FileNotFoundException: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy); 	at java.io.RandomAccessFile.open0(Native Method); 	at java.io.RandomAccessFile.open(RandomAccessFile.java:316); 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:243); 	at htsjdk.samtools.seekablestream.SeekableFileStream.<init>(SeekableFileStream.java:47); 	at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7059:1821,error,error,1821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059,1,['error'],['error']
Availability,"tute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:38084,down,down,38084,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['down'],['down']
Availability,"typeGVCFs. The set of sample calls in a pair should be derived from WGS and WEx data for the same sample.; * </p>; ```. Does it _have_ to be WGS + WEx? Could it be WGS + WGS or WEx + Wex for example?. ```; * <h3>Output</h3>; * <p>; * A combined VCF with combined calls for each pair of samples specified and de-uniquified sample names.; * </p>; *; * <h3>Examples</h3>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * -o output.vcf; * </pre>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * --uniquified_sample_name NA12878.variant \; * --uniquified_sample_name NA12878.variant2; * -o output.vcf; * </pre>; ```. I don't get what's the difference between the first and second example. . In any case I'm not going to push this through now in light of all the TODOs:. ```; /*TODO: when this tool is moved into protected the following will have to be addressed:; * Do more robust error checking on sample name de-uniquification -- right now checks for pairs of <sampleName>.variantX and <sampleName>.variantY but should be extended to allow tagged VCF input into GenotypeGVCFs, which will produce names like <sampleName>.RODtagName; * Move sample name uniqufication/de-uniquification to SampleListUtils.java; * Check to make sure all genotype attributes are preserved after merge, e.g. allele phasing and genotype filters; * Generalize for all ploidies?; * Change GenotypeGVCFs --uniquifySamples argument from hidden (maybe still keep @advanced?); *; */; ```. ---. @ldgauthier commented on [Mon Nov 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-159059807). 1) Could be any two sets of data, I just did WGS + WEx for GTEx; 2) I think the first example probably should have two -V entries (that have the same sample names) compared with the second that has one input -V that has already uniquified samples. ---. @vdauwera commented on [Mo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2485:1863,robust,robust,1863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485,2,"['error', 'robust']","['error', 'robust']"
Availability,"ublic release version 4.1.8.1; java version ; ```; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. ### Description ; keep get the error message like below. ```; Using GATK jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Dsamjdk.compression_level=5 -Xms10G -jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCallerSpark -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I SRR1573206.GatherBamFiles.bam -O SRR1573206.g.vcf.gz -G StandardAnnotation -G StandardHCAnnotation -G AS_StandardAnnotation -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -ERC GVCF; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 09:38:05.655 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 15, 2020 9:38:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:38:05.911 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 09:38:05.912 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.8.1; 09:38:05.912 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:38:05.912 INFO HaplotypeCallerSpark - Executing as xc278@amarel2.amarel.rutgers.edu on Linux v3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6750:1338,Redundant,Redundant,1338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6750,1,['Redundant'],['Redundant']
Availability,"ufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 719847424 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid11513.log; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f27ebfe7d9a, pid=11455, tid=0x00007f27e87e5700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libfml.6198146539708364717.jnilib+0xed9a] rld_itr_init+0x4a; ```. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fd2680a350c, pid=11685, tid=0x00007fd2b02bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14); # Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libbwa.5694772191018335324.jnilib+0x850c] bwa_mem2idx+0xcc; ```. The underlying issue in these cases is likely either ""out of memory"" or, perhaps in the case of the seg faults, ""file not found"" or ""malformed file"", but we could greatly improve our ability to interpret Travis failures if we were more careful about checking return values from system calls. Eg., in the function below from the BWA bindings we could check the return values of the `mmap()` and `calloc()` calls, and die with an appropriate error message if they fail:. ```; bwaidx_t* jnibwa_openIndex( int fd ) {; struct stat statBuf;; if ( fstat(fd, &statBuf) == -1 ) return 0;; uint8_t* mem = mmap(0, statBuf.st_size, PROT_READ, MAP_SHARED, fd, 0);; close(fd);; bwaidx_t* pIdx = calloc(1, sizeof(bwaidx_t));; bwa_mem2idx(statBuf.st_size, mem, pIdx);; pIdx->is_shm = 1;; mem_fmt_fnc = &fmt_BAMish;; bwa_verbose = 0;; return pIdx;; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3209:1975,fault,faults,1975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209,3,"['error', 'failure', 'fault']","['error', 'failures', 'faults']"
Availability,"ug/' \; HaplotypeCallerSpark \; --reference /projects/rdocking_prj/software/bcbio-nextgen/data/genomes/Hsapiens/hg19/ucsc/hg19.2bit \; --annotation MappingQualityRankSumTest --annotation MappingQualityZero \; --annotation QualByDepth --annotation ReadPosRankSumTest \; --annotation RMSMappingQuality --annotation BaseQualityRankSumTest \; --annotation FisherStrand --annotation MappingQuality \; --annotation DepthPerAlleleBySample --annotation Coverage \; -I /projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/work/align/MOLM13_rep1/MOLM13_rep1-dedup.splitN.bam \; -L /projects/karsanlab/rdocking/KARSANBIO-1254_pipeline/KARSANBIO-1390_rna_seq_runs/data/gatk_debug/chr1_70k.bed \; --interval-set-rule INTERSECTION \; --spark-master local[12] \; --conf spark.local.dir=/projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/debug \; --conf spark.driver.host=localhost \; --conf spark.network.timeout=800 \; --conf spark.executor.heartbeatInterval=100 \; --annotation ClippingRankSumTest --annotation DepthPerSampleHC \; --emit-ref-confidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80 \; --output MOLM13_rep1-chr1-70k-gatk-haplotype.vcf; ```. When I run this command on a single chromosome with `-Xmx94349m`, the command completes successfully, but the resulting VCF header does not contain this expected header line:. ```; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ```. (along with most of the other header lines associated with gVCF output). When I up the memory request to 110g for the same input files, the proper VCF header is present. I discovered this in the context of running GATK within the bcbio pipeline, the original descriptions are at: https://github.com/bcbio/bcbio-nextgen/issues/2375. On the linked issue, I have examples of GATK output from runs that produced correct and incorrect output - please let me know if there's any other information you need. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4821:1441,heartbeat,heartbeatInterval,1441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4821,1,['heartbeat'],['heartbeatInterval']
Availability,uildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); 22:05:55.980 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:8062,ERROR,ERROR,8062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,uildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLaunc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4346,ERROR,ERROR,4346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,uildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:230); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$RunTasksAction.execute(DefaultGradleLauncher.java:227); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:161); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:6074,ERROR,ERROR,6074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"um_calling_processes int]; [--learning_rate float]; [--adamax_beta1 float]; [--adamax_beta2 float]; [--log_emission_samples_per_round int]; [--log_emission_sampling_median_rel_error float]; [--log_emission_sampling_rounds int]; [--max_advi_iter_first_epoch int]; [--max_advi_iter_subsequent_epochs int]; [--min_training_epochs int]; [--max_training_epochs int]; [--initial_temperature float]; [--num_thermal_advi_iters int]; [--convergence_snr_averaging_window int]; [--convergence_snr_trigger_threshold float]; [--convergence_snr_countdown_window int]; [--max_calling_iters int]; [--caller_update_convergence_threshold float]; [--caller_internal_admixing_rate float]; [--caller_external_admixing_rate float]; [--disable_sampler str_to_bool]; [--disable_caller str_to_bool]; [--disable_annealing str_to_bool]; cohort_denoising_calling.6786136740079319091.py: error: unrecognized arguments: --random_seed=1984 --num_samples_copy_ratio_approx=200; 23:44:54.590 INFO GermlineCNVCaller - Shutting down engine; [August 3, 2024 at 11:44:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 1.04 minutes.; Runtime.totalMemory()=2147483648; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 2; Command Line: python /tmp/cohort_denoising_calling.6786136740079319091.py --ploidy_calls_path=/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/ploidy/ploidy-calls --output_calls_path=/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/cohort_all/cohort_30-calls --output_tracking_path=/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/cohort_all/cohort_30-tracking --random_seed=1984 --modeling_interval_list=/tmp/intervals15539986661449841065.tsv --output_model_path=/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/cohort_all/cohort_30-model --enable_explicit_gc_bias_modeling=True --read_count_tsv_files /tmp/V30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:40088,down,down,40088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['down'],['down']
Availability,"umber of reads to be filtered for each step. For the six datasets with simulate.sequencing.error=FALSE, I would expect similar results but with even fewer reads to be filtered for the low-quality or low complexity read filter. #### Actual behavior; For the six datasets with simulate.sequencing.error=TRUE, 8,496 - 18,103 reads were filtered by the low complexity or low quality filter (the Salmonella datasets were on the lower end and the Fusobacterium datasets were on the higher end), 115 - 311 reads were filtered by the host k-mer filter and 886 - 1822 reads were filtered by the duplicate read filter. . The number of reads filtered by the low complexity or low quality filter seemed high to me so I repeated the analysis with simulate.sequencing.error=FALSE. For these six datasets, all 100,000 reads are filtered by the low-quality or low complexity read filter. . #### Steps to reproduce; I wrote the workflow using snakemake and conda. In theory, you should be able to reproduce the error using `snakemake --use-conda`; Snakefile; ```; from os.path import join; import pandas as pd. ATCC25586_CDS_URL = ""ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/007/325/GCF_000007325.1_ASM732v1/GCF_000007325.1_ASM732v1_cds_from_genomic.fna.gz""; SL1344_CDS_URL = ""ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/855/GCF_000210855.2_ASM21085v2/GCF_000210855.2_ASM21085v2_cds_from_genomic.fna.gz""; LT2_CDS_URL = ""ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/006/945/GCF_000006945.2_ASM694v2/GCF_000006945.2_ASM694v2_cds_from_genomic.fna.gz"". CDS_FA = join(""data"", ""{patient}_cds_from_genomic.fa""); SL1344_CDS_FA = CDS_FA.format(patient=""SL1344""); ATCC25586_CDS_FA = CDS_FA.format(patient=""ATCC25586""); LT2_CDS_FA = CDS_FA.format(patient=""LT2""); FQ1_PREFIX = join(""output"", ""simulated_{patient}-{sample}""); FQ1 = join(""output"", ""simulated_{patient}-{sample}_R1.fastq.gz""); pathseq_bam = join(""output"", ""PathSeq"", ""{patient}-{sample}"", ""pathseq.bam""). samples = pd.DataFrame.from_dict({""patient"": [""A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6705:1840,error,error,1840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6705,1,['error'],['error']
Availability,"unTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:42422,down,down,42422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,2,['down'],['down']
Availability,"unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:1719,ERROR,ERROR,1719,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,4,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"up from the most recent GATK 4.1.7 on a WES sample like this:. gatk PileupSpark --spark-runner SPARK --spark-master local[{threads}] --conf ""spark.driver.memory=22g"" -I $DATA/NA12878.proper.wes.md.bam -R $DATA/Homo_sapiens_assembly18.fasta -O /tmp/gatk4s_{threads}.pileup'. mwiewior@Mareks-MacBook-Pro ~ % spark-submit --version; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 2.4.5; /_/; ; Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_252; Branch HEAD. No matter how high I set the max file descriptors (even to 1M); mwiewior@Mareks-MacBook-Pro ~ % ulimit -a; -t: cpu time (seconds) unlimited; -f: file size (blocks) unlimited; -d: data seg size (kbytes) unlimited; -s: stack size (kbytes) 8192; -c: core file size (blocks) 0; -v: address space (kbytes) unlimited; -l: locked-in-memory size (kbytes) unlimited; -u: processes 2048; -n: file descriptors 1000000. I'm keep on getting the following error:. 20/06/06 14:56:35 ERROR Utils: Aborting task; org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file file:///private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta. Error was: Fasta index file could not be opened: /private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta.fai; at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:159); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:125); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:110); at org.broadinstitute.hellbender.engine.ReferenceFileSource.<init>(ReferenceFileSourc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6642:1029,ERROR,ERROR,1029,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6642,1,['ERROR'],['ERROR']
Availability,update error message when sample name in VCF cannot be looked up in sampleMap.tsv,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7074:7,error,error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7074,1,['error'],['error']
Availability,update htsjdk downstream tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3235:14,down,downstream,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235,1,['down'],['downstream']
Availability,update htsjdk to a current snapshot and fix the test failures,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3417:53,failure,failures,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3417,1,['failure'],['failures']
Availability,"updating bams, sams, and cram to sam spec version 1.5 (some invalid bams were not updated); updated interval list headers for bed tests from v 1.4 - 1.5; updating several tests to give a better error message if an index IS present when it's expected to not be",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/763:194,error,error,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/763,1,['error'],['error']
Availability,updating the docker image to fix the failure in CreateFilterSet introduced by my older VSQR-Lite merge. [Successful run here.](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/95307258-02d0-4d33-b9bb-1ba1eaac6bff),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8210:37,failure,failure,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8210,1,['failure'],['failure']
Availability,"ups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --cluster svdev-caller; ```. ========================. On the other hand, we see a similar error if the input is changed to the same file but stored in a google bucket (although the cited cause is different):. ```; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://sv-data-dsde-dev/test_data/smallCram.cram; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from gs://sv-data-dsde-dev/test_data/smallCram.cram; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:381); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:5723,ERROR,ERROR,5723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['ERROR'],['ERROR']
Availability,uralLogUtils.logSumExp(NaturalLogUtils.java:84); 2019-10-29T18:18:04.001194549Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 2019-10-29T18:18:04.001367357Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 2019-10-29T18:18:04.001518160Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 2019-10-29T18:18:04.001673083Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-29T18:18:04.001846904Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-29T18:18:04.002024760Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-29T18:18:04.002140012Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-29T18:18:04.002232542Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-29T18:18:04.002242727Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-29T18:18:04.002292461Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 2019-10-29T18:18:04.002301667Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 2019-10-29T18:18:04.002307019Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-29T18:18:04.002311722Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 2019-10-29T18:18:04.002316449Z 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 2019-10-29T18:18:04.002321526Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabili,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6237:1275,Error,ErrorProbabilities,1275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6237,1,['Error'],['ErrorProbabilities']
Availability,"urceUtils.createAndRegisterFeatureInputs(DataSourceUtils.java:328); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createDataSourceFuncotationFactoriesForDataSources(DataSourceUtils.java:277); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.onTraversalStart(Funcotator.java:774); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1037); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Duplicate key 0, for input source: cadd.config; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:263); at htsjdk.tribble.TribbleIndexedFeatureReader.&lt;init&gt;(TribbleIndexedFeatureReader.java:102); at htsjdk.tribble.TribbleIndexedFeatureReader.&lt;init&gt;(TribbleIndexedFeatureReader.java:127); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:120); at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:350); ... 14 more; Caused by: java.lang.IllegalStateException: Duplicate key 0; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1254); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeInt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:2020,error,error,2020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['error'],['error']
Availability,"used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding all this, if you're happy with the code as it stands, feel free to merge.; Back to you, review done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5153:1201,failure,failure,1201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5153,1,['failure'],['failure']
Availability,using --version results in a bizarre error message,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1293:37,error,error,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1293,1,['error'],['error']
Availability,"using IntelDeflater speeds up writing of bams by at least 15% (latest igzip is even faster, though only at compression level 1). We must have a way to use it. . note: the htsjdk jar does not contain the .so file - it's only available in the zip file",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1585:224,avail,available,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1585,1,['avail'],['available']
Availability,"using full (even sites-only) VCFs for BQSR is heavy, especially in spark, when we broadcast them. They can go > 10GB in size. Really, it's just a few million positions, so we could compress it hugely: say we have 4 million variants to consider (common sites) - that's just 4M*32bits = 16 MB. . This would require creating a special format for this (or finding an existing one that works for this case). note that need to represent indel positions (with start and end, which complicates things) too but they are much less common (1 in 10 compared to snps). Or maybe Using a BloomFilter is the way to go. For a 10^-3 probability of failure and 4 million entries we only need ~7MB of size with 10 hash functions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1407:630,failure,failure,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407,1,['failure'],['failure']
Availability,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:14776,reliab,reliability,14776,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['reliab'],['reliability']
Availability,"ut false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.total",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:3737,ERROR,ERROR,3737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['ERROR'],['ERROR']
Availability,"util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:21 INFO storage.BlockManagerMaster: Removal of executor 9 requested; 18/01/09 18:31:21 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 9; 18/01/09 18:31:21 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 9 from BlockManagerMaster.; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms); 18/01/09 18:31:26 INFO server.AbstractConnector: Stopped Spark@283ab206{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:31:26 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.4:4040; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors; 18/01/09 18:31:26 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 18/01/09 18:31:26 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Stopped; 18/01/09 18:31:26 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/01/09 18:31:26 INFO memory.MemoryStore: MemoryStore cleared; 18/01/09 18:31:26 INFO storage.BlockManager: BlockManager stopped; 18/01/09 18:31:26 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/01/09 18:31:26 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/01/09 18:31:26 INFO spark.SparkContext: Successfully stopped SparkContext; 18:31:26.896 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [January 9, 2018 6:31:26 PM CST] org.broadinstitute.hellbender.tools.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:30153,down,down,30153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['down'],['down']
Availability,ution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:13650,ERROR,ERROR,13650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"utor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:47148,failure,failure,47148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['failure'],['failure']
Availability,"utors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$han",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:2956,failure,failure,2956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['failure'],['failure']
Availability,v1.7.20200521s; 10:25:49.785 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 10:25:49.785 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 10:25:49.785 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 10:25:49.788 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/gencode_xrefseq_v75_37.tsv -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg19/gencode_xrefseq_v75_37.tsv; 10:25:49.789 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/achilles_lineage_results.import.txt -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/achilles/hg19/achilles_lineage_results.import.txt; 10:25:49.789 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/clinvar_20180401.vcf -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 10:25:49.790 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/simple_uniprot_Dec012014.tsv -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/simple_uniprot/hg19/simple_uniprot_Dec012014.tsv; 10:25:49.791 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/clinvar_hgmd.tsv -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv. **The input vcf file was from gatk FilterMutectCalls and just keep indel record**. what's the problem with the program? It have run a long time but without any results and errors. Thank you.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7135:5833,error,errors,5833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135,1,['error'],['errors']
Availability,"va.io.tmpdir=/cromwell_root/tmp.H9t5pC; [December 14, 2017 7:41:30 PM UTC] GenomicsDBImport --genomicsDBWorkspace genomicsdb --batchSize 50 --sampleNameMap /cromwell_root/broad-jg-dev-storage/freimer_dutch_fin_wgs_v1/v1/sample_map --readerThreads 5 --intervals chr1:1-391754 --interval_padding 500 --genomicsDBSegmentSize 1048576 --genomicsDBVCFBufferSize 16384 --overwriteExistingGenomicsDBWorkspace false --consolidate false --validateSampleNameMap false --interval_set_rule UNION --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 0 --cloudIndexPrefetchBuffer 0 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [December 14, 2017 7:41:30 PM UTC] Executing as root@7ca892f01ff3 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.6; [December 14, 2017 7:41:30 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=4116185088; ***********************************************************************. A USER ERROR has occurred: Bad input: Expected a file of format; Sample	File; but found line: I-PAL_FR02_000639 001	gs://broad-gotc-prod-storage/pipeline/G87944/gvcfs/I-PAL_FR02_000639_001.023ca2f7-4fba-4617-9f65-cb989818c858.g.vcf.gz. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3979:2677,ERROR,ERROR,2677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3979,1,['ERROR'],['ERROR']
Availability,va:19); 2019-10-29T18:18:04.002140012Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-29T18:18:04.002232542Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-29T18:18:04.002242727Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-29T18:18:04.002292461Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 2019-10-29T18:18:04.002301667Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 2019-10-29T18:18:04.002307019Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-29T18:18:04.002311722Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 2019-10-29T18:18:04.002316449Z 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 2019-10-29T18:18:04.002321526Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 2019-10-29T18:18:04.002358113Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 2019-10-29T18:18:04.002377342Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 2019-10-29T18:18:04.002383406Z 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 2019-10-29T18:18:04.002431769Z 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 2019-10-29T18:18:04.002441351Z 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 2019-10-29T18:18:04.002446409Z 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 2019-10-29T18:18:04.002493533Z 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 2019-10-29T18:18:04.002503311Z 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6237:2290,Error,ErrorProbabilities,2290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6237,1,['Error'],['ErrorProbabilities']
Availability,"va:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:56:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:56:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:56:39 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:56:39 INFO BlockManager: BlockManager stopped; 18/04/24 17:56:39 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:56:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:56:39 INFO SparkContext: Successfully stopped SparkContext; 17:56:39.758 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:56:39 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=821559296; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:37441,down,down,37441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,2,['down'],['down']
Availability,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:6102,heartbeat,heartbeatInterval,6102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,2,"['error', 'heartbeat']","['error', 'heartbeatInterval']"
Availability,"validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:4474,down,down,4474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['down'],['down']
Availability,"var_20180429_hg38.vcf; 14:24:34.614 INFO DataSourceUtils - Resolved data source file path: file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 14:24:34.617 INFO DataSourceUtils - Resolved data source file path: file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/acmg_lof.tsv -> file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 14:24:35.311 INFO Funcotator - Shutting down engine; [October 29, 2020 2:24:35 PM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2055733248; code: 400; message: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Bad Request""; }; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Bad Request""; }; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:439); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:244); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:241); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at shaded.cloud_nio.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at shaded.cloud_nio.com.google.cloud.RetryHelper.runW",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:7254,down,down,7254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['down'],['down']
Availability,variant.variantcontext.VariantContext.validateStop(VariantContext.java:1401); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1383); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); at picard.util.LiftoverUtils.liftVariant(LiftoverUtils.java:92); at picard.vcf.LiftoverVcf.doWork(LiftoverVcf.java:426); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292). ```. #### Steps to reproduce. Download vcf from here:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz. gatk LiftoverVcf \; -I b37/HG002_SVs_Tier1_v0.6.vcf.gz \; -O b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz \; -CHAIN grch37_to_grch38.over.chain.gz \; --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz \; -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa. #### Expected behavior; The original b37 vcf has a deletion here:. 1 532077 ACATTCATGCTCACTCATACACACCCAGATCATATATACACTCGTGCACACATTCACACTCATACACACCCAAATCATACTCACATTCATGCACACATGTT A; SVLEN=-100;;SVTYPE=DEL;END=532177;sizecat=100to299;. The liftover to hg38 should look like this:; chr1 596697 REF=ACATTCATGCTCACTCATACACACCCAGATCATATATACACTCGTGCACACATTCACACTCATACACACCCAAATCATACTCACATTCATGCACACATGTT; ALT=A; INFO Fields; SVLEN=-100; SVTYPE=DEL;END=596797;sizecat=100to299;. The error message suggests LiftoverVcf is not updating the INFO/END field from 532177 to 596797 and an error is being,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:4011,Down,Download,4011,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,1,['Down'],['Download']
Availability,variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); at picard.util.LiftoverUtils.liftVariant(LiftoverUtils.java:92); at picard.vcf.LiftoverVcf.doWork(LiftoverVcf.java:426); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292). ```. #### Steps to reproduce. Download vcf from here:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz. gatk LiftoverVcf \; -I b37/HG002_SVs_Tier1_v0.6.vcf.gz \; -O b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz \; -CHAIN grch37_to_grch38.over.chain.gz \; --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz \; -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa. #### Expected behavior; The original b37 vcf has a deletion here:. 1 532077 ACATTCATGCTCACTCATACACACCCAGATCATATATACACTCGTGCACACATTCACACTCATACACACCCAAATCATACTCACATTCATGCACACATGTT A; SVLEN=-100;;SVTYPE=DEL;END=532177;sizecat=100to299;. The liftover to hg38 should look like this:; chr1 596697 REF=ACATTCATGCTCACTCATACACACCCAGATCATATATACACTCGTGCACACATTCACACTCATACACACCCAAATCATACTCACATTCATGCACACATGTT; ALT=A; INFO Fields; SVLEN=-100; SVTYPE=DEL;END=596797;sizecat=100to299;. The error message suggests LiftoverVcf is not updating the INFO/END field from 532177 to 596797 and an error is being triggered since the END is before the start. An incorrect INFO/END will cause problems with tabix and other programs. #### Actual behavior; It generates an error when the INFO/END is before the start and aborts.. ----. ## Feature request; Liftover INFO/END . ### Description; ; The INFO/END position also needs to be updated-not just the site position.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:4910,error,error,4910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,3,['error'],['error']
Availability,"veGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field AS_SOR - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field FS - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.528 INFO GenotypeGVCFs - Shutting down engine; [September 23, 2023 at 8:09:23 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2801795072; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:463); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:365); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291); at org.broadinstitute.hellbender.engine.VariantLocusWalker.initialize at org.broadinstitute.hellbender.engine.Varia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8527:5730,down,down,5730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527,1,['down'],['down']
Availability,vents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] Total time: 29.153 secs; ```. ```; root# su - portage; portage$ cd /scratch/var/tmp/portage/sci-biology/gatk-9999/work/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:13938,ERROR,ERROR,13938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"ver VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the error message if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5220:6468,down,down,6468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220,2,"['down', 'error']","['down', 'error']"
Availability,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7672:2471,recover,recover-all-dangling-branches,2471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672,2,"['avail', 'recover']","['avail', 'recover-all-dangling-branches']"
Availability,version bump for reliability,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7284:17,reliab,reliability,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7284,1,['reliab'],['reliability']
Availability,"very possible codec hoping to find one and only one that answers yes to the canDecode(FileName) method call. If none does execution fails saying that there is no code available to deal with the input file; if more than one codec returns true then is supposed to throw another error indicating the ambiguity. The former is likely an user cased error whereas the later is rather a bug as Codec developers seems to be responsible to make sure that such a collision never happens... This has a few draw backs:; - Seems to quasi-force to establish a 1-to-1 assignation of Codecs and file extension names; canDecode documentation encourages use the file name as the way to determine whether the codec can decode or not the file. What if the file is a simple tab separated value file (with some column count and format constrains) and general extensions such as .tab or .tsv seem acceptable names in practice?; - The error message when there is no supporting code does not tell what the problem is; whether the extension of the file (due to the the 1-to-1 name to type quasi-restriction above) or a more complex formatting issue in the file (e.g. required header missing, version not supported ... blah blah). ; - All codecs are tried out even when most won't ever apply. Even if the performance impact should in practice be minimal still may cause several file IO open operations as several Codec do actually peek into the file (e.g. BCF and VCF codecs). ; - Codec developers have to make sure their new codec does not collides with others; it would be better if codec development can be totally independent.; - General file extensions such as .tab , .tsv cannot be used by codecs due to possible collisions constraining users to name their files the way GATK needs them to; ""I don't like people telling what file names a have to use... I'm already placing the correct argument name before the file name. What else you need!"". Proposal:. An annotation to tell what codes to try out, the first one that canDe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184:1066,error,error,1066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184,1,['error'],['error']
Availability,"vletContextHandler@b2e1df3{/stages/stage/json,null,AVAILABLE,@Spark}; 10:33:07.359 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6cf3b3d7{/stages/pool,null,AVAILABLE,@Spark}; 10:33:07.360 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:46679,AVAIL,AVAILABLE,46679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample：. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7315:845,ERROR,ERROR,845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315,5,['ERROR'],['ERROR']
Availability,"was created from a contribution made by Chunyang Bao on June 14, 2021 23:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-](https://gatk.broadinstitute.org/hc/en-us/community/posts/1260803844270-ASEReadCounter-ouputs-only-header-). \--. I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.1.8.1 ; ; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \\ ; ; \-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \\ ; ; ASEReadCounter \\ ; ; \-L scattered.interval\_list \\ ; ; \-R Homo\_sapiens\_assembly19.fasta \\ ; ; \-V 1000G\_phase1.snps.high\_confidence.b37.vcf.gz \\ ; ; \-I downsample\_10k.bam \\ ; ; \-O output.txt --verbosity INFO. c) Entire error log:. 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/broad/software/free/Linux/redhat\_7\_x86\_64/pkgs/gatk\_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. Jun 14, 2021 7:13:26 PM shaded.cloud\_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials. WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see [https://cloud.google.com/docs/authentication/](https://cloud.google.com/docs/authentication/). 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------. 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1. 19:13:26",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7327:1364,error,error,1364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7327,1,['error'],['error']
Availability,"we need a canonical set of tests that we run when we upgrade the cluster. We've been running terasort but it's not enough: 1) it does not run our code and 2) it does not even run java8 (recent config error when 2 nodes were running java7 was undetected). The task here is to write, in readme or in scripts directory, a script or set of scripts that must be run after every change to the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1392:200,error,error,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392,1,['error'],['error']
Availability,"when the cluster is created. The default for this value is `false`, per [here](https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml), which is the version of Hadoop used in Dataproc image version 1.3. If left as `false`, one keeps getting errors like below when requesting reference bases localized to the HDFS attached to the dataproc cluster, regardless if using *.fasta.gz or *.fasta.; ```; 19/07/26 20:15:43 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 20.5 in stage 50.0 (TID 45798, shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-w-2.c.broad-dsde-methods.internal, executor 44): htsjdk.samtools.SAMException: Unable to load chr14(100526932, 100526932) from /reference/Homo_sapiens_assembly38.fasta; 	at htsjdk.samtools.reference.AbstractIndexedFastaSequenceFile.getSubsequenceAt(AbstractIndexedFastaSequenceFile.java:207); 	at htsjdk.samtools.reference.IndexedFastaSequenceFile.getSubsequenceAt(IndexedFastaSequenceFile.java:49); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceHadoopSparkSource.getReferenceBases(ReferenceHadoopSparkSource.java:31); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType.extractRefBases(SvType.java:161); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SimpleSVType$DuplicationTandem.<init>(SimpleSVType.java:190); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.inferSimpleTypeFromNovelAdjacency(ContigChimericAlignmentIterativeInterpreter.java:229); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.lambda$discoverVariantsFromChimeras$610a78cb$1(ContigChimericAlignmentIterativeInterpreter.java:84); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6064:273,error,errors,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064,1,['error'],['errors']
Availability,where to download GATK3?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6373:9,down,download,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6373,1,['down'],['download']
Availability,"with LOD <= -5.0000.; 18:03:57.263 INFO GaussianMixtureModel - Initializing model with 100 k-means iterations...; 18:04:05.276 INFO VariantRecalibratorEngine - Finished iteration 0.; 18:04:07.160 INFO VariantRecalibratorEngine - Finished iteration 5. Current change in mixture coefficients = 0.47495; 18:04:09.021 INFO VariantRecalibratorEngine - Finished iteration 10. Current change in mixture coefficients = 0.07996; 18:04:10.871 INFO VariantRecalibratorEngine - Finished iteration 15. Current change in mixture coefficients = 0.02188; 18:04:12.690 INFO VariantRecalibratorEngine - Finished iteration 20. Current change in mixture coefficients = 0.00815; 18:04:14.555 INFO VariantRecalibratorEngine - Finished iteration 25. Current change in mixture coefficients = 0.00334; 18:04:15.663 INFO VariantRecalibratorEngine - Convergence after 28 iterations!; 18:04:15.938 INFO VariantRecalibratorEngine - Evaluating full set of 3826009 variants...; 18:04:20.008 INFO VariantRecalibrator - Shutting down engine; [July 28, 2021 6:04:20 PM EDT] org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator done. Elapsed time: 7.70 minutes.; Runtime.totalMemory()=105907224576; java.lang.IllegalStateException: Gaussian mean vector does not have the same size as the list of annotations; at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.makeMeansTable(VariantRecalibrator.java:986); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.writeModelReport(VariantRecalibrator.java:887); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:680); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:8737,down,down,8737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['down'],['down']
Availability,"wo different write modes. When `WRITE_AND_COPY` is selected, a temporary .pgen file is created and written to during the running of the tool, and then once all records have been written, a new file is created with the index at the top and the contents of the temporary .pgen file appended to it. When `WRITE_SEPARATE_INDEX` is selected, the index is instead written to a separate .pgi file. The default is `WRITE_AND_COPY`. #### max-alt-alleles; The PGEN format can only support up to 254 alt alleles per site. This argument allows you to specify a limit. The default is the max of 254. Any sites with more alt alleles than the specified max will not be written. #### lenient-ploidy-validation; PGEN is a bit quirky in that it requires samples to be diploid but has a special case for sex chromosomes, which are allowed to be haploid. By default, any attempt to write a record with an unsupported ploidy will result in an exception being thrown. If this flag is used, then ploidy failures will instead be logged and the records will be written as missing. #### writer-log-file; The C++ code in the PGEN writer in PGEN-JNI will log sites that exceed max-alt-alleles and with unsupported ploidy (if lenient-ploidy-validation is set) to the specified log file, if this argument is set. #### allow-empty-pgen; Empty PGEN files are not technically valid PGEN files. However, for parallel processing purposes, it is sometimes helpful to allow the creation of empty files when there are no variants to be written. The GvsExtractCallsetPgenMerged workflow relies on this. If this flag is set and no variants are written, an empty .pgen, .psam, and .pvar.zst file will be written in `onShutdown()`. By default (i.e. if this flag is not set), if there are no variants written, an exception will be thrown. . ### Part 3: GvsExtractCallsetPgenMerged; GvsExtractCallsetPgenMerged is a WDL workflow that calls ExtractCohortToPgen to extract data from GVS and write it to PGEN files, and then merges those PGEN file",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708:4866,failure,failures,4866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708,1,['failure'],['failures']
Availability,"working to setup a singularity container for gatk-4.1.4.0. while preparing the gatk conda environment numpy-1.13.3 ins installed but biopython==1.70 requirement from the pip section of the gatkcondaenv.yml. removes it and install numpy-1.18.1. see relevant part of conda env create -n gatk -f gatk-4.1.4.0/gatkcondaenv.yml 2>&1 | tee log; NB full log is attached : [log.txt](https://github.com/broadinstitute/gatk/files/4091802/log.txt). ```; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... done. Downloading and Extracting Packages. keras-preprocessing- | 36 KB | ########## | 100%; astor-0.8.0 | 46 KB | ########## | 100%; setuptools-36.4.0 | 563 KB | ########## | 100%; termcolor-1.1.0 | 8 KB | ########## | 100%; protobuf-3.11.2 | 635 KB | ########## | 100%; keras-applications-1 | 33 KB | ########## | 100%; readline-6.2 | 606 KB | ########## | 100%; libgfortran-ng-7.3.0 | 1006 KB | ########## | 100%; numpy-1.13.3 | 3.1 MB | ########## | 100%; ```. numpy-1.13.3 is corectly installed . but then . ```; Collecting numpy (from biopython==1.70->-r /root/gatk-4.1.4.0/condaenv.g1uyq0ce.requirements.txt (line 1)); Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB); ```. that does . ```; Found existing installation: numpy 1.13.3; Uninstalling numpy-1.13.3:; Successfully uninstalled numpy-1.13.3; ```. this causes ```gatk DetermineGermlineContigPloidy ```; to exit with an error related to numpy.testing.decorators which is deprecated since numpy 1.15.0 see https://docs.scipy.org/doc/numpy-1.15.0/release.html. ```; Deprecations. Aliases of builtin pickle functions are deprecated, in favor of their unaliased pickle.<func> names:; numpy.loads; numpy.core.numeric.load; numpy.core.numeric.loads; numpy.ma.loads, numpy.ma.dumps; numpy.ma.load, numpy.ma.dump - these functions already failed on python 3 when called with a ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6396:549,Down,Downloading,549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6396,1,['Down'],['Downloading']
Availability,"wup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:1222,Redundant,Redundant,1222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['Redundant'],['Redundant']
Availability,xceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:6779,ERROR,ERROR,6779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,xec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:12908,ERROR,ERROR,12908,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"xec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.HintGCAfterBuild.execute(HintGCAfterBuild.java:44); 22:05:55.983 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_reso",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:12008,ERROR,ERROR,12008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,xecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:8962,ERROR,ERROR,8962,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"xecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:3166,heartbeat,heartbeat,3166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['heartbeat'],['heartbeat']
Availability,"xecutor.java:617); at java.lang.Thread.run(Thread.java:748). 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 1]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 3]; 18/04/23 20:42:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/23 20:42:02 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 11.519 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:14877,ERROR,ERROR,14877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['ERROR'],['ERROR']
Availability,xecutorException: ; python exited with 134; Command Line: python /tmp/training.741160770003597505.py --data_dir /tmp/readTensorDir7357298393069910206/ --output_dir /tmp/readTensorDir7357298393069910206/ --tensor_name read_tensor --annotation_set best_practices --conv_width 5 --conv_height 5 --conv_dropout 0.0 --padding valid --fc_dropout 0.0 --annotation_units 16 --epochs 1 --training_steps 5 --validation_steps 2 --gatk_version 4.1.4.1-11-gaa4eded-SNAPSHOT --id test_read_tensor_model --channels_last --mode train_default_2d_model; Stdout: ; Stderr: 2019-12-09 19:55:04.271829: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA; 2019-12-09 19:55:04.287419: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; terminate called after throwing an instance of 'Xbyak::Error'; what(): code is too big. 	at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNVariantTrain.doWork(CNNVariantTrain.java:214); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6307:1285,Error,Error,1285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6307,1,['Error'],['Error']
Availability,"xtHandler: Started o.s.j.s.ServletContextHandler@5463f035{/jobs/job/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10618775{/environment/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a3e10c{/executors,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e2a6991{/executors/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:8477,AVAIL,AVAILABLE,8477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"y 10 samples at a time using the command --genomicsdb-update-workspace-path and the relative .sample_map file containing the path to my g.vcf.gz and g.vcf.gz.tbi files. I tried running the GenomicsDBImport followed by GenotypeGVCFs using only four sampled and it worked appropriately by generating a .vcf.gz file along with the index .vcf.gz.tbi file. However, when I run GenotypeGVCFs with 222 samples I get the error: A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. The GATK version used is gatk-4.4.0.0 and the command used is the following:. python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz. attaches below also the complete program log. and the content of my callset.json file. Any idea about that?. Thank you very much. Stefano. REQUIRED for all errors and issues:; a) GATK version used: gatk-4.4.0.0; b) Exact command used: python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:;     java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /ho",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8709:1168,error,errors,1168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709,1,['error'],['errors']
Availability,"y large numbers of false positives; > with bad mapping quality and very large normal artifact lods. The depth is; > often high due to mapping issues, which aggravates the problem. We should; > be able to modify our active region determination so that these bad sites; > don't trigger the assembly and likelihoods engines.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/997>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdKeWsrA1DojH_u7JMVCvec1o-zOtks5ryCXbgaJpZM4ND1FU>; > .; >. ---. @LeeTL1220 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296208775). We should leverage that list for CNV tools as well. We get a lot of false positive CNVs in centromeres (particularly chr9). ---. @davidbenjamin commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296262676). @ldgauthier Thank you! I have localized the regions, and that HaplotypeCaller interval list seems to exclude most or all of them (I can't say for sure because I have only localized down to about 100 kb). In our DREAM challenge wgs benchmarks we will lose about one in ten thousand true positives, which I can easily live with. This will save us a lot of time. ---. @davidbenjamin commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296267603). @samuelklee if you're not already aware of this wgs intervals whitelist. ---. @samuelklee commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296268137). Excellent, thanks. Looping in @asmirnov239, @mbabadi, and @achevali. ---. @samuelklee commented on [Wed May 03 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-299050342). Looping in @danielrosebrock and @dlivitz as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2975:3156,down,down,3156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2975,1,['down'],['down']
Availability,"y large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:5601,recover,recovery,5601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,2,['recover'],['recovery']
Availability,"y"">; ##FORMAT=<ID=MFRL,Number=R,Type=Integer,Description=""median fragment length"">; ##FORMAT=<ID=MMQ,Number=A,Type=Integer,Description=""median mapping quality"">; ##FORMAT=<ID=MPOS,Number=A,Type=Integer,Description=""median distance from end of read"">; ##FORMAT=<ID=OBAM,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact modes."">; ##FORMAT=<ID=OBAMRC,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact mode complements."">; ##FORMAT=<ID=OBF,Number=A,Type=Float,Description=""Fraction of alt reads indicating orientation bias error (taking into account artifact mode complement)."">; ##FORMAT=<ID=OBP,Number=A,Type=Float,Description=""Orientation bias p value for the given REF/ALT artifact or its complement."">; ##FORMAT=<ID=OBQ,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for a given REF/ALT error."">; ##FORMAT=<ID=OBQRC,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for the complement of a given REF/ALT error."">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SA_MAP_AF,Number=3,Type=Float,Description=""MAP estimates of allele fraction given z"">; ##FORMAT=<ID=SA_POST_PROB,Number=3,Type=Float,Description=""posterior probabilities of the presence of strand artifact"">; etc..; etc..; etc..; 1 237752 . A G . artifact_in_normal;clustered_events;mapping_quality;panel_of_normals;strand_artifact DP=369;ECNT=3;IN_PON;NLOD=14.51;N_ART_LOD=9.23;POP_AF=0.168;P_GERMLINE=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5158:3411,error,error,3411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5158,1,['error'],['error']
Availability,"y, which includes theano graph compilation. This includes 350 iterations of ADVI, but note that convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=le",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2984:2082,error,errors,2082,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984,1,['error'],['errors']
Availability,"y.; getFivePrimeUtrSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:744); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createUtrFuncotation(GencodeFuncotationFactory.java:1568); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnSingleTranscript(GencodeFuncotationFactory.java:983); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:805); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:789). the deletion that is causing the error is 141 base pairs, and I noticed the length of the contig Funcotator is trying to retrieve (895) is equal to the UTR length + deletion length + 1, 753 + 141 + 1. When I looked at the source code around where the error occurs, I see where the length of the retrieved interval is defined (line 738): . > final SimpleInterval transcriptInterval = new SimpleInterval(; > transcriptMapIdAndMetadata.mapKey,; > transcriptMapIdAndMetadata.fivePrimeUtrStart,; > transcriptMapIdAndMetadata.fivePrimeUtrEnd + extraBases; > );. and the logic for how large that extraBases should be (line 1566):. >final int numExtraTrailingBases = variant.getReference().length() < defaultNumTrail ingBasesForUtrAnnotationSequenceConstruction ? defaultNumTrailingBasesForUtrAnnotationSequenceConst ruction : variant.getReference().length() + 1;. I believe line 1566 is the source of the problem; there is no check that UTR-end + deletion length extends past the end of the transcript. #### Steps to reproduce. download funcotator_dataSources.v1.6.20190124s from Broad FTP server. run funcotator using:. `Funcotator -R /tmp/GRCh38.fa -V broken.vcf -O broken.out.vcf --data-sources-path funcotator_dataSources.v1.6.20190124s/ --output-file-format VCF --ref-ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6345:2256,error,error,2256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6345,1,['error'],['error']
Availability,"y: (((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter); 58 read(s) filtered by: ((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter); 1 read(s) filtered by: (((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter); 1 read(s) filtered by: ((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter); 1 read(s) filtered by: (MappingQualityReadFilter AND MappingQualityAvailableReadFilter); 1 read(s) filtered by: MappingQualityReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter . 03:58:35.812 INFO ProgressMeter - 13:115070262 0.0 4029 203313.7; 03:58:35.812 INFO ProgressMeter - Traversal complete. Processed 4029 total regions in 0.0 minutes.; 03:58:35.839 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 8.397000000000001E-4; 03:58:35.839 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0028144000000000003; 03:58:35.839 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 03:58:35.840 INFO HaplotypeCaller - Shutting down engine; ```. #### Steps to reproduce; Command used:; ```; gatk HaplotypeCaller \; --input sample.bam \; --annotation OrientationBiasReadCounts \; --intervals b37.chr13.bed \; --reference hs37d5.fa \; --output sample.vcf.gz; ```. The processings were executed locally with Docker images `broadinstitute/gatk:4.1.1.0`, `broadinstitute/gatk:4.2.2.0` and `broadinstitute/gatk:4.3.0.0`. Other versions apart from these were not tested. #### Expected behavior; F1R2 and F2R1 computed and specified for each variant in recent versions of GATK. #### Actual behavior; F1R2 and F2R1 are described in the header, but they are not calculated in recent versions. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:14263,down,down,14263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['down'],['down']
Availability,"yBlockTransferService - Server created on 172.20.19.130:43279; 10:33:07.210 INFO BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 10:33:07.214 INFO BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.221 INFO BlockManagerMasterEndpoint - Registering block manager 172.20.19.130:43279 with 1076.2 GiB RAM, BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.225 INFO BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.226 INFO BlockManager - Initialized BlockManager: BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.345 INFO ContextHandler - Stopped o.s.j.s.ServletContextHandler@7074da1d{/,null,STOPPED,@Spark}; 10:33:07.347 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6556471b{/jobs,null,AVAILABLE,@Spark}; 10:33:07.349 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7cdb05aa{/jobs/json,null,AVAILABLE,@Spark}; 10:33:07.351 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5cb76070{/jobs/job,null,AVAILABLE,@Spark}; 10:33:07.352 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@443ac5b8{/jobs/job/json,null,AVAILABLE,@Spark}; 10:33:07.354 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@753e4eb5{/stages,null,AVAILABLE,@Spark}; 10:33:07.355 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@63318b56{/stages/json,null,AVAILABLE,@Spark}; 10:33:07.357 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@462f8fe9{/stages/stage,null,AVAILABLE,@Spark}; 10:33:07.358 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@b2e1df3{/stages/stage/json,null,AVAILABLE,@Spark}; 10:33:07.359 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@6cf3b3d7{/stages/pool,null,AVAILABLE,@Spark}; 10:33:07.360 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:45003,AVAIL,AVAILABLE,45003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,yIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Transformers$4.transform(Transformers.java:169); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77); 11:54:40.434 [ERROR] [org.gradle.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:2990,ERROR,ERROR,2990,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"yground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896 INFO GermlineCNVCaller - Parsing the germline contig ploidy annotation table...; 16:55:24.906 INFO ContigGermlinePloidyAnnotationTableReader - Ploidy tags: SEX_XX, SEX_XY; 16:55:25.056 INFO GermlineCNVCaller - Parsing the copy number transition prior table and initializing the caches...; 16:55:28.634 INFO GermlineCNVCaller - Initializing the EM algorithm workspace...; 16:55:32.861 INFO GermlineCNVCaller - Shutting down engine; [June 12, 2017 4:55:32 PM ACST] org.broadinstitute.hellbender.tools.coveragemodel.germline.GermlineCNVCaller done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=1364721664; org.broadinstitute.hellbender.exceptions.GATKException: Nd4j data type must be set to double for coverage modeller routines to function properly. This can be done by setting JVM system property ""dtype"" to ""double"". Can not continue. Thanks. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/39376#Comment_39376",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3098:2420,down,down,2420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098,1,['down'],['down']
Availability,yo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 30 more; Caused by: java.lang.ClassNotFoundException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$85/2028177366; at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:154); ... 50 more; 19/12/16 07:06:57 Thread-2 INFO ShutdownHookManager: Shutdown hook called; ```. I can reproduce a similar error locally:. ```; gatk HaplotypeCallerSpark \; -R src/test/resources/large/human_g1k_v37.20.21.fasta \; -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam \; --emit-ref-confidence GVCF \; -O out.vcf \; -- \; --spark-runner SPARK \; --spark-master spark://wm419-830:7077; ```; ```; Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IndexOutOfBoundsException: Index -2 out of bounds for length 299; Serialization trace:; positionLock (sun.nio.ch.FileChannelImpl); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); sequenceFile (org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile); val$taskReferenceSequenceFile (org.broadinstitute.hellbender.tools.HaplotypeCallerSpark$1); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:731); 	at com.esotericsoftwa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6341:3320,error,error,3320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6341,1,['error'],['error']
Availability,"ype Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Des",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7315:1864,error,error,1864,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315,1,['error'],['error']
Availability,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8616:2049,fault,faulty,2049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616,1,['fault'],['faulty']
Availability,"ype_count 1024 --sample_ploidy 2 --genotyping_mode DISCOVERY --contamination_fraction_to_filter 0.0 --output_mode EMIT_VARIANTS_ONLY --allSitePLs false --readShardSize 5000 --readShardPadding 100 --minAssemblyRegionSize 50 --maxAssemblyRegionSize 300 --assemblyRegionPadding 100 --maxReadsPerAlignmentStart 50 --activeProbabilityThreshold 0.002 --maxProbPropagationDistance 50 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false --minimumMappingQuality 20; [August 9, 2017 10:13:02 AM AST] Executing as nkathiresan@nsnode11 on Linux 3.10.0-229.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.2-14-g4229219-SNAPSHOT; [INFO] Available threads: 32; [INFO] Requested threads: 1024; [WARNING] Using 32 available threads, but 1024 were requested; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.utils.MathUtils$Log10Cache).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; **[August 11, 2017 12:34:22 PM AST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. **Elapsed time: 3,021.34 minutes.****; Runtime.totalMemory()=57773916160; + /gpfs/software/spark/spark-2.1.0-bin-hadoop2.7//sbin/stop-master.sh. ; Thanks a lot,; With Regards,; Naga. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/10340/gatk-3-7-and-gatk-4-beta2/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631:7466,Avail,Available,7466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631,2,"['Avail', 'avail']","['Available', 'available']"
Availability,"ys: disabled; 09:39:55.561 INFO Mutect2 - Initializing engine; 09:39:56.014 INFO FeatureManager - Using codec BEDCodec to read file file:///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 09:39:56.024 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 09:39:56.032 INFO Mutect2 - Done initializing engine; 09:39:56.044 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:39:56.077 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:39:56.139 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:39:56.139 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:39:56.139 INFO IntelPairHmm - Available threads: 36; 09:39:56.139 INFO IntelPairHmm - Requested threads: 4; 09:39:56.139 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:39:56.146 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 09:39:56.146 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 09:39:56.146 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 09:39:56.148 INFO Mutect2 - Shutting down engine; [July 3, 2020 9:39:56 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2233991168; htsjdk.samtools.util.RuntimeIOException: File not found: mutect2/concatenated_ACC5611A1_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 	at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:451); 	at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695:3757,Avail,Available,3757,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695,1,['Avail'],['Available']
Availability,"ython package archive in /home/axverdier/Tools/GATK4/git/gatk/build/gatkPythonPackageArchive.zip; :createPythonPackageArchive (Thread[Daemon worker Thread 2,5,main]) completed. Took 0.058 secs.; :compileJava (Thread[Daemon worker Thread 2,5,main]) started.; :compileJava; Executing task ':compileJava' (up-to-date check took 0.044 secs) due to:; No history is available.; All input files are considered out-of-date for incremental task ':compileJava'.; Compiling with JDK Java compiler API.; /home/axverdier/Tools/GATK4/git/gatk/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SuspectedTransLocDetector.java:13: warning: [unchecked] unchecked conversion; import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;; ^; required: List<String>; found: List; error: warnings found and -Werror specified; 1 error; 1 warning; :compileJava FAILED; :compileJava (Thread[Daemon worker Thread 2,5,main]) completed. Took 4.116 secs. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileJava'.; > Compilation failed; see the compiler error output for details. * Try:; Run with --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':compileJava'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:5160,FAILURE,FAILURE,5160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,1,['FAILURE'],['FAILURE']
Availability,"z; 15:46:44.076 WARN IndexUtils - Feature file ""snp151common_tablebrowser.bed.bgz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 15:46:44.500 WARN IndexUtils - Feature file ""snp151flagged_tablebrowser.bed.bgz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 15:46:44.798 INFO BaseRecalibrator - Done initializing engine; 15:46:44.936 INFO BaseRecalibrationEngine - The covariates being used here:; 15:46:44.936 INFO BaseRecalibrationEngine - 	ReadGroupCovariate; 15:46:44.937 INFO BaseRecalibrationEngine - 	QualityScoreCovariate; 15:46:44.937 INFO BaseRecalibrationEngine - 	ContextCovariate; 15:46:44.937 INFO BaseRecalibrationEngine - 	CycleCovariate; 15:46:44.953 INFO ProgressMeter - Starting traversal; 15:46:44.953 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 15:46:45.866 INFO BaseRecalibrator - Shutting down engine; [March 7, 2019 3:46:45 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=731381760; java.lang.IllegalArgumentException: fromIndex(64) > toIndex(62); 	at java.util.Arrays.rangeCheck(Arrays.java:113); 	at java.util.Arrays.fill(Arrays.java:3044); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateKnownSites(BaseRecalibrationEngine.java:354); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateSkipArray(BaseRecalibrationEngine.java:322); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:137); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:185); 	at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:91); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:5754,down,down,5754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['down'],['down']
Availability,"zed setting. ### Tool(s) or class(es) involved; GenomicsDBImport v.4.2.6.1 (current). ### Description ; As far as I understand it the joint germline variant calling process is like this (imagine 100 samples):; 1. Call variants using `Haplotypecaller` using the gVCF output flag for each sample; 2. use the multiple gVCFs (1 per sample) and a set of intervals (WGS_intervals.bed as an example) to build a Genomics DB store using `GenomicsDBImport`; 3. Use `GenotypeGVCFs` using the output of `GenomicsDBImport` as the input to consolidate the multiple samples into 1 multi-sample vcf. My question comes from the parallelization/interval splitting during step 2. If I parallelize the GenomicsDBImport across each interval. I would end up with ~300 intervals and subsequently, ~300 GenomicsDB directory paths since I am not adding new samples to an existing DB, then the specified output DB path, ""Must be an empty or non-existent directory"", which will contain the relevant interval calls for the 100 samples. . Am I supposed to use the 300 directory paths as input into a single `GenotypeGVCFs` call? Or process each of the 300 intervals into 300 multi-sample vcf files (each with 100 samples) and then merge those into a single vcf file using `GatherVcfs` or some other merging tool. The examples posted and documentation for `GenomicsDBImport` relay the need for intervals to work effectively, and so does [an old broad lecture recording](https://www.youtube.com/watch?v=XrHt5yBlp80&t=1243s). . Essentially it boils down to when and how to process and merge the same set of samples (100) over the many intervals (300). If I had 300 compute nodes (as an example) I want to parallelize as much of this as possible. so that each node can process an interval set, and at the end of the process I have 1 VCF file with 100 samples covering the entire range of intervals. I hope that was clear. Please let me know if you need any more info, or if I should be asking somewhere else. Thanks in advance!. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7898:1708,down,down,1708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7898,1,['down'],['down']
Availability,"{VariantContext@5509} ""[VC HC0 @ chr#:###551-###560 Q. of type=INDEL alleles=[CTTTTTTTTT*, C] attr={} GT=[] filters="" ; ```; And by chance that last variant (which happens to be supported by all haplotypes present) falls outside of our active region in the padding then we try to draw the variant span based on the first 4 haplotypes by the rules of haplotype expansion we end up making our trimming span `chr#:###326-###555` (note ###555 falls inside the span the 5th haplotype). When we go to trim all of our variant haplotypes (which happen to all have variant #5) they run into this code inside `Haplotype.trim()`:; ```; // note: the following returns null if the bases covering the ref interval start or end in a deletion.; final byte[] newBases = AlignmentUtils.getBasesCoveringRefInterval(newStart, newStop, getBases(), 0, getCigar());. if ( newBases == null || newBases.length == 0 ) { // we cannot meaningfully chop down the haplotype, so return null; return null;; }; ```; For all of our variant haplotypes at this site we find deletions at the end base and throw the whole haplotypes away when we try to trim it. In this particular case it meant we lost real variants in the previous 4 haplotypes as a result. I propose remedying this in one of two ways:; 1) Allow `AlignmentUtils.getBasesCoveringRefInterval()` to return partially spanning haplotypes when there are potentially 'shorter' than the reference haplotype span (this could easily cause all sorts of errors as the later code might not account for those mismatches. ; 2) Make `AlignmentUtils.getBasesCoveringRefInterval()` cheat and paste reference bases at the front or back of the haplotype to make it square with the reference offsets (we should never call or worry about deletions at the ends of haplotypes anyway) ; 3) Try to catch this edge case at the `AssemblyRegionTrimmer.trim()` stage, try to make the trimmer aware that there might be deletions overlapping its boundaries and expand them until there are no more overla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7137:1612,down,down,1612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7137,1,['down'],['down']
Availability,"| || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || | ; 12:11:32.828 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_| ; 12:11:32.828 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_) ; 12:11:32.828 WARN Funcotator - |___/ ; 12:11:32.828 WARN Funcotator - --------------------------------------------------------------------------------; 12:11:32.828 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this ; 12:11:32.828 WARN Funcotator - run was misconfigured. ; 12:11:32.828 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 12:11:32.828 WARN Funcotator - ================================================================================; 12:11:32.829 INFO VcfFuncotationFactory - ClinVar_VCF 20180401 cache hits/total: 0/0; 12:11:32.829 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 12:11:32.830 INFO Funcotator - Shutting down engine; [March 24, 2021 12:11:32 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.22 minutes.; Runtime.totalMemory()=1793064960; Tool returned:; true; (gatk) root@75181703d894:/gatk# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-di",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:17995,down,down,17995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['down'],['down']
Availability,"|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|3	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	1|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|1	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; And indeed, with that `--alleles` input with a single condensed record, HaplotypeCaller runs without error. Additionally, omitting the genotypes also runs without error:; ```; ##fileformat=VCFv4.1; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; 22	27658738	rs145982391	G	GGTTT,GGTTTGTTT	.	PASS	.; 22	27658738	rs374358960	GGTTTGTTT	G	.	PASS	.; ```; So it seems to be the combination of the split-record multiallelic and genotypes in `--alleles` file that is problematic here. Probably an edge case by most definitions (and straightforward to work around by either omitting genotypes or condensing multiallelics into a single record) but I figured it was worth pointing out. I should probably also add that many other split multiallelics seem to be processed fine, without crash, e.g.:; ```; ##fileformat=VCFv4.1; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	HG00096	HG00097	HG00099	HG00100	HG00101	HG00102	HG00103	HG00105	HG00106	HG00107	HG00108	HG00109	HG00110	HG00111	HG00112	HG00113	HG00114	HG00115	HG00116	HG00117	HG00118	HG00119	",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5355:73983,error,error,73983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355,1,['error'],['error']
Availability,~okokok calm down it's just a draft!~. It's alive!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8534:13,down,down,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8534,2,"['alive', 'down']","['alive', 'down']"
Availability,"~~The `ALL_TRANSCRIPTS` output from Funcotator is not properly parsed by the built-in parsing methods for the funcotations.~~. ~~This should be fixed so that these parsing methods will work without producing an error.~~. -----------. It turns out that at least for `ClinVar_VCF_CLNVI`, hashes aren't being properly cleaned (i.e. URL encoded) before writing to the VCF fields. This is bad, because hash is our delimiter for `ALL_TRANSCRIPTS` mode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5671:211,error,error,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5671,1,['error'],['error']
Availability," HaplotypeCaller - \* of the above arguments please manually construct the command.         \* ; ; 22:06:40.415 WARN  HaplotypeCaller - \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* ; ; 22:06:40.437 INFO  HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output ; ; 22:06:40.484 INFO  NativeLibraryLoader - Loading libgkl\_utils.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_utils.so ; ; 22:06:40.485 INFO  NativeLibraryLoader - Loading libgkl\_pairhmm\_omp.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_pairhmm\_omp.so ; ; 22:06:40.515 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; ; 22:06:40.516 INFO  IntelPairHmm - Available threads: 4 ; ; 22:06:40.516 INFO  IntelPairHmm - Requested threads: 4 ; ; 22:06:40.517 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; ; 22:06:40.545 INFO  ProgressMeter - Starting traversal ; ; 22:06:40.545 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Regions Processed   Regions/Minute ; ; 22:06:41.344 WARN  InbreedingCoeff - InbreedingCoeff will not be calculated at position chr4:57843320 and possibly subsequent; at least 10 samples must have called genotypes ; ; 22:06:50.557 INFO  ProgressMeter -        chr4:69816964              0.2                   570           3415.9 ; ; 22:07:00.633 INFO  ProgressMeter -        chr4:74352584              0.3                  1340           4002.4 ; ; 22:07:10.827 INFO  ProgressMeter -        chr4:79856475              0.5                  2370           4695.9 ; ; 22:07:20.846 INFO  ProgressMeter -        chr4:88243684              0.7                  3370           5017.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7741:10583,Avail,Available,10583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741,1,['Avail'],['Available']
Availability,"… as well as excluding log4j 1.x. GKL 0.5.6 now uses the log4j 1.x API for logging, and we use the log4j-1.2-api bridge JAR to redirect to log4j2 implementation. See [here](https://logging.apache.org/log4j/2.0/faq.html#which_jars) for details. This change was made because GATK 3.x uses log4j 1.x, and users were reporting errors in the output. This release fixes those errors. GATK 4 uses log4j2 and, in order to make the API compatible with the GKL, we need to add a dependency on the log4j-1.2-api bridge. Unfortunately, the log4j 1.X JAR is also brought in due to some transitive dependency from another package, which causes conflicts with the log4j-1.2-api bridge package. To solve that, we need to exclude log4j 1.X from the dependencies, and let log4j-1.2-api take care of any calls to the log4j 1.X API, redirecting them to the log4j2 implementation. See [here](https://logging.apache.org/log4j/2.0/faq.html#exclusions) for details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416:323,error,errors,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416,2,['error'],['errors']
Availability,… mismatches is too large. Removes an error that is thrown when the number of mismatches is greater than the number of mismatches/mismatches in the cigar. . It appears that some aligners do this. I am seeing it frequently in soft-clipped reads in TCGA RNA-seq BAMs (maybe a STAR bug?). . I would prefer to remove the thrown error rather than assume the dev will be aware of this issue and will catch it externally.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3639:38,error,error,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3639,2,['error'],['error']
Availability,… over the VCF index if its better. Handle sequence interval validation when no sequence length is available. Fixes https://github.com/broadinstitute/gatk/issues/1999 and the downstream genomeLoc parser validation fallout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2091:99,avail,available,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2091,2,"['avail', 'down']","['available', 'downstream']"
Availability,… test. partial fix for #1042 - reenabled testStackOverFlowPairSetSwap - the failure was due to scoring strategy using by picard (total ref bases) vs spark (sum of quals). Spark did not even have a pluggable scoring strategy. Now it does and the test passes. For @davidadamsphd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1156:77,failure,failure,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1156,1,['failure'],['failure']
Availability,"…bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684:294,heartbeat,heartbeats,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684,1,['heartbeat'],['heartbeats']
Availability,…ceContentsAsFile(....). This gets rid of Hierarchical URI error message.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4723:59,error,error,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723,1,['error'],['error']
Availability,"…group for the contig alignments. To do downstream stuff like correlate breakpoints in copy number calls that are in VCF format, and perhaps eventually put a genotype column in our output VCF, it would be helpful to keep track of the sample name. This PR tries to help do that by 1) validating that input read groups contain reads from only one sample, 2) extracting the sample name for future use, and 3) putting a constructed read group in our aligned assemblies output file that contains the sample name, and tagging all of the alignment records in that file with the read group id. . As part of testing this I added an expected aligned contigs file test to `FindBreakpointEvidenceSparkIntegrationTest`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3726:40,down,downstream,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3726,1,['down'],['downstream']
Availability,"…m for shuffle jobs. This is a first attempt at #1403 to get feedback on the approach. For aggregating tools that don’t have a shuffle (like CountReadsSpark), the existing 10MB per split is an issue since it dramatically slows down processing. Increasing the split size can be done via -bps, but that is not at all obvious and shouldn’t be necessary. The change I’ve made here uses the default split size for Hadoop (which is 128MB on HDFS). For tools that do have a shuffle, I’ve added a -P argument for all of them, which sets the level of parallelism to use for the shuffle. If not set it defaults to one partition per 10MB of input, which is the existing default. Note that for tools that write an output BAM, the level of parallelism set by -P is used for writing a single BAM (the default, since shardedOutput is false), since the reads are first sorted and written to multiple BAM files before finally being merged. Question: there’s a lot of duplicated code here. Would it be a good idea to have a ParallelismArgumentCollection and a ShardedOutputParallel collection? Note that some tools need a -P but not -shardedOutput (e.g. CompareDuplicatesSpark).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1432:227,down,down,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432,1,['down'],['down']
Availability,"⛔ DO NOT MERGE this into the EchoCallset branch ⛔ just yet; it's probably better to address this bug in the current EchoCallset run by adding the two missing partitions to the last group and rerunning the last group only rather than rerunning all the groups with a larger group size. Add in `n_rounds - 1` to the group size expression to include all of the partitions in the set of groups, otherwise we omit the final `n_parts % n_rounds` partitions. Concretely for AoU Echo with 145192 total partitions and 5 rounds:. ```; >>> 145192 // 5; 29038; >>> 29038 * 5; 145190; >>> (145192 + 5 - 1) // 5; 29039; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8785:29,Echo,EchoCallset,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8785,3,['Echo'],"['Echo', 'EchoCallset']"
Deployability,"	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-10-01 02:53:03,81] [info] WorkflowManagerActor WorkflowActor-c55a06f3-abc1-4db1-8e0f-ea0303caab2c is in a terminal state: WorkflowFailedState; [2019-10-01 02:53:07,42] [info] Not triggering log of token queue status. Effective log interval = None; [2019-10-01 02:53:08,41] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-10-01 02:53:12,32] [info] Workflow polling stopped; [2019-10-01 02:53:12,33] [info] 0 workflows released by cromid-876ccf5; [2019-10-01 02:53:12,34] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-10-01 02:53:12,34] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-10-01 02:53:12,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-10-01 02:53:12,34] [info] Aborting all running workflows.; [2019-10-01 02:53:12,34] [info] JobExecutionTokenDispenser stopped; [2019-10-01 02:53:12,35] [info] WorkflowStoreActor stopped; [2019-10-01 02:53:12,35] [info] WorkflowLogCopyRouter stopped; [2019-10-01 02:53:12,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-10-01 02:53:12,35] [info] WorkflowManagerActor All workflows finished; [2019-10-01 02:53:12,35] [info] WorkflowManagerActor stopped; [2019-10-01 02:53:12,65] [info] Connection pools shut down; [2019-10-01 02:53:12,65] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:9364,release,released,9364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['release'],['released']
Deployability, 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.MapReferenceResolver.addWrittenObject(MapReferenceResolver.java:41); 	at com.esotericsoftware.kryo.Kryo.writeReferenceOrNull(Kryo.java:658); 	at co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3303:4646,deploy,deploy,4646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303,1,['deploy'],['deploy']
Deployability, 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:61); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2793:2303,deploy,deploy,2303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793,1,['deploy'],['deploy']
Deployability, 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:137); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:157); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at shade,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:3474,deploy,deploy,3474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,1,['deploy'],['deploy']
Deployability, 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:2192); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsData,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517:2200,deploy,deploy,2200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517,1,['deploy'],['deploy']
Deployability, 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$13.hasNext(I,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:12715,deploy,deploy,12715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['deploy'],['deploy']
Deployability," (Février in french) August (Août) or December (Décembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --disable-tool-default-annotations false --enable-all-annotations false --allow-old-rms-mapping-quality-annotation-data false"",Version=""4.1.9.0"",Date=""5 f<E9>vrier 2021 10:42:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7081:1483,update,updates,1483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081,1,['update'],['updates']
Deployability," - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change bac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:26250,release,release,26250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['release'],['release']
Deployability," - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_412-b08; 02:55:32.063 INFO Funcotator - Start Date/Time: July 12, 2024 2:55:31 AM EDT; 02:55:32.063 INFO Funcotator - ------------------------------------------------------------; 02:55:32.063 INFO Funcotator - ------------------------------------------------------------; 02:55:32.063 INFO Funcotator - HTSJDK Version: 2.15.1; 02:55:32.063 INFO Funcotator - Picard Version: 2.18.2; 02:55:32.063 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:55:32.063 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:55:32.063 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 02:55:32.063 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:55:32.063 INFO Funcotator - Deflater: IntelDeflater; 02:55:32.063 INFO Funcotator - Inflater: IntelInflater; 02:55:32.063 INFO Funcotator - GCS max retries/reopens: 20; 02:55:32.063 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 02:55:32.063 WARN Funcotator - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 02:55:32.063 INFO Funcotator - Initializing engine; 02:55:32.318 INFO FeatureManager - Using codec VCFCodec to read file file:///export2/liuhw/wes_test/Mutect2_filter/K001137N_somatic_filtered.vcf.gz; 02:55:32.459 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 02:55:32.466 INFO Funcotator - Shutting down engine; [July 12, 2024 2:55:32 AM EDT] org.broadinstitute.hellbender.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8913:2436,patch,patch,2436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8913,1,['patch'],['patch']
Deployability," - Picard Version: 2.25.0; 14:48:09.081 INFO GenomicsDBImport - Built for Spark Version: 2.4.5; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:48:09.082 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:48:09.082 INFO GenomicsDBImport - Deflater: IntelDeflater; 14:48:09.082 INFO GenomicsDBImport - Inflater: IntelInflater; 14:48:09.082 INFO GenomicsDBImport - GCS max retries/reopens: 20; 14:48:09.082 INFO GenomicsDBImport - Requester pays: disabled; 14:48:09.082 INFO GenomicsDBImport - Initializing engine; 14:48:09.524 INFO IntervalArgumentCollection - Processing 249250621 bp from intervals; 14:48:09.551 INFO GenomicsDBImport - Done initializing engine; 14:48:09.781 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 14:48:09.782 INFO GenomicsDBImport - Vid Map JSON file will be written to /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/vidmap.json; 14:48:09.782 INFO GenomicsDBImport - Callset Map JSON file will be written to /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/callset.json; 14:48:09.782 INFO GenomicsDBImport - Complete VCF Header will be written to /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/vcfheader.vcf; 14:48:09.782 INFO GenomicsDBImport - Importing to workspace - /scratch/PI/boip/Han/WGS/HK_WGS_5X/GenomicsDB/chr1; 14:48:09.783 INFO ProgressMeter - Starting traversal; 14:48:09.783 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 14:48:09.935 INFO GenomicsDBImport - Starting batch input file preload; 14:48:21.686 INFO GenomicsDBImport - Finished batch preload; 14:48:21.686 INFO GenomicsDBImport - Importing batch 1 with 400 samples; </pre>. I am now trying to solve this by reducing batch size, will update once it finished for batch1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7218:8034,update,update,8034,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218,1,['update'],['update']
Deployability," -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.458 INFO ComposeSTRTableFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:44:55.458 INFO ComposeSTRTableFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:3783,install,install,3783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['install'],['install']
Deployability," 1024 cores). The execution time is almost same as GATK 4 Beta2 ( 50 Hours, 21 min). Please help me, how to reduce the execution time for GATK 4 Beta2 HaplotypeCaller? . Please see this below Spark logs:. + /gpfs/software/spark/spark-2.1.0-bin-hadoop2.7//bin/spark-submit --master spark://nsnode11:6311 --driver-java-options -Dsamjdk.use_async_io_read_samtools=false,-Dsamjdk.use_async_io_write_samtools=true,-Dsamjdk.use_async_io_write_tribble=false,-Dsamjdk.compression_level=1 --conf spark.io.compression.codec=snappy --conf spark.yarn.executor.memoryOverhead=6000 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.userClassPathFirst=true --conf spark.driver.maxResultSize=0 --conf spark.executor.cores=1024 --conf spark.reducer.maxSizeInFlight=100m --conf spark.shuffle.file.buffer=512k --conf spark.akka.frameSize=512 --conf spark.akka.threads=10 --conf spark.executor.memory=50g --conf spark.driver.memory=150g --conf spark.local.dir=/gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2Spark/1024cores/tmp --class org.broadinstitute.hellbender.Main /gpfs/software/genomics/GATK/4b.2/gatk/build/libs/hellbender-spark.jar HaplotypeCaller --reference /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa --input /gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2/bam//NA12892.recal.bam --dbsnp /gpfs/projects/NAGA/naga/SparkTest/SPARKCALLER/REF/dbsnp_138.vcf --emitRefConfidence GVCF --readValidationStringency LENIENT --nativePairHmmThreads 1024 --createOutputVariantIndex true --output NA12892.raw.snps.indels.g.vcf; [August 9, 2017 10:13:02 AM AST] HaplotypeCaller --nativePairHmmThreads 1024 --dbsnp /gpfs/projects/NAGA/naga/SparkTest/SPARKCALLER/REF/dbsnp_138.vcf --emitRefConfidence GVCF --output NA12892.raw.snps.indels.g.vcf --input /gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2/bam//NA12892.recal.bam --readValidationStringency LENIENT --reference /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa --createOutputVariantIndex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631:2745,pipeline,pipeline,2745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631,1,['pipeline'],['pipeline']
Deployability," 12:18:11.388 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 12:18:11.388 INFO Mutect2 - Start Date/Time: April 11, 2018 12:18:10 PM PDT; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:2387,patch,patch,2387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['patch'],['patch']
Deployability," 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0hMTukUGLtk1oTOse4Oj3awHf_exks5rnq1CgaJpZM4JNjE->; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287828851). OK well this workaround should really be moved to a ""validation stringency"" level decision, not a hardcoded hack. . @ronlevine Do you know if this hack is also present in GATK4's equivalent code?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:3975,update,update,3975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['update'],['update']
Deployability," 2.21.2; 09:54:54.732 INFO HaplotypeCaller - Picard Version: 2.21.9; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:54:54.732 INFO HaplotypeCaller - Deflater: IntelDeflater; 09:54:54.732 INFO HaplotypeCaller - Inflater: IntelInflater; 09:54:54.732 INFO HaplotypeCaller - GCS max retries/reopens: 20; 09:54:54.732 INFO HaplotypeCaller - Requester pays: disabled; 09:54:54.732 INFO HaplotypeCaller - Initializing engine; 09:55:05.747 INFO HaplotypeCaller - Shutting down engine; [September 11, 2020 9:55:05 AM CEST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=5152178176; ***********************************************************************. A USER ERROR has occurred: Fasta dict file file:///home/averdier/test/dna-seq-pipeline/Triticum_aestivum_Claire_EIv1.1.dict for reference file:///home/averdier/test/dna-seq-pipeline/Triticum_aestivum_Claire_EIv1.1.fa.gz does not exist.; Please see http://gatkforums.broadinstitute.org/discussion/1601/how-can-i-prepare-a-fasta-file-to-use-as-reference for help creating it. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Command 'java -Xmx100G -jar /opt/gatk/gatk-package-4.1.7.0-local.jar HaplotypeCaller -R Triticum_aestivum_Claire_EIv1.1.fa.gz --sequence-dictionary Triticum_aestivum_Claire_EIv1.1.fa.gz.dict -I ClaireTest_MD.bam -O ClaireTest_MD_NoInter; vals_Output.vcf --stand-call-conf 10 --native-pair-hmm-threads 30' failed with 512. ```. I'm using the local gatk of version 4.1.7.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6808:2733,pipeline,pipeline,2733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6808,2,['pipeline'],['pipeline']
Deployability," 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '18570' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; 23:44:42.124 DEBUG ScriptExecutor - Result: 0; 23:44:42.124 INFO GermlineCNVCaller - Done initializing engine; 23:44:42.126 INFO GermlineCNVCaller - Intervals specified...; 23:44:42.534 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:42.534 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:42.534 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:42.534 DEBUG GenomeLocParser - chr3 (198295559 bp); 23:44:42.535 DEBUG GenomeLocParser - chr4 (190214555 bp); 23:44:42.535 DEBUG GenomeLocParser - chr5 (181538259 bp); 23:44:42.535 DE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:5654,release,release,5654,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['release'],['release']
Deployability," 64-Bit Server VM v1.8.0_201-b09; 18:08:13.874 INFO CombineGVCFs - Start Date/Time: May 18, 2019 6:08:13 PM CST; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - HTSJDK Version: 2.14.3; 18:08:13.874 INFO CombineGVCFs - Picard Version: 2.17.2; 18:08:13.874 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:08:13.875 INFO CombineGVCFs - Deflater: IntelDeflater; 18:08:13.875 INFO CombineGVCFs - Inflater: IntelInflater; 18:08:13.875 INFO CombineGVCFs - GCS max retries/reopens: 20; 18:08:13.875 INFO CombineGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:08:13.875 INFO CombineGVCFs - Initializing engine; 18:08:14.718 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340092.HC.g.vcf.gz; 18:08:14.807 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340093.HC.g.vcf.gz; 18:08:14.852 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340094.HC.g.vcf.gz; 18:08:14.897 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340095.HC.g.vcf.gz; 18:08:14.958 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340096.HC.g.vcf.gz; 18:08:15.003 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947:2044,patch,patch,2044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947,1,['patch'],['patch']
Deployability," ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:14452,pipeline,pipeline,14452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability," = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:2951,INSTALL,INSTALLDIRGCC,2951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGCC']
Deployability," @lucidtronix Any update on this since I heard VQSR got ported to GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287905381). It's unlikely the behavior has changed. For gnomad we used hard filters to; address the problem, which is probably a good global recommendation. On Mar 20, 2017 1:35 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @lucidtronix; > <https://github.com/lucidtronix> Any update on this since I heard VQSR; > got ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLRwdezIkmt3uPqIABWLggVjRN3yks5rnrjegaJpZM4Dt4t7>; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287919609). OK, that makes sense, thanks. Do you want me to migrate the issue to GATK4? Otherwise I'll just close it out here as WONTFIX. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-288223457). Somewhere we need a record of updates to filtering best practices until we; publish a new thing, so yeah, please migrate. On Mar 20, 2017 6:36 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > OK, that makes sense, thanks. Do you want me to migrate the issue to; > GATK4? Otherwise I'll just close it out here as WONTFIX.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287919609>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdIeoOhkvNSNkC_XpxSZTt-kPdFDMks5rnv9RgaJpZM4Dt4t7>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2508:9538,update,updates,9538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508,1,['update'],['updates']
Deployability," @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmTiL0aks5rnq_OgaJpZM4FaLwX>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2507:8049,pipeline,pipelines,8049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507,1,['pipeline'],['pipelines']
Deployability," GenomicsDBImport - Start Date/Time: July 10, 2018 2:57:15 AM EDT; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - ------------------------------------------------------------; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 02:57:15.773 INFO GenomicsDBImport - Picard Version: 2.18.7; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 02:57:15.773 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:57:15.774 INFO GenomicsDBImport - Deflater: IntelDeflater; 02:57:15.774 INFO GenomicsDBImport - Inflater: IntelInflater; 02:57:15.774 INFO GenomicsDBImport - GCS max retries/reopens: 20; 02:57:15.774 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 02:57:15.774 INFO GenomicsDBImport - Initializing engine; 02:57:18.389 INFO IntervalArgumentCollection - Processing 11228744 bp from intervals; 02:57:18.437 INFO GenomicsDBImport - Done initializing engine; Created workspace ../RAW_VCF/my_database; 02:57:18.583 INFO GenomicsDBImport - Vid Map JSON file will be written to ../RAW_VCF/my_database/vidmap.json; 02:57:18.583 INFO GenomicsDBImport - Callset Map JSON file will be written to ../RAW_VCF/my_database/callset.json; 02:57:18.583 INFO GenomicsDBImport - Complete VCF Header will be written to ../RAW_VCF/my_database/vcfheader.vcf; 02:57:18.583 INFO GenomicsDBImport - Importing to array - ../RAW_VCF/my_database/genomicsdb_array; 02:57:18.583 INFO ProgressMeter - Starting traversal; 02:57:18.583 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 02:57:31.082 INFO GenomicsDBImport - Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4994:2455,patch,patch,2455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994,1,['patch'],['patch']
Deployability," GenomicsDBImport - Start Date/Time: March 8, 2018 5:00:53 PM CET; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 17:00:53.772 INFO GenomicsDBImport - Picard Version: 2.17.2; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:00:53.772 INFO GenomicsDBImport - Deflater: IntelDeflater; 17:00:53.772 INFO GenomicsDBImport - Inflater: IntelInflater; 17:00:53.772 INFO GenomicsDBImport - GCS max retries/reopens: 20; 17:00:53.772 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:00:53.772 INFO GenomicsDBImport - Initializing engine; 17:00:54.197 INFO IntervalArgumentCollection - Processing 135534747 bp from intervals; 17:00:54.200 INFO GenomicsDBImport - Done initializing engine; Created workspace /scratch/production/cluengo/genomicsdb/gdbworkspace-gatk; 17:00:54.418 INFO GenomicsDBImport - Vid Map JSON file will be written to gdbworkspace-gatk/vidmap.json; 17:00:54.418 INFO GenomicsDBImport - Callset Map JSON file will be written to gdbworkspace-gatk/callset.json; 17:00:54.418 INFO GenomicsDBImport - Complete VCF Header will be written to gdbworkspace-gatk/vcfheader.vcf; 17:00:54.418 INFO GenomicsDBImport - Importing to array - gdbworkspace-gatk/genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514:2398,patch,patch,2398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514,1,['patch'],['patch']
Deployability," I get a error message, which states that one or more of the ALT allele are actually not in the samples provided. A previous user already found a similar error in ValidateVariants (https://gatk.broadinstitute.org/hc/en-us/community/posts/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-), but then for Haplotypecaller, and you have opened a bugreport to add a feature to ValidateVariants: https://github.com/broadinstitute/gatk/issues/6553. However, it would be nice if you could actually investigate the formatting error. Unfortunately my formatting error isn't the same as reported in the other post. I have 105 error in which the 1st alternative allele is a spanning deletion and the 2nd (and 3rd) is either an indel or snp. It's true that the 2nd and 3rd allele is actually not found in my samples. I even have 7 occurances in which the 1st allele (spanning deletion) has allele frequency 1.00. my code is the following for GenotypeGVCFs:. java -Xms32G -Xmx32G -jar ${gatk4} GenotypeGVCFs -R ${ref} -V ${pipeline}/${name}\_v4.1.6.0.g.vcf.gz -O ${vcf}/${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list 2> ${log}/${name}\_v4.1.6.0\_genotype.log. for ValidateVariants:. java -Xms10G -Xmx10G -jar ${gatk4} ValidateVariants -R ${ref} -V ${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list --warn-on-errors 2> ${log}/${name}\_v4.1.6.0\_genotype\_valivar.log. the warning in ValidateVariants and the site look like this:. 14:12:15.126 WARN ValidateVariants - \*\*\*\*\* Input 1st\_v4.1.6.0.vcf.gz fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position chr\_1:1088200 are not observed at all in the sample genotypes \*\*\*\*\* ; ; chr\_1 1088200 . T \*,TAAAAAAAAAAAA 64.39 . AC=8,0;AF=0.667,0.00;AN=12;DP=118;ExcessHet=3.0103;FS=0.000;InbreedingCoeff=0.4286;MLEAC=7,7;MLEAF=0.583,0.583;MQ=58.73;QD=32.19;SOR=2.303 GT:AD:DP:GQ:PL ./.:9,0,0:9:.:0,0,0,0,0,0 0/0:9,0,0:9:0:0,0,113,0,113,113 ./.:10,0,0:10:.:0,0,0,0,0,0 ./.:5,0,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:1450,pipeline,pipeline,1450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,1,['pipeline'],['pipeline']
Deployability," Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Feature request. ### Tool(s) or class(es) involved. VariantRecalibrator. ### Description. VariantRecalibrator automatically runs the generated Rscript to produce recalibration plots. This is usually good and convenient, but it requires that all *R* dependencies must be installed in the same environment in the current running GATK environment. This is not necessarily the case for sandbox-based package managers e.g. docker or conda. A viable fix on the user's side is to include R dependencies with GATK in e.g. docker or conda. But I think I would prefer if my packages were as independent of each other as possible. It would be great if **VariantRecalibrator had an option to write but not run the Rscript for recalibration plots.** Then, the user can call the Rscript in an appropriate e.g. R conda environment or docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7697:1441,install,installed,1441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7697,1,['install'],['installed']
Deployability," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:12506,Update,Update,12506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,6,"['Update', 'update']","['Update', 'updated']"
Deployability, Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:09:41.679 INFO  Base,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:2392,pipeline,pipeline,2392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability, Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1500,Configurat,ConfigurationResolver,1500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Configurat'],['ConfigurationResolver']
Deployability," Since GC bias is a; > property of the fragments that are pulled by the baits, a reasonable; > measure of ""GC content"" of each bait has to be calculated from the expected; > value of the GC content of the fragments that the bait pulls (not the GC; > content of the baits or targets), and this can be easily calculated from; > the previously obtained empirical distributions.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/914>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qEpyk5wss6qvl653UQo-BAiQWfIks5rdjPNgaJpZM4ME4kq>; > .; >. ---. @mbabadi commented on [Sat Feb 18 2017](https://github.com/broadinstitute/gatk-protected/issues/914#issuecomment-280820307). @yfarjoun, thanks for your comments. On the first two points, I agree. Let me clarify: I was going to use the bait-length and insert-length as _hyperparameters_ of the pdf, where the pdf itself gives the probability of having an insert in a certain configuration relative to the bait. I think the parametrization you proposed, i.e. the distance between nearest ends of insert and bait, is very reasonable since the PDF is going to be reflection-symmetric once averaged over all baits; and you're right, the bait length is constant (77bp for ICE) so we can drop it from the analysis. If the fragment capture efficiency is insensitive to the relative position of the bait sequence in the fragment, we expect the pdf to be approximately uniform (save for boundary effects at the scale of bait length), with the 0.5 x (insert length - bait length) setting the upper bound of the distribution. However, some dependency on the position of the bait is expected: e.g. if the bait sequence is on the dangling end of a fragment, it is less likely to stay bound than if it is in the middle of the fragment. I'm curious to see what comes out (who knows -- maybe another 6-bp periodicity!).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2947:6394,configurat,configuration,6394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947,1,['configurat'],['configuration']
Deployability," VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:37:00.531 INFO GenotypeGVCFs - Deflater: IntelDeflater; 12:37:00.531 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:37:00.531 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:37:00.531 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:37:00.531 INFO GenotypeGVCFs - Initializing engine; 12:37:02.095 INFO FeatureManager - Using codec VCFCodec to read file file:///home-1/cvalenc1@jhu.edu/work/cvalenc1/Paralysis/NEW_ALIGNEMENT/Sample_VCF/Multi.g.vcf; 12:37:03.426 INFO GenotypeGVCFs - Done initializing engine; 12:37:03.683 INFO ProgressMeter - Starting traversal; 12:37:03.683 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 12:37:13.942 INFO ProgressMeter - chr1:1034498 0.2 14000 81887.3; 12:37:24.351 INFO ProgressMeter - chr1:1322991 0.3 41000 119030.3; 12:37:34.716 INFO ProgressMeter - chr1:1926324 0.5 83000 160474.3; 12:37:44.726 INFO ProgressMeter - chr1:3786982 0.7 124000 181273.3; 12:37:45.921 INFO GenotypeGVCFs - Shutting down engine; [July 12, 2018 12:37:45 PM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.76 minutes.; Runtime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5009:2374,patch,patch,2374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009,1,['patch'],['patch']
Deployability," With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 21:47:48.270 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 21:47:48.270 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 21:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:5496,install,install,5496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,1,['install'],['install']
Deployability," [hs_err_pid100.log](https://github.com/broadinstitute/gatk/files/6203288/hs_err_pid100.log); [hs_err_pid164.log](https://github.com/broadinstitute/gatk/files/6203289/hs_err_pid164.log); [hs_err_pid274.log](https://github.com/broadinstitute/gatk/files/6203290/hs_err_pid274.log); [hs_err_pid400.log](https://github.com/broadinstitute/gatk/files/6203291/hs_err_pid400.log); [hs_err_pid482.log](https://github.com/broadinstitute/gatk/files/6203292/hs_err_pid482.log); [hs_err_pid711.log](https://github.com/broadinstitute/gatk/files/6203293/hs_err_pid711.log); [hs_err_pid735.log](https://github.com/broadinstitute/gatk/files/6203294/hs_err_pid735.log); [hs_err_pid801.log](https://github.com/broadinstitute/gatk/files/6203295/hs_err_pid801.log); [hs_err_pid825.log](https://github.com/broadinstitute/gatk/files/6203296/hs_err_pid825.log); [hs_err_pid849.log](https://github.com/broadinstitute/gatk/files/6203297/hs_err_pid849.log); [otherFiles.zip](https://github.com/broadinstitute/gatk/files/6203305/otherFiles.zip); [in2510-8.orientationFilter.vcf.txt](https://github.com/broadinstitute/gatk/files/6203356/in2510-8.orientationFilter.vcf.txt); *VCF extension appended with .txt to satisfy GitHub's upload requirements*. #### Expected behavior; Worked on 7 other files generated with the same pipeline. . #### Actual behavior; Unsure why this last one is causing a segfault. The VCF included is not the whole VCF submitted originally. I went cutting out lines from the original until I could isolate it down to a minimal set required to reproduce the crash (I included all of the crash logs generated in case it can help). I was expecting to find a single line or maybe two that were required to reproduce this issue, but that range appears to be needed. Eliminating either the first or last line from the range will make the program work again. Did not attempt to remove lines from the middle of the range yet to see if they're necessary to cause the fault, but it's 2am and I should probably sleep.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7162:7759,pipeline,pipeline,7759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162,1,['pipeline'],['pipeline']
Deployability," addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; GenotypeGVCFs, /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; ### Affected version(s); - [ 4.1.9.0-4.4.0.0] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; The ParaStor file system suffers from low CPU operating efficiency and extremely slow read and write speeds. If I test it on my own mobile hard drive, it's normal. The file format of my mobile hard disk is EXT4; #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; /public2/home/gaoshibin/software/gatk-4.4.0.0/gatk --java-options ""-Xmx160g -Djava.io.tmpdir=./tmp_fat"" GenotypeGVCFs -R /public2/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR9_gvcf_database -G StandardAnnotation -L 9:1-5000000 -O ./test.vcf.gz --genomicsdb-shared-posixfs-optimizations true; #### Expected behavior; _Tell us what should happen_; The ParaStor file system suffers from",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8546:1654,release,release,1654,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546,1,['release'],['release']
Deployability," already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCF. ### Affected version(s); 4.1.7; 4.1.8; 4.1.9. ### Description ; Starting with GATK4.1.7, the AF annotation in the changed from '0' to '.'. This change is cause downstream issues with our processing pipeline. #### Steps to reproduce; CMD using 4.1.6:; gatk GenotypeGVCFs --variant proband_mother_duo.HC.g.vcf.gz -R hs38DH.fa --dbsnp dbsnp151_common.hg38.vcf.gz -O proband_mother_duo_GATK4.1.6.HC.vcf.gz. Looking at one of the sites causing this downstream issue:; CMD; gzcat proband_mother_duo_GATK4.1.6.HC.vcf.gz | grep 83598622; OUTPUT:; chr4 83598622 . AT ATT,A 1337.45 . AC=1,1;AF=0.250,0.250;AN=4;BaseQRankSum=1.26;ClippingRankSum=0.074;DP=145;ExcessHet=4.7712;FS=5.235;GQ_MEAN=625.00;LikelihoodRankSum=1.34;MLEAC=1,1;MLEAF=0.250,0.250;MQ=60.00;MQ0=0;MQRankSum=0.00;NCC=0;NCount=0;QD=10.06;ReadPosRankSum=1.56;SOR=0.375 GT:AD:AF:DP:F1R2:F2R1:GQ:PL 0/2:33,0,35:0.515,0:68:12,15,0:21,18,0:99:713,812,1542,0,731,625 0/1:32,33,0:0.493,0.00:67:9,23,0:19,8,0:99:640,0,588,728,747,1569. CMD using 4.1.7:; gatk GenotypeGVCFs --variant proband_mother_duo.HC.g.vcf.gz -R hs38DH.fa --dbsnp dbsnp151_common.hg38.vcf.gz -O proband_mother_duo_GATK4.1.7.HC.vcf.gz. Looking at one of t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6938:1430,pipeline,pipeline,1430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6938,1,['pipeline'],['pipeline']
Deployability," another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:8405,pipeline,pipeline,8405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:9274,deploy,deploy,9274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,6,['deploy'],['deploy']
Deployability," been added to a panel that was created when requesting a minimum of 30, 45, or even 50 samples displaying a variant at the same site. #### Steps to reproduce; ```; gatk --java-options ""-Xmx30g"" Mutect2 \; -R /ref/Homo_sapiens_assembly38.fasta \; -I /bams/input/WES_Normal/${infile} \; -max-mnp-distance 0 \; -O /bams/output/${outfile}. gatk --java-options ""-Xmx100g"" GenomicsDBImport \; -R /ref/Homo_sapiens_assembly38.fasta -L /mydir/S33266340_hg38_Regions.bed \; --tmp-dir /scratch/ --genomicsdb-workspace-path ${RAMDISK}/PON_db_50_samples \; --merge-input-intervals true \; -V /bams/output/sample1.vcf.gz -V /bams/output/sample2.vcf.gz [....]. gatk --java-options ""-Xmx10g"" CreateSomaticPanelOfNormals \; -R /ref/Homo_sapiens_assembly38.fasta \; -V gendb://${RAMDISK}/PON_db_50_samples \; --germline-resource /gnomad/gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz \; --min-sample-count 50 \; -O /mydir/output/variants_100percent_samples_PON_50_samples.vcf.gz; ```. Am I missing something? Has this been fixed in more recent releases? . #### Expected behavior; For this chr12:25245348 it should not have been included at all in the PON, or only if I had set --min-sample-count to 4 and the fraction should have been 0.08 (don't know about the beta). `chr12 25245348 . C A,G . . BETA=?,?;FRACTION=0.08`. #### Alternative method to create the panel. After I noticed the problem, I ran bcftools isec to get for all the files a stripped VCF where 90% of them had a mutation at the same position and at least some of the alternative alleles were the same. `bcftools isec -n+45 --collapse some -p /mydir/output/isec ${vcfs}`. This shows me sites that are actually shared by 90% of my samples. Can't I just get chromosome, start, and end from one of these files, set INFO to ""."" and use this as the panel of normals with mutect2? Unfortunately, this seems more accurate than using CreateSomaticPanelOfNormals, though I would have liked to. . Importantly, **it seems like this only affects multiallelic sites.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8916:3728,release,releases,3728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8916,1,['release'],['releases']
Deployability, broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:1575,update,updates,1575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,1,['update'],['updates']
Deployability," dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion for gvs scripts; - testing if the exclusion works. [VS-16]: https://broadworkbench.atlassian.net/browse/VS-16?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8251:33076,Update,Updates,33076,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251,1,['Update'],['Updates']
Deployability," dragen mode.  ; ; \-  --alleles is taken from a normal HC run.  ; ; \-  roughly the same heterozygous calls & hom.ALT calls are made with/without --alleles (which is expected behaviour). \=======================. REQUIRED for all errors and issues: ; ; a) GATK version used: 4.2.5.0. b) Exact command used:. gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller \\ ; ;   -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta \\ ; ;   -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam \\ ; ;   -O results/wesep-229191-f.vcf \\ ; ;   --alleles affected\_alleles.vcf \\ ; ;   -L 0005-scattered.interval\_list \\ ; ;   -bamout results/wesep-229191-f.variants.bam \\ ; ;   -G StandardAnnotation -G StandardHCAnnotation \\ ; ;   --dragen-mode \\ ; ;   --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params \\ ; ;   --native-pair-hmm-threads 2.   ; ; c) Entire program log:. (ELPREP) gvandeweyer@ngsvm-pipelines:~/elprep\_streaming/VariantCalling\_Test/scattered$ gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx3g"" HaplotypeCaller    -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta    -I /home/gvandeweyer/elprep\_streaming/results/wesep- ; ; 229191-f.bam    -O results/wesep-229191-f.vcf    --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.interval\_list    -bamout results/wesep-229191-f.variants.bam    -G StandardAnnotation -G StandardHCAnnotation    --dragen-mode    --dragstr-params- ; ; path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params 2>&1 | tee Runtime.log.txt ; ; Using GATK jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7741:2543,pipeline,pipelines,2543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741,1,['pipeline'],['pipelines']
Deployability," even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing peop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:1675,release,released,1675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,3,['release'],"['released', 'releases']"
Deployability," existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----; Thanks in advance!; ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875:10343,release,release,10343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875,1,['release'],['release']
Deployability," false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 11, 2017 2:19:10 PM CST] Executing as hdfs@mg on Linux 3.10.0-514.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: 4.beta.5-70-gdc3237e-SNAPSHOT; 14:19:10.289 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:19:10.290 INFO PrintReadsSpark - Deflater: IntelDeflater; 14:19:10.290 INFO PrintReadsSpark - Inflater: IntelInflater; 14:19:10.290 INFO PrintReadsSpark - GCS max retries/reopens: 20; 14:19:10.290 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:19:10.290 INFO PrintReadsSpark - Initializing engine; 14:19:10.290 INFO PrintReadsSpark - Done initializing engine; 17/10/11 14:19:10 INFO spark.SparkContext: Running Spark version 1.6.0; 17/10/11 14:19:10 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/11 14:19:10 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/11 14:19:10 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs); 17/10/11 14:19:10 INFO util.Utils: Successfully started service 'sparkDriver' on port 43567.; 17/10/11 14:19:11 INFO slf4j.Slf4jLogger: Slf4jLogger started; 17/10/11 14:19:11 INFO Remoting: Starting remoting; 17/10/11 14:19:11 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.131.101.159:45501]; 17/10/11 14:19:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:2963,patch,patch,2963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['patch'],['patch']
Deployability," false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 3, 2017 5:27:51 AM UTC] Executing as centos@master.novalocal on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.beta.5; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 05:27:52.642 INFO PrintReadsSpark - Deflater: IntelDeflater; 05:27:52.642 INFO PrintReadsSpark - Inflater: IntelInflater; 05:27:52.643 INFO PrintReadsSpark - GCS max retries/reopens: 20; 05:27:52.643 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 05:27:52.643 INFO PrintReadsSpark - Initializing engine; 05:27:52.643 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@dcf3e99] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@61df66b6].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```; I can run com",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3651:3190,patch,patch,3190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3651,1,['patch'],['patch']
Deployability," for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RUNNING; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/6 is now RUNNING; 18/04/24 17:55:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.25:54754) with ID 2; 18/04/24 17:55:07 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.25:41354 with 366.3 MB RAM, BlockManagerId(2, xx.xx.xx.25, 41354, None); 18/04/24 17:55:07 INFO FileInputFormat: Total input paths to process : 1; 18/04/24 17:55:07 INFO SparkContext: Starting job: first at ReadsSparkSource.java:221; 18/04/24 17:55:07 INFO DAGScheduler: Got job 0 (first at ReadsSparkSource.java:221) with 1 output partitions; 18/04/24 17:55:07 INFO DAGScheduler: Final stage: ResultStage 0 (first at ReadsSparkSource.java:221); 18/04/24 17:55:07 INFO DAGScheduler: Parents of final stage: List(); 18/04/24 17:55:07 INFO DAGScheduler: Missing parents: List(); 18/04/24 17:55:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at ReadsSparkSource.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:14569,update,updated,14569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,2,['update'],['updated']
Deployability," have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/47819#Comment_47819",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4667:1426,pipeline,pipeline,1426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667,1,['pipeline'],['pipeline']
Deployability," implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:1624,pipeline,pipeline,1624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,1,['pipeline'],['pipeline']
Deployability," information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-env.1.gz; git-lfs usr/share/man/man1/git-lfs-ext.1.gz; git-lfs usr/share/man/man1/git-lfs-fetch.1.gz; git-lfs usr/share/man/man1/git-lfs-filter-process.1.gz; git-lfs usr/share/man/man1/git-lfs-fsck.1.gz; git-lfs usr/share/man/man1/git-lfs-install.1.gz; git-lfs usr/share/man/man1/git-lfs-lock.1.gz; git-lfs usr/share/man/man1/git-lfs-locks.1.gz; git-lfs usr/share/man/man1/git-lfs-logs.1.gz; git-lfs usr/share/man/man1/git-lfs-ls-files.1.gz; git-lfs usr/share/man/man1/git-lfs-merge-driver.1.gz; git-lfs usr/share/man/man1/git-lfs-migrate.1.gz; git-lfs usr/share/man/man1/git-lfs-pointer.1.gz; git-lfs usr/share/man/man1/git-lfs-post-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-post-commit.1.gz; git-lfs usr/share/man/man1/git-lfs-post-merge.1.gz; git-lfs usr/share/man/man1/git-lfs-pre-push.1.gz; git-lfs usr/share/man/man1/git-lfs-prune.1.gz; git-lfs usr/share/man/man1/git-lfs-pull.1.gz; git-lfs usr/share/man/man1/git-lfs-push.1.gz; git-lfs usr/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8320:2240,install,install,2240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320,1,['install'],['install']
Deployability," is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 5.42 minutes.; Runtime.totalMemory()=5618270208; ```; Althought no warning/error messages was issued for the indexing of this big genome, I have tried to debug on 3 things I could think of:. 1. The chromosome and the coordinate are sorted ascendandly, although the chromosome names are not simply numeric continuous because of the A/B subgroup for each chromosome.; 2. The genome size difference, for which no clue was aboserved about the chromosome length limits. ; 3. The chromosome names for this big genome is quite long, but I tried the shorter names as A11 for chr1A_part1, A12 for chr1A_part2, ... B72 for chr7B_part2 (42 chromosomes in total), and the problem stayed exactly the same. Not sure what I may have missed. I appreciate any insight of this problem.; Yifang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5917:6099,continuous,continuous,6099,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917,2,"['A/B', 'continuous']","['A/B', 'continuous']"
Deployability," lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '18570' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; 23:44:42.124 DEBUG ScriptExecutor - Result: 0; 23:44:42.124 INFO GermlineCNVCaller - Done initializing engine; 23:44:42.126 INFO GermlineCNVCaller - Intervals specified...; 23:44:42.534 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:42.534 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:42.534 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:42.534 DEBUG GenomeLocParser - chr3 (198295559 bp); 23:44:42.535 DEBUG GenomeLocParser - chr4 (190214555 bp); 23:44:42.535 DEBUG GenomeLocParser - chr5 (181538259 bp); 23:44:42.535 DEBUG GenomeLocParser - chr6 (170805979 bp); 23:44:42.535 DEBUG GenomeLocParser - chr7 (159345973 bp); 23:44:42.535 DEBUG GenomeLocParser - chr8 (145138636 bp); 23:44:42.535 DEBUG GenomeLocParser - chr9 (138394717 bp); 23:44:42.535 DEBUG GenomeLocParser - chr10 (133797422 bp); 23:44:42.535 DEBUG Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:5952,release,release,5952,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['release'],['release']
Deployability," major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:1400,update,update,1400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['update'],['update']
Deployability," not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:1456,release,released,1456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,2,"['release', 'update']","['released', 'updated']"
Deployability," on the [forum](https://gatkforums.broadinstitute.org/gatk/discussion/13680/variants-with-ad-0-0-and-dp-0#latest)... Aparently some variants with non-zero quals have 0 AD and DPs. Other annotations are also missing from the INFO columns. . After some debugging it turns out that the criteria to determine whether a read should be considered for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434:1052,configurat,configuration,1052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434,1,['configurat'],['configuration']
Deployability," org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:11468,pipeline,pipelines,11468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['pipeline'],['pipelines']
Deployability," pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.16.0+computecanada ; tbb 2021.1.1+computecanada; Theano 1.0.4 ; tqdm 4.19.5+computecanada ; wheel 0.34.2 ; ----. I used python 3.6.10 as suggested in gatkcondaenv.yml.template and respecting these dependencies found here setup_gcnvkernel.py:. ""theano == 1.0.4"",; ""pymc3 == 3.1"",; ""numpy >= 1.13.1"",; ""scipy >= 0.19.1"",; ""tqdm >= 4.15.0"" . ----. mkl is installed in my environment.; When I do : python -c ""import numpy ; numpy.show_config()"". I get this message:. blas_mkl_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/include', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib']; blas_opt_info:; libraries = ['mkl_rt', 'pthread']; library_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/lib/intel64']; define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]; include_dirs = ['/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl', '/cvmfs/soft.computecanada.ca/easybuild/software/2020/Core/imkl/2020.1.217/mkl/inc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:6129,install,installed,6129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['install'],['installed']
Deployability," removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:1841,update,updates,1841,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,1,['update'],['updates']
Deployability," routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4667:1070,install,installed,1070,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667,1,['install'],['installed']
Deployability," save on the cloud but maybe @yfarjoun is willing to help. ---. @yfarjoun commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2959:2471,pipeline,pipeline-help,2471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959,1,['pipeline'],['pipeline-help']
Deployability," spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.alpha.2-98-g8fa5092-SNAPSHOT; 23:25:07.556 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 23:25:07.556 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 5; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREATE_MD5 : false; 23:25:07.556 INFO PrintReadsSpark ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:1510,pipeline,pipelines,1510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['pipeline'],['pipelines']
Deployability," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:10755,pipeline,pipeline,10755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,8,"['Update', 'integrat', 'pipeline', 'update']","['Update', 'integration', 'pipeline', 'updates']"
Deployability," the info field annotations. ---. @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2507:7627,release,release,7627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507,1,['release'],['release']
Deployability," the python package theano(which is a requirement of gcnvkernel) with python 3.6.6 which is compiled with gcc 7.3.0. I am not using the conda environment to install these packages.; Then i tried to run theano-nose, but is giving me the following error:. ```sh. $ theano-nose; --; ; You can find the C code in this temporary file: /tmp/theano_compilation_error_gp0ar1kx; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compil",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:1291,INSTALL,INSTALLDIRGATK,1291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability," to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:4374,release,release,4374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,3,['release'],['release']
Deployability," yes, I know is still in beta but I’ve found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for sort the outputs and after this step the outputs are passed from average of 19 gigabytes to 13 gigabytes average for the all samples. I've opened this Issue because I would to help you with my experiments to improvement your tool.; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5323:1289,pipeline,pipeline,1289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323,2,['pipeline'],['pipeline']
Deployability,""" ; 7: Setting LC_MEASUREMENT failed, using ""C"" ; Error in readRDS(pfile) : ; cannot read workspace version 3 written by R 3.6.0; need R 3.5.0 or newer; Calls: source ... library -> find.package -> lapply -> FUN -> readRDS; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generatePlots(RecalUtils.java:360); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.generatePlots(AnalyzeCovariates.java:329); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.doWork(AnalyzeCovariates.java:341); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. when opening the docker container one can see installed version of R is 3.2.5. ```; Singularity broadinstitute_gatk_sha256_cec850f20311f0686fcf88510bc44e529590d78bec7076a603132115943c09e6.sif:~> R --version. R version 3.2.5 (2016-04-14) -- ""Very, Very Secure Dishes""; Copyright (C) 2016 The R Foundation for Statistical Computing; Platform: x86_64-pc-linux-gnu (64-bit); ```. #### Steps to reproduce; Run any AnalyzeCovariates. #### Expected behavior; create plots. #### Actual behavior; Docker faceplants",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6393:5536,install,installed,5536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6393,1,['install'],['installed']
Deployability,"""""""""; gatk]# ./gradlew; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; ............................................; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/data/md1/zhouyajun/biotools/gatk/gatk/build.gradle' line: 102. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; """"""; what should I do ?; How can I install GATK4 successful?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4669:757,install,install,757,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669,1,['install'],['install']
Deployability,"""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:1863,INSTALL,INSTALLDIRGATK,1863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,"# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-dictionary-validation true --remove-filtered-variants false --five-prime-flank-size 5000 --three-prime-flank-size 0 --force-b37-to-hg19-reference-contig-conversion false --transcript-selection-mode CANONICAL --lookahead-cache-bp 100000 --min-num-bases-for-segment-funcotation 150 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.2.0.0"",Date=""March 24, 2021 12:11:32 PM GMT"">; ## Funcotator 4.2.0.0 | Date 20211124T121132 | Gencode 34 CANONICAL | Achilles 110303 | CGC full_2012_03-15 | ClinVar 12.03.20 | C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:19452,update,updates,19452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['update'],['updates']
Deployability,"# Bug Report . ### Affected tool(s) or class(es); PathSeq. ### Affected version(s); - 4.1.6.0. ### Description . I wanted to better understand the PathSeq pipeline (and in particular, the Host Filter step) so I simulated RNA-seq reads from three microbial genomes of interest (Salmonella eneterica subsp. enterica serovar Typhimurium str. SL1344, Salmonella eneterica subsp. enterica serovar Typhimurium str. LT2, Fusobacterium nucleatum subsp. nucleatum ATCC 25586). I generate six datasets with 100,000 unpaired reads of length 75 bp (2 datasets from each genome) using Rsubread with simulate.sequencing.error=TRUE and ran them through PathSeq. I generated an identical six datasets using Rsubread with simulate.sequencing.error=FALSE. . #### Expected behavior; For the six datasets with simulate.sequencing.error=TRUE, I would expect a small number of reads to be filtered for each step. For the six datasets with simulate.sequencing.error=FALSE, I would expect similar results but with even fewer reads to be filtered for the low-quality or low complexity read filter. #### Actual behavior; For the six datasets with simulate.sequencing.error=TRUE, 8,496 - 18,103 reads were filtered by the low complexity or low quality filter (the Salmonella datasets were on the lower end and the Fusobacterium datasets were on the higher end), 115 - 311 reads were filtered by the host k-mer filter and 886 - 1822 reads were filtered by the duplicate read filter. . The number of reads filtered by the low complexity or low quality filter seemed high to me so I repeated the analysis with simulate.sequencing.error=FALSE. For these six datasets, all 100,000 reads are filtered by the low-quality or low complexity read filter. . #### Steps to reproduce; I wrote the workflow using snakemake and conda. In theory, you should be able to reproduce the error using `snakemake --use-conda`; Snakefile; ```; from os.path import join; import pandas as pd. ATCC25586_CDS_URL = ""ftp://ftp.ncbi.nlm.nih.gov/genomes/all/G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6705:155,pipeline,pipeline,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6705,1,['pipeline'],['pipeline']
Deployability,"# Bug Report. ## Affected tool(s) or class(es); gatk `GenomicsDBImport ` `GenotypeGVCFs`; ## Affected version(s); The Genome Analysis Toolkit (GATK) v4.5.0.0; ## Description; Hi,; Here is my situation, I'm testing the feasibility of incremental GenomicsDB，I have total 400 samples to joint calling, I have no problem directly using `GenomicsDBImport `and `GenotypeGVCFs `for joint calling of all 400 samples. The configuration used is 4c32g for `GenomicsDBImport `and 2c16g for `GenotypeGVCFs`. But when I first built a GenomicsDB of 200 samples using `GenomicsDBImport `successfully, and then use GenomicsDB `--genomicsdb-update-workspace-path` increment 200 samples into the GenomicsDB , use this incremental imported GenomicsDB to `GenotypeGVCFs`. The error happend and report GENOMICSDB_TIMER,Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; Here are my code; ```; gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenomicsDBImport \; --tmp-dir $PWD \; --genomicsdb-workspace-path ~{workspace_dir_name}~{prefix}.~{index} \; --batch-size 50 \; -L ~{intervals} \; --reader-threads 5 \; --merge-input-intervals \; --consolidate \; -V ~{sep = "" -V "" single_sample_gvcfs}. gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenomicsDBImport \; --tmp-dir $PWD \; --genomicsdb-update-workspace-path ~{workspace_dir_name} \; --batch-size 50 \; --reader-threads 5 \; --merge-input-intervals \; --consolidate \; -V ~{sep = "" -V "" single_sample_gvcfs}. gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenotypeGVCFs \; --tmp-dir $PWD \; -R ~{ref} \; -O ~{workspace_dir_name}.vcf.gz \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; -V gendb://~{workspace_dir_name} \; -L ~{intervals} \; --merge-input-intervals \; -all-sites; ```; And I found that before report error the number of threads used by GATK increased, but the memory usage did not exceed the maximum limit of the server.; I also cheched `--max-alternate-alleles` and `--genomicsdb-max-alternate-al",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8777:413,configurat,configuration,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8777,2,"['configurat', 'update']","['configuration', 'update-workspace-path']"
Deployability,"## Bug Report. ### Affected class; AssemblyBasedCallerUtils. ### Affected version(s); - [x] Latest public release version 4.1.9.0; - [x] Latest master branch as of 10/10/2020. ### Description ; When adjusting the base quality of overlapping read pairs, the modifications are made in place. If the modified reads are later used in another active region, the results from the later active region will be changed by the earlier modification. We had previously fixed this issue in #4926. But it looks like the refactoring in https://github.com/broadinstitute/gatk/commit/1353e3201bb11e29039efd89359b0a4cfc11e5c0 reverted to the earlier behavior. `AssemblyBasedCallerUtilsUnitTest.testfinalizeRegion()` will fail due to this behavior if [line 67](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyBasedCallerUtilsUnitTest.java#L67) is changed from:; ```; AssemblyBasedCallerUtils.finalizeRegion(activeRegion, false, false, minbq, header, sampleList, false);; ```; to:; ```; AssemblyBasedCallerUtils.finalizeRegion(activeRegion, false, false, minbq, header, sampleList, true);; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6882:106,release,release,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6882,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es). All Spark tools that takes parameter `-L`. ### Affected version(s); - [x] Latest public release version [4.0.4.0]; - [x] Latest master branch as of [2018-06-30]. ### Description . When running a Spark tool and passing in interval arguments via the standard `-L` argument, if the interval file (only BED file is tested) is stored in HDFS, we see errors like below. ```; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Query interval ""hdfs://shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-m:8020/data/merged_commonFPDel.bed"" is not valid for this input.; 	at org.broadinstitute.hellbender.utils.GenomeLocParser.getUnambiguousInterval(GenomeLocParser.java:350); 	at org.broadinstitute.hellbender.utils.GenomeLocParser.parseGenomeLoc(GenomeLocParser.java:309); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:300); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:226); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:174); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:155); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeIntervals(GATKSparkTool.java:514); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLinePro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4852:138,release,release,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es). FastaAlternateReferenceMaker. ### Affected version(s); - [x] Latest public release version 4.1.4.1; - [ ] Latest master branch as of [date of test?]. ### Description . A null pointer exception in . #### Steps to reproduce. We called variants with HaplotypeCaller & use resulting VCF with FastaAlternateReferenceMaker. See command below, but only reference fasta & HC vcf are given as input (no snp masking or interval list, though error also occurs when using interval list with multiple -L calls). #### Expected behavior. Alternate-adjusted reference file or at least a helpful error message. #### Actual behavior. ```; + latest-gatk/gatk-4.1.4.1/gatk FastaAlternateReferenceMaker -R /g/data/xe2/references/eucalyptus/emel_scott/Emelliodora_CSIROg1_SISH00000000.1.fasta -O consensus_sequences_gatk//CCA0704.fasta.tmp -V pergene_gatk/CCA0704/CCA0704.vcf.gz; Using GATK jar /g/data/xe2/users/stephen-rodgers/latest-gatk/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /g/data/xe2/users/stephen-rodgers/latest-gatk/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar FastaAlternateReferenceMaker -R /g/data/xe2/references/eucalyptus/emel_scott/Emelliodora_CSIROg1_SISH00000000.1.fasta -O consensus_sequences_gatk//CCA0704.fasta.tmp -V pergene_gatk/CCA0704/CCA0704.vcf.gz; 15:43:14.276 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/g/data/xe2/users/stephen-rodgers/latest-gatk/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 3:43:15 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:43:15.230 INFO FastaAlternateReferenceMaker - ------------------------------------------------------------; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6434:125,release,release,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6434,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es). GermlineCNVCaller. ### Affected version(s). - [x] Latest public release version gatk 4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . The same set of hdf5 works fine with another annotated_intervals.tsv . the stack trace:; ```; 11:52:33.788 INFO GermlineCNVCaller - Aggregating read-count file /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/; 20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0e1df603266809/B00HOTD.counts.hdf5 (229 / 347); HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 173 in H5Dread(): can'; t read data; major: Dataset; minor: Read failed; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dio.c line 550 in H5D__read(): ca; n't read data; major: Dataset; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 543 in H5D__contig; _read(): contiguous read failed; major: Dataset; minor: Read failed; #003: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 517 in H5D__scat; gath_read(): file gather failed; major: Low-level I/O; minor: Read failed; #004: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dscatgath.c line 253 in H5D__gath; er_file(): read error; major: Dataspace; minor: Read failed; #005: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 873 in H5D__contig; _readvv(): can't perform vectorized sieve buffer read; major: Dataset; minor: Can't operate on object; #006: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5VM.c line 1457 in H5VM_opvv(): ca; n't perform operation; major: Internal error (too specific to document in detail); minor: Can't operate on object; #007: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Dcontig.c line 696 in H5D_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7202:114,release,release,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es). HC java.lang.IllegalStateException: Padded span must contain active span. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description. ```; Runtime.totalMemory()=2494038016; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7289:169,release,release,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es). HaplotypeCaller. ### Affected version(s); - [x] Latest public release version (4.4.0.0, also 4.1.4.1); - [ ] Latest master branch as of [date of test?]. ### Description . I am using the HaplotypeCaller (GATK 4.4.0.0). When I look at the input BAM file in IGV, I expect the variant `NC_000015.9:g.48760182_48760185delinsGGGT`. However, HaplotypeCaller reports `NC_000015.9:g.48760182_48760185del` as well as an insertion `NC_000015.9:g.48760184_48760185insGGGT` (i.e. two distinct variants instead of a single indel). In the `bamout`, one can clearly see that the local realignment suggests the deletion + insertion and not the indel. ![image](https://user-images.githubusercontent.com/58295931/226553360-bff887ea-3823-44b7-bddb-46f70705c0b3.png). I understand that the local realignment is expected to improve variant calling and that his approach is battle-tested. I am thus not convinced this is a bug. However, the realignment/variant call is not obvious to the human eye - one would expect the indel instead. The variant seems like a clear heterozygous indel. I checked this [blog post](https://gatk.broadinstitute.org/hc/en-us/articles/360035891111-Expected-variant-at-a-specific-site-was-not-called): The bamout is as outlined above, the mapping + base quality seems fine (judging by IGV) and `--max-alternate-alleles` doesn't seem useful here (and indeed doesn't do anything to the result). I didn't got into kmer fiddling as suggested by the blog post. This is not a homopoly region. I also tested with 4.1.4.1 which only reports the deletion. The screenshot from above is from the 4.4.0.0 invocation. Here is the same situation for 4.1.4.1 (realignment is similar, `out.vcf` does not contain the insertion):. ![image](https://user-images.githubusercontent.com/58295931/226554045-0d9dd7e3-65ec-40ce-a6bd-74d73d4a2507.png). FYI, the variant lies on FBN1 / NM_000138.5 (rev strand). cDNA notation would be `NM_000138.5:c.4698_4701del` or `NM_000",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8253:112,release,release,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8253,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es). MuTect2 for the test case, but any caller using the Reference Bases annotation and calling bases near the end of chromosomes. ### Affected version(s). This occurs with the latest release (4.0.8.1) and not with the previous (4.0.7.0). It appears to be related to the addition of the Orientation Bias filter (#4895) and assessing sequence context:. https://github.com/broadinstitute/gatk/pull/4895/files#diff-07e3c8c33f865c5b32b362afe50cfd86R48. ### Description . When identifying variants near the end of chromosome boundaries, MuTect2 fails with:; ```; java.lang.StringIndexOutOfBoundsException: String index out of range: 369; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:48); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:270); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:176); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5130:229,release,release,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5130,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103:156,release,release,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es). VariantAnnotator ... but this is due to an old syntax update perhaps other docs in other tools are also affected. ### Affected version(s); - [X] Latest public release version [version?]; - [Presumptive] Latest master branch as of [date of test?]. ### Description . The argument ```--resource``` example(s) show a wrong syntax in regards to the location of the ""provider"" name ; ; #### Steps to reproduce; Google 'GATK VariantAnnotator'; the first or one of the first hits points to the current GATK doc on the tool. . #### Expected behavior. The example should read ```--resource:foo resource-file.vcf.gz```. #### Actual behavior. The example reads ```--resource foo:resource-file.vcf.gz```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8143:104,update,update,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8143,2,"['release', 'update']","['release', 'update']"
Deployability,## Bug Report. ### Affected tool(s) or class(es). `cnv_germline_cohort_workflow.wdl`. ### Affected version(s); - [x] Latest public release version [4.4.0.0]; - [x] Latest master branch as of [2023-04-14]. ### Description . cnv_germline_cohort_workflow.wdl currently outputs the following ; https://github.com/broadinstitute/gatk/blob/0374937bd7b152ecf1c2c922989371fcccedf184/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl#L405. whereas the task that generates the corresponding result outputs `String`; https://github.com/broadinstitute/gatk/blob/0374937bd7b152ecf1c2c922989371fcccedf184/scripts/cnv_wdl/cnv_common_tasks.wdl#L297. #### Expected behavior. `cnv_germline_cohort_workflow.wdl` should output `Array[String]`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8290:131,release,release,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8290,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7706:237,release,release,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); AddCommentsToBam. ### Affected version(s); - [x] Latest public release version [version?] GATK-4.0.10.1. ### Description ; I was trying to run AddCommentsToBam and with the -C flag, it was crashing when I had a colon in the string, so I had to delete it. #### Steps to reproduce; `gatk AddCommentsToBam -I=In.bam -O=Out.bam -C=""Bad: comment""`; `gatk AddCommentsToBam -I=In.bam -O=Out.bam -C=""Good comment""`. #### Expected behavior; A new BAM with the comment should be created. #### Actual behavior; I get this output: No value found for tagged argument: C=Bad: comment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5315:113,release,release,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5315,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); All Spark tools. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [2020-03-20]. ### Description ; Complains that the `driver-memory` is not a recognized argument.; Does not recognize these arg too; https://github.com/broadinstitute/gatk/blob/00f1e43402d9c8c195de383248099399f261e2cc/gatk#L425. #### Steps to reproduce; ```bash; gatk PrintReadsSpark -I <input.bam> -O <output.bam> \; -- \; --driver-memory 20g; ```. #### Expected behavior; NA; #### Actual behavior; NA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6515:112,release,release,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6515,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); AnalyzeCovariates . ### Affected version(s); - [x] Latest public release version [v4.1.4.0] [hash:cec850f20311f0686fcf88510bc44e529590d78bec7076a603132115943c09e6]. ### Description ; AnalyzeCovariates fails with ; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.4.0-local.jar AnalyzeCovariates -bqsr /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/IN-PM01004_rmd.recal.bam.recalTable -plots /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/AnalyzeCovariates.pdf; 23:15:29.581 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 19, 2020 11:15:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:30.435 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.437 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.1.4.0; 23:15:30.437 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:30.438 INFO AnalyzeCovariates - Executing as shollizeck@papr-res-compute204.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 23:15:30.438 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-8u212-b03-0ubuntu1.16.04.1-b03; 23:15:30.438 INFO AnalyzeCovariates - Start Date/Time: January 19, 2020 11:15:29 PM UTC; 23:15:30.439 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.439 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.439 INFO AnalyzeCovariates - HTSJDK Version: 2.20.3; 23:15:30.439 INFO AnalyzeCovariates - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6393:115,release,release,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6393,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); BaseRecalibrator, `BaseRecalibrationEngine.calculateKnownSites`. ### Affected version(s); - public release version [tested on >=4.1? no idea how to check]. ### Description ; When running BaseRecalibrator, if a read begins with an insertion and the KnownSites file contains an interval that begins at that position, the bases will not be skipped as GetReadCoordinateForReferenceCoordinate returns the read coordinate after the insertion. To properly handle this case, in the line https://github.com/broadinstitute/gatk/blob/9bca5119e996886ee85ef6c890eba79ec5d6cfb1/src/main/java/org/broadinstitute/hellbender/utils/recalibration/BaseRecalibrationEngine.java#L340 ReadUtils.ClippingTail.LEFT_TAIL should be changed to ReadUtils.ClippingTail.RIGHT_TAIL. Making this substitution would ensure that the coordinate returned is always the leftmost coordinate when the read begins with an insertion, the desired effect for the start of the interval. #### Steps to reproduce; Use an alignment with a read that begins with an insertion and a BED that specifies an interval that begins at that position. For example, alignment file:; ```; @HD VN:1.6 SO:coordinate; @SQ SN:ref LN:10; @RG ID:foo SM:bar PU:baz PL:ILLUMINA; r001 0 ref 2 40 6I4M * 0 0 AAAAAAAAAA IIIIIIIIII RG:Z:foo; ```; and ref:; ```; >ref; AAAAAAAAAA; ```. and BED file; ```; ref 0 1; ref 1 2; ref 2 3; ref 3 4; ref 4 5; ref 5 6; ref 6 7; ref 7 8; ref 8 9; ref 9 10; ```; Then run BaseRecalibrator and look at the output:; `gatk BaseRecalibrator -I aln.bam -R ref.fa --known-sites sites.bed.gz -O recal.txt`. #### Expected behavior; The output tables should be empty, since every site in our reference (bases 1-10 inclusive) should be skipped. #### Actual behavior; The output tables include the 6 inserted bases, and the cycle covariate values confirm they are the 6 leading inserted bases:; ```; ReadGroup QualityScore CovariateValue CovariateName EventType EmpiricalQuality Observations Errors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6385:149,release,release,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6385,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Bug when this line is executed: https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L147. Casued because attribute ""oneShotLogger"" is uninitialized. See line (https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L42) and its missing initialization in the constructor method (https://github.com/broadinstitute/gatk/blob/9f77b1fddedb8e047948078b29ac9fbb70d005b0/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java#L65). ### Affected version(s); - [ ] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [12/01/2023]. ### Description ; I work as support for a HPC cluster and this bug has affected one of our users, so I won't be able to provide the exact specifics. Long story short, the user reports that for a high enough value of ploidy (20-50), they start getting null pointer exception errors. Here we can see an example of how they launch the program:. ```; gatk --java-options ""-Xmx4g"" HaplotypeCaller \; -I ${bamfile} \; -R ${reference} \; -O ${outpath}/${sample_id}.ploidy_${SLURM_ARRAY_TASK_ID}.output.g.vcf.gz \; -RF MappingQualityReadFilter \; --minimum-mapping-quality 10 \; --max-alternate-alleles 10 \; --max-genotype-count 75000 \; --dont-use-soft-clipped-bases true \; -ploidy ${SLURM_ARRAY_TASK_ID} \; -ERC GVCF; ```. And this is the stack trace obtained when it fails:. ```; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.logging.OneShotLogger.warn(String)"" because ""this.oneShotLogger"" is null; at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:147); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCall",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8158:796,release,release,796,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Build. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; =======================<phase: build >============================; ===> Building for gatk-4.2.6.1_1. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). FAILURE: Build failed with an exception. * Where:; Build file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.2.6.1/build.gradle' line: 15. * What went wrong:; Plugin [id: 'de.undercouch.download', version: '4.1.2'] was not found in any of the following sources:. - Gradle Core Plugins (plugin is not in 'org.gradle' namespace); - Plugin Repositories (could not resolve plugin artifact 'de.undercouch.download:de.undercouch.download.gradle.plugin:4.1.2'); Searched in the following repositories:; Gradle Central Plugin Repository; ```. #### Steps to reproduce; regular build. Version: 4.2.6.1; Java-17; FreeBSD 13.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7984:102,release,release,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7984,4,"['continuous', 'release']","['continuous', 'release', 'release-notes']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); CalculateContamination. ### Affected version(s); - [x] Latest public release version 4.1.8.1. ### Description . There appears to be an error mode where if not a lot of sites are provided, the contamination estimation tool will estimate the error on contamination as 0.0. We should change this to either error out if enough sites are not provided, or modify the calculation to correctly reflect the uncertainty in contamination.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6727:119,release,release,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6727,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); CollectReadCounts . ### Affected version(s); - [x] Latest public release version [4.1.8.1]; - [x] Latest master branch as of [10/6/2020]. ### Description ; GATK CollectReadCounts successfully processes some reads but then throws an ArrayIndexOutOfBoundsException in the middle of the file. Console output below. I was able to successfully run CollectReadCounts on ~16k other crams, but received the same error on 3 of them (although the last coordinate reported by ProgressMeter was different for each). The error does not occur if I subset out chromosome 6 reads from the main cram file and run CollectReadCounts on each file separately. I don't see any obvious formatting issue with my crams from a quick skim over lines immediately following the last reported coordinate. ; ```; > java -jar gatk-package-4.1.8.1-local.jar CollectReadCounts \; -I input.cram \; --read-index input.cram.crai \; -L my_intervals.bed \; --interval-merging-rule OVERLAPPING_ONLY \; --reference hg38.fa \; --format TSV \; -O output.tsv. 16:56:30.581 INFO CollectReadCounts - ------------------------------------------------------------; 16:56:30.581 INFO CollectReadCounts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:56:30.581 INFO CollectReadCounts - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:56:30.582 INFO CollectReadCounts - Executing as isaac@LAPTOP-K5UOQS3A on Linux v4.19.104-microsoft-standard amd64; 16:56:30.582 INFO CollectReadCounts - Java runtime: Java HotSpot(TM) 64-Bit Server VM v14.0.1+7; 16:56:30.582 INFO CollectReadCounts - Start Date/Time: October 6, 2020 at 4:56:30 PM EDT; 16:56:30.582 INFO CollectReadCounts - ------------------------------------------------------------; 16:56:30.582 INFO CollectReadCounts - ------------------------------------------------------------; 16:56:30.583 INFO CollectReadCounts - HTSJDK Version: 2.23.0; 16:56:30.584 INFO CollectReadCounts - Picard Version: 2.22.8; 16:56:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6865:115,release,release,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6865,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - GATK 4.1.8.1 (Latest release as of 08/24/20). ### Description ; User is running CombineGVCFs and getting a java error java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52). This issue was discussed at the GATK Office Hours meeting. ### Associated forum post; https://gatk.broadinstitute.org/hc/en-us/community/posts/360072644931-Combine-GVCF-generate-java-lang-NullPointerException. Command:; time ""$gatk"" CombineGVCFs \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -R ""$ref_gen""/ucsc.hg19.fasta \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200272.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200273.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200274.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200313.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200314.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200315.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-006.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-007.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/backup_gvcfs/all_wes_samples.g.vcf \; -O /paedwy/disk1/yangyxt/wes/backup_gvcfs/all_wes_samples_plus_${sample_batch}.g.vcf.gz && echo ""Combine_gvcfs done"". Error Log:; ```; 12:01:36.798 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:01:36.824 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 24, 2020 12:01:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect wheth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766:112,release,release,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - [X] Latest public release version [4.2.5.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; The auto-generated wdl for CombineGVCFs on dockstore won't work because it doesn't have any inputs for the indices of the input vcfs. This means GATK cannot access the vcf indices because they never get localized, so the workflow fails. . #### Steps to reproduce; Take any vcfs and run them through the workflow to get an error about missing indices. . #### Expected behavior; Including the indices in the task inputs will allow them to get localized along with the vcfs so GATK can operate normally. . #### Actual behavior; You get an error saying it requires index files to proceed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7681:109,release,release,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7681,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); CountReadsSpark. ### Affected version(s); gatk-4.0.12.0. ### Description ; Reading cram generates the following error when running CountReadsSpark on yarn. . ```; ./gatk-4.0.12.0/gatk CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn; Using GATK jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar; Running:; /share/pkg/spark/2.1.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:625,install,install,625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['install'],['install']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); CreateReadCountPanelOfNormals. ### Affected version(s); - [ ] Latest public release version [4.1.0.0]. ### Description ; When you run it on a single machine, it trys to use _hadoop_ and failed. ```; $ java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5; 12:33:52.103 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:126,release,release,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [ ] v.4.1.3.0 using us.gcr.io/broad-gatk/gatk:4.1.3.0; - [ ] 11/22 failed in Terra. ### Description ; There is a bug in the logic with how Funcotator is handling this variant. It is a variant after ; chr14:24655355 ; **Stacktrace**; <img width=""1248"" alt=""Screen Shot 2019-11-27 at 4 37 31 PM"" src=""https://user-images.githubusercontent.com/13475639/69761316-026f2a00-1135-11ea-9b50-491f5b22971c.png"">. Jonn has the input files, log file, and WDL. #### Steps to reproduce; To reproduce this issue, all the inputs and full pipeline are listed in this[ Zendesk ticket 3847](https://broadinstitute.zendesk.com/agent/tickets/3847). Contact Tiffany for access. #### Expected behavior; The tool should handle this situation more gracefully?. #### Actual behavior; It fails with a java.lang.StringIndexOutOfBoundsException: String index out of range: 776. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289:611,pipeline,pipeline,611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289,1,['pipeline'],['pipeline']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [x] Latest public release version [4.2.1.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When processing a VCF with tumor and matched normal into a MAF, the `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields are not populated. #### Steps to reproduce. `gatk --java-options -Xmx2048m Funcotator --data-sources-path /cromwell_root/datasources_dir --ref-version hg38 --output-file-format MAF -R /cromwell_root/getzlab-workflows-reference_files-oa/hg38/gdc/GRCh38.d1.vd1.fa -V` [`C3N-02729.vcf.gz`](https://github.com/broadinstitute/gatk/files/6977700/C3N-02729.vcf.gz) `-O C3N-02729.maf --annotation-default normal_barcode:C3N-02729_N --annotation-default tumor_barcode:C3N-02729_T --annotation-default Center:broadinstitute.org --annotation-default source:Unknown --transcript-selection-mode BEST_EFFECT`. (NOTE: reference files available at `gs://getzlab-workflows-reference_files-oa/hg38/gdc`). #### Expected behavior; `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields should be populated as appropriate based on the `GT` field for the matched normal in the VCF. #### Actual behavior; `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` are populated with `__UNKNOWN__`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7408:107,release,release,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7408,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; Funcotator appears to output an unnecessary extra tab at the end of each line. The change appears to have happened between gatk 4.1.4.0 and 4.1.6.0. #### Expected behavior; Output correct number of tabs corresponding to the number of column headers. #### Actual behavior; Outputs an extra tab.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6693:107,release,release,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6693,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GATK 4.1.0.0 AnalyzeCovariates. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; The csv produced by AnalyzeCovariates is invalid. It doesn't escape commas in fields, resulting in an error in the R script. #### Steps to reproduce; If you have a comma in the readgroup in a BAM, this will happen. #### Expected behavior; It should produce valid csv files, and then be able to properly produce the plots. #### Actual behavior; Commas in read group names result in malformed (unescaped) csv where it's impossible to parse fields properly. This results in the following R script error:; ```; Error in read.table(file = file, header = header, sep = sep, quote = quote, :; more columns than column names; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5739:127,release,release,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5739,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); GATK 4.2.2.0. ### Description . When running GenotypeGVCFs,; 1. multiple warnings of **No valid combination operation found for INFO field** ; 2. AS_VarDP warnings:; ```; WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - -------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437:985,install,install,985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437,1,['install'],['install']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GATK Haplotype caller. ### Affected version(s); - [x] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. The Genome Analysis Toolkit (GATK) v4.1.0.0; HTSJDK Version: 2.18.2; Picard Version: 2.18.25. ### Description ; HaplotypeCaller is outputting variants which have a no-call as the ALT, which breaks a bunch of downstream tools, this is new behavior in 4.1, AFAICT. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	s1564	s1741	s1851	s1852	s1862	s1901	s1912	s1971	s2017	s2021	s2026	s2056	s2100	s2102	s2104	s2122	s2124	s2151	s2157; 1	937796	.	T	.	179.65	.	AN=38;DP=31;MMQ=60;MQ=60.00	GT:AD:DP	0/0:0:0	0/0:0:0	0/0:4:4	0/0:0:0	0/0:1:1	0/0:1:1	0/0:0:0	0/0:0:0	0/0:2:2	0/0:0:0	0/0:1:1	0/0:3:3	0/0:1:1	0/0:8:8	0/0:1:1	0/0:0:0	0/0:2:2	0/0:7:7	0/0:0:0; ```. #### Steps to reproduce; I'm not doing anything special, so I suspect these variants should exist in other projects as well. I'm doing batch calling on several samples simultaneously; an example:. ```; unset JAVA_HOME && export PATH=/home/rdk4/local/share/bcbio/anaconda/bin:$PATH && gatk --java-options '-Xms4g -Xmx5000m -XX:+UseSerialGC -Djava.io.tmpdir=/n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/bcbiotx/tmpTSg0hJ' HaplotypeCaller -R /n/app/bcbio/dev/genomes/Hsapiens/GRCh37/seq/GRCh37.fa --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation BaseQualityRankSumTest --annotation FisherStrand --annotation MappingQuality --annotation DepthPerAlleleBySample --annotation Coverage -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2017/s2017-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2056/s2056-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2122/s2122-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5650:118,release,release,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5650,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GATK installation. ### Affected version(s); - [x] Latest public release version 4.4.0.0; - [x] Latest master branch as of [11.12.2023]. ### Description ; Latest `conda` versions cannot install pip packages. #### Steps to reproduce; Create `gatk` `env` with `conda` version equal or newer than `23.10`. #### Expected behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transactio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618:55,install,installation,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618,5,"['Install', 'install', 'release']","['Installing', 'install', 'installation', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GATK v4.1.4.0 using FilterMutectCalls. ### Affected version(s); - [x] Latest public release version `4.1.4.0` installed from conda release `gatk4-4.1.4.0-1`; - [ ] Latest master branch as of [date of test?]. ### Description ; This issue reports the same error that is reported in #6237, but on the latest release, and in a mitochondrial calling setting. My command is:; ```bash; gatk FilterMutectCalls -V MT.vcf.gz\; -R human_g1k_v37.main.fasta\; -O MT.filtered.vcf.gz\; --stats MT.vcf.gz.stats\; --mitochondria-mode; ```. I get the following output to STDERR:; ```; 11:15:57.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 07, 2019 11:15:57 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:15:57.328 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.328 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.0; 11:15:57.328 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:15:57.328 INFO FilterMutectCalls - Executing as warkre@fuji on Linux v4.9.0-9-amd64 amd64; 11:15:57.328 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 11:15:57.329 INFO FilterMutectCalls - Start Date/Time: November 7, 2019 11:15:57 AM CET; 11:15:57.329 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.329 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.329 INFO FilterMutectCalls - HTSJDK Version: 2.20.3; 11:15:57.329 INFO FilterMutectCalls - Picard Version: 2.21.1; 11:15:57.329 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:134,release,release,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,4,"['install', 'release']","['installed', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GATK version: 4.1.1.0-VariantRecalibrator-ApplyVQSR. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I am doing VQSR with gatk-VariantRecalibrator-ApplyVQSR, and i got some mistakes in the log file， and i dont konw what was wrong with my script,. #### Steps to reproduce; Below are my complete scripts:; java -Xmx3990m -Djava.io.tmpdir=/gss1/home/ldl20190322/a_haoxiaoshuai/JavaTmpDir -jar /gss1/home/ldl20190322/a_haoxiaoshuai/z_software/gatk/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar VariantRecalibrator -R Gmax_275_v2.0.fa --variant Ztem.gatk.vcf.gz --resource:hapmap,known=false,training=true,truth=true,prior=10.0 final.intersected.snp.vcf.gz -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP -mode SNP -O Ztem.gatk.snp.recal --tranches-file Ztem.gatk.snp.tranches --rscript-file Ztem.gatk.snp.plots.R -tranche 90.0 -tranche 92.0 -tranche 94.0 -tranche 96.0 -tranche 97.0 -tranche 98.0 -tranche 99.0 -tranche 99.9; java -Xmx3990m -Djava.io.tmpdir=/gss1/home/ldl20190322/a_haoxiaoshuai/JavaTmpDir -jar /gss1/home/ldl20190322/a_haoxiaoshuai/z_software/gatk/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar ApplyVQSR -R Gmax_275_v2.0.fa -V Ztem.gatk.vcf.gz --truth-sensitivity-filter-level 99.0 --tranches-file Ztem.gatk.snp.tranches --recal-file Ztem.gatk.snp.recal -mode SNP -O Ztem.gatk.snp.vcf.gz. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; Below is the message of the mistakes and i just omitted some no use information in the log file:; .; .; .; 15:51:14.040 INFO VariantRecalibratorEngine - Evaluating full set of 3660 variants...; 15:51:15.156 INFO VariantRecalibratorEngine - Evaluating full set of 3660 variants...; 15:51:15.373 INFO VariantRecalibrator - Building FS x ReadPosRankSum plot...; 15:51:15.374 INFO VariantRecalibratorEngine - Evaluating full set of 3660 variants...; 15:51:16.493 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6948:148,release,release,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6948,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GatherTranches. ### Affected version(s); Latest public release version 4.2.6.1. ### Description ; I ran `VariantRecalibrator` in scattered (using intervals) mode and now trying to gather the scattered tranches into a single file but somehow the number of novel variants is < 0. This is the exact error:; `Invalid tranche - no. variants is < 0 : known 90357410 novel -1894637320`. #### Steps to reproduce; ```; inputs_cmdl = ' '.join([f'--input {t}' for t in tranches]); j.command(; f""""""set -euo pipefail; gatk --java-options -Xms6g \\; GatherTranches \\; --mode SNP \\; {inputs_cmdl} \\; --output {j.out_tranches}""""""; ); ```. #### Expected behavior; Gathered scattered VQSLOD tranches into a single file. #### Actual behavior; Fails because of what seems like an integer overflow according to @ldgauthier; ```; org.broadinstitute.hellbender.exceptions.GATKException: Invalid tranche - no. variants is < 0 : known 90357410 novel -1894637320; 	at org.broadinstitute.hellbender.tools.walkers.vqsr.Tranche.<init>(Tranche.java:37); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.<init>(VQSLODTranche.java:37); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.mergeAndConvertTranches(VQSLODTranche.java:205); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VQSLODTranche.mergeAndConvertTranches(VQSLODTranche.java:139); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.GatherTranches.doWork(GatherTranches.java:80); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7859:105,release,release,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7859,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport . ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of Apr 4, 2022. ### Description ; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found; [E::faidx_adjust_position] The sequence ""chrX"" was not found. #### Steps to reproduce; Run the first test case for GnarlyGenotyperIntergrationTest::testUsingGenomicsDB() on the branch https://github.com/broadinstitute/gatk/pull/7750. The test contains the argument `--intervals chrX:1000000-5000000`, but I'm not sure why that would be an issue. The tool runs fine and the output is valid. #### Expected behavior; An informative warning or a single output of the existing warning. #### Actual behavior; Excessive logging",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7751:114,release,release,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7751,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [ ] Latest public release version [4.2.5.0]. ### Description ; My gVCF files are block compressed and indexed, but the files have the file extension "".gvcf.gz"" rather than "".vcf.gz"". When I run `GenomicsDBImport` with `--bypass-feature-reader`, the "".gvcf.gz"" file cannot be recognized as a block compressed vcf file. The code of `GenomicsDBImport` validates if input is block compressed by checking if the file extension is "".vcf.gz"". ```; private static void assertVariantFileIsCompressedAndIndexed(final Path path) {; if (!path.toString().toLowerCase().endsWith(FileExtensions.COMPRESSED_VCF)) {; throw new UserException(""Input variant files must be block compressed vcfs when using "" +; BYPASS_FEATURE_READER + "", but "" + path.toString() + "" does not appear to be"");; }; Path indexPath = path.resolveSibling(path.getFileName() + FileExtensions.COMPRESSED_VCF_INDEX);; IOUtils.assertFileIsReadable(indexPath);; }; ```. I understand that this is an issue on my side because I did not name my gVCF files with the standard extension "".vcf.gz"". Is it possible to make this check less stringent in a future release? Maybe make any "".gz""/"".bgz"" file acceptable, or check the "".tbi"" index file to identify block compression (existing index typically means the file is block compressed and indexed). . Thank you. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7691:113,release,release,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7691,2,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [ ] Public release version 4.1.4.1 . ### Description ; Running GenomicsDBImport on an HPC cluster using SLURM, admin mentioned that the jobs are writing inefficiently to shared storage (@spikebike will follow up with HPC specifics and logs). . #### Steps to reproduce; ```; Using GATK jar /share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -Xms60g -jar /share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar GenomicsDBImport --genomicsdb-workspace-path data/interim/combined_database_bpres/0004 --batch-size 50 --reader-threads 6 --sample-name-map data/processed/sample_map --intervals data/processed/scattered_intervals/0004-scattered.intervals --tmp-dir /scratch/sdturner/genomicsdbimport/0004; ```. #### Expected behavior; My understanding is that it may be more efficient to use a small buffer and write the final database in full. . #### Actual behavior; Again my (limited) understanding is that the tool is writing output multiple times and throwing out all but the last write. Here is an example of a log for a 2.6 Mb region and 295 samples: ; ; ```; 07:24:39.198 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 28, 2020 7:24:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:24:39.616 INFO GenomicsDBImport - ------------------------------------------------------------; 07:24:39.617 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.4.1; 07:24:39.617 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487:106,release,release,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); -4.1.8.1, 4.1.6.0. ### Description ; Two users are running GenomicsDBImport and getting a Duplicate Sample Name Error and both have reported that they do not have duplicate sample names in their map files. @nalinigans @mlathara does this look like a user issue or bug with GenomicsDBImport?. ### First Example; Please see this link for more info: https://gatk.broadinstitute.org/hc/en-us/community/posts/360072797951--GenomicsDBException-Duplicate-sample-name-found-?page=1#community_comment_360012681791. `gatk --java-options ""-Xmx16g -Xms16g"" GenomicsDBImport --batch-size 24 --reader-threads 12 --genomicsdb-update-workspace-path /rooted3/langley/work/home/chuck/rad/SFARI/SSC_hg38/WGS/CPRs_100_proto/DB_chr1 --intervals chr1:118739963-147510543 --verbosity DEBUG -V /rooted3/langley/work/home/chuck/rad/SFARI/SSC_hg38/WGS/phase2_CPRs/SSC00007_CPR/SSC00007.haplotypeCalls.CPR.er.raw.vcf.gz`. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -Xms16g -jar /afs/genomecenter.ucdavis.edu/software/gatk/4.1.6.0/lssc0-linux/gatk-package-4.1.6.0-local.jar GenomicsDBImport --batch-size 24 --reader-threads 12 --genomicsdb-update-workspace-path /rooted3/langley/work/home/chuck/rad/SFARI/SSC_hg38/WGS/CPRs_100_proto/DB_chr1 --intervals chr1:118739963-147510543 --verbosity DEBUG -V /rooted3/langley/work/home/chuck/rad/SFARI/SSC_hg38/WGS/phase2_CPRs/SSC00007_CPR/SSC00007.haplotypeCalls.CPR.er.raw.vcf.gz; 16:16:35.954 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/afs/genomecenter.ucdavis.edu/software/gatk/4.1.6.0/lssc0-linux/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:16:36.003 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/libgkl_compression5245166187604030095.so; Aug 28, 2020 4:16:36 PM s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:704,update,update-workspace-path,704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['update'],['update-workspace-path']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs 4.0.0.12. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I've run into a weird case where GenotypeGVCFs is doing something unexpected. I have a gVCF with the following entry in it:. ```; chr11 6637739 . ATTTTT A,AT,ATT,ATTT,ATTTT,ATTTTTT,<NON_REF> 565.73 . BaseQRankSum=-0.014;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;MLEAC=0,0,0,1,0,0,0;MLEAF=0,0,0,0.5,0,0,0;MQRankSum=0;RAW_MQandDP=338400,94;REF_BASES=GCCGGCCTGGATTTTTTTTTT;ReadPosRankSum=-0.812 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/4:9,3,3,11,15,8,3,0:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:56:603,504,1526,335,1171,1118,56,661,640,608,0,362,313,183,335,336,500,389,187,171,527,655,864,622,277,169,466,1026,597,1101,953,645,465,625,861,1133:8,1,33,10; ```. It's a messy site for sure, an indel in a long homopolymer-T, but I think that's a separate issue. If I run the following on that gVCF:. ```; gatk GenotypeGVCFs \; -R hg19.fa -V test.g.vcf -O test.vcf \; -A ClippingRankSumTest -A Coverage -A ExcessHet -A FisherStrand \; -A MappingQualityRankSumTest -A OxoGReadCounts -A QualByDepth -A ReadPosRankSumTest \; -A ReferenceBases -A RMSMappingQuality -A StrandOddsRatio -A TandemRepeat \; -L chr11:6637730-6637750 \; -stand-call-conf 18.0 \; ```. then I get the following output to the VCF just like I'd expect:. ```; chr11 6637739 . ATT A 565.73 . AC=1;AF=0.500;AN=2;BaseQRankSum=-1.400e-02;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;FS=1.779;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.00;QD=23.57;REF_BASES=GCCGGCCTGGATTTTTTTTTT;RPA=15,13;RU=T;ReadPosRankSum=-8.120e-01;SOR=0.386;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:9,15:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:99:603,0,335; ```. QUAL is unchanged since I'm genotyping a single-sample gVCF. However, if I raise my `-stand-call-conf` threshold to 19.0, GenotypeGVCFs no longer outputs any variants. 565.73 >> 19.0, so I'm confused as to why that var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5793:119,release,release,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs with --keep-combined-raw-annotations. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of (not tested). ### Description ; @ldgauthier was kind enough to introduce the `--keep-combined-raw-annotations` option for us after the discussion in issue #5698, and we've been using it extensively. We recently noticed a problem that affects a small fraction of variants though. We're noticing this with `AS_SB_TABLE` but it probably applies to all annotations that are per-allele or per-alt allele. The problem is that when GenotypeGVCFs runs it may chose to output only a subset of the alleles present in the gVCF. When it does this it does not appear to update the annotations to remove the values for the removed alleles. This results in annotations with more values than there are alleles, and no safe/predictable way to interpret those annotations since you don't know the original ordering of alleles and which ones were removed when looking at the resulting VCF. This is happening, in my case, primarily at homopolymer sites and occasionally at STRs with larger repeat units. I've attached a zip file - [AS_SB_TABLE_bug.zip](https://github.com/broadinstitute/gatk/files/3357101/AS_SB_TABLE_bug.zip) - which contains a one-record gVCF, the command to generate the VCF and the resulting VCF, which should be sufficient to demonstrate the problem and reproduce it. Here's what an offending variant looks like:. ```; chr1 100366446 . GTT G 562.64 . AC=1;AF=0.500;AN=2;AS_SB_TABLE=19,6|16,6|4,0|2,2|1,1;...;REF_BASES=ATGTTTTTTTGTTTTTTTTTT;RPA=13,11;RU=T;ReadPosRankSum=-1.296e+00;SOR=0.534;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:25,22:57:19,16:4,4:99:570,0,819; ```. #### Steps to reproduce; See attached zip file. #### Expected behavior; All per-allele and per-alt-allele annotations should be subsetted to only the values for the alleles that are output in the resulting VCF. #### Actual behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6029:147,release,release,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6029,2,"['release', 'update']","['release', 'update']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [X] Latest public release version. ### Description ; GenotypeGVCFs outputs samples (individuals) with no reads (depth) as reference genotypes: ; 0/0:0,0:0:0:.:.:0,0,0. or with very small number of reads, like 1:; 0/0:1,0:1:3:.:.:0,3,31. I believe that this issue was also reported here:; https://gatk.broadinstitute.org/hc/en-us/community/posts/4476803114779-GenotypeGVCFs-Output-no-call-as-reference-genotypes. #### Steps to reproduce; I have followed all GATK steps with default settings. . #### Expected behavior; I believe that samples with no reads (or very small number of reads) should be reported as missing, like:; ./. Thank you for your help,; Marcin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7792:110,release,release,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7792,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; GenotypeGVCFs won't joint call DRAGEN mitochondrial data because of the DRAGEN somatic output format. We should be able to use the DRAGEN SQ in place of Mutect2's TLOD (see line 279 in GenotypeGVCFsEngine); Note that DRAGEN SQ is a Phred-scaled double. #### Steps to reproduce; DRAGEN somatic GVCF entries from version 3.8.4 look like:; chrM 1 . G <NON_REF> . weak_evidence END=1 GT:AD:DP:SQ:MIN_DP 0/0:112,1579:1691:0:1691. Run GenotypeGVCFs with -V to a file like that (reference GenotypeGVCFsIntegrationTest::testGenotypingForSomaticGVCFs() for more details); Must include `--input-is-somatic` as of now. #### Expected behavior; The task should run to completion, calculating a site quality store using the DRAGEN SQ value. #### Actual behavior; Error from AlleleFrequencyCalculator about not having PLs or GQ.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7840:110,release,release,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7840,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); 4.6.0.0. ### Description ; When I was doing GenotypeGVCFs from GenomicsDB of 420 samples, the process interrupted due to significant memory issues. This process was eating up memory continuously. In 4.5.0.0, I did same process, and I confirmed it works fine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918:272,continuous,continuously,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918,1,['continuous'],['continuously']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - [x] Latest public release version [4.5.0.0]; - [x] Latest master branch as of [14.12.2023]. ### Description ; Very different results after update from 4.4.0.0 to 4.5.0.0. We updated test results after https://github.com/broadinstitute/gatk/issues/8619, but now we see big changes (especially in `segments` file). #### Steps to reproduce; Command list:; ```; /soft/gatk-4.5.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.5.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628:114,release,release,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628,3,"['release', 'update']","['release', 'update', 'updated']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); GnarlyGenotyper. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]; - [x] 4.2.3 - snapshot -> https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details. ### Description ; WDL joint genotyping using GnarlyGenotyper after ReblockGVCF (fixed on the snapshot above). #### Steps to reproduce; Joint Genotyper wdl pipeline with ""GatkJointGenotyping.useGnarlyGenotyper"": true , **samples from DRAGEN 3.8+**. #### Expected behavior; Complete the pipeline. #### Actual behavior; Failing with diploid error on Sexual Chromosomes. Hello again everyone.; First of all, thank you @ldgauthier to send us that snapshot docker. It kind of solved reblock problem. As feedback here, I tried with the newest GATK version (4.2.5) as it modified ReblockGVCF, but it didn`t work.; Anyway, I have another issue here...; While I was using only one or few chromosomes, the pipeline with reblock + gnarly was working fine. Once I added all chromosomes I started to get this type of error (GnarlyGenotyper):. ```; A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA18668 has ploidy 1 at position chrY:2789135. or. A USER ERROR has occurred: Bad input: This tool assumes diploid genotypes, but sample NA14734 has ploidy 1 at position chrX:36667858. ```; I checked every failed log, and it's all related to the sexual chromosomes. Any thought/tip about that? ; ps.: From chr1 to chr22 it worked fine!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7690:112,release,release,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690,4,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller --output-mode EMIT_ALL_SITES. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm trying to generate a VCF (not a gVCF) that contains calls spanning all the sites in my regions. Each region is small, and is more or less equivalent to a single variant. Ideally I'd use `GENOTYPE_GIVEN_ALLELES`, but I don't know the alleles, and in some cases the variant location is approximate (e.g. somewhere in _this_ 10bp window). I've been trying to use HaplotypeCaller to produce a VCF that contains calls covering my entire set of regions, but nothing seems to work. I started with just `--output-mode` and eventually ended up with:. ```; gatk HaplotypeCaller \; -R ref.fasta \; -L regions.interval_list \; --disable-optimizations \; --force-active \; --output-mode EMIT_ALL_SITES \; -I my.bam \; -O my.vcf.gz; ```. This does output considerably more records, including a lot of hom-ref records, but still nowhere near to the full set of bases within my regions. E.g. in one test this emits variants spanning 3,468bp which is way better than the ~120bp I get without those options, but nowhere near the 293,570bp with the regions I'm supplying. It would be great if `--output-mode EMIT_ALL_SITES` did as the documentation described, but if that's not possible, then perhaps that mode should simply be removed?. #### Steps to reproduce; Try calling a BAM file with HaplotypeCaller with a 100-1000bp region with `--output-mode EMIT_ALL_SITES`. #### Expected behavior; VCF should contain records spanning the entire input region. #### Actual behavior; VCF contains a minority of sites from the region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6059:141,release,release,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller . ### Affected version(s); - [x] Latest public release version [4.1.6.0]; - [ ] Latest master branch (not tested). ### Description ; I have a sample that has a slightly complicated event in it that is getting miscalled. The easiest way to see it is probably with an IGV screenshot:. ![variant](https://user-images.githubusercontent.com/1609210/78403555-c6077b00-75b9-11ea-96f3-8f9ca6c25e86.png). BWA aligns the reads with a 7bp deletion followed by 2 mismatches, though am inclined to think of it as a 9bp deletion coupled with a 2bp insertion (or a swap of 9bp of reference for 2bp of novel sequence). The original alignments are in the top half of the IGV view. The bottom is the assembly BAM from running the HaplotypeCaller. From what I see the assembly is getting it right. . But the problem is that the event extraction/genotyping goes wrong. I've run it two ways. If I run to generate a called VCF directly using:. ```; gatk HaplotypeCaller \; --input snippet.bam \; --output snippet.vcf \; -R hg19/hg19.fa \; --bam-output assembly.bam \; -L chr1:68896800-68896900 \; --ploidy 2 \; --min-pruning 2 \; --min-dangling-branch-length 2 \; --pcr-indel-model CONSERVATIVE ; ```. Then I get only a single variant reported in the region (the 9bp deletion):. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test-sample; chr1 68896832 . CTTTAGTTTT C 1597.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.000;DP=122;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=14.52;ReadPosRankSum=1.341;SOR=0.350 GT:AD:DP:GQ:PL 0/1:67,43:110:99:1605,0,2683; ```. If i run to generate a gvcf then things get more interesting:. ```; gatk HaplotypeCaller \; --input snippet.bam \; --output snippet.g.vcf \; -R hg19/hg19.fa \; -ERC GVCF \; --bam-output assembly.bam \; -L chr1:68896800-68896900 \; --ploidy 2 \; --min-pruning 2 \; --min-dangling-branch-length 2 \; --pcr-indel-model CONSERVATIVE ; ```. yields:. ```; #CHROM POS ID REF A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6538:113,release,release,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6538,1,['release'],['release']
