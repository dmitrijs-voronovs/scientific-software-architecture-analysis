quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability, 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:363); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:356); 	at lenthall.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:17); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespaceWithImports(MaterializeWorkflowDescriptorActor.scala:356); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespace(MaterializeWorkflowDescriptorActor.scala:372); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:172); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:132); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:2203,Error,ErrorOr,2203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['Error'],['ErrorOr']
Availability," "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:36948,echo,echo,36948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability," "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. # downstream task that depends on upstream task outputing all files; task DownstreamConsumer {; Array[File] txt_array; Array[File] md_array. command {; cat ${sep="" ""} txt_array > merged.txt; cat ${sep="" ""} md_array > merged.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""50"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File cat_txt = ""merged.txt""; File cat_md = ""merged.md""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:2469,Down,DownstreamConsumer,2469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,6,"['Down', 'down', 'echo', 'failure']","['DownstreamConsumer', 'downstream', 'echo', 'failure']"
Availability," ""IpcMode"": ""private"",; ""Cgroup"": """",; ""Links"": null,; ""OomScoreAdj"": 0,; ""PidMode"": """",; ""Privileged"": false,; ""PublishAllPorts"": false,; ""ReadonlyRootfs"": false,; ""SecurityOpt"": null,; ""UTSMode"": """",; ""UsernsMode"": """",; ""ShmSize"": 67108864,; ""Runtime"": ""runc"",; ""ConsoleSize"": [; 0,; 0; ],; ""Isolation"": """",; ""CpuShares"": 0,; ""Memory"": 0,; ""NanoCpus"": 0,; ""CgroupParent"": """",; ""BlkioWeight"": 0,; ""BlkioWeightDevice"": [],; ""BlkioDeviceReadBps"": null,; ""BlkioDeviceWriteBps"": null,; ""BlkioDeviceReadIOps"": null,; ""BlkioDeviceWriteIOps"": null,; ""CpuPeriod"": 0,; ""CpuQuota"": 0,; ""CpuRealtimePeriod"": 0,; ""CpuRealtimeRuntime"": 0,; ""CpusetCpus"": """",; ""CpusetMems"": """",; ""Devices"": [],; ""DeviceCgroupRules"": null,; ""DeviceRequests"": null,; ""KernelMemory"": 0,; ""KernelMemoryTCP"": 0,; ""MemoryReservation"": 0,; ""MemorySwap"": 0,; ""MemorySwappiness"": null,; ""OomKillDisable"": false,; ""PidsLimit"": null,; ""Ulimits"": null,; ""CpuCount"": 0,; ""CpuPercent"": 0,; ""IOMaximumIOps"": 0,; ""IOMaximumBandwidth"": 0,; ""MaskedPaths"": [; ""/proc/asound"",; ""/proc/acpi"",; ""/proc/kcore"",; ""/proc/keys"",; ""/proc/latency_stats"",; ""/proc/timer_list"",; ""/proc/timer_stats"",; ""/proc/sched_debug"",; ""/proc/scsi"",; ""/sys/firmware""; ],; ""ReadonlyPaths"": [; ""/proc/bus"",; ""/proc/fs"",; ""/proc/irq"",; ""/proc/sys"",; ""/proc/sysrq-trigger""; ]; },; ""GraphDriver"": {; ""Data"": {; ""LowerDir"": ""/var/lib/docker/overlay2/aa7c784c947752f9736d649c2f7f1d1ff992e94a295a7d8b3281eb18b60192f0-init/diff:/var/lib/docker/overlay2/pi10oyxckwgkkcbtbhtopthgo/diff:/var/lib/docker/overlay2/ijx4ivzmz9j7r2z9sqnxxkfr2/diff:/var/lib/docker/overlay2/jv5021rm0ro1ncxdxk9z7z4z2/diff:/var/lib/docker/overlay2/l7ti7s4rs2dxrvvlh4xry72fn/diff:/var/lib/docker/overlay2/0ecrq5dfyezvcc19ewmxvgohh/diff:/var/lib/docker/overlay2/vpue5u7q3ouhkmqlyrwbg5n75/diff:/var/lib/docker/overlay2/eh958b0fn0cjj2btiaxfhimxg/diff:/var/lib/docker/overlay2/luqo2vxej2gtqb1i70juiilf3/diff:/var/lib/docker/overlay2/jn9pv5erse0hvoixil8mb2h0k/diff:/var/lib/docker/overlay2/trwrj89a2zln5c2wh9ci255nm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:4788,Mask,MaskedPaths,4788,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,1,['Mask'],['MaskedPaths']
Availability, (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341:1633,recover,recoverWith,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341,1,['recover'],['recoverWith']
Availability," -AF |grep ag457|grep sync; ag457 3388 3387 0 26974 364 9 Oct26 ? 00:00:00 sync; ag457 5036 5035 0 26974 364 2 Oct27 ? 00:00:00 sync; ag457 21705 21704 0 26974 360 11 Oct28 ? 00:00:00 sync; ag457 22304 22303 0 26974 364 12 Oct28 ? 00:00:00 sync; ag457 22345 22344 0 26974 364 26 Oct28 ? 00:00:00 sync; ; and they have been running from a minimum of 46 hours to 85 hours for the oldest, so they are definitely stuck.; ; Bhanu and Jason (the DevOps team leader) checked the node and found out that you have several duplicate processes that are running and doing exactly the same thing.; ; This is never good and it is likely what is causing ""sync"" to hang.; ; More in detail:; ; those are all the PID associated with your username:; ; rp189@compute-p-17-32:~ ps -AF |grep ag457|awk '{printf ""%s "",$2}';echo""""; 2904 2906 3387 3388 5035 5036 12888 12890 14814 15375 15377 21704 21705 21850 21852 22303 22304 22344 22345 24802 24804 29768 29770; ; Looking at the commands being executed by some of those processes we found several duplicates which likely corrupted the file(s) where command sync was potentially flushing the buffer data, those below are the PID and associated commands (cat /proc/PID/cmdline),; you can see how there are 5 duplicates, exactly as the number of stuck ""sync"" commands:; ; 15377, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S003_M1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/d41833e7-2c4a-4a92-a9d7-fd194d3059d3/call-MergeBamAlignment/shard-0/execution/script; 21704, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S003_M1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/d41833e7-2c4a-4a92-a9d7-fd194d3059d3/call-MergeBamAlignment/shard-0/execution/script; 22344, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S005_N1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/b004cf90-7c72-498e-b378-d49cd0b0c2cb/call-SortAndFixTags/execution/script; 2906, /bin/bash/n/no_bac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4347:4987,echo,echo,4987,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4347,1,['echo'],['echo']
Availability," -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2ï¼š; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4703:1493,Error,Error,1493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703,1,['Error'],['Error']
Availability," 1 workflows from the WorkflowStoreActor; 2018-01-17 20:52:54,947 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - MaterializeWorkflowDescriptorActor [UUID(e71c769c)]: Call-to-Backend assignments: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion -> JES, SplitLargeRG.SumSplitAlignedSizes -> JES, SplitLargeRG.GatherBamFiless -> JES, SomaticPairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba -> JES, SplitLargeRG.SamSplitter -> JES, SplitLargeRG.Alignment -> JES; 2018-01-17 20:52:56,323 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-e71c769c-948f-4bd7-8cbe-064a18375966 [UUID(e71c769c)]: Starting calls: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1; 2018-01-17 20:53:02,487 cromwell-system-akka.dispatchers.backend-dispatcher-44 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: `# not setting set -o pipefail here because /bwa has a rc=1 and we dont want to allow rc=1 to succeed because; # the sed may also fail with that error and that is something we actually want to fail on.; /usr/gitc/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //'`; 2018-01-17 20:53:04,348 cromwell-system-akka.dispatchers.backend-dispatcher-56 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: job id: operations/EPXh4LeQLBjT9Z2WvIiz-QggqeCbgo4VKg9wcm9kdWN0aW9uUXVldWU; 2018-01-17 20:53:15,636 cromwell-system-akka.dispatchers.backend-dispatcher-56 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from - to Running; 2018-01-17 20:56:25,285 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from Running to Success; 2018-01-17 20:56:28,223 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156:9204,error,error,9204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156,1,['error'],['error']
Availability," 12:24:32,94] [info] BT-322 b303ae23:expanse_figures.CBL_assoc:-1:1 is eligible for call caching with read = true and write = true; [2023-03-29 12:24:32,97] [info] BT-322 b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1 is eligible for call caching with read = true and write = true; [2023-03-29 12:35:42,07] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1-20000000023 [b303ae23expanse_figures.CBL_hom_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,07] [info] BT-322 b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = 93DAD89F707FA490E2A46FFAC924DFFF.; [2023-03-29 12:35:42,07] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,08] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1-20000000024 [b303ae23expanse_figures.CBL_hom_not_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,08] [info] BT-322 b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:1276,failure,failures,1276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['failure'],['failures']
Availability," 14:45 foo/bar3.wdl; 99 02-07-2017 14:45 foo/bar5.wdl; 99 02-07-2017 14:45 foo/bar4.wdl; 99 02-07-2017 14:45 foo/bar6.wdl; --------- -------; 1089 12 files; ```. The content of all the task dependencies is just a variation on:; ```; [conradL@qimr13054 ~]$ cat foo/bar.wdl ; task doIt {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; }; ```. Submit to the server:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@goodImport.wdl -FwdlDependencies=@foo.zip; ```. Now tailing the server logs, the first time this is submitted, the workflow succeeds and the log shows nothing out of the ordinary. But ""sometimes"" (meaning, I can submit it 5 times and not see it, or twice and see it both times) I see this:; ```; 2017-02-07 15:01:10,781 cromwell-system-akka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.772+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:1569,ERROR,ERROR,1569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability," 2016-08-03 15:20:01,502 cromwell-system-akka.dispatchers.backend-dispatcher-107 INFO - $a [UUID(eaeaa32d)DeliciousFileSpam.StringSpam:215:1]: JesAsyncBackendJobExecutionActor [UUID(eaeaa32d):DeliciousFileSpam.StringSpam:215:1] Status change from Running to Success; 2016-08-03 15:20:01,923 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: Job DeliciousFileSpam.StringSpam:215:1 succeeded!; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionFailedState. Shutting down.; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] done. Shutting down.; 2016-08-03 15:20:03,593 cromwell-system-akka.dispatchers.engine-dispatcher-85 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transitioning from ExecutingWorkflowState to FinalizingWorkflowState; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationPendingState to FinalizationInProgressState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationInProgressState to FinalizationSucceededState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-138 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transitioning from FinalizingWorkflowState to WorkflowFailedState; 2016-08-03 15:20:03,594 cromwell-system",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2887:1191,down,down,1191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2887,1,['down'],['down']
Availability," 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4985,checkpoint,checkpointClose,4985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5442,checkpoint,checkpointClose,5442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5899,checkpoint,checkpointClose,5899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6356,checkpoint,checkpointClose,6356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21:14:50,85] [info] checkpointClose script done; [2022-12-15 21:14:50,85] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6813,checkpoint,checkpointClose,6813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:50,95] [info] checkpointClose synched; [2022-12-15 21:14:50,98] [info] checkpointClose script done; [2022-12-15 21:14:50,98] [info] dataFileCache commit start; [2022-12-15 21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9426,checkpoint,checkpointClose,9426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9883,checkpoint,checkpointClose,9883,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10340,checkpoint,checkpointClose,10340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10797,checkpoint,checkpointClose,10797,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11254,checkpoint,checkpointClose,11254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b254006"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""wr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11711,checkpoint,checkpointClose,11711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 2381; 470 pool-10-t 4751; 470 pool-10-t 2381; 282 G1 4751; 282 G1 2381; 188 blaze-tic 4751; 188 blaze-tic 2381; 94 VM 4751; 94 VM 2381; 94 java 4751; 94 java 2381; 94 db-9 4751; 94 db-9 2381; 94 db-8 4751; 94 db-8 2381; 94 db-7 4751; 94 db-7 2381; 94 db-6 4751; 94 db-6 2381; 94 db-5 4751; 94 db-5 2381; 94 db 4751; 94 db-4 4751; 94 db-4 2381; 94 db-3 4751; 94 db-3 2381; 94 db-2 4751; 94 db 2381; 94 db-2 2381; 94 db-20 4751; 94 db-20 2381; 94 db-19 4751; 94 db-19 2381; 94 db-18 4751; 94 db-18 2381; 94 db-17 4751; 94 db-17 2381; 94 db-16 4751; 94 db-16 2381; 94 db-15 4751; 94 db-15 2381; 94 db-1 4751 ...; ```. this is my java command; ```{shell}; java -Xms10M -Xmx125M -Dconfig.file=SGE.conf -jar cromwell-86.jar run xxx.wdl --inputs xxx.json; ```. SGE.conf file:; ```; # Documentation:; # https://cromwell.readthedocs.io/en/stable/backends/SGE. backend {; default = SGE. providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue = ""xxx""; String? sge_project = ""xxx""; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l num_proc="" + cpu + "",virtual_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; -binding ${""linear:"" + cpu} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7571:1710,alive,alive,1710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7571,4,['alive'],['alive']
Availability," 24 was b71f06f. This works on 19 (yes we're still on 19 =/). Task (stuff removed):. ```; task ValidateSamFile {; File input_bam; File? input_bam_index; String report_filename; File? ref_dict; File? ref_fasta; File? ref_fasta_index; Int? max_output; Array[String]? ignore; Int disk_size; Int preemptible_tries. command {; java -Xmx4000m -jar stuff.jar blah; }; runtime {; memory: ""7 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File report = ""${report_filename}""; }; }; ```. Call (note there is no value supplied for max_output):. ```; call ValidateSamFile as ValidateReadGroupSamFile {; input:; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; input_bam = SortAndFixReadGroupBam.output_bam,; report_filename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".validation_report"",; disk_size = flowcell_medium_disk,; preemptible_tries = preemptible_tries; }; ```. error in server logs:; ```; 2017-01-23 15:09:09 [cromwell-system-akka.actor.default-dispatcher-89] ERROR c.b.i.j.JesAsyncBackendJobExecutionActor - JesAsyncBackendJobExecutionActor [UUID(8f35e32d)PairedEndSingleSampleWorkflow.Vali; dateReadGroupSamFile:1:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.colle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1943:1184,error,error,1184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943,1,['error'],['error']
Availability," = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; runtime-attributes = """"""; String? docker; """"""; submit = ""/usr/bin/env bash ${script}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; ""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')""; """"""; filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; hasing-strategy: ""fingerprint""; check-sibling-md5: true; fingerprint-size: 1048576 # 1 MB ; }; }; }; }; }; # For running jobs by submitting them from an interactive node to the cluster; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:4712,echo,echo,4712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['echo'],['echo']
Availability," > jobs ; > ; > Kristian Cibulskis ; > Director of Platform Engineering, Data Sciences Platform ; > Broad Institute of MIT and Harvard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:4226,failure,failure,4226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failure']
Availability," Array[File?] vcfs = select_first([task_a.vcf_out, task_b.vcf_out]); ```; Due to this bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[String?] or Array[String?]?. [An example WDL, which passes womtool and miniwdl check, is available here.](https://gist.github.com/aofarrel/547c35468c248331b678b3f766f83591) It actually shows the issue twice -- once in the section starting with `if(defined(variant_call_after_earlyQC_filtering.errorcode)) {` and once in the",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:1354,error,errorcode,1354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,2,"['echo', 'error']","['echo', 'errorcode']"
Availability," Cromwell runs the WDL tasks with the correct command line, but somehow the arguments after the initial command aren't being picked up by the binary inside the docker container. It seems like only the first argument is actually being used. This isn't an issue with my python script, because I can run it directly and everything works fine. Cromwell showing the command line:; ```; cromwell_1 | 2018-11-12 06:57:56,451 cromwell-system-akka.dispatchers.backend-dispatcher-40 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(5d4c4459)germline_variant_calling.fastqc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(Pipelines",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1127,error,error,1127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['error'],['error']
Availability," File filtered_vcf_idx = select_first([FilterAlignmentArtifacts.filtered_vcf_idx, Filter.filtered_vcf_idx]); File filtering_stats = Filter.filtering_stats; File mutect_stats = MergeStats.merged_stats; File? contamination_table = CalculateContamination.contamination_table. File? funcotated_file = Funcotate.funcotated_output_file; File? funcotated_file_index = Funcotate.funcotated_output_file_index; File? bamout = MergeBamOuts.merged_bam_out; File? bamout_index = MergeBamOuts.merged_bam_out_index; File? maf_segments = CalculateContamination.maf_segments; File? read_orientation_model_params = LearnReadOrientationModel.artifact_prior_table; }. }; }. task CramToBam {; input {; File ref_fasta; File ref_fai; File ref_dict; #cram and crai must be optional since Normal cram is optional; File? cram; File? crai; String name; Int disk_size; Int? mem; }. Int machine_mem = if defined(mem) then mem * 1000 else 6000. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefail. samtools view -h -T ~{ref_fasta} ~{cram} |; samtools view -b -o ~{name}.bam -; samtools index -b ~{name}.bam; mv ~{name}.bam.bai ~{name}.bai; }. runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai""; }; }. task SplitIntervals {; input {; File? intervals; File ref_fasta; File ref_fai; File ref_dict; Int scatter_count; String? split_intervals_extra_args. # runtime; Runtime runtime_params; }. command {; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. mkdir interval-files; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" SplitIntervals \; -R ~{ref_fasta} \; ~{""-L "" + intervals} \; -scatter ~{scatter_count} \; -O interval-files \; ~{split_intervals_extra_args}; cp interval-files/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:19569,failure,failure,19569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['failure'],['failure']
Availability," Int mem\n\n # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks\n # If not, coverage and target files (received from upstream) for WES are passed downstream\n command {\n if [ ${isWGS} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \\\n --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \\\n else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n File gatk_target_file = \""${entity_id}.coverage.tsv.targets.tsv\""\n }\n}\n\n# Add new columns to an existing target table with various targets\n# Note that this task is optional \ntask AnnotateTargets {\n String entity_id\n File target_file\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then an empty file gets passed downstream\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \\\n else touch ${entity_id}.annotated.tsv; \\\n fi\n }\n\n output {\n File annotated_targets = \""${entity_id}.annotated.tsv\""\n }\n}\n\n# Correct coverage for sample-specific GC bias effects\n# Note that this task is optional \ntask CorrectGCBias {\n String entity_id\n File coverage_file\n File annotated_targets\n String gatk_jar\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then the coverage file gets passed downstream unchanged\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \\\n --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \\\n else ln -s $",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:38561,down,downstream,38561,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['down'],['downstream']
Availability," Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:2014,error,errors,2014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['errors']
Availability," SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6855,alive,alive,6855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['alive'],['alive']
Availability," The memory-error-key is caught and memory is increased as defined in memory-retry-multiplier.; I also see this failure message in metadata.json:; _""message"": ""stderr for job `MemoryRetryTest.TestBadCommandRetry:NA:1` contained one of the `memory-retry-error-keys: [Killed]` specified in the Cromwell config. Job might have run out of memory.""_. Grepping metadata for memory of this job, I see the expected behaviour:; ""memory"": ""1 GB"",; ""memory"": ""2 GB"",. The second task, **TestOutOfMemoryRetry** is designed to fail do to real out of memory error.; The purpose of this task is to shoe that memory-retry mechanism is not working when a task runs out of memory, even if ""Killed"" is written to stderr. Result of TestOutOfMemoryRetry:; When this task is run, it fails but **the job is retried with the same amount of memory**.; This time I see the following failure message:; _""message"": ""Task MemoryRetryTest.TestOutOfMemoryRetry:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running \""/cromwell_root/script\"": unexpected exit status 137 was not ignored\n[UserAction] Unexpected exit status 137 while running \""/cromwell_root/script\"": Killed\n"",_. Grepping metadata for memory of this job, I see the memory expension is not working:; ""memory"": ""1 GB"",; ""memory"": ""1 GB"",; ; I have verified ""Killed"" is written correctly to stderr :; ```; gsutil cat gs://<out_bucket>/cromwell-execution/MemoryRetryTest/3035199e-bf2b-49a2-be87-483; 9e96a08eb/call-TestOutOfMemoryRetry/stderr; Killed ; ``` . We have also noticed that in the out of memory case, no retrurnCode is written to the metadata. **Test wdl for reproduction:**; `version 1.0. workflow MemoryRetryTest {; input {; String message = ""Killed""; }; call TestOutOfMemoryRetry {}; call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; command <<<; echo ""Killed"" >&2; tail /dev/zero; >>>; runtime {; docker: ""ubuntu:latest""; cpu: ""1""; memory: ""1 GB""; disks:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205:1520,error,error,1520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205,1,['error'],['error']
Availability," The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value.`. I get this error whether or not the variant caller task actually ran, even though whether or not it ran should cause an issue, since it's under a defined() check. If the defined() check still is not enough like is the case for setting not_optional_error_code, then that should be caught before runtime. ## backends effected; The womtool validation bug affects at least Terra-womtool and local-womtool. Runtime error happened",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:2445,error,errorcode,2445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability," Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator not found or versions do not match (Service: null; Status Code: 404; Request ID: 9914238b-00c2-11e9-a13d-cdc28a8016c8); ```. Looking at cloudtrail, here is the event associated with that request ID:. [Event](https://gist.github.com/dtenenba/909f16e720a01b00a736cf6e60f7083a). If I pull out just the contents of the `requestParameters` section and call RegisterJobDefinition using t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1652,error,error,1652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['error'],['error']
Availability," WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:21:09,96] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Parsing workflow as WDL draft-2; [2018-10-25 21:21:10,57] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:21:12,86] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Condition met: 'go'. Running conditional section; [2018-10-25 21:21:16,98] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/script; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e705",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:4056,echo,echo,4056,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability," [2018-10-25 21:17:13,87] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:17:13,95] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Parsing workflow as WDL draft-2; [2018-10-25 21:17:14,52] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:17:20,89] [info] WorkflowExecutionActor-e22c6324-5aec-4694-8750-f62160e2ca81 [e22c6324]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-2/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-1/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-0/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324te",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:12895,echo,echo,12895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability," [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4760,Checkpoint,Checkpoint,4760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability," [e22c6324test_opt_array.t1:1:1]: Status change from - to WaitingForReturnCode; [2018-10-25 21:17:28,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: Status change from - to WaitingForReturnCode; [2018-10-25 21:17:28,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: Status change from - to Done; [2018-10-25 21:17:28,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: Status change from - to Done; [2018-10-25 21:17:30,83] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: Status change from WaitingForReturnCode to Done; [2018-10-25 21:17:34,04] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: Status change from WaitingForReturnCode to Done; [2018-10-25 21:17:34,39] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: Status change from WaitingForReturnCode to Done; [2018-10-25 21:17:38,14] [error] WorkflowManagerActor Workflow e22c6324-5aec-4694-8750-f62160e2ca81 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'test_opt_array.arr2' (reason 1 of 1): Evaluating select_first([t1.out, arr1]) failed: assertion failed: base member type WomMaybeEmptyArrayType(WomAnyType) and womtype WomMaybeEmptyArrayType(WomSingleFileType) are not compatible; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:510); at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:73); at cats.instances.ListInstances$$anon$1.loop$2(list.scala:63); at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:63); at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:276); at cats.Eval$.loop$1(Eval.scala:338); at cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); at cats.Eval$Defer.va",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:16128,error,error,16128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['error'],['error']
Availability," `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value.`. I get this error whether or not the variant caller task actually ran, even though whether or not it ran should cause an issue, since it's under a defined() check. If the defin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:2198,error,errorcode,2198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability," `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:1998,error,error,1998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,6,['error'],"['error', 'errors']"
Availability," a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:1545,echo,echo,1545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,1,['echo'],['echo']
Availability," a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Our backend: ; GCP PAPIv2 ; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"". endpoint-url = ""https://genomics.googleapis.com/"". <!-- Paste/Attach your workflow if possible: -->; workflow runtime; runtime {; docker: ""us.gcr.io/cloudypipelines-com/til_segmentation:1.5""; bootDiskSizeGb: 70; disks: ""local-disk 70 SSD""; memory: ""52 GB""; cpu: ""8""; maxRetries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59154 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_quip_lymphocyte_segmentation_v03232020.quip_lymphocyte_segmentation:NA:2 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chrom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:1935,error,errors,1935,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['error'],['errors']
Availability," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:1592,error,error,1592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,4,"['error', 'failure']","['error', 'failures']"
Availability," actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; \""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')\""; ""; """"""; kill-docker = ""scancel ${job_id}"". filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; check-sibling-md5: true; hasing-strategy: ""fingerprint""; fingerprint-size: 1048576 # 1 MB ; }; }; }. }; }; }}; ```. Note: there are some WDL parameters relevant to DNANexus's dxCompiler. I'm hoping this code will be able to run on that system eventually, but I understand that those parameters are not relevant to cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:6287,echo,echo,6287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,2,['echo'],['echo']
Availability," be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:13687,down,downloading,13687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['down'],['downloading']
Availability," builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:1041,Error,Errors,1041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['Error'],['Errors']
Availability," com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1092); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1040); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1350); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchedInserts(ClientPreparedStatement.java:716); 	... 27 common frames omitted; ```. Initially nothing was persisting to the database making debugging tricky, so I updated the database to increase the column size of METADATA_ENTRY.METADATA_KEY from varchar(255) to something silly like varchar(3000) and re-ran the offending script. The culprit showed itself as:. ```; SELECT length(METADATA_KEY) FROM cromwell.METADATA_ENTRY, x.METADATA_KEY xWHERE length(METADATA_KEY) = (SELECT max(length(METADATA_KEY)) FROM METADATA_ENTRY). 323, ""inputs:batch_files:/mnt/data/cromwell-executions/build_bob_ep/40573452-6a92-4e26-8f0d-02bd980970b7/call-build_bob/build_bob/b6e60c8e-2d0a-4db7-8b70-b71cde217b30/call-building_taxonomy/building_taxonomy/c2e537ff-e231-4b51-a423-f750604dca7c/call-classify_f33ef23grwsg32fgv/inputs/2065711490/GTDB_GB_GCA_123456789.1.mask.fasta""; ```. Basically, as the complexity of the workflows increases the potential length of the inputs increases and the limit of varchar(255) is exceeded. Going forward, this will not be our most complicated workflow so I expect to hit this more frequently. So firstly, am I doing anything wrong?. Secondly would it be possible to increase the maximum size of the column METADATA_KEY that can accommodate increasing levels workflow complexity? I can do this post deployment using ansible but that feels a little bit messy. (I have also posted this on your JIRA backlog as Im not sure which is the best place to raise this: https://broadworkbench.atlassian.net/browse/CROM-6721). Best,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6545:4957,mask,mask,4957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6545,1,['mask'],['mask']
Availability," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4709:1718,error,errors,1718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709,3,['error'],['errors']
Availability," default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be create",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2701,alive,alive,2701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['alive'],['alive']
Availability," e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] WorkflowManagerActor Successfully started WorkflowActor-e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-25 21:17:13,86] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-25 21:17:13,87] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:17:13,95] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Parsing workflow as WDL draft-2; [2018-10-25 21:17:14,52] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:17:20,89] [info] WorkflowExecutionActor-e22c6324-5aec-4694-8750-f62160e2ca81 [e22c6324]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-2/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/sh",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:12529,echo,echo,12529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability," events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:2117,error,error,2117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['error']
Availability," field seems to have an inconsistent format:. Compare the failures sections for the following:. ```; {; ""workflowName"": ""echo_strings"",; ""submittedFiles"": {; ""inputs"": ""{...},; ""calls"": {; ""echo_strings.echo_files"": [{; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""machineType"": ""us-central1-c/n1-standard-1"",; ""googleProject"": ""broad-dsde-dev"",; ""executionBucket"": ""gs://cromwell-dev/cromwell-executions"",; ""zone"": ""us-central1-c"",; ""instanceName"": ""ggp-3462354720519617596""; },; ""runtimeAttributes"": {...},; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {...; },; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""jobId"": ""operations/EJiq_oWfKxi8-N-X4qiwhjAgw7vetLsXKg9wcm9kdWN0aW9uUXVldWU"",; ""backend"": ""JES"",; ""end"": ""2017-01-30T19:14:19.708Z"",; ""stderr"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""attempt"": 1,; ""executionEvents"": [...],; ""backendLogs"": {; ""log"": ""gs://fc-2d3fd356-e3be-4953-92f1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:1050,failure,failures,1050,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,2,"['error', 'failure']","['error', 'failures']"
Availability, file or directory); 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048); 	at cromwell.backend.sfs.ProcessRunner.run(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Standard,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1495,recover,recover,1495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Availability," files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised at indexdb.c:2885; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/log/defuse.12.sh: line 6: 7481 Segmentation fault (core dumped) /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; ; real 0m1.262s; user 0m0.046s; sys 0m0.564s. ```. Run within the docker container but not using Cromwell, the output of that command looks like this:; ```; Starting defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /mnt/Workflow_runs/2_fusion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.cdna.psl m; issing; Success for defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d est4 -f psl /mnt/Workflow_runs/2_fus; ion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa > /mnt/Workflow_runs/2_fusion_detection_to; ols/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.est.4.psl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:2768,fault,fault,2768,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['fault'],['fault']
Availability," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:5618,down,downloaded,5618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,1,['down'],['downloaded']
Availability," from the cache, Cromwell seems to lock up after the first handful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:1183,Error,Error,1183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,1,['Error'],['Error']
Availability," function to join arrays of string together. I've followed the general process from: https://github.com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.v",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1144,error,error,1144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability," had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.white_brits_sample_list:-1:1-20000000013 [788d8048main.white_brits_sample_list:NA:1]: Unrecognized runtime attribute keys:; shortTask, dx_timeout; [2022-12-15 21:28:04,01] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.low_genotyping_quality_sample_list:-1:1-20000000014 [788d8048main.low_genotyping_quality_sample_list:NA:1]: Unrecognized ru; ntime attribute keys: shortTask, dx_timeout; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.white_brits_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = 9675960412B5394D5D0816ED198FB6EB.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.white_brits_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.low_genotyping_quality_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 3C891C9939496580DDF747805F991E06, file = AAFFF98AC7D58B07E7CE25978A906B00.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.low_genotyping_quality_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,02] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_mismatch_sample_list:-1:1-20000000015 [788d8048main.sex_mismatch_sample_list:NA:1]: Unrecognized runtime attribute keys; : shortTask, dx_timeout; [2022-12-15 21:28:04,02] [info] BT-322 788d8048:main.sex_mismatch_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 03340ED60152B24B7D0988669F47CF2B, file = EB6A9909BDF3705B7BB543E4096DA08A.; [2022-12-15 21:28:04,02] [i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:36049,failure,failures,36049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability," https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a072",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136:1472,Failure,Failure,1472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136,1,['Failure'],['Failure']
Availability," if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:17:00.575023Z""; },; {; ""startTime"": ""2018-08-14T16:13:13.678Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-08-14T16:13:13.678Z""; },; {; ""startTime"": ""2018-08-14T16:16:42.510303Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:16:43.002063Z""; },; {; ""startTime"": ""2018-08-14T16:17:00.575023Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:19262,echo,echo,19262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:12282,error,errors,12282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,4,['error'],"['error', 'errors']"
Availability," itself will hold most of this granularity, it should be useful for Cromwell to group certain actions into a higher level grouping concept based on its understanding of the job. For example, here's the execution events from a single run: ; ```; ""executionEvents"": [; {; ""startTime"": ""2018-08-14T16:16:40.069663Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:42.510303Z""; },; {; ""startTime"": ""2018-08-14T16:16:43.002063Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:1111,echo,echo,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability," list of globbed files."" > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:19152,echo,echo,19152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability," log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(Liquibas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2210,ERROR,ERROR,2210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['ERROR'],['ERROR']
Availability," make memory retry work on our system without sucess. ; Read all docs and previous issues I could find, but it still doesn't work for us. I have written a test wdl with two tasks, both write ""Killed"" to stderr, and supposed to get retried with more memory. The first task, **TestBadCommandRetry** is designed to fail regularly with rc 127, due to a bad command.; The purpose of this task is to prove the memory-retry mechanism is configured correctly in our system. Result of TestBadCommandRetry:; The memory-error-key is caught and memory is increased as defined in memory-retry-multiplier.; I also see this failure message in metadata.json:; _""message"": ""stderr for job `MemoryRetryTest.TestBadCommandRetry:NA:1` contained one of the `memory-retry-error-keys: [Killed]` specified in the Cromwell config. Job might have run out of memory.""_. Grepping metadata for memory of this job, I see the expected behaviour:; ""memory"": ""1 GB"",; ""memory"": ""2 GB"",. The second task, **TestOutOfMemoryRetry** is designed to fail do to real out of memory error.; The purpose of this task is to shoe that memory-retry mechanism is not working when a task runs out of memory, even if ""Killed"" is written to stderr. Result of TestOutOfMemoryRetry:; When this task is run, it fails but **the job is retried with the same amount of memory**.; This time I see the following failure message:; _""message"": ""Task MemoryRetryTest.TestOutOfMemoryRetry:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running \""/cromwell_root/script\"": unexpected exit status 137 was not ignored\n[UserAction] Unexpected exit status 137 while running \""/cromwell_root/script\"": Killed\n"",_. Grepping metadata for memory of this job, I see the memory expension is not working:; ""memory"": ""1 GB"",; ""memory"": ""1 GB"",; ; I have verified ""Killed"" is written correctly to stderr :; ```; gsutil cat gs://<out_bucket>/cromwell-execution/MemoryRetryTest/3035199e-bf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205:1067,error,error,1067,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205,1,['error'],['error']
Availability," may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:887,error,error,887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['error'],['error']
Availability," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:2583,avail,available,2583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['avail'],['available']
Availability," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:2413,alive,alive,2413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,2,"['alive', 'error']","['alive', 'error']"
Availability," name-for-call-caching-purposes: PAPI; slow-job-warning-time: ""24 hours""; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600; request-workers = 3; genomics {; auth = ""application-default""; endpoint-url = ""https://genomics.googleapis.com/""; location = ""us-west1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""application-default""; project = ""xxxx""; caching {; duplication-strategy = ""copy""; }; }; http { }; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-west1-a"", ""us-west1-b""]; }; include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```; When I run with the above config using:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json; ```; I am getting the following error message:; ```; [2021-08-24 22:05:33,60] [info] WorkflowManagerActor: Workflow 6cc303b4-295d-49fa-a996-b5cf7ec9beea failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Execution failed: allocating: creating instance: inserting instance: Invalid value for field 'resource.networkInterfaces[0].network': ''. The referenced network resource cannot be found.; ```; I have tried passing the vpc and subnet id using the following config:; ```; virtual-private-cloud {; network-label-key = ""xxx""; subnetwork-label-key = ""xxx""; auth = ""application-default""; }; ```. The above values are my actual vpc and subnet id/name. However, it is still giving me that error message. Is there something I am missing from a configuration perspective. Any help would be greatly appreciated. Our VPC network's are not created in auto mode and that is not something we have control over unfortunately. Thanks,; -Simran",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477:1759,error,error,1759,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477,3,['error'],['error']
Availability," new variable based on an optional variable. Instead, Cromwell fails at runtime -- even if it is actually impossible for that optional variable to be undefined. [A working example is available](https://github.com/aofarrel/myco/commit/e7f9ba6951d1b0fe5b3c1a650835312dd2b6e68f), but it is a complex WDL, so a more basic example is listed below. ## background; WDL doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:1040,error,errorcode,1040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability," of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies use",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:87710,avail,available,87710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['avail'],['available']
Availability," optional value: overwrite_given_value. **Example**; _For this examples I use womtool 84 and WDL 1.0. I also tried with womtool 53.1 and 83 which showed the same issue_; The following files are used to show case the issue.; The main workflow:; ```; version 1.0. import ""SubWorkflow.wdl"" as SubWorkflow. workflow MainWorkflow {; input {; String value_2_give = ""default value""; }; call MainTask {; input:; given_value = value_2_give; }. call SubWorkflow.SubWorkflow {; input:; value_2_give = value_2_give; }; }. task MainTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. The subworkflow:; ```; version 1.0. workflow SubWorkflow {; input {; String value_2_give = ""default value""; String? overwrite_value_2_give; }; call SubTask {; input:; given_value = select_first([overwrite_value_2_give, value_2_give]); }; }. task SubTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. To be sure I ran the validation mode of womtools:; ```; $ java -jar womtool-84.jar validate MainWorkflow.wdl; Success!; $ java -jar womtool-84.jar validate SubWorkflow.wdl; Success!; ```. After creating these files, I ran womtool with the ""inputs"" option getting the following output:; ```; $ java -jar womtool-84.jar inputs MainWorkflow.wdl; {; ""MainWorkflow.SubWorkflow.overwrite_value_2_give"": ""String? (optional)"",; ""MainWorkflow.MainTask.overwrite_given_value"": ""String? (optional)"",; ""MainWorkflow.value_2_give"": ""String (optional, default = \""default value\"")""; }; ```; This output json shows which variables you can (or must) provide in order to be able to run in this case the main workflow. here we see that we are able to provide values for the Mainworkflow, MainTask and SubWorkflow but not the SubTask.; If we do the same for just the subworkflow:; ```; $ java -jar womtool-84.jar inputs SubWorkflow.wdl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6841:1210,echo,echo,1210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6841,1,['echo'],['echo']
Availability," provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.â€‚â€‚. Current deploy modes supported for any spark job:; â€‚â€‚a - Client deploy mode using the spark standalone cluster manager; â€‚â€‚b - Cluster deploy mode using the spark standalone cluster manager; â€‚â€‚c - Client deploy mode using Yarn resource manager; â€‚â€‚d - Cluster deploy mode using Yarn resource manager; â€‚â€‚; Future PR Plans:; â€‚â€‚In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).; â€‚â€‚This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation. â€‚â€‚In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.; â€‚â€‚Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:; â€‚â€‚In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.; â€‚â€‚Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339:1721,error,error,1721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339,1,['error'],['error']
Availability," runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 300000ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1791,echo,echo,1791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,7,"['alive', 'echo']","['alive', 'echo']"
Availability," script, because I can run it directly and everything works fine. Cromwell showing the command line:; ```; cromwell_1 | 2018-11-12 06:57:56,451 cromwell-system-akka.dispatchers.backend-dispatcher-40 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(5d4c4459)germline_variant_calling.fastqc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsync",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1381,error,error,1381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['error'],['error']
Availability," submitted dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] WorkflowManagerActor Successfully started WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,67] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [2016-07-13 10:12:46,07] [info] Running with database db.url = jdbc:hsqldb:mem:937e84db-703a-4f18-8e6d-1a2a18227cf5;shutdown=false;hsqldb.tx=mvcc; [2016-07-13 10:12:46,43] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: Call-to-Backend assignments: three_step.ps -> JES, three_step.cgrep -> JES, three_step.wc -> JES; [2016-07-13 10:12:46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:1682,down,down,1682,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,1,['down'],['down']
Availability," the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInsta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1471,error,error,1471,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability," the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:18886,echo,echo,18886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability," the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_â€¦ ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_â€¦ . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcompone",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:2470,Down,Downloading,2470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['Down'],['Downloading']
Availability," to access the specified bucket:; filesystems {; gcs {; # A reference to the auth to use for storing and retrieving metadata:; auth = ""user-service-account""; }; }. # Which bucket to use for storing the archived metadata; bucket = ""{{ backend_bucket }}""; }; ```. when the user-service-account auth is declared up in the configuration :; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```; We got the following error in Cromwell server initialization :; cromwell_1 | [ERROR] [06/21/2023 11:55:25.094] [cromwell-system-akka.actor.default-dispatcher-30] [akka://cromwell-system/user] Failed to parse the archive-metadata config:; cromwell_1 | Failed to construct archiver path builders from factories (reason 1 of 1): Missing parameters in workflow options: user_service_account_json; cromwell_1 | akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService: exception during creation; cromwell_1 | 	at akka.actor.ActorInitializationException$.apply(Actor.scala:202); cromwell_1 | 	at akka.actor.ActorCell.create(ActorCell.scala:698); cromwell_1 | 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:549); cromwell_1 | 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); cromwell_1 | 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); cromwell_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260). this code line causes the error above:; https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/metadata/impl/archiver/ArchiveMetadataConfig.scala#L38:. `PathBuilderFactory.instantiatePathBuilders(pathBuilderFactories.values.toList, WorkflowOptions.empty)`. `WorkflowOptions.empty` probably doesn't have a user_service_account_json set...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7171:2463,error,error,2463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171,1,['error'],['error']
Availability," trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:3956,echo,echo,3956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['echo'],['echo']
Availability," two runs at the same time since cromwell db gets locked by the previous run until it is finished? If yes, is there any other way to do it?. PS: I understand that cromwell provides `server` mode where we can submit runs via REST API end points. However, we are working on HPC cluster where we don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1375,alive,alive,1375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,2,['alive'],['alive']
Availability, type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4920:1469,recover,recoverWith,1469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920,1,['recover'],['recoverWith']
Availability," what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:10016,down,down,10016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['down'],['down']
Availability," with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.categorical_covariates' (scatter index: Some(0), attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.pcs' (scatter index: None, attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.ethnicity_self_report' (scatter index: None, attempt 1); [2022-12-15 21:28:01,17] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Starting main.white_brits_sample_list, main.sex_aneuploidy_sample_list, main.low_genotyping_quality_sample_list, m; ain.sex_mismatch_sample_list, main.load_shared_covars; [2022-12-15 21:28:03,68] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 5; [2022-12-15 21:28:03,69]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:32083,failure,failures,32083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability," work happens inside several levels of sub-workflows. ![image](https://github.com/broadinstitute/cromwell/assets/47044104/c2fa4113-3f6b-47aa-ad8d-62ab55fb374c). ## Details about the example WDL. * A root workflow (`main_workflow.wdl`) creates a subworkflow (LEVEL_1), `call outer.outer_workflow`; ``` wdl; import ""outer_subworkflow.wdl"" as outer. workflow main_workflow {; call outer.outer_subworkflow; }; ```; * LEVEL_1 `outer_subworkflow.wdl` then creates a scatter of 2 across another subworkflow (`call inner.inner_subworkflow`/LEVEL_2A and LEVEL_2B); ``` wdl; import ""inner_subworkflow.wdl"" as inner. workflow outer_subworkflow {; scatter (i in range(2)) {; call inner.inner_subworkflow as inner_subworkflow; }; }; ```; * `inner_subworkflow.wdl`/LEVEL_2A and LEVEL_2B then runs a task with a scatter and a scatter of 3 across a final subworkflow (`call sub_workflow.sub_subworkflow`/ LEVEL_2_X__3_Y); ``` wdl; import ""sub_subworkflow.wdl"" as sub_subworkflow. task hello_world {; command {; echo 'Hello, world!'; echo 'blah' > output.txt ; }. output {; String message = read_string(stdout()); File outputFile = ""output.txt""; }. runtime {; docker: ""ubuntu:latest""; }; }. workflow inner_subworkflow {; scatter (i in range(4)) {; call hello_world; }; scatter (i in range(3)) {; call sub_subworkflow.sub_subworkflow; }; }; ```; * This final `sub_subworkflow.wdl` then runs a scatter across a task:; ``` wdl; task sub_hello_world {; command {; echo 'Hello from sub.sub_workflow, world!'; }. output {; String message = read_string(stdout()); }. runtime {; docker: ""ubuntu:latest""; }; }. workflow sub_subworkflow {; scatter (i in range(2)) {; call sub_hello_world; }; }; ```. In tree form you have something like this:; * ROOT_WORKFLOW `main_workflow.wdl`; * LEVEL_1 `outer_subworkflow.wdl`; * LEVEL_2A `inner_subworkflow.wdl`; * LEVEL_2_A__3_A `sub_subworkflow.wdl`; * LEVEL_2_A__3_B `sub_subworkflow.wdl`; * LEVEL_2_A__3_C `sub_subworkflow.wdl`; * LEVEL_2B `inner_subworkflow.wdl`; * LEVEL_2_B__3_A `su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7387:2772,echo,echo,2772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7387,2,['echo'],['echo']
Availability," workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.04 and int=3 together I get `No coercion defined from wom value(s) '3.0' of type 'Float' to 'Int?'.` which is strange (seems like it's casting the 3 to 3.0 etc.). In any case this only seems to be a problem with defining structs inline. The conversion seems to work okay if you use input.json or if you read_json from some params.json at runtime (however I want to fix parameters inline for my current use case). The current workaround is to use object definition inline:. ```; Params p = object {; boolean: true,; float: 0.04; } ; ```. This is okay but it seems like the object syntax is going to be deprecated in newer versions (and the WDL 1.0 spec doesn't have any examples of this). Also it's generally awkward that the JSON syntax works for some combinations but not others. Thanks for any insights!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414:1879,echo,echo,1879,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414,1,['echo'],['echo']
Availability," {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; auth = ""cromwell-service-account"". # For billing; project = ""${billing_project}"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:2519,down,downloading,2519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['down'],['downloading']
Availability," {; input:; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; input_bam = SortAndFixReadGroupBam.output_bam,; report_filename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".validation_report"",; disk_size = flowcell_medium_disk,; preemptible_tries = preemptible_tries; }; ```. error in server logs:; ```; 2017-01-23 15:09:09 [cromwell-system-akka.actor.default-dispatcher-89] ERROR c.b.i.j.JesAsyncBackendJobExecutionActor - JesAsyncBackendJobExecutionActor [UUID(8f35e32d)PairedEndSingleSampleWorkflow.Vali; dateReadGroupSamFile:1:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at scala.util.Try$.apply(Try.scala:192); ```; in metadata:; ```; failures: [; {; causedBy: {; message: ""Could not find declaration for WdlOptionalValue(WdlIntegerType,None)""; },; message: ""java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1943:2611,failure,failures,2611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943,1,['failure'],['failures']
Availability," }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""eu-west4-a"",""eu-west4-b"",""eu-west4-c""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. Other info:; Debian GNU/Linux 10 (buster); openjdk version ""11.0.9.1-internal"" 2020-11-04 (through MiniConda, also tried with openjdk version ""11.0.12"" 2021-07-20, no difference to failure message). Permissions for service-account (quite liberal); ![image](https://user-images.githubusercontent.com/36060453/129350599-b68eee59-f08b-458f-b164-c48210b140de.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:15910,failure,failure,15910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['failure'],['failure']
Availability,"!. I've followed the instructions at https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/ and I've run into the following errors during 'sbt assembly': . [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:111:57: type mismatch; ; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:999,error,error,999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['error']
Availability,"![image](https://user-images.githubusercontent.com/165320/46151480-da3c2080-c23c-11e8-97a4-ecfa39139c11.png). We're seeing intermittent connectivity issues w/ message of ""socket timeout, cannot connect to server"" in Pingdom. They last 1-3 minutes and seem to be off and on:; ![image](https://user-images.githubusercontent.com/165320/46151547-05267480-c23d-11e8-865a-f9c1fc1c4e4d.png). From the looks of things this looks to be between pingdom and the load balancer or proxy, as neither Cromwell nor proxy logs are showing signs of distress during these times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164:216,Ping,Pingdom,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164,2,"['Ping', 'ping']","['Pingdom', 'pingdom']"
Availability,"![image](https://user-images.githubusercontent.com/45682016/93409722-13f30f80-f8ca-11ea-89cd-bc544cea69ad.png). cromwell version: 53. config file. [aws.txt](https://github.com/broadinstitute/cromwell/files/5235741/aws.txt). ###wdl part. ```; version 1.0. task task1 {. input {; File simg; }. command {; du /cromwell_root/; du /yuce/; find *.simg; singularity exec ${simg} echo hello > hello.txt; du /cromwell_root/; }. runtime{; docker:""kongdeju/singularity:v3.4.0""; }. output {; File outfile = ""hello.txt""; }; }. ```. ### json part. ```; {; ""test.task2.simg"": ""s3://yuce/simgs/alpine.simg"",; ""test.task1.simg"": ""s3://yuce/simgs/alpine.simg""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5861:372,echo,echo,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5861,1,['echo'],['echo']
Availability,"![screen shot 2015-06-14 at 8 36 19 am](https://cloud.githubusercontent.com/assets/58551/8148606/0f7b94f6-1273-11e5-8f6e-8fb7b23aa935.png). No rush to review this. This is ancillary to the sprint but it'd be nice if we could get it in by the end of the sprint. Changes:. 1) SLF4J logging hooked in with the actor system too. 2) Two modes of logging, set by the Java Property `CROMWELL_LOGGER=[SERVER|CONSOLE]`:; - In SERVER mode, it logs to a rolling file appender with all the bells and whistles. This will default to DEBUG level.; - In CONSOLE mode, there's code in `cromwell.logging` that handles these messages from SLF4J and prints them out to the console is as human-readable way as possible. I welcome comments about how to make it more readable. Though, if you are going to do that make sure you first run it so you can see the colors, which are an important aspect of this! CONSOLE logs on INFO, WARN, ERROR.; - The modes are toggled either by explicitly setting CROMWELL_LOGGER, or based on the CLI sub-command you chose: `server` will do SERVER logging and every other sub-command uses CONSOLE logging. 3) I've tried to establish some conventions for logging:; - INFO, WARN, ERROR is meant to be read by _users_ to debug their WDL executions. It should equally be helpful for _developers_ to debug many issues. We must keep in mind that these are also show up in the server logs so they could also help us add context to debugging an issue easier if we're used to these messages from the command line.; - Messages should contain the workflow UUID wherever appropriate. Anything that exists only in a context of a workflow execution: CallActors, WorkflowActors, SymbolStores, etc.; - Messages should be chosen to craft a story about how a workflow is progressing. Highlight the big points (something starts, something finishes, something is launched, symbol store entry is updated, etc)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/44:911,ERROR,ERROR,911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/44,2,['ERROR'],['ERROR']
Availability,"![screenshot-2019-2-28 kibana](https://user-images.githubusercontent.com/1087943/53603858-d5c7bb00-3b80-11e9-9330-a9ac9f9032dc.png). [Kibana link](https://kibana.logit.io/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-90d,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(analyze_wildcard:!t,query:'host:%22gce-cromwell-prod601%22%20AND%20%22Communications%20link%20failure%22')),sort:!('@timestamp',desc))). The same error message showed up in #4360, #3387, and #2519 but in those the ""last packet"" time was short and more or less random, while here it's repeatedly 929,284 milliseconds - or precisely 15 minutes, 30 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4689:498,error,error,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689,1,['error'],['error']
Availability,""": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""des",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:1504,failure,failures,1504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['failure'],['failures']
Availability,"""; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t; String declarationDependingOnCallOutput = t.o; }; ```. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261092137). Oh no! This actually makes using zips infeasible, since I'd imagine in most cases the things you want to zip will be outputs from previous tasks. I suppose I can use a workaround where inside of a scatter loop I can create a task that takes in a File and Array[File] and outputs a Pair, then scatter over the output of that task outside of the original scatter. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095003). I tried that workaround with a task like this:. ```; task ZipUpWorkaround {; File unmapped_bam; Array[File] fastqs. command {; #do nothing; }; output {; Pair[File, Array[File]] p = [unmapped_bam, fastqs]; }; }; ```. and got this error message (after it submitted that task):; `Failed to evaluate outputs.: WdlTypeException: Arrays/Maps must have homogeneous types`. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095284). I think `Pair`s are declared with parenthesis and not brackets. Does . ```; output {; Pair[File, Array[File]] p = (unmapped_bam, fastqs); }; ```. work ?. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132). Also, as long as you don't declared the zip as a workflow variable you should be fine. For example, this should work:. ```; task t {; command {; echo ""hello""; echo ""world""; }; output {; Array[String] o = read_lines(stdout()); }; }. task t2 {; Array[Pair[String, String]] p; command {; #do something; }; output {; Array[Pair[String, String]] o = p; }; }. workflow w {; call t; call t as u; call t2 { input: zip(t.o, u.o) }; }; ```. ---. @meganshand commented on [Thu Nov 17 2016](https://github.com/br",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:1770,error,error,1770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,1,['error'],['error']
Availability,"""No coercion defined"" error in optional output in CWL with wildcard glob.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['error'],['error']
Availability,"""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s = read_string(stdout()) }; }. task mirror {; Array[String] s; command {}; output { Array[String] out = s }; }. workflow helloArray {; Array[Int] ints = range(100); scatter(i in ints) {; call helloWorld; }; call mirror { input: s = helloWorld.s }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1461:1959,echo,echo,1959,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461,1,['echo'],['echo']
Availability,"""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; }; }. services {; MetadataService {; metadata-read-row-number-safety-threshold = 5000000; }; }; ```; The main issue that I can see is that Cromwell is ignoring the increased metadata row count. this is despite my separating out the metadata database and increasing the thresholds on both databases. Prior to running the changes listed above I have ensured that the working directory is completely purged of logs and metadata so as to ensure an unobstructed run. The documentation currently provides no additional guidance on how to overcome the error. Any assistance will be appreciated.; Best wishes,. Matthieu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:4124,error,error,4124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['error'],['error']
Availability,"""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020.bam\""},\""normal_sample\"":\""seqcii_N020\"",\""output_dir\"":\""output\"",\""promiscuous_five_csv_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/GRIDSS-Purple-Linx-Docker/hg19/dbs/knowledgebases/output/knownPromiscuousFive.csv\""},\""promiscuous_three_csv_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/GRIDSS-Purple-Linx-Docker/hg19/dbs/knowledgebases/output/knownPromiscuousThree.csv\""},\""reference\"":{\""class\"":\""File\"",\""location\"":\""/efs/umccr-refdata/bwa/hg38.fa\""},\""replication_origins_file_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/heli_rep_origins.bed\""},\""sample_name\"":\""SBJ_seqcii_020\"",\""snvvcf\"":{\""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020.vcf.gz\""},\""tumor_bam\"":{\""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020_tumor.bam\""},\""tumor_sample\"":\""seqcii_T020\"",\""viral_hosts_file_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/viral_host_ref.csv\""}}"",; ""workflowUrl"": """",; ""labels"": ""{}""; },; ""calls"": {},; ""outputs"": {},; ""actualWorkflowLanguage"": ""CWL"",; ""id"": ""8681f8fa-7624-4bba-bc94-a697d1d2d179"",; ""inputs"": {},; ""labels"": {; ""cromwell-workflow-id"": ""cromwell-8681f8fa-7624-4bba-bc94-a697d1d2d179""; },; ""submission"": ""2020-09-02T09:23:04.304Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""FileStepUUID(file:///tmp/tmp.Olzr17Zus8_cromwell/cwl_temp_dir_5247448030953921891/cwl_temp_file_8681f8fa-7624-4bba-bc94-a697d1d2d179.cwl,Some(main),out_vcf,gridss_step,) (of class cwl.FileStepUUID)""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ""workflowLog"": ""wf_logs/workflow.8681f8fa-7624-4bba-bc94-a697d1d2d179.log"",; ""end"": ""2020-09-02T09:23:06.270Z"",; ""start"": ""2020-09-02T09:23:04.925Z""; }; ```. </details>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:129882,failure,failures,129882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['failure'],['failures']
Availability,"""cromwell --help"" should return zero exit code (success) rather than one (failure)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1870:74,failure,failure,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1870,1,['failure'],['failure']
Availability,"""failures"": [{""causedBy"": [{""causedBy"": [],""message"": ""the local copy message must have path set.""}],""message"": ""Unable to complete JES Api Request""}]. See workflow metadata at: https://cromwell-v29.dsde-methods.broadinstitute.org/api/workflows/v1/4ff9cb8a-cade-482a-8492-66ea3b7a2eaa/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791:1,failure,failures,1,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791,1,['failure'],['failures']
Availability,"""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:4825,heartbeat,heartbeat,4825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,3,"['failure', 'heartbeat']","['failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,"""string"",; ""inputBinding"": {; ""prefix"": ""-A="",; ""separate"": false; }; },; {; ""name"": ""itemB"",; ""type"": ""string"",; ""inputBinding"": {; ""prefix"": ""-B="",; ""separate"": false; }; }; ]; }; }; ],; ""outputs"": {; ""example_out"": {; ""type"": ""stdout""; }; },; ""stdout"": ""output.txt"",; ""baseCommand"": ""echo""; }; ```; This was run with: `java -jar cromwell-36.jar run works.json --inputs inputs.json`. There are two issues:; - clearly the `name` key is being ignored. Since it is not required (see next item), this is by itself quite minor.; - a `name` key is *not* required per the CWL spec (https://www.commonwl.org/v1.0/CommandLineTool.html#InputRecordSchema). As mentioned, ignoring the `name` parameter is probably acceptable, BUT if I remove that parameter, the execution fails. The failing example is the same, but with ` ""name"": ""SOME JUNK VALUE"",` removed:; ```; $ diff works.json fails.json ; 9d8; < ""name"": ""SOME JUNK VALUE"",; ```; The stack trace reports:; ```; [2018-10-30 21:46:32,22] [error] WorkflowManagerActor Workflow de935a6c-85a6-476f-845f-cf5360bbef03 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; error when parsing file /tmp/cwl_temp_dir_9897655526044348367/cwl_temp_file_de935a6c-85a6-476f-845f-cf5360bbef03.cwl; DecodingFailure at .inputs[0].type: DecodingFailure at .inputs[0].type: DecodingFailure at .inputs[0].type: String; ``` ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4338:1995,error,error,1995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4338,1,['error'],['error']
Availability,"""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/9ea737cd-a512-4c62-820c-dd1505ea7676/workflow.logs/workflow.3608d6ca-fbb4-4232-b197-268058470bfc.log"",; ""end"": ""2016-12-02T15:05:42.868Z"",; ""start"": ""2016-12-02T15:05:40.873Z""; }; ```. Here there's no ""message"" and there are ""timestamp"" and ""failure"". ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b\""\n },\n \""google_project\"": \""broad-dsde-dev\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-dev\"",\n \""refresh_token\"": \""cleared\"",\n \""account_name\"": \""abaumann.firecloud@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5\""\n}"",; ""inputs"": ""{\""aggregate_data_workflow.aggregate_data.input_array\"":[\""bar, baz\""]}"",; ""workflow"": ""task aggregate_data {\n\tArray[File] input_array\n\n\tcommand {\n echo \""foo\""\n\n\t}\n\n\toutput {\n\t\tArray[Array[File]] output_array = [input_array]\n\t}\n\n\truntime {\n\t\tdocker : \""broadgdac/aggregate_data:31\""\n\t}\n\n\tmeta {\n\t\tauthor : \""Tim DeFreitas\""\n\t\temail : \""timdef@broadinstitute.org\""\n\t}\n\n}\n\nworkflow aggregate_data_workflow {\n\tcall aggregate_data\n}""; },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stdout.log"",; ""shardIndex"": -1,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadgdac/aggregate_data:31"",; ""cpu"": ""1"",; ""zones"": ""us-central1-b"",; ""memory"": ""2GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_array"": [""bar, baz""]; },; ""failures"": [{; ""timestamp"": ""2016-08-01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:6037,echo,echo,6037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['echo'],['echo']
Availability,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817:1667,reliab,reliably,1667,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817,1,['reliab'],['reliably']
Availability,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161:485,failure,failure,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161,3,"['failure', 'robust']","['failure', 'failures', 'robust']"
Availability,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3666:351,down,down,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666,1,['down'],['down']
Availability,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4105:716,Reboot,Rebooted,716,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105,5,"['Error', 'Reboot', 'error']","['Error', 'Rebooted', 'error']"
Availability,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4056:65,mainten,maintenance,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056,7,"['degraded', 'down', 'error', 'mainten', 'outage', 'resilien']","['degraded', 'down', 'errors', 'maintenance', 'outages', 'resilient']"
Availability,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4138:627,resilien,resiliency,627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138,2,"['failure', 'resilien']","['failures', 'resiliency']"
Availability,"## About this PR; ðŸ“¦ Updates ; * [ch.qos.logback:logback-access](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-classic](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-core](https://github.com/qos-ch/logback). from `1.2.11` to `1.2.12`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7260:1005,down,down,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7260,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [com.dimafeng:testcontainers-scala-mariadb](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-mysql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-postgresql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-scalatest](https://github.com/testcontainers/testcontainers-scala). from `0.40.10` to `0.40.17`. ðŸ“œ [GitHub Release Notes](https://github.com/testcontainers/testcontainers-scala/releases/tag/v0.40.17) - [Version Diff](https://github.com/testcontainers/testcontainers-scala/compare/v0.40.10...v0.40.17). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.dimafeng"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.dimafeng"" }; }]; ```; </details>. <sup>; labels: test-library-update, early-semver-minor, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7270:1416,down,down,1416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7270,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [io.circe:circe-core](https://github.com/circe/circe); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from `0.14.1` to `0.14.6`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.6) - [Version Diff](https://github.com/circe/circe/compare/v0.14.1...v0.14.6). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7292:1299,down,down,1299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7292,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [org.http4s:http4s-ember-client](https://github.com/http4s/http4s); * [org.http4s:http4s-ember-server](https://github.com/http4s/http4s). from `0.21.31` to `0.21.34`. ðŸ“œ [GitHub Release Notes](https://github.com/http4s/http4s/releases/tag/v0.21.34) - [Version Diff](https://github.com/http4s/http4s/compare/v0.21.31...v0.21.34). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.http4s"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.http4s"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7311:1095,down,down,1095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7311,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [org.junit.jupiter:junit-jupiter-api](https://github.com/junit-team/junit5); * [org.junit.jupiter:junit-jupiter-engine](https://github.com/junit-team/junit5); * [org.junit.jupiter:junit-jupiter-params](https://github.com/junit-team/junit5). from `5.9.3` to `5.10.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.junit.jupiter"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.junit.jupiter"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7312:1041,down,down,1041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7312,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [org.typelevel:alleycats-core](https://github.com/typelevel/cats); * [org.typelevel:cats-core](https://github.com/typelevel/cats). from `2.7.0` to `2.10.0`. ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/cats/releases/tag/v2.10.0) - [Version Diff](https://github.com/typelevel/cats/compare/v2.7.0...v2.10.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.7.0).; You might want to review and update them manually.; ```; services/src/test/scala/cromwell/services/database/QueryTimeoutSpec.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7320:1384,down,down,1384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7320,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [cglib:cglib-nodep](https://github.com/cglib/cglib) from `3.2.7` to `3.2.12`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""cglib"", artifactId = ""cglib-nodep"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""cglib"", artifactId = ""cglib-nodep"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7259:864,down,down,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7259,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure.resourcemanager:azure-resourcemanager](https://github.com/Azure/azure-sdk-for-java) from `2.18.0` to `2.33.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7269:938,down,down,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7269,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-http-okhttp](https://github.com/Azure/azure-sdk-for-java) from `1.11.10` to `1.11.17`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-http-okhttp"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-http-okhttp"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7262:910,down,down,910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7262,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-management](https://github.com/Azure/azure-sdk-for-java) from `1.7.1` to `1.11.9`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.7.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-management"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-management"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7263:1156,down,down,1156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7263,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-test](https://github.com/Azure/azure-sdk-for-java) from `1.18.0` to `1.18.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.18.0).; You might want to review and update them manually.; ```; cloud-nio/cloud-nio-impl-drs/src/test/scala/cloud/nio/impl/drs/DrsPathResolverSpec.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-test"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-test"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7264:1208,down,down,1208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7264,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core](https://github.com/Azure/azure-sdk-for-java) from `1.40.0` to `1.45.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7261:884,down,down,884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7261,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-identity-extensions](https://github.com/azure/azure-sdk-for-java) from `1.1.4` to `1.1.10`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity-extensions"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity-extensions"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7266:913,down,down,913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7266,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-identity](https://github.com/Azure/azure-sdk-for-java) from `1.9.1` to `1.9.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7265:890,down,down,890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7265,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-storage-blob](https://github.com/Azure/azure-sdk-for-java) from `12.23.0-beta.1` to `12.23.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-blob"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-blob"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7267:909,down,down,909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7267,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-storage-common](https://github.com/Azure/azure-sdk-for-java) from `12.22.0-beta.1` to `12.22.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-common"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-common"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7268:913,down,down,913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7268,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from `1.1.1` to `2.1.5` âš . ðŸ“œ [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v2.1.5) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.1...v2.1.5). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.1).; You might want to review and update them manually.; ```; womtool/src/test/resources/validate/wdl_draft3/valid/arrays_v1/arrays_v1.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-major, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7271:1356,down,down,1356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7271,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.fasterxml.jackson.dataformat:jackson-dataformat-xml](https://github.com/FasterXML/jackson-dataformat-xml) from `2.13.3` to `2.13.5`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.fasterxml.jackson.dataformat"", artifactId = ""jackson-dataformat-xml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.fasterxml.jackson.dataformat"", artifactId = ""jackson-dataformat-xml"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7272:962,down,down,962,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7272,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.github.cb372:sbt-explicit-dependencies](https://github.com/cb372/sbt-explicit-dependencies) from `0.2.16` to `0.3.1`. ðŸ“œ [GitHub Release Notes](https://github.com/cb372/sbt-explicit-dependencies/releases/tag/v0.3.1) - [Version Diff](https://github.com/cb372/sbt-explicit-dependencies/compare/v0.2.16...v0.3.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.cb372"", artifactId = ""sbt-explicit-dependencies"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.cb372"", artifactId = ""sbt-explicit-dependencies"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7273:1126,down,down,1126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7273,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.github.sbt:junit-interface](https://github.com/sbt/junit-interface) from `0.13.2` to `0.13.3`. ðŸ“œ [GitHub Release Notes](https://github.com/sbt/junit-interface/releases/tag/v0.13.3) - [Version Diff](https://github.com/sbt/junit-interface/compare/v0.13.2...v0.13.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.sbt"", artifactId = ""junit-interface"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.sbt"", artifactId = ""junit-interface"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7274:1321,down,down,1321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7274,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.api-client:google-api-client-jackson2](https://github.com/googleapis/google-api-java-client) from `2.1.4` to `2.2.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-api-java-client/releases/tag/v2.2.0) - [Version Diff](https://github.com/googleapis/google-api-java-client/compare/v2.1.4...v2.2.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api-client"", artifactId = ""google-api-client-jackson2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api-client"", artifactId = ""google-api-client-jackson2"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7276:1142,down,down,1142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7276,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.api.grpc:proto-google-cloud-batch-v1](https://github.com/googleapis/google-cloud-java) from `0.18.0` to `0.30.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-cloud-java/releases/tag/v0.30.0) - [Version Diff](https://github.com/googleapis/google-cloud-java/compare/v0.18.0...v0.30.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-batch-v1"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-batch-v1"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7277:1130,down,down,1130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7277,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.api.grpc:proto-google-cloud-resourcemanager-v3](https://github.com/googleapis/google-cloud-java) from `1.17.0` to `1.32.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.17.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-resourcemanager-v3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-resourcemanager-v3"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7278:1214,down,down,1214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7278,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.api:gax-grpc](https://github.com/googleapis/sdk-platform-java) from `2.25.0` to `2.38.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.25.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api"", artifactId = ""gax-grpc"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api"", artifactId = ""gax-grpc"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7275:1146,down,down,1146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7275,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.auth:google-auth-library-oauth2-http](https://github.com/googleapis/google-auth-library-java) from `1.5.3` to `1.20.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.20.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.5.3...v1.20.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7281:1149,down,down,1149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7281,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-batch](https://github.com/googleapis/google-cloud-java) from `0.18.0` to `0.30.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-cloud-java/releases/tag/v0.30.0) - [Version Diff](https://github.com/googleapis/google-cloud-java/compare/v0.18.0...v0.30.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-batch"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-batch"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7282:1106,down,down,1106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7282,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-bigquery](https://github.com/googleapis/java-bigquery) from `2.25.0` to `2.34.2`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/java-bigquery/releases/tag/v2.34.2) - [Version Diff](https://github.com/googleapis/java-bigquery/compare/v2.25.0...v2.34.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.25.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7283:1352,down,down,1352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7283,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/google-cloud-java) from `1.17.0` to `1.32.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.17.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7284:1190,down,down,1190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7284,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-storage](https://github.com/googleapis/java-storage) from `2.17.2` to `2.29.1`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/java-storage/releases/tag/v2.29.1) - [Version Diff](https://github.com/googleapis/java-storage/compare/v2.17.2...v2.29.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.17.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7285:1347,down,down,1347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7285,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.typesafe:config](https://github.com/lightbend/config) from `1.4.2` to `1.4.3`. ðŸ“œ [GitHub Release Notes](https://github.com/lightbend/config/releases/tag/v1.4.3) - [Version Diff](https://github.com/lightbend/config/compare/v1.4.2...v1.4.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe"", artifactId = ""config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.typesafe"", artifactId = ""config"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7286:1033,down,down,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7286,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [commons-codec:commons-codec](https://github.com/apache/commons-codec) from `1.15` to `1.16.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.15).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_wdlom.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-codec"", artifactId = ""commons-codec"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-codec"", artifactId = ""commons-codec"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7287:1363,down,down,1363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7287,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [commons-io:commons-io](https://commons.apache.org/proper/commons-io/) from `2.11.0` to `2.15.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.11.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-io"", artifactId = ""commons-io"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-io"", artifactId = ""commons-io"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7288:1140,down,down,1140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7288,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [commons-net:commons-net](https://commons.apache.org/proper/commons-net/) from `3.8.0` to `3.10.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-net"", artifactId = ""commons-net"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-net"", artifactId = ""commons-net"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7289:892,down,down,892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7289,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [eu.timepit:refined](https://github.com/fthomas/refined) from `0.10.1` to `0.10.3`. ðŸ“œ [GitHub Release Notes](https://github.com/fthomas/refined/releases/tag/v0.10.3) - [Version Diff](https://github.com/fthomas/refined/compare/v0.10.1...v0.10.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""eu.timepit"", artifactId = ""refined"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""eu.timepit"", artifactId = ""refined"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7290:1033,down,down,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7290,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.circe:circe-config](https://github.com/circe/circe-config) from `0.8.0` to `0.10.1`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe-config/releases/tag/v0.10.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-config"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7291:965,down,down,965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7291,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.circe:circe-optics](https://github.com/circe/circe-optics) from `0.14.1` to `0.15.0`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe-optics/releases/tag/v0.15.0) - [Version Diff](https://github.com/circe/circe-optics/compare/v0.14.1...v0.15.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-optics"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-optics"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7293:1300,down,down,1300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7293,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.grpc:grpc-core](https://github.com/grpc/grpc-java) from `1.54.1` to `1.54.2`. ðŸ“œ [GitHub Release Notes](https://github.com/grpc/grpc-java/releases/tag/v1.54.2) - [Version Diff](https://github.com/grpc/grpc-java/compare/v1.54.1...v1.54.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.grpc"", artifactId = ""grpc-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.grpc"", artifactId = ""grpc-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7295:1028,down,down,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7295,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.projectreactor:reactor-test](https://github.com/reactor/reactor-core) from `3.4.29` to `3.4.34`. ðŸ“œ [GitHub Release Notes](https://github.com/reactor/reactor-core/releases/tag/v3.4.34) - [Version Diff](https://github.com/reactor/reactor-core/compare/v3.4.29...v3.4.34). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.projectreactor"", artifactId = ""reactor-test"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.projectreactor"", artifactId = ""reactor-test"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7296:1072,down,down,1072,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7296,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.sentry:sentry-logback](https://github.com/getsentry/sentry-java) from `5.7.4` to `7.0.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/getsentry/sentry-java/releases/tag/7.0.0) - [Version Diff](https://github.com/getsentry/sentry-java/compare/5.7.4...7.0.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.sentry"", artifactId = ""sentry-logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.sentry"", artifactId = ""sentry-logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7297:1057,down,down,1057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7297,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.swagger:swagger-parser](https://github.com/swagger-api/swagger-parser) from `1.0.56` to `1.0.68`. ðŸ“œ [GitHub Release Notes](https://github.com/swagger-api/swagger-parser/releases/tag/v1.0.68) - [Version Diff](https://github.com/swagger-api/swagger-parser/compare/v1.0.56...v1.0.68). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.swagger"", artifactId = ""swagger-parser"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.swagger"", artifactId = ""swagger-parser"" }; }]; ```; </details>. <sup>; labels: test-library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7298:1080,down,down,1080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7298,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [net.minidev:json-smart](https://github.com/netplex/json-smart-v2) from `2.4.10` to `2.4.11`. ðŸ“œ [GitHub Release Notes](https://github.com/netplex/json-smart-v2/releases/tag/2.4.11) - [Version Diff](https://github.com/netplex/json-smart-v2/compare/2.4.10...2.4.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""net.minidev"", artifactId = ""json-smart"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""net.minidev"", artifactId = ""json-smart"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7300:1056,down,down,1056,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7300,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.apache.commons:commons-lang3](https://commons.apache.org/proper/commons-lang/) from `3.12.0` to `3.14.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-lang3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-lang3"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7301:912,down,down,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7301,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.apache.tika:tika-core](https://tika.apache.org/) from `2.3.0` to `2.9.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.aws.inputs.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.tika"", artifactId = ""tika-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.tika"", artifactId = ""tika-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7303:1382,down,down,1382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7303,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.codehaus.janino:janino](https://github.com/janino-compiler/janino) from `3.1.7` to `3.1.11`. ðŸ“œ [GitHub Release Notes](https://github.com/janino-compiler/janino/releases/tag/v3.1.11) - [Version Diff](https://github.com/janino-compiler/janino/compare/3.1.7...3.1.11) - [Version Diff](https://github.com/janino-compiler/janino/compare/v3.1.7...v3.1.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.codehaus.janino"", artifactId = ""janino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.codehaus.janino"", artifactId = ""janino"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7307:1151,down,down,1151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7307,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.glassfish.jersey.inject:jersey-hk2](https://github.com/eclipse-ee4j/jersey) from `2.32` to `2.41`. ðŸ“œ [GitHub Release Notes](https://github.com/eclipse-ee4j/jersey/releases/tag/2.41) - [Version Diff](https://github.com/eclipse-ee4j/jersey/compare/2.32...2.41). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.32).; You might want to review and update them manually.; ```; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7308:1562,down,down,1562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7308,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.gnieh:diffson-spray-json](https://github.com/gnieh/diffson) from `4.1.1` to `4.4.0`. ðŸ“œ [GitHub Release Notes](https://github.com/gnieh/diffson/releases/tag/v4.4.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7309:967,down,down,967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7309,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.hsqldb:hsqldb](http://hsqldb.org) from `2.6.1` to `2.7.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.6.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7310:1101,down,down,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7310,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.mariadb.jdbc:mariadb-java-client](https://github.com/mariadb-corporation/mariadb-connector-j) from `2.7.4` to `2.7.11`. ðŸ“œ [GitHub Release Notes](https://github.com/mariadb-corporation/mariadb-connector-j/releases/tag/2.7.11) - [Changelog](https://github.com/mariadb-corporation/mariadb-connector-j/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/mariadb-corporation/mariadb-connector-j/compare/2.7.4...2.7.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7314:1235,down,down,1235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7314,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.mockito:mockito-core](https://github.com/mockito/mockito) from `4.11.0` to `5.7.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/mockito/mockito/releases/tag/v5.7.0) - [Version Diff](https://github.com/mockito/mockito/compare/v4.11.0...v5.7.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockito"", artifactId = ""mockito-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockito"", artifactId = ""mockito-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7315:1044,down,down,1044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7315,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.scala-graph:graph-core](https://github.com/scala-graph/scala-graph) from `1.13.1` to `1.13.6`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-graph"", artifactId = ""graph-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-graph"", artifactId = ""graph-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7316:895,down,down,895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7316,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.scala-lang:scala-library](https://github.com/scala/scala) from `2.13.9` to `2.13.12`. ðŸ“œ [GitHub Release Notes](https://github.com/scala/scala/releases/tag/v2.13.12) - [Version Diff](https://github.com/scala/scala/compare/v2.13.9...v2.13.12). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-lang"", artifactId = ""scala-library"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-lang"", artifactId = ""scala-library"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7317:1044,down,down,1044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7317,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.scalatest:scalatest](https://github.com/scalatest/scalatest) from `3.2.15` to `3.2.17`. ðŸ“œ [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.17) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.15...release-3.2.17). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalatest"", artifactId = ""scalatest"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scalatest"", artifactId = ""scalatest"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7318:1076,down,down,1076,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7318,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from `2.0.4` to `2.0.9`. ðŸ“œ [GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.9) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v2.0.4...v2.0.9). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7319:1070,down,down,1070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7319,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.typelevel:kittens](https://github.com/typelevel/kittens) from `2.3.2` to `3.1.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/kittens/releases/tag/v3.1.0) - [Version Diff](https://github.com/typelevel/kittens/compare/v2.3.2...v3.1.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""kittens"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""kittens"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7321:1293,down,down,1293,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7321,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.typelevel:mouse](https://github.com/typelevel/mouse) from `1.0.11` to `1.2.2`. ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/mouse/releases/tag/v1.2.2) - [Version Diff](https://github.com/typelevel/mouse/compare/v1.0.11...v1.2.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.11).; You might want to review and update them manually.; ```; .sdkmanrc; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""mouse"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""mouse"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7322:1267,down,down,1267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7322,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.yaml:snakeyaml](https://bitbucket.org/snakeyaml/snakeyaml/src) from `1.33` to `2.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. <sup>; labels: test-library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7324:1484,down,down,1484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7324,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [se.marcuslonnberg:sbt-docker](https://github.com/marcuslonnberg/sbt-docker) from `1.9.0` to `1.11.0`. ðŸ“œ [GitHub Release Notes](https://github.com/marcuslonnberg/sbt-docker/releases/tag/v1.11.0) - [Changelog](https://github.com/marcuslonnberg/sbt-docker/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/marcuslonnberg/sbt-docker/compare/v1.9.0...v1.11.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.9.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""se.marcuslonnberg"", artifactId = ""sbt-docker"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""se.marcuslonnberg"", artifactId = ""sbt-docker"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7325:1417,down,down,1417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7325,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates bio.terra:workspace-manager-client from `0.254.452-SNAPSHOT` to `0.254.966-SNAPSHOT`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.254.452-SNAPSHOT).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7258:1153,down,down,1153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7258,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates com.google.apis:google-api-services-cloudkms from `v1-rev20230421-2.0.0` to `v1-rev20231012-2.0.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7279:913,down,down,913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7279,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates com.google.apis:google-api-services-lifesciences from `v2beta-rev20220916-2.0.0` to `v2beta-rev20230707-2.0.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7280:929,down,down,929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7280,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates mysql:mysql-connector-java from `8.0.28` to `8.0.33`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""mysql"", artifactId = ""mysql-connector-java"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""mysql"", artifactId = ""mysql-connector-java"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7299:849,down,down,849,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7299,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.apache.commons:commons-text from `1.10.0` to `1.11.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-text"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-text"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7302:859,down,down,859,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7302,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-google from `0.21-5c9c4f6` to `0.30-2147824`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7304:909,down,down,909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7304,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-google from `0.21-5c9c4f6` to `0.30-5781917`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9ac858c7e61f43ed3648f0fabc7104d0951cce67/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7331:909,down,down,909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7331,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-model from `0.15-f9f0d4c` to `0.19-8376167`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-model"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-model"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7305:907,down,down,907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7305,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-util from `0.6-65bba14` to `0.10-8376167`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-util"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-util"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7306:904,down,down,904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7306,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.liquibase:liquibase-core from `4.8.0` to `4.25.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7313:852,down,down,852,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7313,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.webjars:swagger-ui from `4.5.2` to `4.19.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.webjars"", artifactId = ""swagger-ui"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.webjars"", artifactId = ""swagger-ui"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7323:840,down,down,840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7323,1,['down'],['down']
Availability,"## Bug. I am trying to run a workflow using the GCP backend, however no matter what set of configurations I use, I am unable to have it succeed. The workflows Batch task appears to fail on the 3rd task, just after the `Setup Container`. This is basically causing every task to fail for some strange reason. ```; docker: invalid spec: /mnt/disks/cromwell_root:/mnt/disks/cromwell_root:: empty section between colons.; ```. [This](https://cromwellhq.slack.com/archives/CGQ7WK5A6/p1697484861117659) thread suggested that the logic in [these two lines](https://github.com/broadinstitute/cromwell/blob/86/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/runnable/RunnableBuilder.scala#L63-L64) may be the culprit under specific condirtions. ## Information. Cromwell Version: 87-c9d4ce4; <!-- Which backend are you running? -->; Backend: GCP Batch; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. task hello {. input {; String name; }; command <<<; echo 'hello ~{name}!'; >>>. output {; File response = stdout(); }. runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""3.75 GB""; }; }; workflow test {; call hello. output {; File response = hello.response; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```hoco; backend {; default = ""batch""; providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:990,echo,echo,990,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['echo'],['echo']
Availability,"## Bug; Cromwell appears to be improperly tokenizing string interpolations when using a variable that is prepended by the letters: `if` in any format. It seems to think that this is actually the start of a conditional `if _ then _ else` block instead of a variable name. The parser does not appear to be discriminating against the lack of whitespace in variables with the form `if[a-zA-Z0-9_]+` and fails to parse with an error. ### How to reproduce. ```; cat <<EOF > test.wdl; version 1.0; task test_task {; String ifl_token=""a""; command <<<; echo ""~{ifl_token}""; >>>; }. workflow test {; call test_task; }; EOF. java -jar cromwell-84.jar run. test.wdl; ```. ### Expected; 1. The workflow to parse correctly and to echo `a` when running. ### Actual Error. The workflow fails to parse with the following error:. ```; [2022-11-25 11:10:39,68] [info] WorkflowManagerActor: Workflow 45701495-7113-40d6-ac32-dab5247f37e7 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; ERROR: Unexpected symbol (line 5, col 20) when parsing 'e'. Expected then, got ""}"". echo ""~{ifl_token}""; ^. $e = :identifier <=> :lparen $_gen23 :rparen -> FunctionCall( name=$0, params=$2 ); ; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:257); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:227); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:222); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35); ```. ### Environment:; Tested on:; - Cromwell 84; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6956:422,error,error,422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6956,5,"['Error', 'echo', 'error']","['Error', 'echo', 'error']"
Availability,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:918,reliab,reliable,918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['reliab'],['reliable']
Availability,"## From @yfarjoun . I lost a whole day on this stupid bug....; when the input file specified in a JSON is missing, submitting the job takes a long time and returns with:. Resource representation is only available with these Content-Types:; text/plain; charset=UTF-8; text/plain. Which, needless to say is not pointing at the problem...I only managed to figure this out by slowly transitioning from a functioning JSON to the ""problematic"" one... Cheers,. Yossi.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703:203,avail,available,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703,1,['avail'],['available']
Availability,"## Symptom; I can run test and assembly tasks succesfully on cromwellApiClient subproject, but If I write my own small test class that uses `cromwell.api.CromwellClient` it fails at runtime with:; ```; Detected java.lang.NoSuchMethodError error, which MAY be caused by incompatible Akka versions on the classpath. Please note that a given Akka version MUST be the same across all modules of Akka that you are using, e.g. if you use akka-actor [2.5.3 (resolved from current classpath)] all other core Akka modules MUST be of the same version. External projects like Alpakka, Persistence plugins or Akka HTTP etc. have their own version numbers - please make sure you're using a compatible set of libraries. ; Uncaught error from thread [default-akka.actor.default-dispatcher-5]: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for for ActorSystem[default]; java.lang.NoSuchMethodError: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;; ...; ```; I'm essentially seeing exactly the behaviour described in reference [1] below, which is eviction warnings at compile time and then the runtime blow-up. The root cause seems to be that akka-http depends on an older version of akka-actor (2.4.19) than that specified for the project (2.5.3). Running `dependencyTree` task confirms:; ```; [info] +-com.typesafe.akka:akka-http-spray-json_2.12:10.0.9 [S]; [info] | +-com.typesafe.akka:akka-http_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-http-core_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-parsing_2.12:10.0.9 [S]; [info] | | | +-com.typesafe.akka:akka-actor_2.12:2.4.19 (evicted by: 2.5.3); ```; If I explicitly add dependency on the latest akka-stream as suggested in [2] and [3], the problem goes away:; ```; diff --git a/project/Dependencies.scala b/project/Dependencies.scala; index 0d77e2d3..7254fc61 100644; --- a/project/Dependencies.scala; +++ b/project",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2579:239,error,error,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2579,4,"['down', 'error']","['down', 'error']"
Availability,## What it does do; Gives you the field that does not parse. (This is better than the previous `CanBuildFrom` brick wall.). ## What it does not do; * Tell you the offending value; * Distinguish between CWL's when multiple are submitted in one file. This is because `id` field is optional and made random by SALAD preprocessing. Example of 2 errors combined:; ```; DecodingFailure at .inputs[1].inputBinding.position: Int; DecodingFailure at .stdout: DecodingFailure at .stdout: DecodingFailure at .stdout: String; ```; So not amazing (don't know why `at .stdout` is repeated thrice) but better.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3180:341,error,errors,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3180,1,['error'],['errors']
Availability,"## basic issue; `womtool validate` does not catch all scenarios where you are defining a new variable based on an optional variable. Instead, Cromwell fails at runtime -- even if it is actually impossible for that optional variable to be undefined. [A working example is available](https://github.com/aofarrel/myco/commit/e7f9ba6951d1b0fe5b3c1a650835312dd2b6e68f), but it is a complex WDL, so a more basic example is listed below. ## background; WDL doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.er",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:271,avail,available,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,4,"['avail', 'error']","['available', 'errorcode']"
Availability,"### Description. After playing a while with GCP Batch:; 1. Batch can automatically retry preemption errors.; 2. When Batch retries, there is no signal in the Job status events, we need to check the VM logs.; 3. Cromwell does not get any details about Batch retries, hence, the same jobId is kept even if a VM is recreated.; 4. When the job status events mention that the job failed due to a preemption error, this is final, Batch already exhausted the retries. This removes all the code related to handling preemption errors and parses the job status events to derive the failure reason. Also, this tries detecting the other potential exit codes mapping them to a better error message. Refs:; - [Batch automated task retries](https://cloud.google.com/batch/docs/automate-task-retries); - [Batch exit codes](https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes). <!-- What is the purpose of this change? What should reviewers know? -->. Fixes #7407. This is an example error log produced when getting a preemption error:. ```; [2024-06-21 12:30:09,28] [info] WorkflowManagerActor: Workflow 2cdef371-703c-4c1e-92b5-0e013dcda6c8 failed (during ExecutingWorkflowState): java.lang.Exception: Task myWorkflow.myTask:NA:1 failed: A Spot VM for the job was preempted during run time; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7457:100,error,errors,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7457,7,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"### Description. As part of preparing for the fall 2024 audit, we were asked to fix the permissions on the various healthcheck buckets such as `gs://cromwell-ping-me-dev`. It would have taken some tinkering to make sure the permissions are secure enough _and_ the healthcheck still works, so I decided to drop the healthcheck. I am not aware of any times it's helped us and doesn't pass the ""would we add this today"" test. The only notable bucket we'd want to make sure Cromwell itself has permissions on is the workflow archiver - and it uses [a separate service account from the rest of Cromwell](https://github.com/broadinstitute/terra-helmfile/blob/master/charts/cromwell/templates/config/_cromwell.conf.tpl#L267-L271), so it's not a valid test. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7533:158,ping,ping-me-dev,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533,1,['ping'],['ping-me-dev']
Availability,"### Description. Currently, and as described in https://github.com/broadinstitute/cromwell/issues/7535, only general-purpose machine types are supported in Google Backend, which prevents running wdl workflows on many machine types available on GCP, including those provisioned with modern GPUs. I believe the simplest and most general solution would be to pass the machine type directly from the wdl configuration to the Google Batch API. The idea is that this approach would be more resilient to machine types being added or deprecated on GCP, as users would only need to update their wdl workflows in such cases. An alternative approach of mapping machine specs (e.g.: cpu platform and gpu requirements) to standard machine types would potentially introduce an additional layer of maintenance with little benefit. This PR adds support for a new standardMachineType key in the runtime section, which is only parsed for the Google backend. ### Testing. I deployed this internally and verified I can successfully run the following wdl workflow:. ```; version 1.0. task nvidia_smi {; input {; String docker_version; }. command <<<; nvidia-smi. touch .done; echo ""Finished at $(date)""; >>>. runtime {; docker: <internal image>; disks: ""local-disk 50 SSD""; memory: ""32G""; preemptible: 0; gpuCount: 1; gpuType: ""nvidia-tesla-a100""; standardMachineType: ""a2-highgpu-1g""; }. output {; File done = "".done""; }; }. workflow nvidia_smi_wf {; input {; String docker_version; }; ; call nvidia_smi as nvidia_smi_call {; input:; docker_version = docker_version; }. output {; File done = "".done""; }; }; ```. ### Next steps. - [ ] Confirm this approach is in the right direction with the cromwell team.; - [ ] Work on proper unit tests and get this PR ready to be merged. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7545:231,avail,available,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545,3,"['avail', 'mainten', 'resilien']","['available', 'maintenance', 'resilient']"
Availability,"### Description. Fixes CI failures caused by errors like:; ```; $ docker pull quay.io/broadinstitute/cromwell-docker-test:centaur; centaur: Pulling from broadinstitute/cromwell-docker-test; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of quay.io/broadinstitute/cromwell-docker-test:centaur to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. The very old images need to be updated to get around this. For Python, we can use a newer version. For the Quay image we manage, using a different one because I fear push access to the current one has been lost in the mists of time. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [X] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [X] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7450:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7450,2,"['error', 'failure']","['errors', 'failures']"
Availability,"### Description. Fixes job recovery on restart for GCP Batch, addresses #7495. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:27,recover,recovery,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,1,['recover'],['recovery']
Availability,### Description. Part 2 of https://github.com/broadinstitute/cromwell/pull/7432. Detects and retries the new fatal quota errors we've been seeing. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7439:121,error,errors,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7439,1,['error'],['errors']
Availability,"### Description. Resolves intermittent build breakage caused by 404s of `paleo-core` artifacts. `paleo-core` is deprecated, and so is the library that depends on it, `swagger2markup`. - Remove code and build components; - Clean up docs and provide reasonable replacements when necessary; - Removed the term ""REST"" as redundant because it has taken over as the dominant API type; - Reorganize current `CHANGELOG.md` into sections because we have a substantial number of release notes ðŸŽ‰ ; - Unrelated one-line change to add timezone to debug image. ```; > docker run -it --entrypoint /bin/bash broadinstitute/cromwell:88-648e536-DEBUG; Version 88-648e536-DEBUG built at 2024-08-08 15:04:21 EDT; root@4ec372b744a8:/# ; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7488:317,redundant,redundant,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7488,1,['redundant'],['redundant']
Availability,"### Description. The PR exercises the ""retry with more memory"" Centaur tests on the GCP Batch backend. Minimal changes to production code, all of which are in the GCP Batch backend:. - The constant`RunnableUtils#MountPoint` was created with value `/mnt/disks/cromwell_root` and applied where appropriate.; - A copy/paste bug in code brought over from PAPIv2 was corrected (the `/cromwell_root` of PAPIv2 has become `/mnt/disks/cromwell_root` in Batch), using the constant described above.; - If a job fails, the *last* event message is now propagated rather than the first event message. The first event message is often a benign state transition, while the last event message is more likely to contain the actual reason for job failure.; ; Unfortunately Cromwell does not allow for dynamic backend selection (i.e. the backend name cannot be a variable), which necessitated copy/paste/renaming the Centaur test WDLs from their PAPIv2 versions, hence the magnitude of these diffs. The existing `preemptible_and_memory_retry ` Centaur test is heavily tailored to the quirks of Papi v2: a preemptible PAPI VM deletes itself and depends on the Lifesciences system mistaking that for a preemption event. tbh this is kind of a weird test and as I don't know how to induce a preemption on demand, I simply `ignore`d the GCPBATCH version. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7494:729,failure,failure,729,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7494,1,['failure'],['failure']
Availability,"### Description. The `google_legacy_machine_selection` workflow option that exists on PAPI v2 to pick JES-shaped machines was not completely wired in to the GCP Batch backend. However when I completed the wiring job and ran the `hello_google_legacy_machine_selection` Centaur test I got this:. ```; Task wf_hello.hello:NA:1 failed: Job failed when Batch tries to schedule it:; Batch Error: code - CODE_MACHINE_TYPE_NOT_FOUND, description - ; machine type predefined-1-2048 for job job-xyz, project 8675309, region us-central1, zones (if any) us-central1-b is not available.; ```. So basically `google_legacy_machine_selection` does not seem to work on GCP Batch. If anyone is using this undocumented feature to emulate the behavior of JES from many years ago, we should let them know that this support is going to be dropped when GCP Batch is rolled out. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7547:383,Error,Error,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7547,2,"['Error', 'avail']","['Error', 'available']"
Availability,"### Description. The current Centaur backend selection mode within a *.test file is 'all', which means if a given backend does not have all the backends enumerated in the backend specification then the test will be ignored. IMHO this is a dangerous default behavior because it can cause tests to be unexpectedly ignored, creating a false sense of confidence in the correctness of the code that was intended to be tested. These changes make the default backend selection mode 'any'. The failure mode here is that a test may run on backends that the test author did not intend, but if this happens that should be a noisy failure that the test author cannot ignore and must correct. Test authors can always explicitly override the `backendsMode` in a Centaur .test file if they want. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7513:486,failure,failure,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7513,2,['failure'],['failure']
Availability,"### Description. This should have no effect on our existing Bard usage, just removes errors logged when NOT using Bard. * Base config was incorrect, so Bard was not registered in the Service Registry by default; * `BardEventingActor.receive` had no handling for receiving a `BardEvent` when eventing was disabled... and also nothing has ever checked for enablement before creating and sending these events. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7566:85,error,errors,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7566,1,['error'],['errors']
Availability,### Description. Turn on 90ish Centaur tests for GCPBATCH. In all but one case this was just adding the GCPBATCH backend to the Centaur .test file. The one exception involved different error message text coming from the Batch system than what we get from Lifesciences. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7496:185,error,error,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7496,1,['error'],['error']
Availability,"### Description. UPDATE: issues with special characters in passwords appear to be resolved. PR to demo broken private Docker repo support in GCP Batch. There are actually multiple existing PAPI v2 Centaur tests in this vein; the one test enabled here for GCP Batch seems to be the simplest and demonstrates the issues clearly enough. The crux of this test is that the Docker image that is specified for the task is in a private repo to which the Centaur service account has been granted access. This test passes on PAPI v2 but on GCP Batch jobs fail with messages like the following visible in `gcloud batch jobs describe`:. ```; Job state is set from RUNNING to FAILED for job projects/1005074806481/locations/us-central1/jobs/job-27607753-d2d5-404d-89af-a786da8ad383.Job; failed due to task failure. Specifically, task with index 0 failed due to the; following task event: ""Task state is updated from RUNNING to FAILED on zones/us-central1-b/instances/8098872438472929780; with exit code 125."". ```. Exit code 125 being a typical ""[something's wrong with that Docker invocation](https://stackoverflow.com/questions/53640424/exit-code-125-from-docker-when-trying-to-run-container-programmatically)"" error. in Cloud Logging I see the following, including what looks like a plaintext password which I have x'd out below:. ```; Executing runnable container:{image_uri:""broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4"" commands:""/mnt/disks/cromwell_root/script"" entrypoint:""/bin/bash"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root"" username:""firecloud"" password:""xxxxx""} labels:{key:""tag"" value:""UserRunnable""} for Task task/job-27607753-d2d5-132dc052-df92-4db100-group0-0/0/0 in TaskGroup group0 of Job job-27607753-d2d5-132dc052-df92-4db100.; ```. So it looks like the GCP Batch backend has acquired and plumbed through the required Docker credentials, but the login to Docker Hub doesn't seem to have happened. ### Release Notes Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515:793,failure,failure,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515,1,['failure'],['failure']
Availability,"### Description. We make this query constantly, looks like the single most frequent one against metadata. All it does is check whether the workflow ID is valid by checking whether >1 metadatum exists for it. We already started down the path of checking summary instead of metadata, see https://github.com/broadinstitute/cromwell/pull/4617. It just makes way more sense to me to check a table with 77M rows than 36B. ```; select ; exists(; select ; `CALL_FQN`, ; `METADATA_KEY`, ; `WORKFLOW_EXECUTION_UUID`, ; `METADATA_TIMESTAMP`, ; `JOB_SCATTER_INDEX`, ; `METADATA_JOURNAL_ID`, ; `JOB_RETRY_ATTEMPT`, ; `METADATA_VALUE_TYPE`, ; `METADATA_VALUE` ; from ; `METADATA_ENTRY` ; where ; `WORKFLOW_EXECUTION_UUID` = '602a4913-d666-4182-b2f1-242fbda817d2'; );; ```. It is potentially implicated in the 2021 database migration that failed at the very end, you can see a bunch of them in this screenshot (2021-11-09):. <img width=""1792"" alt=""Screen Shot 2021-11-09 at 1 14 03 AM"" src=""https://github.com/user-attachments/assets/d3eee3da-9636-4406-a6f9-9862d33cd650"">. ```; Lock wait timeout exceeded; try restarting transaction; [for Statement ""RENAME TABLE `cromwell`.`METADATA_ENTRY` TO `cromwell`.`_METADATA_ENTRY_old`,; `cromwell`.`_METADATA_ENTRY_new` TO `cromwell`.`METADATA_ENTRY`""]; at /usr/bin/pt-online-schema-change line 10922.; ```; [Slack link to contemporary discussion.](https://broadinstitute.slack.com/archives/C02LCC8968N/p1636439602084200); [Contemporary analysis in JIRA.](https://broadworkbench.atlassian.net/browse/WM-906?focusedCommentId=53394). ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575:227,down,down,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575,1,['down'],['down']
Availability,"### Description. We need to propagate the Google credentials while pulling metadata from private GCR repositories. This is likely fixes #7356. Before this change, we'd get a log error when cromwell tries pulling the metadata, this occurs because `GoogleRegistry` implementation does not have a valid auth token:. ```; [2024-06-28 01:14:19,56] [info] Assigned new job execution tokens to the following groups: 5fe16e0e: 1; [2024-06-28 01:14:20,38] [warn] BackendPreparationActor_for_5fe16e0e:myWorkflow.myTask:-1:1 [5fe16e0e]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for gcr.io/<REDACTED>/debian:latest Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/<REDACTED>/locations/us/repositories/gcr.io\"" (or it may not exist)""}]}; ```. <details>; <summary>An example Workflow.wdl to test this</summary>. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }. runtime {; docker: ""gcr.io/<REDACTED>/debian:latest""; bootDiskSizeGb: 50; preemptible: 0; }; }; ```. </details>. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7464:178,error,error,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7464,4,"['down', 'echo', 'error']","['downloadArtifacts', 'echo', 'error', 'errors']"
Availability,"### Description. When Cromwell restarts during a failure, it must fail the 'next' upcoming tasks in order to cleanly terminate the workflow. During this process, it was logging an error that isn't really an error (and was very confusing to users). . This PR: ; - Changes the failure reason to something more relevant.; - Removes the failure reason from the 'Workflow' level failures, since it was not the reason the workflow failed. The failure reason is still attached to the task that never ran. . ### Release Notes Confirmation; #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7449:49,failure,failure,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7449,7,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"### Description; Google has shut down their Genomics (a.k.a. PAPI v2Alpha1) API, this PR cleans up associated code. This is not to be confused with the Cloud Life Sciences API (a.k.a. PAPI v2beta, deprecated, but still in use for the next few months) or the Google Batch backend (the new hotness). . #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7532:33,down,down,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7532,1,['down'],['down']
Availability,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4933:168,heartbeat,heartbeat,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933,4,"['down', 'error', 'heartbeat']","['downstream', 'error', 'errors', 'heartbeat']"
Availability,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740:582,error,errors,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740,1,['error'],['errors']
Availability,"### What happened. On 10/10/2018, around 11:15 AM, there was a spike in backpressure and 403 copy failures. It was discovered that a user had submitted workflows attempting to access buckets it did not have access to. . ![image](https://user-images.githubusercontent.com/16748522/46764755-59087300-ccab-11e8-9163-afd953710adf.png); Purple line- backpressure; Light green line- 403 copy failures. ### What was done to fix it. The situation was discussed with the user, and once he aborted all his workflows, Cromwell slowly returned to its normal state. The issue was resolved around 1:50 PM. ### Potential causes. The user had reused a WDL from another user, but he didn't have access to their Google Cloud buckets. This workflow contained job that ran 5000 split intervals against dataset of approx 1300 samples. Each of the 5000 outputs would be copied, per workflow, per sample. Depending on the number of samples the other user had previously run, each interval-output-for-each-sample tried call caching to other user's workspace. This resulted in a lot of attempts to copy files and then failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:98,failure,failures,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,3,['failure'],['failures']
Availability,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260,5,['error'],"['error', 'errors']"
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. endpoint-url = ""https://genomics.googleapis.com/""; Cromwell version 55. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; All job submissions stopped working today with errors:; Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager)"". Error messages from Cromwell logs:; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anon$1: A batch of PAPI status requests failed. The request manager will retry automatically up to 10 times. The error was: 404 Not Found; POST https://genomics.googleapis.com/batch; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 404 (Not Found)!!1</title>; ...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:1280,error,errors,1280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; The backend the workflow pipelines is https://genomics.googleapis.com/. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Error message: ; The job was stopped before the command finished. PAPI error code 14. Execution failed: worker was terminated. The job was running on non-preemptible VM, with one instance of nvidia-tesla-t4 attached, nvidiaDriverVersion: 418.40.04. . What does ""PAPI error code 14"" mean? Can you suggest what we should do with it?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306:1233,Error,Error,1233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306,3,"['Error', 'error']","['Error', 'error']"
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. I'm using cromwell to kick off 1000 mutect2 jobs at a time, but right now the workflow doesn't kick off all 1000 sub-workflows simultaneously. When I look at my google cloud quotas none are close to being full. I'm wondering what settings I might need to change in the system or some other section of the google conf to better use all the compute I have available. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; I'm kicking off google cloud jobs using cromwell from a local VM.; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. #; # Description of inputs; # intervals: genomic intervals; # ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary; # vi : arrays of normal bams; # scatter_count: number of parallel jobs when scattering over intervals; # pon_name: the resulting panel of normals is {pon_name}.vcf; # m2_extra_args: additional command line parameters for Mutect2. This should not involve --max-mnp-distance,; # which the wdl hard-codes to 0 because GenpmicsDBImport can't handle MNPs. #import """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352:591,avail,available,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352,1,['avail'],['available']
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:404,error,error,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,2,['error'],['error']
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. I'm trying to run the workflow below. There is a working version for a single sample (https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl), but I want to run the workflow on many samples concurrently. My attempt at creating a workflow to do so failed with the following error:. ```; Failed to process scatter block (reason 1 of 1): No conversion defined for Ast with name Outputs to WorkflowGraphElement; ```. Any help would be greatly appreciated. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:528,error,error,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['error'],['error']
Availability,#3984 restored streaming logs. A/C for this ticket would add a regression test for the functionality. One possible centaur implementation for a task:; - Writes to local stdout & stderr; - Calculates the gcloud stdout & stderr path; - Sleep a bit to allow streaming; - Use gsutil to download the stdout & stderr from gcloud; - Validate that the stdout & stderr are as expected,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4187:282,down,download,282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4187,1,['down'],['download']
Availability,$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(Wom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:6708,Error,ErrorOr,6708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5789,error,errors,5789,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:5213,error,error,5213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['error'],['error']
Availability,"(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:3298,error,error,3298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Then I see:. ```; [ERROR] [05/01/2017 17:36:04.203] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 5; 3e95ead-9026-4c13-89f9-f6c675214523 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:1:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the follow; ing files: ""gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD; /DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A2; 5E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n; File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.; py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrappin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:2592,error,error,2592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['error'],['error']
Availability,"(First, I've registered at; https://gatkforums.broadinstitute.org/wdl/categories/wdl#; but I cannot figure out how to post a question there. If you help me with that, I would be happy to re-ask in the ""user forum"" there, but so far I do not see how.). My question is on `write_json` versus `write_lines`. The following works fine (using Cromwell-36):; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_lines(results)}""; echo ${files_fn}; >>>; }; ```; But this fails:; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_json(results)}""; echo ${files_fn}; >>>; }; ```; It says `write_json` needs an ""Object"". Am I doing something wrong?; ```; Failed to process task definition 'foo' (reason 1 of 1): Failed to process expression 'write_js; on(results)' (reason 1 of 1): Invalid parameter 'IdentifierLookup(results)'. Expected 'Object' but got 'Array[File]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4625:415,echo,echo,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625,4,['echo'],['echo']
Availability,"(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14676,error,errors,14676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"(JdbcBackend.scala:491); at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:660); at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:517); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:25); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); at java.base/java.lang.Thread.run(Thread.java:1589); Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.Table.enforceTypeLimits(Unknown Source); at org.hsqldb.Table.generateAndCheckData(Unknown Source); at org.hsqldb.Table.insertSingleRow(Unknown Source); at org.hsqldb.StatementDML.insertRowSet(Unknown Source); at org.hsqldb.StatementInsert.getResult(Unknown Source); at org.hsqldb.StatementDMQL.execute(Unknown Source); at org.hsqldb.Session.executeCompiledStatement(Unknown Source); at org.hsqldb.Session.execute(Unknown Source); ... 17 common frames omitted; Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.types.CharacterType.convertToTypeLimits(Unknown Source); ... 25 common frames omitted; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:15:1]: Status change from - to WaitingForReturnCode; [2022-11-10 13:45:54,45] [info] Backg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:3928,Error,Error,3928,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['Error'],['Error']
Availability,"(Mailbox.scala:268); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); cromwell_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); cromwell_1 | ; cromwell_1 | 2024-01-11 11:09:38 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - BT-322 0845428a:myworkflow.mytask:-1:1 is not eligible for call caching; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 1280",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:6778,echo,echo,6778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['echo'],['echo']
Availability,"(No links to failed builds, as clicking Travis ""Retry Build"" erased the logs.). Travis has been having network errors recently attempting to download artifacts from maven central, and in at least one case downloading the ubuntu:latest docker image. In the various scripts under src/bin/travis, the scripts should retry:. - `sbt update` before any other sbt commands; - `docker pull ubuntu:latest`; - possibly: `sudo apt-get â€¦`. The retries should wait some time (60 seconds?) between failures, and try at least twice? Possibly this could be put into a utility bash script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1791:111,error,errors,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1791,4,"['down', 'error', 'failure']","['download', 'downloading', 'errors', 'failures']"
Availability,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4570:510,error,error,510,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570,2,"['Error', 'error']","['ErrorOr', 'error']"
Availability,"(during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687:1206,error,error,1206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687,1,['error'],['error']
Availability,) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:32) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:29) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_74]; > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_74]; > at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_74]; > Caused by: org.hsqldb.HsqlException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.error.Error.error(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Constraint.getException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.index.IndexAVLMemory.insert(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.persist.RowStoreAVL.indexRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.TransactionManagerMVCC.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Table.insertSingleRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementDML.insertRowSet(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementInsert.getResult(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementDMQL.execute(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.executeCompiledStatement(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.execute(Unknown Source) ~[cromwell-0.19.jar:0.19]; > ... 23 common frames omitted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:4285,error,error,4285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['error'],['error']
Availability,"); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:3361,error,errors,3361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['error'],['errors']
Availability,); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:86); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(Part,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:4092,robust,robustExecuteOrRecover,4092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['robust'],['robustExecuteOrRecover']
Availability,); 	at cromwell.backend.wdl.ReadLikeFunctions$class.read_string(ReadLikeFunctions.scala:62); 	at cromwell.backend.impl.jes.JesExpressionFunctions.read_string(JesExpressionFunctions.scala:16); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at wdl4s.expression.WdlFunctions$$anonfun$getFunction$1.apply(WdlFunctions.scala:12); 	at wdl4s.expression.WdlFunctions$$anonfun$getFunction$1.apply(WdlFunctions.scala:12); 	at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:181); 	at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 	at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:180); 	... 38 more; Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoog,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:7178,Error,Error,7178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['Error'],['Error']
Availability,"* Addresses the ""programmer error"" in JES API manager; * Makes the logging in these classes more usable and traceable; * Replaces strings like `""JES API polling worker""` with strings like `""PAPI request worker""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671,1,['error'],['error']
Availability,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:531,error,error,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,1,['error'],['error']
Availability,"* LEVEL_1 `outer_subworkflow.wdl` then creates a scatter of 2 across another subworkflow (`call inner.inner_subworkflow`/LEVEL_2A and LEVEL_2B); ``` wdl; import ""inner_subworkflow.wdl"" as inner. workflow outer_subworkflow {; scatter (i in range(2)) {; call inner.inner_subworkflow as inner_subworkflow; }; }; ```; * `inner_subworkflow.wdl`/LEVEL_2A and LEVEL_2B then runs a task with a scatter and a scatter of 3 across a final subworkflow (`call sub_workflow.sub_subworkflow`/ LEVEL_2_X__3_Y); ``` wdl; import ""sub_subworkflow.wdl"" as sub_subworkflow. task hello_world {; command {; echo 'Hello, world!'; echo 'blah' > output.txt ; }. output {; String message = read_string(stdout()); File outputFile = ""output.txt""; }. runtime {; docker: ""ubuntu:latest""; }; }. workflow inner_subworkflow {; scatter (i in range(4)) {; call hello_world; }; scatter (i in range(3)) {; call sub_subworkflow.sub_subworkflow; }; }; ```; * This final `sub_subworkflow.wdl` then runs a scatter across a task:; ``` wdl; task sub_hello_world {; command {; echo 'Hello from sub.sub_workflow, world!'; }. output {; String message = read_string(stdout()); }. runtime {; docker: ""ubuntu:latest""; }; }. workflow sub_subworkflow {; scatter (i in range(2)) {; call sub_hello_world; }; }; ```. In tree form you have something like this:; * ROOT_WORKFLOW `main_workflow.wdl`; * LEVEL_1 `outer_subworkflow.wdl`; * LEVEL_2A `inner_subworkflow.wdl`; * LEVEL_2_A__3_A `sub_subworkflow.wdl`; * LEVEL_2_A__3_B `sub_subworkflow.wdl`; * LEVEL_2_A__3_C `sub_subworkflow.wdl`; * LEVEL_2B `inner_subworkflow.wdl`; * LEVEL_2_B__3_A `sub_subworkflow.wdl`; * LEVEL_2_B__3_B `sub_subworkflow.wdl`; * LEVEL_2_B__3_C `sub_subworkflow.wdl`. LEVEL_2 `inner_subworkflow.wdl` task outputs end up here:; ```; cromwell-executions/main_workflow/ecb081a4-0166-4f9f-a2a8-20a50f8e9b19/; call-outer_subworkflow/outer.outer_subworkflow/53f62151-1c36-4e2f-8bff-0a2a90d7d8c5/; call-inner_subworkflow/shard-0/inner.inner_subworkflow/cd65e57c-12ee-4213-a698-c98dd0a9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7387:3220,echo,echo,3220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7387,1,['echo'],['echo']
Availability,* Re-pin conformance test hash to the current HEAD of master.; * Allow workflow inputs to be recycled back as outputs (part 1/2 fixing conformance 20).; * Allow arrays to be coerced to Anys (part 2/2 fixing conformance 20).; * Fix prefix handling with empty arrays (fix conformance 125).; * Adjust expected conformance failures for all of the above.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3467:319,failure,failures,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3467,1,['failure'],['failures']
Availability,"* Upgrade from deprecated `trusty` dist to `xenial` (needed to get psutil 5.6.4 working); * Switch from Oracle JDK to OpenJDK (needed for the change above, xenial only supports Java 9 to 14); * Fix mistakes and deprecations in `mkdocs.yml` since the configuration of the `xenial` image treats `mkdocs` warnings as errors which fail the `checkPublish` build; * Don't explicitly start `munged` for the SLURM build since it seems to already be started in `xenial`. The `munged` bit would especially benefit from @kshakir 's input, I'm pretty sure that could be done better.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5262:314,error,errors,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5262,1,['error'],['errors']
Availability,"* Wiring in the remaining draft 3 engine functions' value evaluators; * Modified value evaluation to keep a running track of its side-effect file generation; * Also allowed value evaluation to run value mapping if used in placeholder interpolation. Note: red thumb required because there's some changes to WomIdentifier undoing some of the changes in the forkjoin PR and making them again, but correcter this time. . This PR ended up with far wider-ranging changes than I originally expected (even requiring changes in draft 2 and CWL) so if you see something that smells wrong... âš ï¸ â—ï¸ â—ï¸ âš ï¸ . ~~NB: this PR will go down by about 700 lines once the `forkjoin` PR merges~~ DONE!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3505:617,down,down,617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3505,1,['down'],['down']
Availability,"* `cromwell-30.jar`; * local; * single workflow mode. ```; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true	; 	if ( b0 ) {; 		if ( b2 ) {; 			call t0 as t2 { input: i=2 }; 		}; 		if ( b1 ) {; 			call t0 as t1 { input: i=1 }			; 		}; 		#Boolean tmp = b1 && b2; 		#if ( tmp ) {	; 		if ( b1 && b2 ) {; 			call t0 as t12 { input: i=12 }				; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo test > ${i}.txt; 	}; 	output {; 		File out = glob('*.txt')[0]; 	}; }; ```; This code does not work.; ```; $ java -jar ../cromwell-30.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 09:40:22,36] [info] Running with database db.url = jdbc:hsqldb:mem:ee347d5b-2cdf-4b76-b68a-dc5d09a93aeb;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 09:40:28,42] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 09:40:28,44] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 09:40:28,54] [info] Running with database db.url = jdbc:hsqldb:mem:68a1b424-aa08-4f22-bc04-952c5eb83e7e;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 09:40:29,02] [info] Slf4jLogger started; [2017-12-05 09:40:29,28] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 09:40:29,29] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 09:40:29,30] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 09:40:29,35] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 09:40:30,63] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 09:40:30,68] [info] Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 submitted.; [2017-12-05 09:40:30,68] [info] SingleWorkflowRunnerActor: Workflow submitted 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,69] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992:411,echo,echo,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992,1,['echo'],['echo']
Availability,"* disabled build failures due to deprecation warnings... it's a hotfix, deadend branch!; * support public http-based imports in cromwell; * fixed a few necessary classes due to newer cats being pulled in",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2734:17,failure,failures,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2734,1,['failure'],['failures']
Availability,"* lost a few hours debugging this test, made it more robust",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2749:53,robust,robust,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2749,1,['robust'],['robust']
Availability,"** /dev""; ls -l /dev; ; if [ -d /dev/disk ]; then; echo; echo ""**** /dev/disk""; ls /dev/disk; fi; ; echo; echo ""**** /proc/mounts""; cat /proc/mounts; ; echo; echo ""**** /etc/mtab""; cat /etc/mtab; ; echo; echo ""**** /sys/block""; find -L /sys/block -maxdepth 2; ; echo; echo ""**** /sys/block/sdb/size (converted to integer GB)""; echo ""$(($(cat /sys/block/sdb/size) * 512 / 2**30))""; ; echo; echo ""**** /sys/devices""; find -L /sys/devices -maxdepth 3; >>>; ; runtime {; docker: ""talkowski/delly""; memory: ""1.7 GB""; cpu: ""1""; disks: ""local-disk 250 HDD""; preemptible: 3; }; }; ```; Snips of relevant output from cromwell 36 (edited for brevity):; ```; **** df -h; Filesystem Size Used Available Use% Mounted on; /dev/disk/by-id/google-local-disk; 245.1G 60.0M 245.0G 0% /cromwell_root; **** /dev; total 0; lrwxrwxrwx 1 root root 11 Nov 14 21:16 core -> /proc/kcore; lrwxrwxrwx 1 root root 13 Nov 14 21:16 fd -> /proc/self/fd; crw-rw-rw- 1 root root 1, 7 Nov 14 21:16 full; drwxrwxrwt 2 root root 40 Nov 14 21:16 mqueue; crw-rw-rw- 1 root root 1, 3 Nov 14 21:16 null; lrwxrwxrwx 1 root root 8 Nov 14 21:16 ptmx -> pts/ptmx; drwxr-xr-x 2 root root 0 Nov 14 21:16 pts; crw-rw-rw- 1 root root 1, 8 Nov 14 21:16 random; drwxrwxrwt 2 root root 40 Nov 14 21:16 shm; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stderr -> /proc/self/fd/2; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stdin -> /proc/self/fd/0; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stdout -> /proc/self/fd/1; crw-rw-rw- 1 root root 5, 0 Nov 14 21:16 tty; crw-rw-rw- 1 root root 1, 9 Nov 14 21:16 urandom; crw-rw-rw- 1 root root 1, 5 Nov 14 21:16 zero. **** /proc/mounts; /dev/disk/by-id/google-local-disk /cromwell_root ext4 rw,relatime,data=ordered 0 0. **** /etc/mtab; /dev/disk/by-id/google-local-disk /cromwell_root ext4 rw,relatime,data=ordered 0 0. **** /sys/block/sdb/size (converted to integer GB); 250; ```. Whereas on cromwell 30; ```; **** df -h; Filesystem Size Used Available Use% Mounted on; /dev/sdb 246.0G 59.1M 233.4G 0% /cromwell_root; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388:3145,Avail,Available,3145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388,1,['Avail'],['Available']
Availability,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4901:477,echo,echo,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901,1,['echo'],['echo']
Availability,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805:624,Error,Error,624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805,1,['Error'],['Error']
Availability,"**Command**:; `sudo java -jar cromwell-84.jar run ngs-ubuntu-20-04/iletisim/warp/pipelines/broad/dna_seq/germline/single_sample/exome/local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl -i ngs-ubuntu-20-04/iletisim/json/S736Nr1.json -o ngs-ubuntu-20-04/iletisim/json/options2.json`. **Platform**:; Ubuntu 20.04 via WSL2 on Windows 10 pro. **Java**:; openjdk 11.0.17 2022-10-18; OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu220.04); OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu220.04, mixed mode, sharing). **Docker**:; Docker version 20.10.22, build 3a2c30b; Docker desktop v.4.16.3. **Inputs & Options JSON:**; Please find attached in the zip.; [forgithub.zip](https://github.com/broadinstitute/cromwell/files/10610687/forgithub.zip). **Workflow & Error**:; I am running the main workflow (WDL attached) named ""local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl"". It first successfully finishes the first task from ""BamProcessing.wdl"" (WDL attached) specifically running the task ""GenerateSubsettedContaminationResources"" via docker: ""us.gcr.io/broad-gotc-prod/bedtools:2.27.1"". In the next step it starts the next task from ""paired-fastq-to-unmapped-bam.wdl"" (WDL attached) via docker: ""broadinstitute/gatk:latest"". Inspection of the container's log during this step shown below:. ```; 2023-02-05 12:55:43 mkfifo: cannot create fifo '/cromwell-executions/ExomeGermlineSingleSample/9053ae04-ca8c-4d23-848d-7a04313af725/call-ConvertPairedFastQsToUnmappedBamWf/ConvertPairedFastQsToUnmappedBamWf/c19284af-355b-4a83-bc7d-5fed437ea8e7/call-PairedFastQsToUnmappedBAM/tmp.8cbe1f4a/out.1': Operation not supported; 2023-02-05 12:55:43 mkfifo: cannot create fifo '/cromwell-executions/ExomeGermlineSingleSample/9053ae04-ca8c-4d23-848d-7a04313af725/call-ConvertPairedFastQsToUnmappedBamWf/ConvertPairedFastQsToUnmappedBamWf/c19284af-355b-4a83-bc7d-5fed437ea8e7/call-PairedFastQsToUnmappedBAM/tmp.8cbe1f4a/err.1': Operation not supported; 2023-02-05 12:55:4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7002:793,Error,Error,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7002,1,['Error'],['Error']
Availability,"**Edit:** I've encountered more issues here, so putting this on pause. ---. **Short version:** rather than an InstrumentationPath being a `NonEmptyList[String]`, it is now [a more complex type](https://github.com/broadinstitute/cromwell/blob/DDO-1728-handle-metric-variance/services/src/main/scala/cromwell/services/instrumentation/CromwellInstrumentation.scala#L48). Everything else is refactoring to match that change. **Long version:**. There's basically two competing models for metric names--""lots of metrics, no labels"" and ""smaller a number of metrics, with labels for variation"". Statsd uses the first, and Prometheus (and Stackdriver too, ideally) use the second. By way of example:. ```; api.response.count.200; ```. versus. ```; api_response_count{code=""200""}; ```. This boils down to how the different systems query metrics. In a StatsD world, you can have a query like `api.response.count.*`, but that's not possible for Prometheus/Stackdriver--the actual metric name needs to be fully static, and you conceptually do `api_response_count{code=""*""}`. Cromwell metric names right now are this:. ```scala; type InstrumentationPath = NonEmptyList[String]. CromwellBucket(prefix: List[String], path: InstrumentationPath); ```. Metric names are guaranteed to not be empty by the path. The path is what is actually assembled in code and passed around, and prefix (or an empty list) is added at the outer edge--most of Cromwell treats the `NonEmptyList[String]` as the metric name. When the metrics are actually sent to StatsD, the strings are joined with periods and sent off. I spent several days trying to figure out a way to reliably parse labels out from these names, and I couldn't figure it out. There's two key pieces of info known at the time the `NonEmptyList[String]` is assembled that are not recorded:; - When a particular string being added to the list is expected to vary frequently (and thus needs to be a label); - What that string means (labels are key-value pairs and we're usu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6681:788,down,down,788,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681,1,['down'],['down']
Availability,"**Finalized Ticket**. For the `read_x()` functions, limit the size of data which can be read in. Check the file size prior to attempting to read as reading can also incur network charges. Any attempt to read an oversized file will result in a failure. See notes from Geraldine down below about how to phrase the error. `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB. **Original text for posterity**; Note to @kcibul - this is both a request for the WDL spec and Cromwell's implementation, not just the latter. Our read_X() functions (e.g. read_int, read_string, read_lines) have a flaw in that they'll dutifully read in the entire file. The problem with this is that a malicious and/or less than careful user could take down Cromwell or another WDL implementing engine (unless it was written in Erlang!) by reading in an enormous file. For instance a careless user might `read_string(SomeEnormousBam)`. The point of these functions are more of a convenience, if users are trying to sling around huge chunks of data they should be passing files around. In particular things like `read_boolean()` are particularly egregious as it will only interpret `true` or `false`. Similarly it seems unlikely that someone would have a valid use case to read in the first 9 billion digits of Pi into a `Float`. . I propose that we place a cap on how much data we will read to some reasonable amount of data (e.g. CWL uses 64 KiB, which seems a little excessively small to me).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762:243,failure,failure,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762,4,"['down', 'error', 'failure']","['down', 'error', 'failure']"
Availability,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:322,error,error,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['error']
Availability,"**NB** - we're happy to have more than one person working on this as long as it's temporally close. If you find this interesting and there's already someone assigned speak up. Also we're happy to have people doing this in spare cycles unofficially (as long as there's at least one official person), so if that's you also speak up. Timeboxed to 1 week. Take a deep dive into CWL in whatever form you think will be the most effective for you. The output of this should be some form of show & tell to the group, whatever you think will be most effective for how you did this. To give a little bit of guidance: Imagine a spectrum between becoming a generalized CWL expert on one side or having a perfect proof of concept of how we could implement one specific thing. The desired outcome would be to trend towards the former. Our goal here is to have a goto person (or persons) available when we start putting shovels to the ground.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2238:873,avail,available,873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2238,1,['avail'],['available']
Availability,"**PR**: https://github.com/broadinstitute/cromwell/pull/7224. **Idea:** Add an auth mode for GCP that allows Cromwell to impersonate a service account and use [short-lived credentials](https://cloud.google.com/iam/docs/create-short-lived-credentials-direct) for calls to GCP. . **Context:** Service account impersonation is now the [defacto recommendation](https://cloud.google.com/docs/authentication#auth-decision-tree) when choosing an authentication mechanism that relies on service accounts. Some teams (such as Verily) might prefer this approach over the current option to rely on downloaded service account keys. **How it works**; Users would specify an auth block in the .conf file with one of the following formats. The first specifies . - Application default credentials used for **source service account**; ```; {; name = ""user-service-account""; scheme = ""user_service_account_impersonation""; }; ```. - A specified JSON file used for **source service account**; ```; {; name = ""user-service-account""; scheme = ""user_service_account_impersonation""; json-file= ""path/to/file.json""; }; ```. Users would then add the following option when making workflow requests:; ```; {; ""user_service_account_email"": ""someemail@domain.com; }; ```. Cromwell would use the **source service account** to impersonate the **target service account** from the workflowOptions. It would then mint and refresh access tokens from this target service account for all GCP requests. . In order for this to work, the source service account would need the IAM role roles/iam.serviceAccountTokenCreator. **If the team would prefer a smaller PR**; We could change this to only ever use applicationDefaultCredentials, in which case we could remove parts of the code that are altering ServiceAccountMode",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7223:587,down,downloaded,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7223,1,['down'],['downloaded']
Availability,"**WORKAROUND:** Explicitly `mkdir` (as necessary) and `export` a new `$TMPDIR` at the top of your task command, for example `export TMPDIR=/tmp` should work. Setting the `TMPDIR` environment variable to a long path will cause an error in Python `mulitprocessing` library. ```; Process SyncManager-1: ; Traceback (most recent call last):; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 258, in _bootstrap; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 114, in run; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 550, in _run_server; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3647:229,error,error,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647,1,['error'],['error']
Availability,"**What Happened**; On 9/12/18 5:40 pm, after a Firecloud release, Cromwell 402 stopped responding to status checks. It only recovered after being restarted at 10pm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4094:124,recover,recovered,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094,1,['recover'],['recovered']
Availability,"**What happened:**; On 9/12/18 at 3:41 pm, the Cromwell servers' (801 and 802) CPU were pegged. 801 was restarted (4:11 pm) and immediately got pegged again but recovered 35 min later without further intervention. As for 802, the CPU remained pegged so, at 5:21 pm, 802 was restarted and recovered right away.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093:161,recover,recovered,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093,2,['recover'],['recovered']
Availability,"*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".sorted.bam\"",\n# disk_size = sort_sam_disk_size\n# }\n\n call ValidateSamFile {\n input:\n input_bam = RevertSam.unmapped_bam,\n report_filename = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".unmapped.bam$\"", \""\"") + \"".validation_report\"",\n disk_size = validate_sam_file_disk_size\n }\n\n output {\n RevertSam.*\n ValidateSamFile.*\n }\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""engle-macarthur-ccdd\"",\n \""auth_bucket\"": \""gs://cromwell-auth-engle-macarthur-ccdd\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflow.logs\"",\n \""account_name\"": \""kcibul@broadinstitute.org\"",\n \""jes_gcs_root\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""a714b11b-0162-4585-afa5-abbd7433af51"",; ""inputs"": {; ""BamToUnmappedBams.input_bam"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/batch04/S64-2_Illumina.bam""; },; ""submission"": ""2017-01-19T18:17:12.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Google credentials are invalid: connect timed out""; }],; ""workflowLog"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflow.logs/workflow.a714b11b-0162-4585-afa5-abbd7433af51.log"",; ""end"": ""2017-01-19T18:17:39.673Z"",; ""start"": ""2017-01-19T18:17:19.606Z""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:4250,failure,failures,4250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['failure'],['failures']
Availability,"*See comment below on how to fix/address*. - cromwell-27-c89c83f-SNAP.jar; - JES backend; - server mode; - local mysql. I have a database block that looks exactly like the one in the example (from the error message), yet I still get the error message. I tried a diff on the database blocks, between the example and my database block, so I am sure that they match. Is this just a mistake in the error message itself? . This is blocking me. The error:. ```; Caused by: java.lang.Exception:; *******************************; ***** DEPRECATION MESSAGE *****; *******************************. Use of configuration path 'database.driver' has been deprecated. Replace with a ""profile"" element instead, e.g:. database {; #driver = ""slick.driver.MySQLDriver$"" #old; profile = ""slick.jdbc.MySQLProfile$"" #new; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; }. Cromwell thanks you. at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:70); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 22 more. ```. My conf file for database:; ```; database {; #driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell_24?useSSL=false&rewriteBatchedStatements=true""; user = ""root""; password = ""blahblah""; connectionTimeout = 5000; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217:201,error,error,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217,4,['error'],['error']
Availability,", ggp-1817239588482439788, us-east1-c/n1-standard-2; > broad-wgs-prod5, 2018-01-16T23:46:08Z, 2018-01-17T14:57:19Z, ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've bee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:9693,down,down,9693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['down'],['down']
Availability,", not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:1069,echo,echo,1069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,1,['echo'],['echo']
Availability,",89] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-01 20:01:02,94] [info] Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 submitted.; [2017-12-01 20:01:02,94] [info] SingleWorkflowRunnerActor: Workflow submitted 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,95] [info] 1 new workflows fetched; [2017-12-01 20:01:02,95] [info] WorkflowManagerActor Starting workflow 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] WorkflowManagerActor Successfully started WorkflowActor-132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-01 20:01:03,27] [info] MaterializeWorkflowDescriptorActor [132d7527]: Call-to-Backend assignments: test.t1 -> Local; [2017-12-01 20:01:04,64] [info] WorkflowExecutionActor-132d7527-a0af-4f08-8291-d935e7cd5632 [132d7527]: Starting calls: test.t1:NA:1; [2017-12-01 20:01:04,82] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: echo test1 > test1.txt; echo test2 > test2.txt; [2017-12-01 20:01:04,86] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: executing: /bin/bash /users/leepc12/code/atac-seq-pipeline/test/cromwell-executions/test/132d7527-a0af-4f08-8291-d935e7cd5632/call-t1/execution/script; [2017-12-01 20:01:04,91] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:2510,echo,echo,2510,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['echo'],['echo']
Availability,",; ""Mutect2_Multi.unfiltered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-unfilteredOutputList/execution/unfiltered.list"",; ""Mutect2_Multi.ob_filtered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-orientationBiasFilteredOutputList/execution/ob_filtered.list""; },; ""id"": ""0239d302-1154-4c39-9870-55574d000765""; }; [2017-03-20 15:30:35,34] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/debug_m2_wdl/test_m2_wdl.metadata; [2017-03-20 15:30:35,46] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-21-0-unknown-operation#1356917576]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-17-0-unknown-operation#-291022515]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-15-0-unknown-operation#-925665144]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-3-0-unknown-operation#-2130885356]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-4-0-unknown-operation#-1268876796]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-16-0-unknown-operation#-371454906]] terminated abruptly; [2017-03-20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:3587,error,error,3587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,",; ""doc"": ""Minimum spread within candidate purities before somatics can be used. Default 0.15\n"",; ""id"": ""#somatic_min_purity_spread_purple""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Minimum number of somatic variants required to assist highly diploid fits. Default 300.\n"",; ""id"": ""#somatic_min_total_purple""; },; {; ""type"": [; ""null"",; ""float""; ],; ""doc"": ""Proportion of somatic deviation to include in fitted purity score. Default 1.\n"",; ""id"": ""#somatic_penalty_weight_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\nSample name must match tumor parameter. GZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#somatic_vcf_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of structural variant vcf for more accurate segmentation.\nGZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#structural_vcf_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of failing structural variants that may be recovered.\nGZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#sv_recovery_vcf_purple""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads used for amber step\n"",; ""id"": ""#threads_amber""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads to run cobalt command\n"",; ""id"": ""#threads_cobalt""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads to use - set to 8 by default"",; ""id"": ""#threads_gridss""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads\n"",; ""id"": ""#threads_purple""; },; {; ""type"": ""File"",; ""doc"": ""tumour BAM file\n"",; ""secondaryFiles"": [; "".bai""; ],; ""id"": ""#tumor_bam""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""sample name of tumor. Must match the somatic snvvcf sample name. (Default: \\${sample}_T)\n"",; ""id"": ""#tumor_sample""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""htsjdk SAM/BAM validation level (STRICT (default), LENIENT, o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:46840,recover,recovered,46840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Availability,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:117,down,down,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,1,['down'],['down']
Availability,"- 0.23; - SGE backend; - single workflow; - no docker. The wdl in question (a simpler WDL can be made easily). Look at how it scatters over a variable that does not exist. I would expect cromwell to give an error message and exit.; ```; # This is **broken.wdl**; # This simple, *unsupported* WDL takes in a VCF from M2 and a tumor bam file.; # It produces a new VCF with the filtering results. workflow test_ob_filter {; # tsv; # entity_id vcf tumor_bam_file; File input_table; Array[Array[String]] m2_vcfs = read_tsv(input_table); File db_snp; String gatk_jar; File ref_fasta. scatter (row in THIS_VAR_DOES_NOT_EXIST) {; call CollectSequencingArtifactMetrics {; input:; entity_id=row[0],; bam_file=row[2],; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; output_location_prepend=row[0]; }; call FilterByOrientationBias {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; m2_vcf=row[1],; pre_adapter_detail_metrics=CollectSequencingArtifactMetrics.pre_adapter_detail_metrics; }; }. call MakeSummaryFileList {; input:; files=FilterByOrientationBias.orientation_bias_vcf_summary,; output_file=""summary_table.txt""; }; }. task CollectSequencingArtifactMetrics {; String entity_id; File bam_file; String output_location_prepend; String gatk_jar; # /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; File ref_fasta. command {; java -jar ${gatk_jar} CollectSequencingArtifactMetrics -I ${bam_file} -O ${output_location_prepend} -R ${ref_fasta} --VALIDATION_STRINGENCY SILENT; }. output {; File pre_adapter_detail_metrics = ""${output_location_prepend}.pre_adapter_detail_metrics""; File pre_adapter_summary_metrics = ""${output_location_prepend}.pre_adapter_summary_metrics""; File bait_bias_detail_metrics = ""${output_location_prepend}.bait_bias_detail_metrics""; File bait_bias_summary_metrics = ""${output_location_prepend}.bait_bias_summary_metrics""; }; }. task FilterByOrientationBias {; String entity_id; String gatk_jar; File m2_vcf; File pre_adapter_detail_metrics. command {; java -jar ${ga",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1774:207,error,error,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774,1,['error'],['error']
Availability,"- 0.23; - Single Workflow; - SGE backend. Task fails, but cromwell, never realizes and simply hangs. SGE has already exited cleanly. In this case, I believe cromwell would simply exit with non-zero return code. stderr:; ```; /var/spool/sge/node1403/job_scripts/9226232: line 16: warning: here-document at line 4 delimited by end-of-file (wanted `CODE'); /var/spool/sge/node1403/job_scripts/9226232: line 17: syntax error: unexpected end of file. ```. Task is broken due to unforeseen tab issues (unlikely to be reproduced correctly here):. ```wdl; command {; python <<CODE; 	 	import shutil; 		import os; 		str_file_list = """"""${sep=""\n"" files}""""""; 		fp = fopen(${output_file}, 'w'); 		fp.write(str_file_list); 		fp.close(); CODE; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1773:415,error,error,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1773,1,['error'],['error']
Availability,"- 0.24; - SGE backend. I accidentally gave an Int parameter a String value in the json. I would prefer an error specific to parameter type, rather than a generic invalid runtime attribute error message (below). Proposed solution: ; ``Task m1_task was given an invalid type for cpu = ""${cpu}"". A String was given, though parameter is an Int``. Current error message:; ```; [ERROR] [02/08/2017 10:38:57.225] [cromwell-system-akka.dispatchers.engine-dispatcher-8] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 07a3f007-8c62-4cd4-8668-6ac034ff42f1 failed (during InitializingWorkflowState): Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; java.lang.IllegalArgumentException: Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:156); at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963:106,error,error,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"- 0.24; - SGE backend. When used for a list of files (as Strings), I received a lot of file not found errors. Seems like CRLF (0D0A) is interpreted as two lines. Proposed solution: have ``read_lines`` do a strip/trim on each line before returning the Array of String/File. Offending WDL:. `` Array[String] oncotated_m1_files = read_lines(input_oncotated_m1)``",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876:102,error,errors,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876,1,['error'],['errors']
Availability,"- 0.24; - SGE backend; - single workflow mode; - sub-workflows involved; - no docker. I can provide exact WDL, but replicating this may need new WDL. Feel free to ask me questions. I have a full WDL that calls two subworkflows, serially. . - The first takes a really long time and completed successfully on the first run. ; - The second subworkflow failed on the first run. This was due to an error in the call block of the WDL (marked below); - Fixed the error ; - Ran it again for a second run. Unexpected: In the second run, the first subworkflow call did not call cache and I do not know why not. Nothing was changed in the call block (nor parameters, etc etc). ```; import ""dl_ob_training.wdl"" as dl_ob_training; import ""m1/m1.wdl"" as m1. workflow full_dl_ob_training_with_m1 {. .....snip.....; Array[Pair[File, File]] tumor_bam_pair = zip(tumor_bam_files, tumor_bam_indices); scatter (p in tumor_normal_pairs) {. # All of the m1.m1 calls completed just fine; call m1.m1 {; input: ; tumorBam=p.left.left,; tumorBamIdx=p.left.right,; ....snip....; ; }. call dl_ob_training.dl_ob_training {; input:; # I fixed an error in the two lines below. Below is the corrected output; bam_file=p.left.left,; bam_file_index=p.left.right,; .....snip..........; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1970:393,error,error,393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970,3,['error'],['error']
Availability,"- 0.24; - single workflow mode; - JES backend. When I run the workflow, I get a localization permission error, but when I try again from the command line, there is no issue.; From cromwell:; ```; ....snip....; java.lang.RuntimeException: Task 773d051e-2e93-4248-bca4-e40292e0e59d:generate_true_positives failed: error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list -> /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list (cp failed: gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list, command failed: AccessDeniedException: 403 Caller does not have storage.objects.list access to bucket firecloud-tcga-open-access.\nCommandException: 1 file/object could not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:489); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:61); ....snip....; ```; ; BUT I would think this next operation would fail and it does not:; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/tutorial/reference/; gs://firecloud-tcga-open-access/tutorial/reference/CNV.hg19.bypos.111213.txt; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.dict; gs://firecloud-tcga-open-access/tutori",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:104,error,error,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,2,['error'],['error']
Availability,"- 0.25; - Local+docker backend (though I doubt it matters); - single workflow (though I doubt it matters). Workflow output is optional and sometimes both ``oncotate_m2_ob.oncotated_m2_vcf`` and ``oncotate_m2_no_ob.oncotated_m2_vcf`` are not populated. Proposed solution: select_first returns null if no inputs are populated. Offending workflow output:; ```; File? oncotated_m2_vcf = select_first([oncotate_m2_ob.oncotated_m2_vcf, oncotate_m2_no_ob.oncotated_m2_vcf]); ```. Error message:; ```; [ERROR] [02/17/2017 14:18:45.923] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow e241e2bc-95cd-4a8c-a814-20bb8852c6b5 failed (during ExecutingWorkflowState): select_first failed. All provided values were empty.; java.lang.IllegalArgumentException: select_first failed. All provided values were empty.; at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1$$anonfun$apply$10.apply(WdlStandardLibraryFunctions.scala:180); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1$$anonfun$apply$10.apply(WdlStandardLibraryFunctions.scala:180); at scala.Option.getOrElse(Option.scala:121); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1.apply(WdlStandardLibraryFunctions.scala:180); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1.apply(WdlStandardLibraryFunctions.scala:175); at scala.util.Success.flatMap(Try.scala:231); at wdl4s.expression.WdlStandardLibraryFunctions$class.select_first(WdlStandardLibraryFunctions.scala:175). ....snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2005:473,Error,Error,473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2005,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4744:218,failure,failures,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744,2,"['error', 'failure']","['error', 'failures']"
Availability,"- Added recovery functionality using KV service.; - In the next iteration will refactor to use a Doc store (Mongo, Couchbase) generic service implementation or continue using KV service but with a refactor in order to support not just SQL DBs as KV store but any other kind of DB. I think the best may be to work on a DAL or if it's not possible just modify the service to support other providers. Let me know what do you think on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1250:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250,1,['recover'],['recovery']
Availability,- Added the `referenceDiskManifestBuilderApp` test to GithubActions; - Implemented workaround for an issue related to Github Action Runners and their shell environments. See [WX-938](https://broadworkbench.atlassian.net/browse/WX-938?atlOrigin=eyJpIjoiZTU2YWJhMGVjZTM3NDg2NmFiZThkNmI2NGQwYzUwMTEiLCJwIjoiaiJ9) for a full run down. . [WX-938]: https://broadworkbench.atlassian.net/browse/WX-938?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7017:325,down,down,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7017,1,['down'],['down']
Availability,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4785:199,failure,failures,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785,1,['failure'],['failures']
Availability,- Adds a `JesError` class that maps some known JES errors to custom Exceptions to provide better error messages. Simplistic for now but avoid unnecessary stacktrace and give more explicit error messages.; - Tries to read the return code regardless of the final status of the JES job (even if it failed). If it can read it then the return code will be available in metadata.; - Sets the exec.sh content-type to `text/plain` in gcs so it opens in the browser instead of downloading a files.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1856:51,error,errors,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1856,5,"['avail', 'down', 'error']","['available', 'downloading', 'error', 'errors']"
Availability,"- Allow a `0` value for CWL `outDirMin` and `tmpDirMin` resource attributes; - Adds an optional section to the language factory to define a command to run after the user's action that will return output files that can only be known at runtime; - Only defined for CWL for now, which will remove unnecessary pull of jq for WDL tasks on PAPI2; - Docker image and command can both be changed in the configuration; - The PAPI2 logic that handles delocalization of those file strips away some redundant pieces in the delocalized paths to reduce the overall length of the path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4358:487,redundant,redundant,487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4358,1,['redundant'],['redundant']
Availability,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4021:126,alive,alive,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021,1,['alive'],['alive']
Availability,"- Backend: AWS; - Cromwell: 36. When running certain highly parallel WDL workflows, I'm getting the error `cromwell.core.CromwellFatalException: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cffe6e45-d66c-11e8-a1df-05402551b0ba)`. The specific case where this happens is in the `gatk3-data-processing` workflow, when running the `ApplyBQSR` task, which is run in parallel over some calculated intervals. The full error trace I get is:. ```; 2018-10-23 02:39:07,631 cromwell-system-akka.dispatchers.backend-dispatcher-53345 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(6d97fef4)GPPW.ApplyBQSR:15:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:100,error,error,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,- Closes #4158 ; - [x] @ruchim could you confirm that [this error message](https://github.com/broadinstitute/cromwell/pull/4174/files#diff-aade89887d9abbfe15dc3bf8b809aec5R31) meets your AC?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4174:60,error,error,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4174,1,['error'],['error']
Availability,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4962:86,redundant,redundant,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962,1,['redundant'],['redundant']
Availability,"- Disabled redundant `lots_of_inputs.test` test, which uses `lots_of_inputs.wdl` like `lots_of_inputs_papiv2.test` and makes analysis confusing; - Removed unused configs, mostly from PAPIv2 Alpha; - Removed unused suites, mostly from PAPIv2 Alpha; - Removed Travis, Jenkins, and CircleCI references from `test.inc.sh`. This includes `case` statements, as well as all functions that were called exclusively in the removed `case` statements.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7336:11,redundant,redundant,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7336,1,['redundant'],['redundant']
Availability,"- Fix broken calls to any workflow that is already in the workflow call tree (including itself).; - This would allow the equivalent of `do-while` loops via recursion and conditionals. E.g. we could iteratively improve a result until it is ""good enough""; - Here's an example that would allow us to work out square roots to a predefined accuracy:; ```; task refine {; 	Int number; 	Float currentApproximation. 	command {}. 	output {; 		Float nextApproximation = 0.5 * (currentApproximation + (1.0 / currentApproximation * number)); 	}; }. workflow sqrt {; 	Int number; 	Float currentApproximation. 	Float errorAllowed = 0.05 . 	if (currentApproximation * currentApproximation - number > errorAllowed) {; 		call refine { input: number = number, currentApproximation = currentApproximation }; 		call sqrt as recurse { input: number = number, currentApproximation = refine.nextApproximation }; 	}. 	output {; 		Float result = select_first( [ recurse.result, currentApproximation ] ); 	}; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1842:603,error,errorAllowed,603,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1842,2,['error'],['errorAllowed']
Availability,- Fixes #4081 ; - Fixes the draft-2 error and makes slight readability improvements to the `1.0` and later messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4175:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4175,1,['error'],['error']
Availability,"- JES Backend; - v29. See #2844 . This error causes cromwell to enter a funny state when the error described in #2844 occurs in a subworkflow. Cromwell states that the workflow (and subworkflow) are running, though the server log shows the exception. Not only that, the backend status is `Success` for the task that generated the invalid filename, though the task status remains `Running`, just like the workflows. Without looking at the server logs, a user cannot determine what is going on, easily.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845,2,['error'],['error']
Availability,"- JES backend; - 0.23; - single workflow. This workflow used to complete successfully (though cromwell did not exit), but with release 0.23, the workflow itself fails; Looks like cromwell can no longer handle spaces in the output file name. I believe that @kshakir had a similar issue in one of the develop builds. Did the fix make it into release 0.23? . ```; ...snip...; java.lang.RuntimeException: Task 5d13ddf0-dcf9-4b99-bd13-40b4321a954a:aggregate_results_html failed: error code 5. Message: 9: Failed to localize files: failed to copy; the following files: ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity; _series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-gatk-protect; ed/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small Amplificati; ons.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-r; un_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Amplifications.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0; -dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754:474,error,error,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754,1,['error'],['error']
Availability,"- JES backend; - 0.24; - single workflow mode. When JES returns a 403 AccessDeniedException, should cromwell keep retrying? It delays the result getting back to the user and should have no way of recovering with retries. Proposed solution: When AccessDeniedException is seen from JES, simply end there, instead of initiating any retries...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961:196,recover,recovering,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961,1,['recover'],['recovering']
Availability,"- JES backend; - cromwell server; - localhost mysql; - cromwell-27-c89c83f-SNAP.jar; - I set the database queue size to 3000.; - I have *not* changed the metadata batch size. *Should I attempt to restart this workflow?* This took over 4 hours to get this error message and I do not want to incur the cost if it will fail the same way again. Side issues:; - My workflow failed and yet cromwell is still *mauling* the mysql server.; - The call cache lookups are taking >1 hour per task. Main issue:. I do not understand the error messages, but my workflow has entered a Failed state and I am not sure why. First, I see a bunch of NPE:; ```; [ERROR] [05/01/2017 17:36:00.055] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:255,error,error,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"- JES/PAPI backend; - Cromwell v29. I have a task that dropped its outputs into ""./"", since output directory was a parameter and that is how it was set. So when cromwell/PAPI saw an output file `...././my_output.txt`, they created a bucket that actually contains the ""."" ... . The task was allowed to complete successfully and then downstream tasks failed. Proposed solutions:; - check the output location either when the first/upstream task completes (or, if possible, before it starts).; - Simple prune the ./ from the output path. ```; java.lang.IllegalArgumentException: I/O not allowed on dot-dirs or extra slashes when !permitEmptyPathComponents: /lichtens/cromwell-executions-test-dl-oxoq-full/CNVValidation/25df0c1d-5a13; -4bc1-8712-0cf90a78dfda/call-cnvPair/CNVSomaticPairWorkflow/6fe3dc67-c5f0-4c61-8881-474ceac4c8d7/call-ModelSegmentsTumor/./G25783.TCGA-55-6986-01A-11D-1945-08.2.modelFinal.seg; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2844:332,down,downstream,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2844,1,['down'],['downstream']
Availability,- Prefix default locations in CWL with `gs://...` root for PAPI conformance tests; - Retrieve size of files early to avoid unnecessary I/O (there's still too much redundant I/O but it's a step); - Uses `WomObject` to map back JS objects instead of `WomMap` that needs homogoneous value type (or it ends up being `WomAnyType`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3492:163,redundant,redundant,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3492,1,['redundant'],['redundant']
Availability,"- Refactored the DrsLocalizer to better handle multiple large downloads.; - Now, the DrsLocalizer will resolve all URLs up front, and then invoke the `getm` tool with a manifest containing all files to download. This improves download performance.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7214:62,down,downloads,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7214,3,['down'],"['download', 'downloads']"
Availability,- Remove the throttling of jobs during PapiV1 cron.; - Assemble jars if they haven't been already.; - Ensure an error is printed before exiting.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3681:112,error,error,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3681,1,['error'],['error']
Availability,"- Removes the ability to abort Finalization Actor; - Ensures that the finalization actor runs if the Workflow reaches the `Initialize` state, regardless of what happens next (failure, success, abort), or when it happens (initialization, execution).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/916:175,failure,failure,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/916,1,['failure'],['failure']
Availability,"- Removes the awkward plateauing of running jobs at 2k, 4k, 6k, etc when running several thousand jobs concurrently.; - Does not introduce very long delays into execution store processing like the previous attempt to ""fix"" the execution store.; - Allows us to get a more accurate count of total jobs queued in the system because they will express themselves as EJEAs waiting for tokens rather than pre-queue-queued items of which we have no visibility.; - Adds a dummy backend to test all of the above. Review Notes:; * Start with the `Remove redundant WaitingForQueueSpace status` commit. That's the one which fixes the bug. Everything else is just dummy backend and test infrastructure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6047:543,redundant,redundant,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047,1,['redundant'],['redundant']
Availability,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4132:272,down,down,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132,1,['down'],['down']
Availability,- Renames the existing `Parse` to `IOChecked` <- I changed the name because it is now used in places that have nothing to do with parsing but this is still not a great name so suggestions welcome; - Uses `IOChecked` in place of `ErrorOr` or `Checked` to keep the operation async as long as possible. It's not all the way async yet but at least pushes the sync call further up; - Provides a value for the `cats.Parallel` typeclass over `CheckedIO` using `CheckedIOPar`. This gives the ability to use `parTraverse` and `parSequence` on collections and have the IO execute in parallel over the elements of the collection.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4423:229,Error,ErrorOr,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4423,1,['Error'],['ErrorOr']
Availability,"- Retries the creation of the `Pipeline` and `Run`; - If all retries fail, a proper failure is propagated and the call will fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/227:84,failure,failure,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/227,1,['failure'],['failure']
Availability,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4123:420,down,down,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123,1,['down'],['down']
Availability,"- SGE backend (though I bet backend does not matter); - server mode; - cromwell 29. WDL takes in a list of filenames and scatters over a read_lines call. Each line is a file.; If the list file has DOS line endings, read_lines preserves the `\r` character in the file name. After running dos2unix, the issue disappeared. Here is the error message and you can even see the appended `\r`... ```; Could not localize /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r:\n\t/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r doesn't exists\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r""; ```. Hash error:; ```; ""Cannot hash file /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r because it can't be found"". ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632:332,error,error,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632,2,['error'],['error']
Availability,"- SGE backend; - 0.24; - single workflow; - call caching is on. Failure of intermediate task (CollectSequencingArtifactMetrics) on a single shard caused the entire workflow to stop immediately and cromwell to exit. Hence, successful tasks could not cache results, though those did complete. . The next tasks in the series (ExtractReadInfo) do not appear to ever be run, even for samples that did not fail CollectSequencingArtifactMetrics. ```; [ERROR] [01/23/2017 14:00:09.277] [cromwell-system-akka.dispatchers.engine-dispatcher-74] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 92c98fd6-003e-4f1b-b61b-9ab610f4961d failed (during ExecutingWorkflowState): Call dl_ob_training.dl_ob_training.CollectSequencingArtifactMetrics:NA:1: return code was 3; java.lang.RuntimeException: Call dl_ob_training.dl_ob_training.CollectSequencingArtifactMetrics:NA:1: return code was 3. ```. Command:; ```; java -Xmx6G -Dconfig.file=${PWD}/sge_application.conf -jar \; cromwell.jar \; run full_dl_ob_training.wdl \; full_dl_ob_training.json \; sge_runtimes \; full_dl_ob_training.metadata; ```. full_dl_ob_training.wdl:; ```; import ""dl_ob_training.wdl"" as dl_ob_training. workflow full_dl_ob_training {. ....snip.... scatter (p in variant_files_pair) {; call dl_ob_training.dl_ob_training {; input:; ....snip....; }; }; }. ```. dl_ob_training.wdl:; ```; workflow dl_ob_training {. ....snip.... call CollectSequencingArtifactMetrics {; input:; .....snip.....; }. call CreateObIntervalList {; input:; .....snip.....; }. call ExtractReadInfo {; input:; ....snip.....; }. output {; ExtractReadInfo.read_infos; }; }. task CollectSequencingArtifactMetrics {; ....snip....; output {; File pre_adapter_detail_metrics = ""${output_location_prepend}.pre_adapter_detail_metrics""; File pre_adapter_summary_metrics = ""${output_location_prepend}.pre_adapter_summary_metrics""; File bait_bias_detail_metrics = ""${output_location_prepend}.bait_bias_detail_metrics""; File ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1895:64,Failure,Failure,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895,2,"['ERROR', 'Failure']","['ERROR', 'Failure']"
Availability,"- SGE backend; - cromwell v29. The following WDL works:; ```; # Runtime parameters; Int? mem; String gatk_docker; Int? preemptible_attempts; Int? disk_space_gb. Int final_mem=select_first([mem, 3]); ... snip....; runtime {; memory: select_first([mem, 3]) + "" GB""; ....snip....; ```. BUT the below WDL gives me an error that the + operator is not supported for optional variables, please use select_first. However, the variable final_mem is not optional:. ```; ....; # Everything is the same as the working WDL, except:; runtime {; memory: final_mem + "" GB""; ....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2643:313,error,error,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2643,1,['error'],['error']
Availability,- Some IoCommands cannot be created so return a `Failure` when that happens; - FYI: in some cases throwing-and-re-catching the first `Failure`; - No longer passing `overwrite = true` since it was always true; - Resealed `GcsBatchIoCommand` by moving the test instance into main,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6019:49,Failure,Failure,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6019,2,['Failure'],['Failure']
Availability,- Starter-for-10 PR.; - Lets us start up Cromwell with a simple; ```; # cromwell.server start; ```; - Lets us stop Cromwell with a simple; ```; # cromwell.server stop; ```; - Lets us see whether Cromwell is already running with a simple; ```; # cromwell.server status; Cromwell is stopped. # echo $?; 1; ```; - My ultimate dream is to make this part of the [cromwell quickstart process](https://github.com/broadinstitute/cromwell/issues/2624) to get people up and ready for server mode in just a few simple commands.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2635:292,echo,echo,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2635,1,['echo'],['echo']
Availability,- The engine now retries both of the known Requester pays errors; - The backend (localization/delocalization logic) retries copy failures (no matter what the error) with a project flag.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3892:58,error,errors,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3892,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,- To the changelog:; - Added entry for already released 62; - Added placeholder for next release 63; - For homebrew:; - Remove extra slash added to generated URLs; - Changed default publishing instructions to include homebrew; - Added validation of brew style according to guidelines; - Fixed casing of 'cromwell' in PR name; - For the publishing GitHub token scopes:; - updated instructions; - updated validation; - gracefully error with helpful messages; - added an example image; - Removed attempt to publish from dbms tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6338:428,error,error,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6338,1,['error'],['error']
Availability,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4133:264,error,errors,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133,1,['error'],['errors']
Availability,- Wait longer for containers to download and startup; - Use the latest PostgreSQL jdbc driver; - Updated liquibase and schema comparison tests; - Empty LOBs are always stored as null; - Fixed test description for empty lobs; - Test the various databases with centaur local; - Don't start a db when CI checking publishing; - Help liquibase by using valid location of S3FSP; - Longer leeway for logging events for longer liquibase,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6345:32,down,download,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6345,1,['down'],['download']
Availability,"- We have a lot of actors.; - The actor hierarchy can be quite deep in places; - Each actor is given a name, which must make the overall path unique; - Unwieldy actor paths are making debugging awkward. The error lines are often hundreds of characters long!; - We should decide how to name actors, in a way that ensures they have workflow, call and attempt information in the name... but **at most** once.; - We should also retroactively update the actor naming so that all existing actors have appropriate paths.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1367:207,error,error,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1367,1,['error'],['error']
Availability,- Workaround Travis Docker issue; - Added heartbeats to docker tests; - DRYed conformance tests; - Fixed centaur tests where an empty CBCTAP early terminated getopts; - Introduced non-Travis-specific variable for GitHub PR branch names; - Switched from cd to pushd/popd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4930:42,heartbeat,heartbeats,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930,1,['heartbeat'],['heartbeats']
Availability,"- [ ] Usage message; - Update the usage message to better describe how to use the commands. - [ ] Parameters; - Switch from positional parameters to parameter arguments so that users explicitly include inputs and other parameters (workflow options, metadata, imports, lables, etc).; - Make all parameters optional, so if a user doesn't include the parameter argument, then no error.; - Get rid of functionality that Cromwell looks for files with specific extensions, like `.imputs`, `.options`, etc. To Be Continued.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1939:376,error,error,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1939,1,['error'],['error']
Availability,"- [ ] `missing_optional_output`:; The error message is now `""No input x.out_except_undeclared found evaluating inputs for expression x.out_except_undeclared""` which is significantly less friendly than the previous:; ```; out_except_undeclared is not declared as an output of the task x.; Make sure to declare it as an output to be able to use it in the workflow.; ```. - [x] `missing_input_failure`:; We used to get information saying which call, and which input, were given an invalid file. Now we just get `""Workflow Failed""` caused by: `""nonexistingbucket/path/doesnt/exist""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2871:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2871,1,['error'],['error']
Availability,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1682:777,avail,available,777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682,1,['avail'],['available']
Availability,- [x] Some of the error messages will change/improve following #3628 (or vice versa); - Red thumb required for the womtool changes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3641:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3641,1,['error'],['error']
Availability,"- branch 0.19_hotfix a6f7c00b71dd22485d5e95c9a30f3dedd2ddeaba; - running with default application.conf. If I abort a running job via POST to the API endpoint `worflows/v1/<uuid>/abort`, this appears in the server logs:. > 2016-05-23 10:21:55,192 cromwell-system-akka.actor.default-dispatcher-6 INFO - CallActor [UUID(87ebf02f):Godot]: Abort function called.; > 2016-05-23 10:21:55,201 cromwell-system-akka.actor.default-dispatcher-5 INFO - WorkflowActor [UUID(87ebf02f)]: Beginning transition from Running to Aborting.; > 2016-05-23 10:21:55,201 cromwell-system-akka.actor.default-dispatcher-5 INFO - WorkflowActor [UUID(87ebf02f)]: transitioning from Running to Aborting.; > 2016-05-23 10:22:00,175 cromwell-system-akka.actor.default-dispatcher-8 INFO - LocalBackend [UUID(87ebf02f):Godot]: Return code: 0; > 2016-05-23 10:22:00,313 cromwell-system-akka.actor.default-dispatcher-2 ERROR - WorkflowActor [UUID(87ebf02f)]: Completion work failed for call Godot.; > java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at com.zaxxer.hikari.proxy.PreparedStatementProxy.executeUpdate(PreparedStatementProxy.java:61) ~[cromwell-0.19.jar:0.19]; > at com.zaxxer.hikari.proxy.PreparedStatementJavassistProxy.executeUpdate(PreparedStatementJavassistProxy.java) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$anonfun$run$8$$anonfun$apply$1.apply(JdbcActionComponent.scala:520) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:882,ERROR,ERROR,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['ERROR'],['ERROR']
Availability,"- cromwell 0.25; - local backend + docker; - single workflow mode; - call caching disabled. Seems like the workflow completed just fine, but I still get an error. *This is transient* I have run the exact same workflow in exact same configuration multiple times and the error appears to happen ~50% of the time. ```; ....snip....; [2017-03-20 15:30:35,10] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""Mutect2_Multi.contamination_tables"": [""null"", ""null""],; ""Mutect2_Multi.filtered_vcf_files"": [""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-0/Mutect2/7b579210-dfee-4740-ab6e-c1f65bc64014/call-Filter/execution/synthetic.challenge.set1.tumor-vs-synthetic.challenge.set1.normal-filtered.vcf"", ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-1/Mutect2/56dd28f2-d4af-449d-961a-eface7c9a288/call-Filter/execution/background.synth.challenge2.snvs.svs.tumorbackground-vs-synthetic.challenge.set2.normal-filtered.vcf""],; ""Mutect2_Multi.filtered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-filteredOutputList/execution/filtered.list"",; ""Mutect2_Multi.unfiltered_vcf_files"": [""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-0/Mutect2/7b579210-dfee-4740-ab6e-c1f65bc64014/call-MergeVCFs/execution/synthetic.challenge.set1.tumor-vs-synthetic.challenge.set1.normal.vcf"", ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-1/Mutect2/56dd28f2-d4af-449d-961a-eface7c9a288/call-MergeVCFs/execution/background.synth.challenge2.snvs.svs.tumorbackground-vs-synthetic.challenge.set2.normal.vcf""],; ""Mutect2_Multi.ob_filtered_vcf_files"": [""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-987",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:156,error,error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,"- cromwell 26; - JES backend; - call caching on local mysql instance; - server mode. Ran a bunch of the initial jobs, but once it really started fan out (thousands of jobs), I got this error message. Trying to replicate now, but not sure if I can. Might be transient. . Regardless, error message is not particularly helpful. Any ideas? . ```; cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: 410 Gone; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. at cromwell.core.CromwellFatalException$.apply(core.scala:17); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:185,error,error,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,8,"['Error', 'error', 'recover']","['Error', 'error', 'errors', 'recoverWith']"
Availability,"- cromwell 26; - call caching is on (local mysql); - server mode, but only running one workflow; - lots of subworkflows; - JES backend. This workflow has > 20k tasks. Most of the questions are in the title. I ran a lot of tasks and the workflow eventually failed with the same error as reported in issue #2215 . Therefore, I am not sure whether this is a side effect of the failure. This could also just be an issue with the timing diagram. Regardless, see attached image. . Suggestion (which you probably thought, already): Do not investigate until after #2215 is remedied. . Once I run with a fix for #2215 , I'll check to see if this is still an issue. . Is there a parameter that would let me increase the dispatch rate? Would that alleviate this issue?. ![queuedincromwellissue](https://cloud.githubusercontent.com/assets/2152339/25530922/f9208b78-2bf5-11e7-9553-5a7f69a79dc4.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2216:277,error,error,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2216,2,"['error', 'failure']","['error', 'failure']"
Availability,"- cromwell 30; - JES backend. ```""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""key not found: <cnv_somatic_pair_workflow.wdl:63:64 identifier \""UHJlcHJvY2Vzc0ludGVydmFscw==\"">""; }; ],; ""message"": ""Workflow input processing failed""; }; ],```. That WDL file (subworkflow) is packaged correctly as a subworkflow and being submitted to cromwell. I'm pretty sure that this worked just fine in cromwell 28.2. I've had to do a bunch of workarounds for issues in cromwell 30, so confounds abound.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3039:34,failure,failures,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3039,1,['failure'],['failures']
Availability,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1449:214,failure,failure,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449,3,"['down', 'error', 'failure']","['download', 'error', 'failure']"
Availability,"- cromwell pre-0.21 dev snapshot; - JES backend; - command line execution (single workflow) . Current behavior, which happens frequently:; - Mysterious error 500 appears on cromwell stdout. Apologies that I do not have example. ; - cromwell hangs. One time, cromwell was left running overnight and no progress was made.; - ctl-c which ends cromwell; - up arrow and return; - job completes successfully. Can cromwell detect these errors on JES and retry the jobs?. More observations:; - These were never seen on local backend. On JES, these were common.; - All jobs were self-contained. I.e. did not hit a web service nor make a HTTP request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1450:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450,2,['error'],"['error', 'errors']"
Availability,"- cromwell v27; - SGE backend; - server mode. Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2464:200,reliab,reliable,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464,1,['reliab'],['reliable']
Availability,"- cromwell-23-79f6e12-SNAPSHOT.jar; - SGE backend; - Broad Internal filesystem location: ``/dsde/working/lichtens/test_pon_cromwell/cromwell-executions/pon_gatk_workflow/3c28c49b-c243-4371-b80b-d14fb5286c43/``; - no docker; - Being run on Broad VM. The first task takes in a bam file and creates an entity_id, which is passed into the second task. This works fine on local backend. Feel free to contact me if you need more information. Error message:. ```; [ERROR] [11/03/2016 10:37:24.334] [cromwell-system-akka.dispatchers.engine-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 3c28c49b-c243-4371-b80b-d14fb5286c43 failed (during ExecutingWorkflowState): wdl4s.util.AggregatedException: Input evaluation for Call pon_gatk_workflow.CalculateTargetCoverage failedFailed to find index Success(WdlInteger(1)) on array:. Success([""/seq/picard_aggregation/C1850/GTEX-1A3MW-0004/current/GTEX-1A3MW-0004.bam""]). 1. ```. Relevant WDL:. ```; ...snip... scatter (row in bam_file_names) {. call GetBamFileName {; input:; input_bam=row[0]; }. call CalculateTargetCoverage {; input:; entity_id=GetBamFileName.name,; padded_target_file=PadTargets.padded_target_file,; input_bam=row[0],; bam_idx=row[1],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; disable_all_read_filters=disable_all_read_filters,; keep_duplicate_reads=keep_duplicate_reads,; transform=transform,; grouping=grouping,; isWGS=isWGS,; mem=calculate_target_coverage_memory; }; ...snip... # Helper task to get the name of the given bam file; task GetBamFileName {; File input_bam. command <<<; echo $(basename ""${input_bam}"" .bam); >>>. output {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean kee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1646:436,Error,Error,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"- cromwell-25-c398490; - Local backend; - single workflow ; - using docker; - Google VM. This has happened multiple times. Three days apart. Reading google buckets from a local backend has worked fine in the past. The error states that a file cannot be found, yet the error messages look more like a HTTP 500. ``gsutil ls ...`` shows that the file is there. . Apologies if I am missing something obvious (this may just be a misleading error message)... ```; lichtens@lichtens-big:~/test_onco_m2$ gsutil ls gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; lichtens@lichtens-big:~/test_onco_m2$ gsutil ls gs://broad-dsde-methods/takuto/na12878-crsp-ice/; gs://broad-dsde-methods/takuto/na12878-crsp-ice/; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V3.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V3.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/na12878-replicate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:218,error,error,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,3,['error'],['error']
Availability,"- cromwell-27-aab4763-SNAP.jar; - JES backend ; - localhost MySQL with call caching enabled.; - server mode. This error is above my ability to debug. Cromwell failed after that. It did not exit, but was no longer responsive. ```[ERROR] [05/12/2017 21:47:03.777] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 6c383c35-d791-4971-aecd-0723726c8a7b failed (during ExecutingWorkflowState): JesAsyncBackendJobExecutionActor failed and didn't catch its exception.; java.lang.RuntimeException: JesAsyncBackendJobExecutionActor failed and didn't catch its exception.; at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:147); at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:144); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:114,error,error,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,3,"['ERROR', 'Fault', 'error']","['ERROR', 'FaultHandling', 'error']"
Availability,"- cromwell-27-c89c83f-SNAP; - server mode; - JES backend; - call caching on localhost mysql server. Is this a matter of hitting some sort of ceiling in number of concurrent jobs? Can I increase this?. ```; ....snip....; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@122f57e rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@35ca91f1[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 2293]""; }; ],; ""message"": ""JobStore write failure: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@122f57e rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@35ca91f1[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 2293]""; }; ],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219:221,failure,failures,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219,2,['failure'],"['failure', 'failures']"
Availability,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1465:141,error,error,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465,4,"['Error', 'error']","['Error', 'error']"
Availability,"- develop branch from 0.22; - local backend; - single workflow. I think one of the tasks failed, but got the message below. cromwell did not exit and I believe it should have. ```; [2016-10-24 14:44:19,47] [error] head of empty list; java.util.NoSuchElementException: head of empty list; at scala.collection.immutable.Nil$.head(List.scala:420); at scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1615:207,error,error,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615,1,['error'],['error']
Availability,"- develop; - server mode. ```; SELECT MAX(aggregated) as group_concat_max_len FROM; (; SELECT cche.CALL_CACHING_ENTRY_ID, SUM(LENGTH(cche.HASH_VALUE)) AS aggregated; FROM CALL_CACHING_HASH_ENTRY cche; GROUP BY cche.CALL_CACHING_ENTRY_ID; ) aggregation; ```. yields a value of 1440. Yet, even when I set ``group_concat_max_len`` to 30000, I get the error message that the migration cannot be completed successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2262:348,error,error,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2262,1,['error'],['error']
Availability,- more logging in EjeaMultipleCallCacheCopyAttemptsSpec; - more logging of the centaur config during app startup; - all tests now output the heartbeat after common setup; - shorter centaur max workflow length for non-cron jobs; - exit horicromtal test when any of the containers fail,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6202:141,heartbeat,heartbeat,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6202,1,['heartbeat'],['heartbeat']
Availability,"- our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:4663,failure,failure,4663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failure']
Availability,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748:505,error,error,505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748,1,['error'],['error']
Availability,"- v28; - JES backend; - continue while possible is true. I would expect the below workflow to have failed. However, cromwell listed many tasks as running that had failed (quickly) with an out-of-storage error. Metadata is attached. ; [lee_metadata_bd884082.json.txt](https://github.com/broadinstitute/cromwell/files/1205626/lee_metadata_bd884082.json.txt). Remaining running tasks look like this one (as near as I can tell):. ```; ....snip....; 2017/08/07 17:03:25 E: command failed: [Errno 28] No space left on device; [Errno 28] No space left on device; [Errno 28] No space left on device; [Errno 28] No space left on device; CommandException: Some components of /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/KIRP/DNA/WGS/BCM/ILLUMINA/TCGA-A4-A48D-10A-01D-A25F-10_wgs_Illumina.bam were not downloaded successfully. Please retry this download.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/08/07 17:03:27 W: cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/KIRP/DNA/WGS/BCM/ILLUMINA/TCGA-A4-A48D-10A-01D-A25F-10_wgs_Illumina.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/KIRP/DNA/WGS/BCM/ILLUMINA/TCGA-A4-A48D-10A-01D-A25F-10_wgs_Illumina.bam, command failed: [Errno 28] No space left on device; [Errno 28] No space left on device; [Errno 28] No space left on device; [Errno 28] No space left on device; CommandException: Some components of /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/KIRP/DNA/WGS/BCM/ILLUMINA/TCGA-A4-A48D-10A-01D-A25F-10_wgs_Illumina.bam were not downloaded successfully. Please retry this download.; CommandException: 1 file/object could not be transferred.; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2526:203,error,error,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2526,5,"['down', 'error']","['download', 'downloaded', 'error']"
Availability,"- v29; - Local backend. `java -jar cromwell.jar test.wdl test.json` is missing the `-i` flag and yet cromwell returns a zero error code. Since this is invalid, it should return non-zero.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3094:125,error,error,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3094,1,['error'],['error']
Availability,"--annotation-override "" else """"; String filter_funcotations_args = if defined(filter_funcotations) && (filter_funcotations) then "" --remove-filtered-variants "" else """"; String excluded_fields_args = if defined(funcotator_excluded_fields) then "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:36676,echo,echo,36676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['echo'],['echo']
Availability,"--mem-per-cpu=${requested_memory_mb_per_core} causes WomLong error on Cromwell 45.1, as reported here: . https://github.com/ENCODE-DCC/chip-seq-pipeline2/issues/59. Editting syntax to --mem-per-cpu ${requested_memory_mb_per_core} fixes this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5151:61,error,error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5151,1,['error'],['error']
Availability,"-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the asy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7122,down,down,7122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['down'],['down']
Availability,"-12-15 21:14:45,71] [info] dataFileCache open start; [2022-12-15 21:14:45,74] [info] dataFileCache open end; [2022-12-15 21:14:46,59] [info] checkpointClose start; [2022-12-15 21:14:46,59] [info] checkpointClose synched; [2022-12-15 21:14:46,71] [info] checkpointClose script done; [2022-12-15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4416,checkpoint,checkpointClose,4416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:5703,down,down,5703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['down'],['down']
Availability,"-306de3037265; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from - to Initializing; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from - to Initializing; [2019-01-10 17:36:24,60] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Initializing to Running; [2019-01-10 17:36:32,19] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Initializing to Running; [2019-01-10 18:21:56,23] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Running to Succeeded; [2019-01-10 18:23:09,43] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Running to Succeeded; [2019-01-10 18:23:10,45] [error] WorkflowManagerActor Workflow 00d0c2df-8f87-42af-9439-b45593930c84 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3://s3.amazonaws.com/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4537:3915,error,error,3915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537,1,['error'],['error']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz; 1608597138537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz; 1608597144746,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:55290,down,download,55290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz; 1608597486185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi; 1608597487788,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:151272,down,download,151272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi; 1608597306259,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:98900,down,download,98900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz; 1608597133659,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi; 1608597136464,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz; 1608597138537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:54042,down,download,54042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:133217,down,download,133217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi; 1608597261055,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:87057,down,download,87057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz; 1608597087902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi; 1608597089027,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi; 1608597091077,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:40947,down,download,40947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz; 1608596981096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi; 1608596983896,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:12267,down,download,12267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:169007,down,download,169007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz; 1608597529228,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi; 1608597531104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi; 1608597532744,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:163985,down,download,163985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz; 1608597132184,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:52175,down,download,52175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz; 1608597612565,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:185187,down,download,185187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:77091,down,download,77091,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz; 1608597196671,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:70845,down,download,70845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz; 1608597234131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi; 1608597236161,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz; 1608597239225,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:80827,down,download,80827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz; 1608597116706,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi; 1608597118429,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:49046,down,download,49046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz; 1608597404588,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi; 1608597406434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:126969,down,download,126969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:6644,down,download,6644,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:29094,down,download,29094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi; 1608597072537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:36578,down,download,36578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz; 1608597466639,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi; 1608597468522,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:145066,down,download,145066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi; 1608597402775,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz; 1608597404588,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:125721,down,download,125721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz; 1608597083236,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi; 1608597085601,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz; 1608597087902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:39699,down,download,39699,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz; 1608597249388,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:85188,down,download,85188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz; 1608596956029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi; 1608596958265,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:4773,down,download,4773,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz; 1608597345651,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi; 1608597348432,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz; 1608597351053,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:110739,down,download,110739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:141326,down,download,141326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Ret,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3185,recover,recoverAsync,3185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recoverAsync']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:29723,down,download,29723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi; 1608597533973,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi; 1608597536250,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi; 1608597538865,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:165870,down,download,165870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:19130,down,download,19130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi; 1608597487788,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:151901,down,download,151901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:77720,down,download,77720,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi; 1608597540849,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi; 1608597542416,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:167755,down,download,167755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,-dc48-4409-bbd1-6f1411211f42 failed (during ExecutingWorkflowState): Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1$$anon$1: Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:200); 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:199); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.backend.sfs.SharedFileSystem$class.localizeInputs(SharedFileSystem.scala:199); 	at cromwell.backend.sfs.SharedFileSystemJobCachingActorHelper$$anon$1.localizeInputs(SharedFileSystemJobCachingActorHelper.scala:40); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(Shar,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:1477,Failure,Failures,1477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['Failure'],['Failures']
Availability,"-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:2185,recover,recoverWith,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['recover'],['recoverWith']
Availability,-dsde-methods/takuto/na12878-crsp-ice/SM-612V3.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/na12878-replicate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:106); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:105); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at scala.util.Failure.recoverWith(Try.scala:203); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:198); at cromwell.backend.impl.sfs.confi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:1835,Error,Error,1835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,4,['Error'],['Error']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz; 1608597374474,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz; 1608597376806,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:117602,down,download,117602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi; 1608596983896,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:12886,down,download,12886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi; 1608596958265,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:5392,down,download,5392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz; 1608596971072,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:8519,down,download,8519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi; 1608597093130,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz; 1608597095782,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz; 1608597098791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:42822,down,download,42822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz; 1608597132184,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz; 1608597133659,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:52794,down,download,52794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi; 1608597236161,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz; 1608597239225,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz; 1608597241175,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:81446,down,download,81446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi; 1608597263910,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz; 1608597266985,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz; 1608597268460,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:88932,down,download,88932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz; 1608597560491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz; 1608597562850,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:172753,down,download,172753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:140697,down,download,140697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi; 1608597216321,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:76462,down,download,76462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz; 1608597523198,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz; 1608597524955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:161487,down,download,161487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz; 1608597286657,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:93918,down,download,93918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597062290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz; 1608597066051,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:33469,down,download,33469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi; 1608597402775,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:125092,down,download,125092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi; 1608597105908,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz; 1608597108506,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz; 1608597111201,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:45927,down,download,45927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:28465,down,download,28465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz; 1608597573618,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz; 1608597577251,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:176483,down,download,176483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:98271,down,download,98271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-queries-per-100-seconds = 1000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Number of workers to assign to PAPI requests; request-workers = 3. genomics {; # A reference to an auth defined in the `google` stanza at the top.; # This auth is used to create pipelines and manipulate auth JSONs.; auth = ""application-default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-west1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for the requests; project = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:3126,error,error,3126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,2,"['error', 'failure']","['error', 'failure']"
Availability,"-r--r-- 2 root root 13K Jan 14 20:31 genomicsdb-6.vcf.gz.tbi. glob-b34dfc006a981a93d6da067cf50036fe:; total 512; -rw-r--r-- 1 root root 277 Jan 14 20:32 cromwell_glob_control_file. glob-ce2a0ab5d8c37a6d061c814f835853ee:; total 3.6M; -rw-r--r-- 1 root root 277 Jan 14 20:32 cromwell_glob_control_file; -rw-r--r-- 3 root root 5.7K Jan 14 20:17 genomicsdb-0.vcf.gz; -rw-r--r-- 3 root root 927K Jan 14 20:32 genomicsdb-1.vcf.gz; -rw-r--r-- 3 root root 554K Jan 14 20:31 genomicsdb-2.vcf.gz; -rw-r--r-- 3 root root 813K Jan 14 20:30 genomicsdb-3.vcf.gz; -rw-r--r-- 3 root root 620K Jan 14 20:32 genomicsdb-4.vcf.gz; -rw-r--r-- 3 root root 50K Jan 14 20:17 genomicsdb-5.vcf.gz; -rw-r--r-- 3 root root 673K Jan 14 20:31 genomicsdb-6.vcf.gz; ```; As you can see, here `vcf.gz` and `vcf.gz.tbi` are stored under different directories.; However, the next `picard sort` step will be only looking at the directory where all `vcf.gz` live, which leads to the error:; ```; Could not localize /mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-genomel_pdc_gatk4_cohort_genotyping/execution/glob-ce2a0ab5d8c37a6d061c814f835853ee/genomicsdb-0.vcf.gz.tbi -> /mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-picard_sortvcf/inputs/2004815296/genomicsdb-0.vcf.gz.tbi:',; u""\t/mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-genomel_pdc_gatk4_cohort_genotyping/execution/glob-ce2a0ab5d8c37a6d061c814f835853ee/gen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4546:4497,error,error,4497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4546,1,['error'],['error']
Availability,"-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor Successfully started WorkflowActor-cfc7b055-b8a3-40c4-a0eb-b4636d5c7286; 2018-08-02 02:23:03,861 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-08-02 02:23:03,863 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - MaterializeWorkflowDescriptorActor [UUID(cfc7b055)]: Parsing workflow as CWL v1.0; 2018-08-02 02:23:03,869 INFO - Pre-Processing /tmp/cfc7b055-b8a3-40c4-a0eb-b4636d5c7286.temp.1620266732058368635/cfc7b055-b8a3-40c4-a0eb-b4636d5c7286.cwl; 2018-08-02 02:23:05,504 INFO - Pre-Processing /modules/file-parsers/parsetxtxy.cwl; 2018-08-02 02:23:07,089 INFO - Pre-Processing /modules/processing-modules/13C-NMR.cwl; 2018-08-02 02:23:08,703 INFO - Pre-Processing /modules/core-modules/uploadsamples.cwl; 2018-08-02 02:23:10,570 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow cfc7b055-b8a3-40c4-a0eb-b4636d5c7286 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Input frequency is required and is not bound to any value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:170); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3958:2202,ERROR,ERROR,2202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3958,1,['ERROR'],['ERROR']
Availability,"-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:3675,ERROR,ERROR,3675,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,". # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The defau",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:12899,avail,available,12899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['avail'],['available']
Availability,". engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://mybucket/cromwell-execution""; 	auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<account-id>:job-queue/MyHighPriorityQue-ae4256f76f07d96"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }. call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users.; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This is used to blacklist cache hit paths based on the; # prefixes of cache hit paths that Cromwell previously failed to copy for authorization reasons.; enabled: false; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 1000; }; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://cromwell-db-rdscluster.cluster.us-east-1.rds.amazonaws.com/cromwell""; user = ""myuser""; password = ""my password""; connectionTimeout = 5000; }; }. my hello.wdl is:; task hello {; String name; command {; echo 'Hello ${name}!' > ""hello${name}.txt""; }; output {; File response = ""hello${name}.txt""; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow test {; call hello; }. My hello_inputs.json is:; {; ""test.hello.name"": ""World"",; }. I ran java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl several times, each time there is a new job submitted to aws batch. Should it submit aws batch only once when first time ran it? . Your assistance will be highly appreciated. Thanks; Jing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412:1767,echo,echo,1767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412,1,['echo'],['echo']
Availability,"...because we're seeing failures with gsutil with the newest image. Need to fix that for sure, but this is more like a temporary patch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2370:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2370,1,['failure'],['failures']
Availability,...until such time as 100KB can be read from OSS in under 60 seconds. Multiple PRs are backed up behind this chronic test failure.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5102:122,failure,failure,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5102,1,['failure'],['failure']
Availability,".5) :. 1. Download and unzip [cromwell_24_bug.zip](https://github.com/broadinstitute/cromwell/files/745711/cromwell_24_bug.zip). 2. Get v0.21 of cromwell, to verify that the existing WDL runs; ```bash; $ cd cromwell_24_bug; $ curl -L -o cromwell-21.jar https://github.com/broadinstitute/cromwell/releases/download/0.21/cromwell-0.21.jar; $ java -jar cromwell-21.jar run tool_icomut.wdl inputs.json; ```; The workflow should succeed.; ```; ...; [2017-02-01 13:18:32,36] [info] WorkflowManagerActor WorkflowActor-5e2fd288-37a5-44d2-ab8a-a65b2fb5179d is in a terminal state: WorkflowSucceededState; {; ""outputs"": {; ""tool_icomut_workflow.tool_icomut.iCoMut_table"": ""/Users/timdef/tmp/cromwell_24_bug/cromwell-executions/tool_icomut_workflow/5e2fd288-37a5-44d2-ab8a-a65b2fb5179d/call-tool_icomut/execution/TCGA-ACC.coMut_table.txt""; },; ""id"": ""5e2fd288-37a5-44d2-ab8a-a65b2fb5179d""; }; [2017-02-01 13:18:34,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; ```. 3. Now download v0.24, and retry:; ```bash; $ curl -L -o cromwell-24.jar https://github.com/broadinstitute/cromwell/releases/download/24/cromwell-24.jar; $ java -jar cromwell-24.jar run tool_icomut.wdl inputs.json; ```. The workflow now fails:; ```; ...; [2017-02-01 13:08:17,13] [error] BackgroundConfigAsyncJobExecutionActor [c290b1fftool_icomut_workflow.tool_icomut:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlFileType,None); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937:1521,down,download,1521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937,3,"['Error', 'down', 'error']","['Error', 'download', 'error']"
Availability,".<snip>......... scatter (p in tumor_normal_pairs) {; # This call works fine; call m2.Mutect2 as m2_tn {; input:; gatk4_jar = ""/root/gatk-protected.jar"",; intervals = mutectIntervals,; ref_fasta = ref_fasta,; ref_fasta_index = ref_fai,; ref_dict = ref_dict,; tumor_bam = p.left.left,; tumor_bam_index = p.left.right,; tumor_sample_name = sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; normal_bam = p.right.left,; normal_bam_index = p.right.right,; normal_sample_name = sub(sub(p.right.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; scatter_count = scatter_count,; dbsnp = dbSNPVCF,; dbsnp_index = dbsnp_index,; cosmic = cosmicVCF,; cosmic_index = cosmic_index,; is_run_orientation_bias_filter = true,; is_run_oncotator = false,; oncotator_docker = ""broadinstitute/oncotator:1.9.2.0-eval-gatk-protected"",; m2_docker = ""broadinstitute/gatk-protected@sha256:08bf9835dabb5b694164dae8312bac8d8012b9d907341f30d3c8e262a0f121d6"",; preemptible_attempts = preemptible,; onco_ds_local_db_dir = ""/root/onco_dbdir/"",; artifact_modes = [""G/T"", ""C/T""],; picard_jar = picard_jar; }; # New WDL added here that calls a task, not a workflow; # NOTE: Even when I put the VcfToIntervals task inline in this WDL file, I get the same exact error.; call dl_ob_training_m2.VcfToIntervals as vcf2i {; input:; vcf = m2_tn.filtered_vcf,; entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"); }; ### End new WDL; }. output {; Array[File] unfiltered_vcf = m2_tn.unfiltered_vcf; Array[File] unfiltered_vcf_index = m2_tn.unfiltered_vcf_index; Array[File] filtered_vcf = m2_tn.filtered_vcf; Array[File] filtered_vcf_index = m2_tn.filtered_vcf_index; }; }; ```. Here is the error message I get using wdltool 0.8 and 0.12 (and cromwell):. ```; ERROR: Expression will not evaluate (line 82, col 42):. entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"). ```. I tried a few things:; - ``entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """")`` --> ``entity_id=p.left.left`` gives same error; - ``entity_id=sub(sub",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2334:2383,error,error,2383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2334,1,['error'],['error']
Availability,".ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-04-11 22:41:00,528 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,543 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,556 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,617 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,841 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,912 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2676] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]. **java.lang.OutOfMemoryError: Java heap space**; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.lang.StringBuilder.toString(StringBuilder.java:407); at scala.StringContext.standardInterpolator(StringContext.scala:128); at scala.StringContext.s(StringContext.scala:95); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:885); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:881); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:9816,down,down,9816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['down'],['down']
Availability,".GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedFileSystemAsyncJobExecutionActor.scala:147); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:201); 	 at wom.values.WomSingl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1737,error,errors,1737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['errors']
Availability,".JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:39); 	at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:36); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; 	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3014); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472); 	... 16 common frames omitted. Â  | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42973] ERROR c.s.m.impl.MetadataServiceActor - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.ex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:5891,Error,Error,5891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,1,['Error'],['Error']
Availability,.JesAsyncBackendJobExecutionActor.jobPaths(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardCachingActorHelper$class.startMetadataKeyValues(StandardCachingActorHelper.scala:76); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(JesAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.startMetadataKeyValues(JesAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:516); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.executeOrRecover(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:54); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:54); at cromwell.core.retry.Retry$.withRetry(Retry.scala:36); at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:50); at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:54); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:77); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunct,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:4311,robust,robustExecuteOrRecover,4311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['robust'],['robustExecuteOrRecover']
Availability,".aggregate_data.input_array\"":[\""bar, baz\""]}"",; ""workflow"": ""task aggregate_data {\n\tArray[File] input_array\n\n\tcommand {\n echo \""foo\""\n\n\t}\n\n\toutput {\n\t\tArray[Array[File]] output_array = [input_array]\n\t}\n\n\truntime {\n\t\tdocker : \""broadgdac/aggregate_data:31\""\n\t}\n\n\tmeta {\n\t\tauthor : \""Tim DeFreitas\""\n\t\temail : \""timdef@broadinstitute.org\""\n\t}\n\n}\n\nworkflow aggregate_data_workflow {\n\tcall aggregate_data\n}""; },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stdout.log"",; ""shardIndex"": -1,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadgdac/aggregate_data:31"",; ""cpu"": ""1"",; ""zones"": ""us-central1-b"",; ""memory"": ""2GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_array"": [""bar, baz""]; },; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ""backend"": ""JES"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""stderr"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:7062,failure,failures,7062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['failure'],['failures']
Availability,".apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:5919,failure,failure,5919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['failure'],['failure']
Availability,".backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --exclusive --timeout 900 $LOCK_FILE \; apptainer exec --containall /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM. 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap \; ""module load apptainer; apptainer exec \; --containall \; --bind /mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references:/mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references \; --bind ${cwd}:${docker_cwd} \; --bind /tmp:/tmp \; /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; ${job_shell} \; ${docker_script}""'; """""". kill = ""'scancel ${job_id}'"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:2130,echo,echo,2130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['echo'],['echo']
Availability,".com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1158,error,error,1158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability,.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute v,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:4948,Fault,FaultHandling,4948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Fault'],['FaultHandling']
Availability,.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6223,recover,recover,6223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5649,error,error,5649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['error'],['error']
Availability,".forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:10129,error,errors,10129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,".gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true',",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:83447,down,down,83447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['down'],['down']
Availability,".html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:19325,echo,echo,19325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,".jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.nativeUpsert(JdbcActionComponent.scala:561); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.f$1(JdbcActionComponent.scala:544); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.run(JdbcActionComponent.scala:557); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); 2017-07-13 22:14:36,622 cromwell-system-akka.actor.default-dispatcher-552 ERROR - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Deadlock found when trying to get lock; try restarting transaction; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:951); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:24",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:9811,ERROR,ERROR,9811,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,".phenotype:-1:1 cache hit copying success with aggregated hashes: initial = 018D1BC619E22671C2125EEDE82AB210, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,36] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.phenotype:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,37] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.date_of_death:-1:1-20000000026 [9e4f5894main.date_of_death:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,37] [info] BT-322 9e4f5894:main.date_of_death:-1:1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A657136E0208581A7B6A7F020F, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,68] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.white_brits:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.year_of_birth' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.phenotype' (scatter inde",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:26523,failure,failures,26523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:4540,alive,alive,4540,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,2,"['alive', 'fault']","['alive', 'fault']"
Availability,".runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:17036,error,error,17036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:7084,down,down,7084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,2,['down'],['down']
Availability,".stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedFileSystemAsyncJobExecutionActor.scala:147); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:201); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:182); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.mapCommandLineWomFile(SharedFileSystemAsyncJobExecutionActor.scala:145); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.mapCommandLineWomFile$(SharedFil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1970,Error,Error,1970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['Error'],['Error']
Availability,.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:5247,Error,ErrorOr,5247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:2896,Error,ErrorOr,2896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_â€¦ ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_â€¦ . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostExcepti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:2582,Down,Downloading,2582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['Down'],['Downloading']
Availability,"/257) and associated cromwell PRs. But if the uninitialized optional declaration on [this line](https://github.com/broadinstitute/centaur/pull/242/files#diff-cc04c14d68a6a1a6d8d8366fc0c2f88cR48) is uncommented, the workflow fails. (Deliberately not quoting since lines don't wrap and anyway the thumbs downs are apropos). 2017-11-14 18:00:05,062 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2017-11-14 18:00:05,093 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - MaterializeWorkflowDescriptorActor [UUID(4b725606)]: Call-to-Backend assignments: decls.sub_decls.second_task -> Local, decls.sub_decls.first_task -> Local; 2017-11-14 18:00:06,129 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowExecutionActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd [UUID(4b725606)]: Starting calls: SubWorkflow-sudecls:-1:1; 2017-11-14 18:00:06,130 cromwell-system-akka.actor.default-dispatcher-50 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreRegisterSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#-388497585] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd/WorkflowExecutionActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd/SubWorkflowExecutionActor-SubWorkflow-sudecls:-1:1#-1890869436] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; 2017-11-14 18:00:06,132 cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - WorkflowManagerActor Workflow 4b725606-6d2a-4cf2-b23b-e5971f52b7dd failed (during ExecutingWorkflowState): ; 2017-11-14 18:00:06,133 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor WorkflowActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd is in a terminal state: WorkflowFailedState",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2902:2288,ERROR,ERROR,2288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2902,1,['ERROR'],['ERROR']
Availability,"/` in strings inconsistently. In some cases, it is dropped without throwing an error, in other cases it will cause an error immediately. If the string is in the WDL file itself, womtool does not detect any issues with it but it will not be handled as expected as runtime. ## use case and how to reproduce; [goleft indexcov ](https://github.com/brentp/goleft/tree/master/indexcov#indexcov) defaults to this value for --excludePattern:; `""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""`. So I set `String excludePattern = ""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""` in my WDL. That passes miniwdl check and womtool. But... * Terra will accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$ `as a variable default or as hardcoded variable, but will handle it incorrectly -- it will not error, but it will be changed into `^chrEBV$|^NC|_random$|Un_|^HLA-|_alt$|hapd$`; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable via JSON; it will fail to import; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable if entered manually; it will throw token recognition error in the workflow menu and not allow you to submit; * Terra will accept the escaped version `^chrEBV$|^NC|_random$|Un_|^HLA\\-|_alt$|hap\\d$` as an input if entered manually or hardcoded, and will interpret it as `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$`. Only tested via Terra-Cromwell, as I was previously told local-Cromwell is a lower development priority. ## expected behavior; 1. A user inputting a string as a variable vs that exact same string being a hardcoded default should be handled the same way.; 2. If Cromwell is supposed to handle `/` by requiring they be escaped as `//`, that should be documented if it isn't already.; 3. womtool should throw a warning when it sees a hardcoded variable/default with a `/` inside of it, and that warning should guide the user as to how it will be interpreted at runtime.; 4. The same workflow running in Cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7167:886,error,error,886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7167,2,['error'],['error']
Availability,"/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:21335,recover,recoverWith,21335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['recover'],['recoverWith']
Availability,"/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator not found or versions do not match (Service: null; Status Code: 404; Request ID: 9914238b-00c2-11e9-a13d-cdc28a8016c8); ```. Looking at cloudtrail, here is the event associated with that request ID:. [Event](https://gist.github.com/dtenenba/909f16e720a01b00a736cf6e60f7083a). If I pull out just the contents of the `requestParameters` section and call RegisterJobDefinition using the AWS CLI as follows, it works fine. ```; aws batch register-job-definition --cli-input-json file://event_history.json; {; ""jobDefinitionArn"": ""arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator:207"",; ""jobDefinitionName"": ""PreProcessingForVariantDiscovery_GATK4-B",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1876,Error,Error,1876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"/codes.html#StaticLoggerBinder for further details."",; ""endTime"": ""2018-08-14T16:16:33.718991Z""; },; {; ""startTime"": ""2018-08-14T16:13:13.678Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-08-14T16:13:14.570Z""; },; {; ""startTime"": ""2018-08-14T16:14:33.113135Z"",; ""description"": ""Started running \""\/bin\/sh -c while true; do retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry 2> \/dev\/null || true; sleep 60; done\"""",; ""endTime"": ""2018-08-14T16:14:33.759759Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.755Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-08-14T16:13:25.071851Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.571Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2018-08-14T16:13:14.755Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.570Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-08-14T16:13:14.571Z""; },; {; ""startTime"": ""2018-08-14T16:16:37.479167Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:8683,echo,echo,8683,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"/cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) example is out of date. In `google.conf` it still lists the configuration for ""JES"" backend. 2) In the same tutorial ([Setting up PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2)), the instructions for which roles to assign to the GCP service account are outdated. 3) Once the user puzzles together which parts to replace, the execution is still failing (for me at least).; I run the following command ` java -Dconfig.file=cromwell.BROADexamples.v4.conf -jar cromwell-66.jar run hello.wdl -i hello.inputs`, which results in the following `Request contains an invalid argument.` error (abbreviated to the relevant section):; ```; [2021-08-13 10:44:39,31] [info] Running with database db.url = jdbc:hsqldb:mem: ...; ...; [2021-08-13 10:44:54,04] [info] Reference disks feature for PAPIv2 backend is not configured.; [2021-08-13 10:44:54,46] [info] Slf4jLogger started; [2021-08-13 10:44:54,73] [info] Workflow heartbeat configuration:; ...; [2021-08-13 10:44:55,42] [info] Running with 3 PAPI request workers; ...; [2021-08-13 10:44:55,79] [info] Unspecified type (Unspecified version) workflow a15c46b7-5f93-46d6-94a2-28f656914866 submitted; ...; [2021-08-13 10:44:56,46] [info] Request manager PAPIQueryManager created new PAPI request worker PAPIQueryWorker-58e6b395-916e-4ba4-965a-0ec8f1c0760d with batch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job ex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:1497,heartbeat,heartbeat,1497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['heartbeat'],['heartbeat']
Availability,"/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:1274,recover,recoverWith,1274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['recover'],['recoverWith']
Availability,"/home/jeremiah/gdc-dnaseq-cwl/tools/samtools_idxstats_to_sqlite.cwl; [2018-09-14 13:21:50,17] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats.cwl; [2018-09-14 13:21:50,38] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:19155,heartbeat,heartbeat,19155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,/na12878-replicate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:106); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:105); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at scala.util.Failure.recoverWith(Try.scala:203); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:198); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.standard.StandardAsyncExecutionActor$class.commandScriptContents(StandardAsyncExecutionActor.scala:170); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:2317,Failure,Failure,2317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,1,['Failure'],['Failure']
Availability,"/opt/execution; ln -sT `pwd`/../inputs /opt/inputs. /opt/src/algutil/monitor_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${bo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:3154,echo,echo,3154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,1,['echo'],['echo']
Availability,"/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0mtwo errors found[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0mtwo errors found[0m; [0m[[31merror[0m] [0m(cwl/compile:[31mdoc[0m) Scaladoc generation failed[0m; [0m[[31merror[0m] [0m(cwl/compile:[31mcompileIncremental[0m) Compilation failed[0m; [0m[[31merror[0m] [0mTotal time: 273 s, completed Oct 25, 2017 1:03:49 PM[0m. restoring stty: 500:5:bf:8a3b:3:1c:7f:15:4:0:1:0:11:13:1a:0:12:f:17:16:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0. travis_time:end:1f45f95e:start=1508935437087656153,finish=1508936629752201761,duration=1192664545608; [0Ktravis_fold:end:after_success; [0K[33;1mSkipping a deployment with the script provider because this is not a tagged commit[0m. Done. Your build exited with 0.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2788:2057,error,errors,2057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2788,1,['error'],['errors']
Availability,"/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When trying to configure metadata-archive in cromwell server by adding the configuration below:; ```; archive-metadata {; # A filesystem able to access the specified bucket:; filesystems {; gcs {; # A reference to the auth to use for storing and retrieving metadata:; auth = ""user-service-account""; }; }. # Which bucket to use for storing the archived metadata; bucket = ""{{ backend_bucket }}""; }; ```. when the user-service-account auth is declared up in the configuration :; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```; We got the following error in Cromwell server initialization :; cromwell_1 | [ERROR] [06/21/2023 11:55:25.094] [cromwell-system-akka.actor.default-dispatcher-30] [akka://cromwell-system/user] Failed to parse the archive-metadata config:; cromwell_1 | Failed to construct archiver path builders from factories (reason 1 of 1): Missing parameters in workflow options: user_service_account_json; cromwell_1 | akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService: exception during creation; cromwell_1 | 	at akka.actor.ActorInitializationException$.apply(Actor.scala:202); cromwell_1 | 	at akka.actor.ActorCell.create(ActorCell.scala:698); cromwell_1 | 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:549); cromwell_1 | 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); cromwell_1 | 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); cromwell_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7171:1312,error,error,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"/www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:968,heartbeat,heartbeat,968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,4,"['error', 'failure', 'heartbeat']","['error', 'failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,"0 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,81] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,84] [info] Database closed; [2019-02-11 10:13:36,84] [info] Stream materializer shut down; [2019-02-11 10:13:36,84] [info] WDL HTTP import resolver closed. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:17396,down,down,17396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,3,['down'],['down']
Availability,"0.0-alpha1.2.4.jar.; 2017/03/20 15:37:09 I: Copying gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar to /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; 2017/03/20 15:37:09 I: Running command: sudo gsutil -q -m cp gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; ```. And the `exec.sh` that it generates is:; ```bash; #!/bin/bash; export _JAVA_OPTIONS=-Djava.io.tmpdir=/cromwell_root/tmp; export TMPDIR=/cromwell_root/tmp. (; cd /cromwell_root; if [ false = false ]; \; then java -Xmx1g -jar gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar PadTargets --targets /cromwell_root/broad-dsde-methods/th/target/ice_targets.tsv --output targets.padded.tsv \; --padding 250 --help false --version false --verbosity INFO --QUIET false; \; else touch targets.padded.tsv; \; fi; ); echo $? > /cromwell_root/PadTargets-rc.txt.tmp; (; cd /cromwell_root. ); mv /cromwell_root/PadTargets-rc.txt.tmp /cromwell_root/PadTargets-rc.txt; ```. The WDL that has this issue is:; ```; workflow BrokenFilePath {; File targets = ""gs://broad-dsde-methods/th/target/ice_targets.tsv""; File GATK_protected_jar = ""gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar""; Boolean isWGS = false; Int padding = 250. call PadTargets {; input:; target_file=targets,; gatk_jar=GATK_protected_jar,; isWGS=isWGS,; mem=1,; padding=padding; }; }. task PadTargets {; File target_file; Int padding; File gatk_jar; Boolean isWGS; Int mem. command {; if [ ${isWGS} = false ]; \; then java -Xmx${mem}g -jar ${gatk_jar} PadTargets --targets ${target_file} --output targets.padded.tsv \; --padding ${padding} --help false --version false --verbosity INFO --QUIET false; \; else touch targets.padded.tsv; \; fi; }. output {; File padded_target_file = ""targets.padded.tsv""; }. runtime {; docker: ""broadinstitute/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078:1399,echo,echo,1399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078,1,['echo'],['echo']
Availability,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1103:61,down,download,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103,2,['down'],['download']
Availability,"0.5) :. 1. Download and unzip [cromwell_24_bug.zip](https://github.com/broadinstitute/cromwell/files/745711/cromwell_24_bug.zip). 2. Get v0.21 of cromwell, to verify that the existing WDL runs; ```bash; $ cd cromwell_24_bug; $ curl -L -o cromwell-21.jar https://github.com/broadinstitute/cromwell/releases/download/0.21/cromwell-0.21.jar; $ java -jar cromwell-21.jar run tool_icomut.wdl inputs.json; ```; The workflow should succeed.; ```; ...; [2017-02-01 13:18:32,36] [info] WorkflowManagerActor WorkflowActor-5e2fd288-37a5-44d2-ab8a-a65b2fb5179d is in a terminal state: WorkflowSucceededState; {; ""outputs"": {; ""tool_icomut_workflow.tool_icomut.iCoMut_table"": ""/Users/timdef/tmp/cromwell_24_bug/cromwell-executions/tool_icomut_workflow/5e2fd288-37a5-44d2-ab8a-a65b2fb5179d/call-tool_icomut/execution/TCGA-ACC.coMut_table.txt""; },; ""id"": ""5e2fd288-37a5-44d2-ab8a-a65b2fb5179d""; }; [2017-02-01 13:18:34,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; ```. 3. Now download v0.24, and retry:; ```bash; $ curl -L -o cromwell-24.jar https://github.com/broadinstitute/cromwell/releases/download/24/cromwell-24.jar; $ java -jar cromwell-24.jar run tool_icomut.wdl inputs.json; ```. The workflow now fails:; ```; ...; [2017-02-01 13:08:17,13] [error] BackgroundConfigAsyncJobExecutionActor [c290b1fftool_icomut_workflow.tool_icomut:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlFileType,None); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937:1403,down,download,1403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937,1,['down'],['download']
Availability,"0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; 2018-06-13 14:29:48,873 INFO - reconfiguredScript: #!/bin/bash; mkdir -p /cromwell_root; #!/bin/bash. cd /cromwell_root; tmpDir=`mkdir -p ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1"" && echo ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:15182,echo,echo,15182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,"019-05-22 19:19:26,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: job id: 7c2d29c2-f04e-4b3f-8579-915a6fbc9033; [2019-05-22 19:19:26,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:10398,error,error,10398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['error'],['error']
Availability,"022-12-15 21:14:45,74] [info] dataFileCache open end; [2022-12-15 21:14:46,59] [info] checkpointClose start; [2022-12-15 21:14:46,59] [info] checkpointClose synched; [2022-12-15 21:14:46,71] [info] checkpointClose script done; [2022-12-15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4471,checkpoint,checkpointClose,4471,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/inputs/download.rockylinux.org/pub/rocky/9/isos/aarch64/Rocky-9.3-aarch64-minimal.iso; 2023-12-20 18:42:08.431 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:42:08.436 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:05.5983839 Bandwidth: 264.71 MiB/s; 2023-12-20 18:42:08.439 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 1,553,926,298; /cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/execution; ```. `develop` with foreign Blob URL:. ![Screenshot 2023-12-20 at 11 41 33](https://github.com/broadinstitute/cromwell/assets/1087943/b46da630-ad80-4388-9642-867e11516177)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:2813,Down,Download,2813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['Down'],['Download']
Availability,"04 using conda-installed Cromwell:. `cromwell run ngs-ubuntu-20-04/iletisim/warp/pipelines/broad/dna_seq/germline/single_sample/exome/local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl -i ngs-ubuntu-20-04/iletisim/json/S736Nr1.json -o ngs-ubuntu-20-04/iletisim/json/options2.json`. Getting the error:. ```; [2023-02-04 08:55:00,61] [info] Running with database db.url = jdbc:hsqldb:mem:bc9ad7e3-efc7-4f37-aecb-b283b104cbcd;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,54] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2023-02-04 08:55:06,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2023-02-04 08:55:06,64] [info] Running with database db.url = jdbc:hsqldb:mem:a487ea75-b617-4523-a254-d0e694e68ff9;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,92] [info] Slf4jLogger started; [2023-02-04 08:55:07,18] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b625dba"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2023-02-04 08:55:07,22] [info] Metadata summary refreshing every 2 seconds.; [2023-02-04 08:55:07,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2023-02-04 08:55:07,63] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2023-02-04 08:55:07,64] [info] SingleWorkflowRunnerActor: Version 34-unknown-SNAP; [2023-02-04 08:55:07,65] [info] SingleWorkflowRunnerActor: Submitting workflow; [2023-02-04 08:55:07,68] [info] Unspecified type (Unspecified version) workflow 48f62f22-25fe-4f0f-b5fe-21191f035abd submitted; [2023-02-04 08:55:07,72] [info] SingleWorkflowRunnerActor: Workflow submit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:966,heartbeat,heartbeat,966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"0748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:19498,echo,echo,19498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz; 1608597579845,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz; 1608597581880,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz; 1608597584919,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:178344,down,download,178344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:86428,down,download,86428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz; 1608597247776,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz; 1608597249388,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:84559,down,download,84559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz; 1608597194778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz; 1608597196671,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:70216,down,download,70216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz; 1608597581880,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz; 1608597584919,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz; 1608597587281,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:178965,down,download,178965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597062290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:32844,down,download,32844,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:139451,down,download,139451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz; 1608597293653,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz; 1608597295559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:96398,down,download,96398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz; 1608597650698,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz; 1608597651470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi; 1608597651575,*** COMPLETED LOCALIZATION ***; 1608597657265,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597658212,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:197060,down,download,197060,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz; 1608597286657,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz; 1608597289714,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:94539,down,download,94539,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz; 1608596992645,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz; 1608596994248,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:14757,down,download,14757,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz; 1608597376806,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz; 1608597378569,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:118842,down,download,118842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz; 1608597229301,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz; 1608597232457,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:78960,down,download,78960,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz; 1608597098791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz; 1608597101844,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz; 1608597103907,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:44062,down,download,44062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz; 1608597502968,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz; 1608597504024,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:156268,down,download,156268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A657136E0208581A7B6A7F020F, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,68] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.white_brits:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.year_of_birth' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.phenotype' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.date_of_death' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.white_brits' (scatter index: None, attempt 1); [2022-12-15 21:23:03,67] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 3; [2022-12-15 21:23:03,69] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 is eligible for call caching with read = true and write = tru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:27158,failure,failures,27158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"1. Looking specifically for feedback on what places JesCacheHitCopyingActor would require more error handling.; 2. Within the JesCacheHitCopyingActor, are there any messages not being sent to the metadata service that should be sent ?; 3. Currently, not re-saving the JobOutputs that JesCacheHitCopyingActor is copying, since we shouldn't require multiple copies of the same outputs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1394:95,error,error,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394,1,['error'],['error']
Availability,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3399:142,heartbeat,heartbeats,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3399,2,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a randomly generated UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like the following when using the MySQL profile: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3401:161,heartbeat,heartbeats,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3401,2,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"1. When the AWS backend job actually failed with memory error, instead of returning that error code the commandScript returned zero. Fixed it to return the error code so that it a failed job in batch.; 2. Added AWS-EFS expression post mapping function so the output expressions with files with relative paths get mapped to the correct path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5516:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5516,3,['error'],['error']
Availability,"1.1.0...v1.1.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.0).; You might want to review and update them manually.; ```; CHANGELOG.md; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsCloudNioFileSystemProvider.scala; cwl/src/test/resources/cwl/lodash.js; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6850:1927,down,down,1927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6850,1,['down'],['down']
Availability,"1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,270 cromwell-system-akka.actor.default-dispatcher-4 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:9184,ERROR,ERROR,9184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,2,"['ERROR', 'Failure']","['ERROR', 'Failures']"
Availability,"1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,270 cromwell-system-akka.actor.default-dispatcher-5 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:5107,ERROR,ERROR,5107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,2,"['ERROR', 'Failure']","['ERROR', 'Failures']"
Availability,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1016:3972,error,error,3972,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016,1,['error'],['error']
Availability,"1.splicegraph"": ""<absolute_path_to_file2>""; }. ```; I tried with and without the ""runtime"" spec block (local run with voila on system path) with the same result. . In the cromwell log as it runs I see it says it runs this command:; ```; voila tsv \ ; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/test.psi.voila \; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/splicegraph.sql \; -f something.tsv; ```. However my program (voila) seems to somehow be receiving an argument for a different file (?) which it can not find:. ```; usage: voila tsv [-h] -f FILE_NAME [--threshold THRESHOLD]; [--non-changing-threshold NON_CHANGING_THRESHOLD]; [--probability-threshold PROBABILITY_THRESHOLD] [--show-all]; [--lsv-types-file LSV_TYPES]; [--lsv-types [LSV_TYPES [LSV_TYPES ...]]]; [--lsv-ids-file LSV_IDS] [--lsv-ids [LSV_IDS [LSV_IDS ...]]]; [--gene-names-file GENE_NAMES]; [--gene-names [GENE_NAMES [GENE_NAMES ...]]]; [--gene-ids-file GENE_IDS]; [--gene-ids [GENE_IDS [GENE_IDS ...]]] [-j NPROC] [--debug]; [-l LOGGER] [--silent]; files [files ...]; ```; voila tsv: error: argument files: cannot find ""/home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/execution/ "". The input files exist correctly at the path /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs . However, it appears that somehow a third path is being specified as an argument with the path ""/home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/execution/"" ? I don't understand where this is coming from but I am new at the language so most likely it is just a noob mistake. . Can anyone let me know why my program might be receiving the extra argument that causes it to crash?. Os: ubuntu 18.04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5285:2490,error,error,2490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285,1,['error'],['error']
Availability,1043c failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341:1625,Failure,Failure,1625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341,1,['Failure'],['Failure']
Availability,"10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); 	at java.nio.file.Files.newInputStream(Files.java:152); 	at java.nio.file.Files.newBufferedReader(Files.java:2784); 	at java.nio.file.Files.readAllLines(Files.java:3202); 	at java.nio.file.Files.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3857,echo,echo,3857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.ni",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3586,echo,echo,3586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,10K copies of the same error message per Cloud NAT run gets old fast.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5589:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5589,1,['error'],['error']
Availability,"11Z"",; ""description"": ""Stopped pulling \""broadinstitute\/cromwell-dos:34-d8acfe3\"""",; ""endTime"": ""2018-08-14T16:14:28.018319Z""; },; {; ""startTime"": ""2018-08-14T16:14:34.534911Z"",; ""description"": ""Started running \""\/bin\/bash -c mkdir -p \/cromwell_root && chmod -R a+rwx \/cromwell_root\"""",; ""endTime"": ""2018-08-14T16:14:34.810415Z""; },; {; ""startTime"": ""2018-08-14T16:16:45.288207Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:16:45.309551Z""; },; {; ""startTime"": ""2018-08-14T16:16:39.695039Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:15897,echo,echo,15897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retryi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:3906,down,download,3906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,2,"['down', 'error']","['download', 'error']"
Availability,"13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:20718,ERROR,ERROR,20718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['ERROR'],['ERROR']
Availability,"15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4707,checkpoint,checkpointClose,4707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4928,checkpoint,checkpointClose,4928,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5385,checkpoint,checkpointClose,5385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5842,checkpoint,checkpointClose,5842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6299,checkpoint,checkpointClose,6299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21:14:50,85] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6756,checkpoint,checkpointClose,6756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:50,95] [info] checkpointClose start; [2022-12-15 21:14:50,95] [info] checkpointClose synched; [2022-12-15 21:14:50,98] [info] checkpointClose script done; [2022-12-15 21:14:50,98] [info] dataFileCache commit start; [2022-12-15 21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9369,checkpoint,checkpointClose,9369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9826,checkpoint,checkpointClose,9826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10283,checkpoint,checkpointClose,10283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10740,checkpoint,checkpointClose,10740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11197,checkpoint,checkpointClose,11197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b254006"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11654,checkpoint,checkpointClose,11654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; concurrent-job-limit = 1000; root = ""s3://cromwell-aws-hello/cromwell-execution""; auth = ""default""; default-runtime-attributes {; queueArn = ""arn:aws:batch:us-east-1:XXXXXXXXX:job-queue/python-batch"" ,; scriptBucketName = ""cromwell-aws-hello"" ; }; filesystems {; s3 {; auth = ""default""; }; }; # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in the cloud.; slow-job-warning-time: 24 hours; }; }; }; }. [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026013/Cromwell-Error.docx); ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153040332-625cb61a-062b-4766-96ea-8e129efb2b20.PNG); [config file.docx](https://github.com/broadinstitute/cromwell/files/8026025/config.file.docx). How to give Timeout options for Job definitions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:3017,Error,Error,3017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,2,['Error'],['Error']
Availability,"17](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334557027). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577609). The performance issues aren't down the road. When I try to build a WOM; graph right now, it slows down after the first 100 nodes and never finishes. On Thu, Oct 5, 2017 at 4:01 PM, Dan Billings <notifications@github.com>; wrote:. > I suggest we leave this as-is with the understanding that it could be a; > performance issue down the road.; >; > rework the whole thing later; > This is a specific anti-goal.; >; > As I suggested, I would like to discuss w/ Chris when he gets back next; > week as we introduced the reference equality in the first place and I'm not; > aware of his motivation to do so.; >; > â€”; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG_4aJnzYP8ru5JvHrjbR5jwKwO9Brncks5spTV8gaJpZM4PttJd>; > .; >. -- ; Oliver Ruebenacker; Senior Software Engineer, Diabetes Portal; <http://www.type2diab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:2956,down,down,2956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['down'],['down']
Availability,"182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Calculated TES inputs (found 1):; Input(Some(commandScript),Some(wf_hello.hello.commandScript),None,/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE),Some(#!/bin/bash. cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; tmpDir=`mkdir -p ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32"" && echo ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. echo ""Hello World! Welcome to Cromwell . . . on AWS!""; ) > '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout' 2> '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr'; echo $? > /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; sync. ); mv /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; )). 2018-06-07 13:09:22,723 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:4640,echo,echo,4640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['echo'],['echo']
Availability,190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2266,recover,recoverAsync,2266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Availability,"19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83e163cce3cbd8e1c1446db372/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/callcaching/CallCacheDiffActor.scala#L164-L168)] is missing a `case (key, subObjec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5348:1155,error,errors,1155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348,1,['error'],['errors']
Availability,"1: I attempted to use your Jira tracker, it let me log in but told me I don't have permission to see anything or do anything; 2: https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team no longer exists, and the support staff respond to questions with ""we only answer GATK isues""; 3: I am using womtool 65 and Cromwell 62. I get the same failure in both, which is that if the first line of my file is:. `version development`. As per the [WDL specifications](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#versioning) I get the error:. `ERROR: Finished parsing without consuming all tokens.`. If I do not include that line, then I get this error:. ```; Expected rbrace, got Directory.; Directory	OutputDir; ```. Does Cromwell support WDL versions pther than the default? if so, how do I specify which version to use?. Thank you,; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIV",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438:351,failure,failure,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438,4,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"1] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:188); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at scala.util.Failure.recoverWith(Try.scala:232); at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); at scala.collection.Iterator.foreach(Iterator.scala:929); at scala.collection.Iterator.foreach$(Iterator.scala:929); at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:181); at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEval",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:3823,Failure,Failure,3823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['Failure'],['Failure']
Availability,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz; 1608597612565,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz; 1608597615294,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz; 1608597617667,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:186423,down,download,186423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1036,Failure,Failure,1036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Failure'],['Failure']
Availability,2 workflows failing with JES 503 errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:33,error,errors,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,1,['error'],['errors']
Availability,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:14402,Error,Error,14402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['Error'],['Error']
Availability,"2+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3042,echo,echo,3042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"2-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4823,Checkpoint,Checkpoint,4823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4873,checkpoint,checkpointClose,4873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5217,Checkpoint,Checkpoint,5217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5330,checkpoint,checkpointClose,5330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5787,checkpoint,checkpointClose,5787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5674,Checkpoint,Checkpoint,5674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6131,Checkpoint,Checkpoint,6131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6244,checkpoint,checkpointClose,6244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6588,Checkpoint,Checkpoint,6588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6701,checkpoint,checkpointClose,6701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:50,95] [info] Checkpoint start; [2022-12-15 21:14:50,95] [info] checkpointClose start; [2022-12-15 21:14:50,95] [info] checkpointClose synched; [2022-12-15 21:14:50,98] [info] checkpointClose script done; [2022-12-15 21:14:50,98] [info] dataFileCache commit start; [2022-12-15 21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9314,checkpoint,checkpointClose,9314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9658,Checkpoint,Checkpoint,9658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9771,checkpoint,checkpointClose,9771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10228,checkpoint,checkpointClose,10228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10115,Checkpoint,Checkpoint,10115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10572,Checkpoint,Checkpoint,10572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10685,checkpoint,checkpointClose,10685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11029,Checkpoint,Checkpoint,11029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11142,checkpoint,checkpointClose,11142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11486,Checkpoint,Checkpoint,11486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b254006"",; ""heartbeatInterval""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11599,checkpoint,checkpointClose,11599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:1875,heartbeat,heartbeat,1875,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"2/call-lo; ad_shared_covars/execution/stderr.; [First 3000 bytes]:Traceback (most recent call last):; File ""/home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py"",; line 87, in <module>; load_covars(); File ""/home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py"",; line 51, in load_covars; assert not np.any(np.isnan(data)); AssertionError. [2022-12-15 21:28:38,49] [info] WorkflowManagerActor: Workflow actor for 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff completed with status 'Failed'. The workflow will be removed from the workflow store.; [2022-12-15 21:28:52,23] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2022-12-15 21:28:53,46] [info] Workflow polling stopped; [2022-12-15 21:28:53,46] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2022-12-15 21:28:53,46] [info] Aborting all running workflows.; [2022-12-15 21:28:53,46] [info] 0 workflows released by cromid-b254006; [2022-12-15 21:28:53,47] [info] WorkflowStoreActor stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] WorkflowLogCopyRouter stopped; [2022-12-15 21:28:53,47] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] JobExecutionTokenDispenser stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor: All workflows finished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:49140,down,down,49140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['down'],['down']
Availability,"21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5164,checkpoint,checkpointClose,5164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5621,checkpoint,checkpointClose,5621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6078,checkpoint,checkpointClose,6078,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6535,checkpoint,checkpointClose,6535,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21:14:50,85] [info] checkpointClose script done; [2022-12-15 21:14:50,85] [info] dataFileCache commit start; [2022-12-15 21:14:50,85] [info] dataFileCache commit end; [2022-12-15 21:14:50,87] [info] checkpointClose end; [2022-12-15 21:14:50,88] [info] Checkpoint end - txts: 101879; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6992,checkpoint,checkpointClose,6992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9605,checkpoint,checkpointClose,9605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10062,checkpoint,checkpointClose,10062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10519,checkpoint,checkpointClose,10519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10976,checkpoint,checkpointClose,10976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11433,checkpoint,checkpointClose,11433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"22-12-15 21:14:46,59] [info] checkpointClose start; [2022-12-15 21:14:46,59] [info] checkpointClose synched; [2022-12-15 21:14:46,71] [info] checkpointClose script done; [2022-12-15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4528,checkpoint,checkpointClose,4528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:7:1#142554972] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility2_run_create_seg_gt_table:NA:1#1336176467] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,67] [error] WorkflowManagerActor Workflow 54e13b6c-33e4-4777-a4bd-f7b2876c2df5 failed (during ExecutingWorkflowState): java.lang.Exception: Call crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:3:1: return code was -1. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:3072,error,error,3072,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,3,['error'],['error']
Availability,"235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(88b21d2d)]: Call failed to initialize: failed to create call actor for PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log: None.get; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(88b21d2d)]: persisting status of PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log to Failed.; 2016-06-01 16:10:15,230 cromwell-system-akka.actor.default-dispatcher-20 INFO - WorkflowActor [UUID(88b21d2d)]: Beginning transition from Running to Failed.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:4353,ERROR,ERROR,4353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['ERROR'],['ERROR']
Availability,"23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n\nreal\t0m0.042s\nuser\t0m0.014s\nsys\t0m0.003s\n[INFO 2021-02-22 23:09:21 UTC] Found toolchain path file locally\nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] /build/cos-tools: \nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:4222,Down,Downloading,4222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['Down'],['Downloading']
Availability,"27). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577609). The performance issues aren't down the road. When I try to build a WOM; graph right now, it slows down after the first 100 nodes and never finishes. On Thu, Oct 5, 2017 at 4:01 PM, Dan Billings <notifications@github.com>; wrote:. > I suggest we leave this as-is with the understanding that it could be a; > performance issue down the road.; >; > rework the whole thing later; > This is a specific anti-goal.; >; > As I suggested, I would like to discuss w/ Chris when he gets back next; > week as we introduced the reference equality in the first place and I'm not; > aware of his motivation to do so.; >; > â€”; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG_4aJnzYP8ru5JvHrjbR5jwKwO9Brncks5spTV8gaJpZM4PttJd>; > .; >. -- ; Oliver Ruebenacker; Senior Software Engineer, Diabetes Portal; <http://www.type2diabetesgenetics.org/>, Broad Institute; <http://www.broadinstitute.org/>. ---",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:3024,down,down,3024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['down'],['down']
Availability,"28000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; enabled = ""false""; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:4038,avail,available,4038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['avail'],['available']
Availability,"31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:2870,error,error,2870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['error'],['error']
Availability,"333-8f3c-7c69317ba0a2/call-PairedFastQsToUnmappedBAM/inputs/-2135135022/S000021_S7367Nr1.2.fastq.gz --OUTPUT S7367Nr1.unmapped.bam --READ_GROUP_NAME S7367Nr1 --SAMPLE_NAME S4431Nr1 --LIBRARY_NAME TwistCore+RefSeq+Mito-Panel --PLATFORM_UNIT platform_unit --PLATFORM Illumina --SEQUENCING_CENTER CeGaT --RUN_DATE 2021-10-10T06:00:00+0000 --USE_SEQUENTIAL_FASTQS false --SORT_ORDER queryname --MIN_Q 0 --MAX_Q 93 --STRIP_UNPAIRED_MATE_NUMBER false --ALLOW_AND_IGNORE_EMPTY_LINES false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; 2023-02-03 12:38:34 [Fri Feb 03 09:38:34 GMT 2023] Executing as root@d65fc5b7d470 on Linux 5.15.49-linuxkit amd64; OpenJDK 64-Bit Server VM 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.3.0.0; 2023-02-03 12:38:35 INFO 2023-02-03 09:38:35 FastqToSam Auto-detected quality format as: Standard.; 2023-02-03 12:39:08 INFO 2023-02-03 09:39:08 FastqToSam Processed 1,000,000 records. Elapsed time: 00:00:32s. Time for last 1,000,000: 32s. Last read position: */*`. I tried via Java 18.0.1.1 JDK and also later with 1.8.0_202 JDK. I also tried with the conda installation where Java dependency of OpenJDK 11.0.15 is automatically installed. I also tried combinations with Cromwell 69, 80 and 84. None of them works. They all have the same problem. It only works if I use Cromwell version 55 along with Java 1.8.0_202 JDK. It would be amazing if you look into this, as we would love to use the latest Cromwell versions and benefit from the conda environment. Thanks!. Machine info: `Darwin Ibrahims-MacBook-Pro.local 22.2.0 Darwin Kernel Version 22.2.0: Fri Nov 11 02:04:44 PST 2022; root:xnu-8792.61.2~4/RELEASE_ARM64_T8103 arm64`. MacOS = Ventur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998:2678,avail,available,2678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998,1,['avail'],['available']
Availability,"34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:7:1#142554972] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:2630,error,error,2630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['error'],['error']
Availability,"3:25.071851Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.571Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2018-08-14T16:13:14.755Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.570Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-08-14T16:13:14.571Z""; },; {; ""startTime"": ""2018-08-14T16:16:37.479167Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:39.695039Z""; },; {; ""startTime"": ""2018-08-14T16:16:57.673071Z"",; ""description"": ""Started running \""\/bin\/sh -c cat \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt 2>\/dev\/null | xargs -I % sh -c 'gsutil -m cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\"") 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -m -u dos-testing cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:10061,echo,echo,10061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"3:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files; [INFO 2020-08-04 23:41:42 UTC] Running Nvidia installer. ERROR: The kernel module failed to load, because it was not signed by a key; that is trusted by the kernel. Please try installing the driver; again, and set the --module-signing-secret-key and; --module-signing-public-key options on the command line, or run the; installer in expert mode to enable the interactive module signing; prompts. ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most; frequently when this kernel module was built against the wrong or; improperly configured kernel sources, with a version of gcc that; differs from the one used to build the target kernel, or if another; driver, such as nouveau, is present and prevents the NVIDIA kernel; module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA; GPU installed in this system is supported by this NVIDIA Linux; graphics driver release. Please see the log entries 'Kernel module load error' and 'Kernel; messages' at the end of the file; '/usr/local/nvidia/nvidia-installer.log' for more information. ERROR: Installation has failed. Please see the file; '/usr/local/nvidia/nvidia-installer.log' for details. You may find; suggestions on fixing installation problems in the README available; on the Linux driver download page at www.nvidia.com.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:5375,ERROR,ERROR,5375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,5,"['ERROR', 'avail', 'down', 'error']","['ERROR', 'available', 'download', 'error']"
Availability,"3] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7199,down,down,7199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['down'],['down']
Availability,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:2883,error,error,2883,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,1,['error'],['error']
Availability,"4+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3313,echo,echo,3313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:134465,down,download,134465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz; 1608597504024,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:156895,down,download,156895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz; 1608597637215,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi; 1608597639911,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:193276,down,download,193276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz; 1608597439856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:137588,down,download,137588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz; 1608597602076,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi; 1608597604365,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:183935,down,download,183935,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz; 1608597335401,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi; 1608597338143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz; 1608597340856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:108253,down,download,108253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz; 1608597031084,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi; 1608597033701,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz; 1608597036753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:25352,down,download,25352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz; 1608597352822,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi; 1608597355348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz; 1608597359185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:112608,down,download,112608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz; 1608597378569,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:119469,down,download,119469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:170884,down,download,170884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz; 1608597308537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi; 1608597310848,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:100775,down,download,100775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz; 1608597295559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:97025,down,download,97025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz; 1608597014237,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi; 1608597016404,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:20999,down,download,20999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz; 1608597361344,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi; 1608597362768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi; 1608597364585,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:114477,down,download,114477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz; 1608597393258,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:123846,down,download,123846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz; 1608597156183,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi; 1608597157882,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:59643,down,download,59643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz; 1608597241175,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi; 1608597243204,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi; 1608597245149,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:82692,down,download,82692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,406 errors from status endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775:4,error,errors,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775,1,['error'],['errors']
Availability,"46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from FinalizingWorkflowState to WorkflowFailedState: shutting down; [2016-07-13 10:12:46,64] [error] WorkflowManagerActor Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 failed (during FinalizingWorkflowState): java.lang.Throwable: Google credentials are invalid: 401 Unauthorized; java.util.NoSuchElementException: None.get; [2016-07-13 10:12:46,64] [info] WorkflowManagerActor WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 is in a terminal state: WorkflowFailedState; [2016-07-13 10:12:49,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:2911,down,down,2911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,2,"['down', 'error']","['down', 'error']"
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:32227,down,download,32227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz; 1608597524955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi; 1608597527253,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz; 1608597529228,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:162737,down,download,162737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:18501,down,download,18501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz; 1608597333778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:106384,down,download,106384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi; 1608597646131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:194526,down,download,194526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi; 1608597157882,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:60268,down,download,60268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz; 1608597374474,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:116983,down,download,116983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597418094,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:129467,down,download,129467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:30973,down,download,30973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz; 1608597112463,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi; 1608597115454,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz; 1608597116706,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:47798,down,download,47798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz; 1608597560491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:172134,down,download,172134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:50927,down,download,50927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz; 1608597169668,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:62145,down,download,62145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:7271,down,download,7271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi; 1608597604365,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:184562,down,download,184562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz; 1608596992645,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:14140,down,download,14140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz; 1608597465182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:143199,down,download,143199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:37205,down,download,37205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:60895,down,download,60895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.job.preparation.JobPreparationActor$$anonfun$1$$anon$1: Call input and runtime attributes evaluation failed for ls:; Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""womtool-31.jar""' of type 'File' to 'Array[File]'.; .; .; Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 transitioned to state Failed. [issue.zip](https://github.com/broadinstitute/cromwell/files/2759990/issue.zip). I've attached the `wdl` and `json` files I've used, the input filenames are the `jar` files of both cromwell and womtool used to run the test workflow. I'm not attaching those because they are very large.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550:2505,heartbeat,heartbeat,2505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550,3,"['error', 'heartbeat']","['error', 'heartbeat', 'heartbeatInterval']"
Availability,48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2932,recover,recover,2932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.trans,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:4781,Error,ErrorOr,4781,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:2430,Error,ErrorOr,2430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"49.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:159399,down,download,159399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"49:46.495] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$kb] $$kb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$kb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb/WorkflowInitializationActor-7218c3a1-5155-4921-9adb-d96c52c32200] State is now terminal. Shutting down.; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.544] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$mb] $$mb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$mb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$mb] $$mb transitioning from In",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:2853,down,down,2853,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,1,['down'],['down']
Availability,"4:main.year_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 09247459DDA5EA8DF661D5F490C81E8B, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,84] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.year_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,36] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.phenotype:-1:1-20000000025 [9e4f5894main.phenotype:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,36] [info] BT-322 9e4f5894:main.phenotype:-1:1 cache hit copying success with aggregated hashes: initial = 018D1BC619E22671C2125EEDE82AB210, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,36] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.phenotype:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,37] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.date_of_death:-1:1-20000000026 [9e4f5894main.date_of_death:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,37] [info] BT-322 9e4f5894:main.date_of_death:-1:1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:25880,failure,failures,25880,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.pcs:-1:1 cache hit copying success with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.categorical_covariates' (scatter index: Some(0), attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:31406,failure,failures,31406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"5) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Sin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:3039,ERROR,ERROR,3039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['ERROR'],['ERROR']
Availability,500 Internal error post success processing of task (call caching turned off),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/577:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/577,1,['error'],['error']
Availability,500 Internal error when trying to get task hash for call caching,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/600:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/600,1,['error'],['error']
Availability,"54-4c39-9870-55574d000765/call-Mutect2/shard-1/Mutect2/56dd28f2-d4af-449d-961a-eface7c9a288/call-FilterByOrientationBias/execution/background.synth.challenge2.snvs.svs.tumorbackground-vs-synthetic.challenge.set2.normal.ob_filtered.vcf""],; ""Mutect2_Multi.unfiltered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-unfilteredOutputList/execution/unfiltered.list"",; ""Mutect2_Multi.ob_filtered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-orientationBiasFilteredOutputList/execution/ob_filtered.list""; },; ""id"": ""0239d302-1154-4c39-9870-55574d000765""; }; [2017-03-20 15:30:35,34] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/debug_m2_wdl/test_m2_wdl.metadata; [2017-03-20 15:30:35,46] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-21-0-unknown-operation#1356917576]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-17-0-unknown-operation#-291022515]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-15-0-unknown-operation#-925665144]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-3-0-unknown-operation#-2130885356]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-4-0-unknown-operation#-1268876796]] terminated abruptly; [2017-03-20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:3351,error,error,3351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,"55ee7c; [2018-10-25 21:21:09,93] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-25 21:21:09,93] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:21:09,96] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Parsing workflow as WDL draft-2; [2018-10-25 21:21:10,57] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:21:12,86] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Condition met: 'go'. Running conditional section; [2018-10-25 21:21:16,98] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/script; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:3934,echo,echo,3934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,"5sbg5oeqevqw-6cylhedz/job-1/a7123170_f41bbba17a6f4409940127a60234695d-1/wd/wd/cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/inputs/lz304a1e79fd7359e5327eda.blob.core.windows.net/sc-705b830a-d699-478e-9da6-49661b326e77/inputs/Rocky-9.2-aarch64-dvd.iso; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/inputs/download.rockylinux.org/pub/rocky/9/isos/aarch64/Rocky-9.3-aarch64-minimal.iso; 2023-12-20 18:42:08.431 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsReader[0] All part read operations completed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:1319,Down,Download,1319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['Down'],['Download']
Availability,6); at cats.effect.internals.IOBracket$.$anonfun$apply$1$adapted(IOBracket.scala:33); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:328); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:117); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.IO.unsafeRunAsync(IO.scala:258); at cats.effect.IO.unsafeToFuture(IO.scala:345); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:6591,robust,robustExecuteOrRecover,6591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['robust'],['robustExecuteOrRecover']
Availability,"6-08-08 08:33:10,291] [info] WorkflowActor [â†[38;5;2m4e20eafcâ†[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [â†[38;5;1merrorâ†[0m] BackendCallExecutionActor [â†[38;5 ;2m4e20eafcâ†[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:9338,error,error,9338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['error'],['error']
Availability,"613). Admittedly, I have never written a parser before, so I don't know how feasible this is, but can you write the grammar/parser such that everything on a line after a `#` character is ignored?. The only edge cases I imagine are when the `#` character is in a quoted string. ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200505343). @tmdefreitas Yes, that is definitely possible. However, we try to not make assumptions about the type of characters that your script can have in it. I'm perhaps being a little overly cautious, but I'd hate for there to be a case where somebody wants to use a `#` in their command but it gets interpreted as a comment. That could lead to the same kind of confusion that we're seeing now. I vacillate on this because I also see the pragmatism in implementing your suggestion for the common case. In most cases I can think of, a `#` is a comment. Maybe some approach like Eddie's where I can have the parser give a better error message is the best solution. ---. @eddiebroad commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200508578). Scott,. Not only can I imagine a command where # is used (and in a WDL no less),; but I have actually written such a command. In the merge step for mutect, I use ""#"" to select for header lines in; merging call_stats files and VCF files!. command <<<; #increase verbosity; set -x. #mutect1 call_stats merging; MUTECT1_CS=""MuTect1.call_stats.txt""; head --lines=2 ${mutect1_cs[0]} > $MUTECT1_CS; cat ${sep =' ' mutect1_cs} | grep -Pv '#'|grep -Pv '^contig' >> $MUTECT1_CS. #mutect2 call_stats merging; MUTECT2_CS=""MuTect2.call_stats.txt""; cat ${mutect2_cs[0]} |grep -P '^#' > $MUTECT2_CS ;; cat ${sep=' ' mutect2_cs} |grep -Pv '^#' >> $MUTECT2_CS ;; -eddie. On Wed, Mar 23, 2016 at 3:25 PM, Scott Frazer notifications@github.com; wrote:. > @tmdefreitas https://github.com/tmdefreitas Yes, that is definitely; > possible.; >",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:3006,error,error,3006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,1,['error'],['error']
Availability,"636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi; 1608597646131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp; 1608597648902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:195153,down,download,195153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz; 1608597186282,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi; 1608597189769,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz; 1608597192326,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:68359,down,download,68359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz; 1608597146753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi; 1608597149087,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz; 1608597152166,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:57157,down,download,57157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz; 1608597208254,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi; 1608597210297,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi; 1608597211840,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:73960,down,download,73960,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz; 1608597014237,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:19751,down,download,19751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz; 1608597333778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz; 1608597335401,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:107005,down,download,107005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz; 1608597070520,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi; 1608597072537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:35953,down,download,35953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi; 1608597348432,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz; 1608597351053,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz; 1608597352822,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:111360,down,download,111360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:30344,down,download,30344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz; 1608597103907,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi; 1608597105908,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz; 1608597108506,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:45308,down,download,45308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:103267,down,download,103267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:104513,down,download,104513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz; 1608596994248,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi; 1608596996095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:16003,down,download,16003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz; 1608597175918,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi; 1608597177960,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz; 1608597179945,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:65250,down,download,65250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz; 1608597277147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi; 1608597279104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:92672,down,download,92672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz; 1608597567063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi; 1608597569034,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:175237,down,download,175237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz; 1608596971072,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:9146,down,download,9146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:120715,down,download,120715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz; 1608597619328,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi; 1608597622374,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz; 1608597624684,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:188288,down,download,188288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi; 1608597509827,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:158141,down,download,158141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:147566,down,download,147566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:13513,down,download,13513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz; 1608597626312,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi; 1608597627949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:190153,down,download,190153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:102021,down,download,102021,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz; 1608597023863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:22245,down,download,22245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz; 1608596952115,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:2287,down,download,2287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz; 1608597038611,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi; 1608597040452,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:27219,down,download,27219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz; 1608597594378,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi; 1608597597343,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz; 1608597600688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:182066,down,download,182066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz; 1608597268460,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi; 1608597270319,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:90178,down,download,90178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi; 1608597435583,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi; 1608597438407,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:135711,down,download,135711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz; 1608597169668,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz; 1608597172290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:62766,down,download,62766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz; 1608597419738,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:131340,down,download,131340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:170257,down,download,170257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz; 1608597023863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz; 1608597026409,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:22866,down,download,22866,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:142572,down,download,142572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz; 1608597144746,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz; 1608597146753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:55911,down,download,55911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz; 1608597393258,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:123219,down,download,123219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:141947,down,download,141947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:133838,down,download,133838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:85809,down,download,85809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi; 1608597438407,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz; 1608597439856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:136961,down,download,136961,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi; 1608597355348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz; 1608597359185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz; 1608597361344,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:113229,down,download,113229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:6019,down,download,6019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi; 1608597639911,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:193897,down,download,193897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:140078,down,download,140078,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz; 1608597637215,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:192028,down,download,192028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:128217,down,download,128217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:105134,down,download,105134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:138834,down,download,138834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:17251,down,download,17251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597367102,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:116354,down,download,116354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:171505,down,download,171505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi; 1608597597343,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz; 1608597600688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz; 1608597602076,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:182687,down,download,182687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz; 1608597204085,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz; 1608597206512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:72093,down,download,72093,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:61516,down,download,61516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:131969,down,download,131969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi; 1608597468522,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:145693,down,download,145693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi; 1608597364585,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597367102,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:115733,down,download,115733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi; 1608597089027,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi; 1608597091077,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi; 1608597093130,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:41574,down,download,41574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz; 1608597204085,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:71472,down,download,71472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi; 1608597531104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi; 1608597532744,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi; 1608597533973,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:164612,down,download,164612,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi; 1608597261055,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi; 1608597263910,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:87684,down,download,87684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi; 1608597406434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:127596,down,download,127596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz; 1608597523198,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:160868,down,download,160868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi; 1608597306259,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz; 1608597308537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:99527,down,download,99527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi; 1608597118429,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:49673,down,download,49673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi; 1608597536250,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi; 1608597538865,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi; 1608597540849,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:166497,down,download,166497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi; 1608597213995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi; 1608597216321,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:75843,down,download,75843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597418094,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz; 1608597419738,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:130094,down,download,130094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:152528,down,download,152528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:31600,down,download,31600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi; 1608597243204,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi; 1608597245149,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz; 1608597247776,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:83319,down,download,83319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:50302,down,download,50302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:169636,down,download,169636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp; 1608597648902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz; 1608597650698,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz; 1608597651470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:196439,down,download,196439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6de7fff-EngineJobExecutionActor-main.assessment_ages:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:50,48] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.assessment_ages' (scatter index: None, attempt 1); [2022-12-15 21:27:50,82] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.genetic_sex:-1:1-20000000011 [9e4f5894main.genetic_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,82] [info] BT-322 9e4f5894:main.genetic_sex:-1:1 cache hit copying success with aggregated hashes: initial = FD7DC79B974CF6706FC3376F067965B9, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,82] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.genetic_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:54,15] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.genetic_sex' (scatter index: None, attempt 1); [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.categorical_covariates:0:1-20000000027 [9e4f5894main.categorical_covariates:0:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:29933,failure,failures,29933,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz; 1608597111201,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz; 1608597112463,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi; 1608597115454,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:47169,down,download,47169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:105755,down,download,105755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi; 1608597189769,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz; 1608597192326,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz; 1608597194778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:68976,down,download,68976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi; 1608597149087,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz; 1608597152166,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz; 1608597154187,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:57776,down,download,57776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:17872,down,download,17872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:103886,down,download,103886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:128838,down,download,128838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:102640,down,download,102640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi; 1608597622374,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz; 1608597624684,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz; 1608597626312,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:188907,down,download,188907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi; 1608597435583,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:135084,down,download,135084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:138207,down,download,138207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi; 1608597310848,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:101394,down,download,101394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi; 1608597177960,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz; 1608597179945,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi; 1608597182124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:65869,down,download,65869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:120088,down,download,120088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:146939,down,download,146939,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi; 1608596944807,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:1660,down,download,1660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz; 1608597081079,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz; 1608597083236,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:38451,down,download,38451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi; 1608597509827,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:157514,down,download,157514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz; 1608597499938,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz; 1608597502968,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:155028,down,download,155028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz; 1608597465182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz; 1608597466639,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:143818,down,download,143818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:51546,down,download,51546,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi; 1608597016404,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:21618,down,download,21618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi; 1608597338143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz; 1608597340856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz; 1608597343594,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:108872,down,download,108872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz; 1608596952115,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz; 1608596954593,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:2906,down,download,2906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz; 1608596979652,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz; 1608596981096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:11019,down,download,11019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentCollection - Processing 292450248 bp from intervals; 21:45:16.221 INFO PrintReads - Done initializing engine; 21:45:16.367 INFO ProgressMeter - Starting traversal; 21:45:16.368 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 21:45:17.033 INFO PrintReads - No reads filtered by: WellformedReadFilter; 21:45:17.035 INFO ProgressMeter - chr6:96496576 0.0 5660 509909.9; 21:45:17.036 INFO ProgressMeter - Traversal complete. Processed 5660 total reads in 0.0 minutes.; 21:45:17.414 INFO PrintReads - Shutting down engine; ```. This is running on Google Cloud on Cromwell 32-c7bcab8.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3929:2534,down,down,2534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929,1,['down'],['down']
Availability,"7c74-4c5c-4314-8463-072e7055ee7c is in a terminal state: WorkflowSucceededState; [2018-10-25 21:21:44,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test_opt_array.t1.out"": [""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-0/execution/out.txt"", ""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/out.txt"", ""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/out.txt"", ""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-3/execution/out.txt"", ""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-4/execution/out.txt""]; },; ""id"": ""0bb77c74-4c5c-4314-8463-072e7055ee7c""; }; ```. But I got the following error when I tried with 36.; ```; $ java -jar ~/cromwell-36.jar run test_opt_array.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2018-10-25 21:17:04,83] [info] Running with database db.url = jdbc:hsqldb:mem:bb200ed8-7db5-49a0-a250-ca46b3332697;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,03] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:17:12,04] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:17:12,13] [info] Running with database db.url = jdbc:hsqldb:mem:c7a7ec22-dec6-4fae-a53b-6c9933402fa9;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,59] [info] Slf4jLogger started; [2018-10-25 21:17:12,88] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-f5ccf1c"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:17:12,90] [info] Metadata summary refreshi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:9533,error,error,9533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['error'],['error']
Availability,"7e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:35187,ERROR,ERROR,35187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi; 1608597542416,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:168382,down,download,168382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:146320,down,download,146320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:124473,down,download,124473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:153782,down,download,153782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:9773,down,download,9773,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:191407,down,download,191407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz; 1608597081079,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:37832,down,download,37832,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi; 1608597569034,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz; 1608597573618,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:175864,down,download,175864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz; 1608597499938,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:154409,down,download,154409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:153155,down,download,153155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz; 1608597480242,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:148193,down,download,148193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:121969,down,download,121969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi; 1608597040452,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:27846,down,download,27846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi; 1608596996095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:16630,down,download,16630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:121342,down,download,121342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz; 1608596979652,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:10400,down,download,10400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi; 1608597270319,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz; 1608597275740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:90805,down,download,90805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:97652,down,download,97652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi; 1608597210297,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi; 1608597211840,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi; 1608597213995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:74587,down,download,74587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi; 1608597627949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:190780,down,download,190780,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi; 1608597279104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:93299,down,download,93299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"8:33:10,121] [â†[38;5;220mwarnâ†[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,251] [info] WorkflowActor [â†[38;5;2m4e20eafcâ†[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-08 08:33:10,251] [info] WorkflowActor [â†[38;5;2m4e20eafcâ†[0m]: createdcall actor for hello.; [2016-08-08 08:33:10,271] [â†[38;5;220mwarnâ†[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,291] [info] LocalBackend [â†[38;5;2m4e20eafcâ†[0m:hello]: â†[38;5;5mecho 'Hello String!'â†[0m; [2016-08-08 08:33:10,291] [info] WorkflowActor [â†[38;5;2m4e20eafcâ†[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [â†[38;5;1merrorâ†[0m] BackendCallExecutionActor [â†[38;5 ;2m4e20eafcâ†[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDisp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:7952,error,error,7952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['error'],['error']
Availability,"8d8048main.low_genotyping_quality_sample_list:NA:1]: Unrecognized ru; ntime attribute keys: shortTask, dx_timeout; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.white_brits_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = 9675960412B5394D5D0816ED198FB6EB.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.white_brits_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.low_genotyping_quality_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 3C891C9939496580DDF747805F991E06, file = AAFFF98AC7D58B07E7CE25978A906B00.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.low_genotyping_quality_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,02] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_mismatch_sample_list:-1:1-20000000015 [788d8048main.sex_mismatch_sample_list:NA:1]: Unrecognized runtime attribute keys; : shortTask, dx_timeout; [2022-12-15 21:28:04,02] [info] BT-322 788d8048:main.sex_mismatch_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 03340ED60152B24B7D0988669F47CF2B, file = EB6A9909BDF3705B7BB543E4096DA08A.; [2022-12-15 21:28:04,02] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.sex_mismatch_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,35] [info] BackgroundConfigAsyncJobExecutionActor [788d8048main.load_shared_covars:NA:1]: /home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main; /788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_cov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:36508,failure,failures,36508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsert,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5661,error,error,5661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['error'],['error']
Availability,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:3137,error,error,3137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,4,"['down', 'error']","['downloading', 'error']"
Availability,"9kdWN0aW9uUXVldWU"",; ""backend"": ""JES"",; ""end"": ""2017-01-30T19:14:19.708Z"",; ""stderr"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""attempt"": 1,; ""executionEvents"": [...],; ""backendLogs"": {; ""log"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files.log""; },; ""start"": ""2017-01-30T19:00:03.896Z""; }]; },; ""outputs"": {. },; ""workflowRoot"": ""/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/"",; ""id"": ""c386672d-0248-4968-9b1a-114f5f5c4706"",; ""inputs"": {...; },; ""submission"": ""2017-01-30T19:00:00.796Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/workflow.logs/workflow.c386672d-0248-4968-9b1a-114f5f5c4706.log"",; ""end"": ""2017-01-30T19:14:20.002Z"",; ""start"": ""2017-01-30T19:00:03.040Z""; }. ```; Here it's an array of ""message""s; ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {... },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:2539,failure,failures,2539,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,2,"['error', 'failure']","['error', 'failures']"
Availability,: cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:1644,Failure,Failure,1644,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['Failure'],['Failure']
Availability,":-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = tru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:2688,failure,failures,2688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['failure'],['failures']
Availability,":. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:2145,Error,ErrorOr,2145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,"://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. I executed `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapExit$ (file:/home/jaruga/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.5/run_2.12-1.5.5.jar); WARNING: Please consider reporting this to the maintainers of sbt.TrapExit$; WARNING: System::setSecurityManager will be r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:1159,error,error,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,1,['error'],['error']
Availability,":09,96] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Parsing workflow as WDL draft-2; [2018-10-25 21:21:10,57] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:21:12,86] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Condition met: 'go'. Running conditional section; [2018-10-25 21:21:16,98] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/script; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-3/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74te",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:4178,echo,echo,4178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7388,Error,ErrorOr,7388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,":14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5280,Checkpoint,Checkpoint,5280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5737,Checkpoint,Checkpoint,5737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6194,Checkpoint,Checkpoint,6194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6651,Checkpoint,Checkpoint,6651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9721,Checkpoint,Checkpoint,9721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10178,Checkpoint,Checkpoint,10178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10635,Checkpoint,Checkpoint,10635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11092,Checkpoint,Checkpoint,11092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11549,Checkpoint,Checkpoint,11549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Succeeded; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.sho,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:4523,Error,ErrorHandling,4523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['Error'],['ErrorHandling']
Availability,":30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1400,down,down,1400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['down'],['down']
Availability,":40:29,28] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 09:40:29,29] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 09:40:29,30] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 09:40:29,35] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 09:40:30,63] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 09:40:30,68] [info] Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 submitted.; [2017-12-05 09:40:30,68] [info] SingleWorkflowRunnerActor: Workflow submitted 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,69] [info] 1 new workflows fetched; [2017-12-05 09:40:30,69] [info] WorkflowManagerActor Starting workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,70] [info] WorkflowManagerActor Successfully started WorkflowActor-6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,70] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 09:40:31,66] [error] WorkflowManagerActor Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.b1; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.b2; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.b1; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.b2; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992:2377,error,error,2377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992,1,['error'],['error']
Availability,":469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14519,error,error,14519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,":47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2234,failure,failure,2234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['failure'],['failure']
Availability,":48:20,484 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Starting workflow UUID(075e0cf3-194b-4f53-a43d-d31f0b370f79); 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Successfully started WorkflowActor-075e0cf3-194b-4f53-a43d-d31f0b370f79; 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2021-09-27 13:48:20,547 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; Sep 27, 2021 1:48:20 PM com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 2021-09-27 13:48:21,326 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Parsing workflow as WDL draft-2; 2021-09-27 13:48:22,359 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; 2021-09-27 13:48:24,671 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-075e0cf3-194b-4f53-a43d-d31f0b370f79 [UUID(075e0cf3)]: Starting wf_hello.hello; 2021-09-27 13:48:29,304 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Assigned new job execution tokens to the following groups: 075e0cf3: 1; 2021-09-27 13:48:31,233 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:9916,error,error,9916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['error'],['error']
Availability,":491); at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:660); at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:517); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:25); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); at java.base/java.lang.Thread.run(Thread.java:1589); Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.Table.enforceTypeLimits(Unknown Source); at org.hsqldb.Table.generateAndCheckData(Unknown Source); at org.hsqldb.Table.insertSingleRow(Unknown Source); at org.hsqldb.StatementDML.insertRowSet(Unknown Source); at org.hsqldb.StatementInsert.getResult(Unknown Source); at org.hsqldb.StatementDMQL.execute(Unknown Source); at org.hsqldb.Session.executeCompiledStatement(Unknown Source); at org.hsqldb.Session.execute(Unknown Source); ... 17 common frames omitted; Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.types.CharacterType.convertToTypeLimits(Unknown Source); ... 25 common frames omitted; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:15:1]: Status change from - to WaitingForReturnCode; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:3934,error,error,3934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['error'],['error']
Availability,":54,70] [info] BT-322 12ceda02:test.task_A:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; [2022-10-06 15:14:54,70] [info] 12ceda02-4906-4840-80a2-514af3ccb801-EngineJobExecutionActor-test.task_A:NA:1 [ESC[38;5;2m12ceda02ESC[0m]: Could not copy a suitable cache hit for 12ceda02:test.task_A:-1:1. No copy attempts were made. Based on [StackOverflow, the issue seems to be simply that subqueries must be aliased.](https://stackoverflow.com/q/1888779/4107809) Is MariaDB not supported? . The workflow runs jobs that complete as normal. When rerunning, no call caching results are used, and all jobs simply run again. . Cromwell connects to the call caching database and successfully creates tables, for example `CALL_CACHING_AGGREGATION_ENTRY`. . <!-- Which backend are you running? -->; I am running with a SLURM backend. . <!-- Paste/Attach your workflow if possible: -->; I have a very simple example workflow. ; ```; workflow test{; call task_A {}; }. task task_A{; command{; echo 'testing'; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:2203,echo,echo,2203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,1,['echo'],['echo']
Availability,":55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] JobStoreActor stopped; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor stopped; [2023-02-04 08:55:17,33] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] KvWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] DockerHashActor stopped; [2023-02-04 08:55:17,34] [info] IoProxy stopped; [2023-02-04 08:55:17,34] [info] ServiceRegistryActor stopped; [2023-02-04 08:55:17,37] [info] Database closed; [2023-02-04 08:55:17,37] [info] Stream materializer shut down; [2023-02-04 08:55:17,40] [info] Automatic shutdown of the async connection; [2023-02-04 08:55:17,40] [info] Gracefully shutdown sentry threads.; [2023-02-04 08:55:17,40] [info] Shutdown finish",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:16484,down,down,16484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,12,['down'],['down']
Availability,":56:30,264 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - 384e88c5-eba8-400c-aaef-5d618ffdce88-SubWorkflowActor-SubWorkflow-SplitRG:-1:1 [UUID(384e88c5)]: Starting calls: SplitLargeRG.SamSplitter:NA:1; 2018-01-17 20:56:30,293 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: `set -e; mkdir output_dir. total_reads=$(samtools view -c /cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam). java -Dsamjdk.compression_level=2 -Xms3000m -jar /usr/gitc/picard.jar SplitSamByNumberOfReads \; INPUT=/cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam \; OUTPUT=output_dir \; SPLIT_TO_N_READS=48000000 \; TOTAL_READS_IN_INPUT=$total_reads`; 2018-01-17 20:56:36,955 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: job id: operations/EOvc7beQLBiwi6fk-aX5yBEgqeCbgo4VKg9wcm9kdWN0aW9uUXVldWU; 2018-01-17 20:56:48,780 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: Status change from - to Running; ```. Here is the scattered [SomaticPairedSingleSampleWf.scattered.txt](https://github.com/broadinstitute/cromwell/files/1641317/SomaticPairedSingleSampleWf.scattered.txt) runnable version that gets stuck running. and the non scattered [SomaticPairedSingleSampleWf.single.txt](https://github.com/broadinstitute/cromwell/files/1641318/SomaticPairedSingleSampleWf.single.txt) runnable version that works great. Here is the dependencies zip [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1641320/SomaticPairedSingleSampleWfDependencies.zip). @kcibul i was asked to ping you on this issue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156:12252,ping,ping,12252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156,1,['ping'],['ping']
Availability,:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1218,recover,recover,1218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,":NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13084,error,error,13084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,2,"['Error', 'error']","['Error', 'error']"
Availability,; 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:7688,robust,robustExecuteOrRecover,7688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['robust'],['robustExecuteOrRecover']
Availability,"; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Position: 73; 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440); 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183); 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308); 	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441); 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365); 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307); 	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293); 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270); 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:266); 	at com.zaxxer.hikari.pool.ProxyStatement.execute(ProxyStatement.java:95); 	at com.zaxxer.hikari.pool.HikariProxyStatement.execute(HikariProxyStatement.java); 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:352); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:37439,ERROR,ERROR,37439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/inputs/download.rockylinux.org/pub/rocky/9/isos/aarch64/Rocky-9.3-aarch64-minimal.iso; 2023-12-20 18:42:08.431 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipelin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:1737,down,download,1737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['down'],['download']
Availability,"; Int max_retries_or_default = select_first([max_retries, 2]). Boolean compress = select_first([compress_vcfs, false]); Boolean run_ob_filter = select_first([run_orientation_bias_mixture_model_filter, false]); Boolean make_bamout_or_default = select_first([make_bamout, false]); Boolean run_funcotator_or_default = select_first([run_funcotator, false]); Boolean filter_funcotations_or_default = select_first([filter_funcotations, true]). # Disk sizes used for dynamic sizing; Int ref_size = ceil(size(ref_fasta, ""GB"") + size(ref_dict, ""GB"") + size(ref_fai, ""GB"")); Int tumor_reads_size = ceil(size(tumor_reads, ""GB"") + size(tumor_reads_index, ""GB"")); Int gnomad_vcf_size = if defined(gnomad) then ceil(size(gnomad, ""GB"")) else 0; Int normal_reads_size = if defined(normal_reads) then ceil(size(normal_reads, ""GB"") + size(normal_reads_index, ""GB"")) else 0. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = if defined(funco_data_sources_tar_gz) then ceil(size(funco_data_sources_tar_gz, ""GB"") * 3) else 100; Int gatk_override_size = if defined(gatk_override) then ceil(size(gatk_override, ""GB"")) else 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size + select_first([emergency_extra_disk,0]). # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = basename(basename(tumor_reads, "".bam""),"".cram"") #hacky way to strip either .bam or .cram; String unfiltered_name = output_basename + ""-unfiltered""; String filtered_name = output_basename + ""-filtered""; String funcotated_name = output_basename + ""-funcotated"". String output_vcf_name = output_basename + "".vcf"". Int tumor_cram_to_bam_disk = ceil(tumor_reads_size * cram_to_bam_multiplier); Int normal_cram_to_bam_disk = ceil(normal_reads_size * cram_to_bam_multiplier). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:9983,down,downloads,9983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['down'],['downloads']
Availability,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:7388,down,down,7388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,13,['down'],['down']
Availability,"; ```java; 2017-02-22 13:49:48,448 cromwell-system-akka.dispatchers.api-dispatcher-23 INFO - Workflow 56b1f228-8054-4947-8dc3-372363c5e94b submitted.; 2017-02-22 13:49:57,791 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - 1 new workflows fetched; 2017-02-22 13:49:57,792 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - WorkflowManagerActor Starting workflow UUID(56b1f228-8054-4947-8dc3-372363c5e94b); 2017-02-22 13:49:57,798 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - WorkflowManagerActor Successfully started WorkflowActor-56b1f228-8054-4947-8dc3-372363c5e94b; 2017-02-22 13:49:57,799 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2017-02-22 13:49:58,220 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - MaterializeWorkflowDescriptorActor [UUID(56b1f228)]: Call-to-Backend assignments: ; 2017-02-22 13:49:58,373 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - Unexpected engine failure; java.lang.RuntimeException: Unexpected engine failure; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:384); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$2.applyOrElse(WorkflowExecutionActor.scala:77); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$2.applyOrElse(WorkflowExecutionActor.scala:75); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:1176,ERROR,ERROR,1176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:1754,error,error,1754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['error'],['error']
Availability,"<!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Bug is full covered here:; https://github.com/sbt/sbt/issues/7117. The error I was getting was this:; ```; Exception in thread ""sbt-socket-server"" java.lang.NoClassDefFoundError: Could not initialize class org.scalasbt.ipcsocket.JNIUnixDomainSocketLibraryProvider; 	at org.scalasbt.ipcsocket.UnixDomainSocketLibraryProvider.get(UnixDomainSocketLibraryProvider.java:26); 	at org.scalasbt.ipcsocket.UnixDomainSocketLibraryProvider.maxSocketLength(UnixDomainSocketLibraryProvider.java:31); 	at sbt.internal.server.Server$$anon$1$$anon$2.$anonfun$run$1(Server.scala:75); 	at scala.util.Try$.apply(Try.scala:213); 	at sbt.internal.server.Server$$anon$1$$anon$2.run(Server.scala:63); Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.UnsatisfiedLinkError: darwin/aarch64/libsbtipcsocket.dylib not found on classpath [in thread ""main""]; ```; <!-- Which backend are you running? -->; This was trying to load set 1.10.2 in the cromwell directory. So I edited project/build.properties to have. sbt.version=1.8.2. And everything worked. Share I submit a PR with that change?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7564:176,error,error,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7564,1,['error'],['error']
Availability,"<!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Validating a workflow with circular imports causes a stack overflow in womtool. ## Expected behavior; I'm the fool who wrote a workflow with circular imports, but if womtool is here to check for errors, I'd like for it to suggest exactly how I'm being foolish. Something like miniwdl's output would work, where it follows the trail of imports and eventually says ""hey, this could be circular.""; ```; >miniwdl check ../fairyland/ld-pruning/ld-pruning-wf.wdl. (../fairyland/ld-pruning/ld-pruning-wf.wdl Ln 5 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6964:300,error,errors,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6964,1,['error'],['errors']
Availability,<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Submitting an empty file as a `workflowInputs` causes cromwell to return an unhelpful ; `There was an internal server error.%`. <!-- Which backend are you running? -->; It happens on both PAPIv1 and PAPIv2 (tested on cromwell 33 and 34) . <!-- Paste/Attach your workflow if possible: -->; running the following:. ` ``; curl -s -v -F workflowSource=empty -F workflowInputs=empty https://cromwell-v34.dsde-methods.broadinstitute.org/api/workflows/v1; ```; results in:; ```; There was an internal server error.%; ```. The file `empty` is an empty file. It would be nice if it returned a more useful error message to tell you what the problem was.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4086:215,error,error,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086,3,['error'],['error']
Availability,"<!-- Which backend are you running? -->; Backend: AWS backend. Link to Jira: https://broadworkbench.atlassian.net/browse/CROM-6712. Issue: ; From time to time I get the following error running a workflow on AWS. ```java; Could not build the path \""s3://bean-cromwell/cromwell-execution\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Failures: \ns3: Unable to load region from any of the providers in the chain software.amazon.awssdk.regions.providers.DefaultAwsRegionProviderChain@7c812b7e: [software.amazon.awssdk.regions.providers.SystemSettingsRegionProvider@759440a5: Unable to load region from system settings. Region must be specified either via environment variable (AWS_REGION) or system property (aws.region)., software.amazon.awssdk.regions.providers.AwsProfileRegionProvider@2a8c03c1: No region provided in profile: default, software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider@1bafe7dd: Unable to contact EC2 metadata service.] (SdkClientException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; ```. This usually happens to one task generated from a scatter task.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6233:179,error,error,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6233,2,"['Failure', 'error']","['Failures', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082:891,error,error,891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:917,error,error,917,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:798,error,error,798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,2,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:842,echo,echo,842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['echo'],['echo']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend: Local. Several basic array functions are not working on optional arrays. Test code passes wdltool validate with no errors.; Tested types: Array[String]? and Array[Int]?; ### Length(); WDL code:; ```; Array[String]? strings; Int num = length(strings); ```; Error:; ```; [2018-10-08 13:12:09,55] [error] WorkflowManagerActor Workflow 3dfb9c92-4e2e-4754-a35e-cfcbf9d6c006 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.num:; length() expects one parameter of type Array but got one parameter of type Array[String]?; ```; ### Indexing; WDL code:; ```; Array[String]? strings. scatter (idx in range(4)) { # strings is provided in the JSON file as an array of 4 strings; call testtask{input: str=strings[idx]}; }; ```; Error:; ```; [2018-10-08 13:27:31,22] [error] WorkflowManagerActor Workflow c2ac7273-c209-4e74-b1f0-a208e89922d8 failed (during ExecutingWorkflowState): Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); wdl4s.wdl.WdlExpressionException: Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); ```; ### Zip(); WDL code:; ```; Array",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218:767,error,errors,767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218,3,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587:1263,error,error,1263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4595:718,error,error,718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595,2,"['avail', 'error']","['available', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:685,error,error,685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,3,"['Error', 'error']","['Error', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I donâ€™t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4602:932,error,error,932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602,3,"['Avail', 'error']","['Available', 'error', 'errors']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. # Description. I believe this is a bug. I tried to use `stderr()` in the `output` section of a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre err",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:862,error,error,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,2,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. I executed `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:812,error,error,812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Default application.json not found in classpath in precompiled jar on github (affecting multiple releases tested version 86 and 85). ; Tested to not be affected version 79/56. Main error below. Should be an easy fix. . ```; Exception in thread ""main"" com.typesafe.config.ConfigException$IO: application: application.conf: java.io.IOException: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:652,error,error,652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Hi!. We've been looking into migrating from PAPIv2 backend to GCPBATCH backend. Callcaching fails on GCPBATCH but not on PAPIv2 when using a private docker image in gcr.io. ; Is this a missing feature or a bug? The documentation on the subject could go either way, depending on whether GCPBATCH is part of the other backends or a subset of the pipelines backend (https://cromwell.readthedocs.io/en/latest/cromwell_features/CallCaching/). ; I do not think this is a configuration error, since the same config works with PAPIv2 backend, but if it is, what configuration options would be necessary for configuring gcr.io authentication when using GCPBATCH?. Errors from cromwell logs when task is being callcached:; ```; cromwell_1 | 2024-01-11 11:09:38 pool-9-thread-9 INFO - Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithoutHash(Some(eu.gcr.io),Some(project),image_name,tag); cromwell_1 | cromwell.docker.registryv2.DockerRegistryV2Abstract$Unauthorized: 401 Unauthorized {""errors"":[{""code"":""UNAUTHORIZED"",""message"":""You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:950,error,error,950,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; The GCP batch backend preemption handling seems to have issue. When preemption happens the job had very high possibility to be error. The typical error would be : time=â€œâ€¦â€ level=error msg=â€œerror waiting for container:â€ . It will take the preempt events as the error from Cromwell logs. However, in the google batch console, it shows clearly ""preemption notice has received and will be processed"". . <!-- Which backend are you running? -->; GCP batch ; <!-- Paste/Attach your workflow if possible: -->; The workflow works perfectly in GCP life science backend; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407:598,error,error,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407,5,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi, . I wrote my first WDL (yay!) and troubleshot it locally using miniwdl. Now, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:553,error,error,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,3,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi,. Since last week, our cromwell server instance on GCP started to encounter the following error in all the jobs:. ```; 2024-07-31 19:08:59 cromwell-system-akka.dispatchers.backend-dispatcher-35 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:09:33 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:10:06 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); ```. However, with `Unknown Error` message, I don't know where to go for help. Do you have any suggestion?. Here are the configurations:. * Cromwell v85; * Genomics API; * PAPIv2 with `actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""`. Many thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7482:459,error,error,459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7482,11,"['Error', 'error', 'failure']","['Error', 'error', 'failures']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. I believe there is an error in the information provided by the Reference Disk Support in the documents for using reference disk images in gcpbatch. I believe there is a ""["" missing or this bit is additional as there are 3 ""["" present in the example. When trying to run this on my cromwell config, I get a syntax error. This is regarding using GCPBatch and not PAPI V2 as mentioned in some examples:. This is what I think it should look like:. ``` ; reference-disk-localization-manifests = [ ; {; ""imageIdentifier"": ""projects/pmc-bdc-private-test-cromwell/global/images/omics-reference-disk-image"",; ""diskSizeGb"": 10, ; ""files"": [ ; {; ""path"": ""pmc-bdc-test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.dict"",; ""crc32c"": 2158779318; },; {; ""path"": ""pmc-bdc-test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta"",; ""crc32c"": 420322484; },; {; ""path"": ""pmc-bdc-test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai"",; ""crc32c"": 1970999569; }; ]; }; ] ; ```; Not sure if reference-disk-localization = [] is also correct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7486:388,error,error,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7486,2,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. I'm using Cromwell v87 on GCP Genomics API. When submitting a job, the error I got is the following:. ```; Caused by: java.lang.IllegalStateException: You are currently running with version 2.2.0 of google-api-client. You need at least version 1.31.1 of google-api-client to run version 1.32.1 of the Genomics API library.; at com.google.common.base.Preconditions.checkState(Preconditions.java:534); at com.google.api.client.util.Preconditions.checkState(Preconditions.java:113); at com.google.api.services.genomics.v2alpha1.Genomics.<clinit>(Genomics.java:44); ... 12 common frames omitted; ```. It seems that I need to downgrade the version of `google-api-client`. However, I don't know how to do it on my machine. Could anyone help? Thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7481:437,error,error,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7481,2,"['down', 'error']","['downgrade', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; Hi, I would like to get help on troubleshooting.; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; I am running the workflow [here](https://github.com/PacificBiosciences/pb-human-wgs-workflow-wdl) using Cromwell 73-04a69e5 on Azure with GA4GH TES backend.; The workflow fails with the following stack trace, but I couldn't find a clue what specific tasks/jobs Cromwell had the issues with. The backend doesn't look to log any failures. Would you guide me how to identify the tasks/jobs where the error occurred? Thanks!. > Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: 2022-09-01 16:44:46,864 cromwell-system-akka.dispatchers.engine-dispatcher-27538 INFO - **WorkflowManagerActor: Workflow 2cd0993c-94df-4663-923d-48bbce3feead failed (during ExecutingWorkflowState): java.lang.Exception: The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.**; Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:275); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:209); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at scala.PartialFunction$OrElse.apply(Parti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6904:848,failure,failures,848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6904,2,"['error', 'failure']","['error', 'failures']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I am using `checkpointFile` in the `runtime` section of a WDL `task`. . I accidentally included a space in the checkpoint file name, and I see in the logs that this (probably) breaks checkpointing. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Log file shows; ```; CHECKPOINTING: Making local copy of /cromwell_root/noise_prompting_classical monocyte_H_shard0.csv; cp: can't create 'monocyte_H_shard0.csv-tmp/noise_prompting_classical': No such file or directory; cp: can't create 'monocyte_H_shard0.csv-tmp/monocyte_H_shard0.csv': No such file or directory; cp: can't create 'monocyte_H_shard0.csv-tmp/noise_prompting_classical': No such file or directory; CHECKPOINTING: Uploading new checkpoint content; ```. <!-- Which backend are you running? -->; Running on GCP via Terra. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When I remove the space in the filename, I see this in the logs, which appears to be working fine:. ```; CHECKPOINTING: Making local copy of /cromwell_root/noise_prompting_classical_monocyte_H_shard0.csv; CHECKPOINTING: Uploading new checkpoint content; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7441:378,checkpoint,checkpointFile,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7441,9,"['CHECKPOINT', 'checkpoint']","['CHECKPOINTING', 'checkpoint', 'checkpointFile', 'checkpointing']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I think the minimum to reproduce the bug is just. ```; Array[File] foo = []; Array[String]? bar = foo; ```. which fails with. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 'bar' (reason 1 of 1): Evaluating foo failed: assertion failed: base member type WomMaybeEmptyArrayType(WomStringType) and womtype WomMaybeEmptyArrayType(WomSingleFileType) are not compatible"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ],; ```. Interestingly enough, this passes if the array is non-empty, or if the target is not optional, or if the source is type `Array[String]`. I am running cromwell ""v85 (ish)"" according to the administrator. Backend is AWS batch.; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7399:498,failure,failures,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7399,1,['failure'],['failures']
Availability,"<< being discussed in Google Doc, copy here when it settles down >>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1342:60,down,down,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1342,1,['down'],['down']
Availability,"= 8BB8C81C27BFD2533FC9743A70F55DB1, file = 51C3D11209F9A7985345B2FD76E1C699.; [2022-12-15 21:28:23,82] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:4:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,86] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:0:1-20000000038 [788d8048main.all_qced_sample_lists:0:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,86] [info] BT-322 788d8048:main.all_qced_sample_lists:0:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 801EC388A847FBAB78685AE96643853A.; [2022-12-15 21:28:23,86] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:0:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(5), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(1), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(3), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(2), attempt 1); [2022-12-15 21:28:26,55] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:46110,failure,failures,46110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"= [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c504",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:1615,echo,echo,1615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['echo'],['echo']
Availability,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5453:2390,error,error,2390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453,3,['error'],['error']
Availability,"=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); out5d4c4459=""${tmpDir}/out.$$"" err5d4c4459=""${tmpDir}/err.$$""; mkfifo ""$out5d4c4459"" ""$err5d4c4459""; trap 'rm ""$out5d4c4459"" ""$err5d4c4459""' EXIT; tee '/cromwell_root/stdout' < ""$out5d4c4459"" &; tee '/cromwell_root/stderr' < ""$err5d4c4459"" >&2 &; (; cd /cromwell_root. /app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq; ) > ""$out5d4c4459"" 2> ""$err5d4c4459""; echo $? > /cromwell_root/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print0 | xargs -0 -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84/cromwell_glob_control_file. # hardlink or symlink all the files into the glob directory; ( ln -L /cromwell_root/*fastqc.zip /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 2> /dev/null ) || ( ln /cromwell_root/*fastqc.zip /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-c36f18b89b7c5f50e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:5175,echo,echo,5175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['echo'],['echo']
Availability,"=/). Task (stuff removed):. ```; task ValidateSamFile {; File input_bam; File? input_bam_index; String report_filename; File? ref_dict; File? ref_fasta; File? ref_fasta_index; Int? max_output; Array[String]? ignore; Int disk_size; Int preemptible_tries. command {; java -Xmx4000m -jar stuff.jar blah; }; runtime {; memory: ""7 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File report = ""${report_filename}""; }; }; ```. Call (note there is no value supplied for max_output):. ```; call ValidateSamFile as ValidateReadGroupSamFile {; input:; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; input_bam = SortAndFixReadGroupBam.output_bam,; report_filename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".validation_report"",; disk_size = flowcell_medium_disk,; preemptible_tries = preemptible_tries; }; ```. error in server logs:; ```; 2017-01-23 15:09:09 [cromwell-system-akka.actor.default-dispatcher-89] ERROR c.b.i.j.JesAsyncBackendJobExecutionActor - JesAsyncBackendJobExecutionActor [UUID(8f35e32d)PairedEndSingleSampleWorkflow.Vali; dateReadGroupSamFile:1:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1943:1283,ERROR,ERROR,1283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943,1,['ERROR'],['ERROR']
Availability,"=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:2042,failure,failures,2042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failures']
Availability,@Horneth commented on [Mon Sep 11 2017](https://github.com/broadinstitute/wdl4s/issues/204). CWL has a lot of meta information that is currently not available in WOM.; This could be part of a larger work and include re-work of WDL runtime attributes but those bits need to make it to Cromwell somehow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2726:149,avail,available,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2726,1,['avail'],['available']
Availability,"@Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48). ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261072803). The attached wdl results in an error message:. `Workflow has invalid declarations: : AggregatedException: : VariableNotFoundException: Variable 'generateArray' not found`. [scratch_3.wdl.txt](https://github.com/broadinstitute/wdl4s/files/595927/scratch_3.wdl.txt). ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261089538). @meganshand . This is actually a different problem - Cromwell doesn't support (yet) Workflow Declarations that reference call outputs. This wouldn't work either:. ```; task t {; command {; echo ""hello""; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t; String declarationDependingOnCallOutput = t.o; }; ```. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261092137). Oh no! This actually makes using zips infeasible, since I'd imagine in most cases the things you want to zip will be outputs from previous tasks. I suppose I can use a workaround where inside of a scatter loop I can create a task that takes in a File and Array[File] and outputs a Pair, then scatter over the output of that task outside of the original scatter. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095003). I tried that workaround with a task like this:. ```; task ZipUpWorkaround {; File unmapped_bam; Array[File] fastqs. command {; #do nothing; }; output {; Pair[File, Array[File]] p = [unmapped_bam, fastqs]; }; }; ```. and got this error message (after it submitted that task):; `Failed to evaluate outputs.: WdlTypeException: Arrays/Maps must have homogeneous types`. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:246,error,error,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,2,"['echo', 'error']","['echo', 'error']"
Availability,"@Horneth in https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairString",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:831,echo,echo,831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['echo'],['echo']
Availability,"@MatthewMah commented on [Thu Jun 15 2017](https://github.com/broadinstitute/wdltool/issues/32). The following example passes validation, and I think it should not. I think validation should be able to identify that a nonexistent output field is trying to be read. . ```; workflow ShouldNotValidate{; 	call A{}; 	Array[File] simple = [A.nonexistent]; 	call B{ input:; 		in = simple; 	}; }. task A{; 	command{; 		echo ""A"" > out; 	}; 	output{; 		File out = ""out""; 	}; }. task B{; 	Array[File] in. 	command{; 		cat ${sep=' ' in}; 	}; 	output{; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2881:412,echo,echo,412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2881,1,['echo'],['echo']
Availability,"@anton-khodak commented on [Wed Jan 25 2017](https://github.com/broadinstitute/wdltool/issues/22). I use `wdltool` to parse descriptions from the main repository, for instance, [this one](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_dsde_workflows/ValidateBamsWf_170107.wdl) and [this](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_dsde_workflows/ConvertPairedFastQToUnmappedBamWf_170107.wdl) . Both descriptions have valid syntax (`validate` and `parse` run smoothly). However, when I run `highlight` on either of them, I get the following error:; ``` ; $ java -jar ~/Downloads/wdltool-0.8.jar highlight ""/media/anton/ECFA959BFA95631E/Programming/wdl2cwl/ValidateBamsWf_170107.wdl"" console. Exception in thread ""main"" scala.MatchError: [Declaration type=Array[File] name=validation_reports expr=Some(ValidateBAM.validation_report)] (of class wdl4s.WorkflowOutput); at wdl4s.formatter.SyntaxFormatter.wdl4s$formatter$SyntaxFormatter$$formatScope(SyntaxFormatter.scala:188); at wdl4s.formatter.SyntaxFormatter$$anonfun$4.applyOrElse(SyntaxFormatter.scala:153); at wdl4s.formatter.SyntaxFormatter$$anonfun$4.applyOrElse(SyntaxFormatter.scala:153); at scala.PartialFunction$$anonfun$runWith$1.apply(PartialFunction.scala:141); at scala.PartialFunction$$anonfun$runWith$1.apply(PartialFunction.scala:140); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.collect(TraversableLike.scala:271); at scala.collection.AbstractTraversable.collect(Traversable.scala:104); at wdl4s.formatter.SyntaxFormatter.wdl4s$formatter$SyntaxFormatter$$formatWorkflow(SyntaxFormatter.scala:153); at wdl4s.formatter.SyntaxFormatter$$anonfun$2.applyOrElse(SyntaxFormatter.scala:73); at wdl4s.formatter.SyntaxFormatter$$anonfun$2.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2878:580,error,error,580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2878,2,"['Down', 'error']","['Downloads', 'error']"
Availability,"@cjllanwarne commented on [Fri Sep 15 2017](https://github.com/broadinstitute/wdl4s/issues/217). EG this can be converted from WDL to WOM:; ```; import ""import_me.wdl"" as import_me. workflow outer {; ; Array[Int] xs; scatter (x in xs) {; 	Boolean b; if (b) {; call import_me.inner as inner { input: i = x }; }; }; output {; Array[String?] outer_out = inner.out; }; }; ```. But if we move the `b` outside the scatter:; ```; import ""import_me.wdl"" as import_me. workflow outer {; Boolean b; Array[Int] xs; scatter (x in xs) {; if (b) {; call import_me.inner as inner { input: i = x }; }; }; output {; Array[String?] outer_out = inner.out; }; }; ```. Then we get an error:; ```; Exception in thread ""main"" java.lang.Exception: Can't build WOM executable from WDL namespace:; No input b found evaluating inputs for expression b; key not found: b; ```. ---. @Horneth commented on [Mon Sep 18 2017](https://github.com/broadinstitute/wdl4s/issues/217#issuecomment-330214878). This has implications in Cromwell. Namely if `b` was a Call instead of being a boolean, and `import_me.inner` depended on an output of `b`, when we evaluate the inputs of `import_me.inner` it will make a difference whether or not `b` is a sibling of `import_me.inner`. If it is we want to get the output with the same shard number from the output store, otherwise the output with no index (if we rule out nested scatters). We could simplify and say ""always look for the same index and if it's not there take the output with no index"" but it would be better to know for sure which one we need. ---. @mcovarr commented on [Fri Sep 22 2017](https://github.com/broadinstitute/wdl4s/issues/217#issuecomment-331568932). Sorry if this is a dumb question, but do you understand what's going wrong here? It's obvious looking at this statically what `b` is supposed to be whether it's inside or outside the scatter. Also it seems a little weird to me that `b` can even be a `GraphInputNode` inside the scatter...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2724:663,error,error,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2724,1,['error'],['error']
Availability,"@cjllanwarne commented on [Mon May 22 2017](https://github.com/broadinstitute/wdl4s/issues/112). This problem presents itself when using `wdltool` but it looks like it's a match error coming from inside `WDL4S`. null.wdl:; ```; task empty{; command {}; output {; File out = ""${output}""; }; }; ```. On validate:; ```; $ java -jar target/scala-2.12/wdltool-0.11.jar validate ~/myWorkflows/null.wdl; null; ```. We can see more details when we try to graph it:; ```; $ java -jar target/scala-2.12/wdltool-0.11.jar graph ~/myWorkflows/null.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:44); 	at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 	at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 	at wdl4s.expression.ValueEvaluator.replaceInterpolationTag(ValueEvaluator.scala:20); 	at wdl4s.expression.ValueEvaluator.$anonfun$interpolate$2(ValueEvaluator.scala:33); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2703:178,error,error,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2703,1,['error'],['error']
Availability,"@cjllanwarne helped me with this issue but its led to new ones:; https://github.com/broadinstitute/cromwell/issues/5793. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:418,error,error,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,2,['error'],['error']
Availability,@danbills commented on [Thu Aug 17 2017](https://github.com/broadinstitute/wdl4s/issues/177). Once it's available it would be nice to have scaladocs published for CWL as they are for WDL,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2715:104,avail,available,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2715,1,['avail'],['available']
Availability,"@davidbenjamin commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87). I have the following toy workflow, which I invoke with `-jar cromwell-25.jar run example.wdl empty_inputs.json`. It reads a tsv that has either one column or two and scatters a task over each row of the tsv. The task prints the second column if it is present. When the input `fake.tsv` has one column, everything is fine. However, when it has two columns eg; ```; 1</TAB>1; 2</TAB>2; ```; it fails with ""Could not construct array of type WdlMaybeEmptyArrayType(WdlOptionalType(WdlIntegerType)) with this value: List(WdlInteger(1), WdlInteger(2))"". (*Side question: why is it trying to make a list out of values in two different scattered rows?*) Using `Int?` in the conditional instead of `Int` does not make a difference. Another bizarre twist: if instead of reading in from a file I hardcode the array, the error persists when each row of the array has the same number of columns but goes away when some rows have two columns and some do not. That is: `Array[Array[Int]] table = [[1,1,1], [2,2]]` works, but `Array[Array[Int]] table = [[1,1], [2,2]]` gives the same error as above. ```; task printInt {; Int? int. command { echo ""${int}"" > out.txt }; output { File out = ""out.txt"" }; }. workflow optional {. Array[Array[Int]] table = read_tsv(""fake.tsv""); scatter (row in table) {. if (length(row) == 2) {; Int int = row[1]; }. call printInt {input: int=int }; }; }; ```. ---. @davidbenjamin commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241). Pinging @LeeTL1220 because this is blocking Mutect. ---. @LeeTL1220 commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277095282). @kcibul This is important. On Feb 2, 2017 4:34 PM, ""David Benjamin"" <notifications@github.com> wrote:. > Pinging @LeeTL1220 <https://github.com/LeeTL1220> because this is; > blocking Mutect.; >; > â€”; > You are receiving this because yo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1952:905,error,error,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1952,1,['error'],['error']
Availability,"@eddiebroad commented on [Wed Aug 10 2016](https://github.com/broadinstitute/wdltool/issues/12). I try to use wdltool validate on a WDL and it seems to _not_ catch; an error in the WDL. In the example ""bad"" WDL attached the call to VCF_to_MAF_task; has the line ""inputVCF=inputVCF"" but the ""inputVCF"" exists only; in the task but not at the workflow level ; but invoking wdltool 0.4 on it seems to _NOT_; cause an error. Shouldn't it be saying the WDL has an error; because inputVCF does _not_ exist at the workflow level?. I downloaded the wdltool from the latest release; https://github.com/broadinstitute/wdltool/releases/download/0.4/wdltool-0.4.jar. the two WDLs are attached. [wdl_files.zip](https://github.com/broadinstitute/wdltool/files/412067/wdl_files.zip). ```; wm8b1-75c:red_bug esalinas$ find *.wdl -exec java -jar wdltool-0.4.jar validate {} \;. wm8b1-75c:red_bug esalinas$ diff good.wdl bad.wdl ; 188c188; < inputVCF=inVCF,. ---; > inputVCF=inputVCF,; wm8b1-75c:red_bug esalinas$ ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2876:168,error,error,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2876,5,"['down', 'error']","['download', 'downloaded', 'error']"
Availability,"@egor-broad commented on [Fri Jun 09 2017](https://github.com/broadinstitute/wdltool/issues/31). The sample script is attached below.; Given a task call like that: ; ```; if (condition) {; call select as selectCondition {; input: ; sampleName=name, ; RefFasta=undeclaredVariable, #this variable does not exist; GATK=gatk, ; RefIndex=refIndex, ; RefDict=refDict, ; type=""SNP"", ; rawVCF=haplotypeCaller.rawVCF; }; }; ```; `wdltool inputs` command will generate the inputs just like everything is fine, but since the undeclaredVariable does not exist, when the task is executed, the run will exit with an error. You can try it with the attached script. ; However, this tool http://pb.opensource.epam.com:10000/ is able to notice the error (try and paste the script there and click ""build"", it will say ""Error: Undeclared variable is referenced: 'undeclaredVariable'). . [simpleVariantSelection.txt](https://github.com/broadinstitute/wdltool/files/1064633/simpleVariantSelection.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2880:602,error,error,602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2880,3,"['Error', 'error']","['Error', 'error']"
Availability,"@francares reports the following. He said that it disappeared in 29 but it's concerning that it just disappeared without (I think) intentionally being fixed. Reproduce and track this error down in 28 and then demonstrate that a) this *is* actually resolved in 29 and b) it didn't get ""resolved"" via another bug. ```; Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-57] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.StackOverflowError; 	at scala.collection.immutable.Set$EmptySet$.seq(Set.scala:68); 	at scala.collection.SetLike$class.$plus$plus(SetLike.scala:141); 	at scala.collection.AbstractSet.$plus$plus(Set.scala:47); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:27); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	...; ```; It's easy to reproduce, just run +100 hello world workflows and then query for all those workflows ids using query POST API.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450:183,error,error,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450,5,"['down', 'error']","['down', 'error']"
Availability,"@geoffjentry ; Here is an example of a situation:. ```; workflow wf { ; String firstname; String lastname; call Greeting as GreetingFirstName { input: name=firstname }; call Greeting as GreetingLastName { input: name=lastname }; }; task Greeting {; String name; String greeting; command {; echo ""${greeting} ${name}""; }; }; ```. The idea is to make it possible to give a value to greetings for all aliased tasks in json like so:. ```; {; ""wf.firstname"": ""Andrey"",; ""wf.lastname"": ""Smirnov"",; ""wf.Greeting.greeting"": ""howdy""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1335:290,echo,echo,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1335,1,['echo'],['echo']
Availability,"@geoffjentry commented on [Fri May 05 2017](https://github.com/broadinstitute/wdl/issues/109). **Needs Refinement**. We want to be able to define types for the values of objects. One suggestion was something like the following (note `struct` is using as a possible replacement for `object`, see below):. struct MyType {; o_f: File; x: Array[String]; }. MyType foo = read_object(...). It will coerce to the types it expects and if it can't that's a failure. Open questions:. - Do we make a new construct (e.g. `struct` above), or replace objects; - If replace, who (if anyone) is currently using `object`; - What's the right syntax, regardless of the name of the construct. This needs focus grouping.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283:448,failure,failure,448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283,1,['failure'],['failure']
Availability,"@geoffjentry commented on [Tue Apr 26 2016](https://github.com/broadinstitute/centaur/issues/36). Initially Centaur loaded in its files for each test (wdl, inputs, etc) using a function which would throw an exception if it wasn't there. As the framework has evolved it has moved to a model that is more robust and would more accurately report what was going on there. Modify these file slurps to play nicer with the rest of the system",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2885:303,robust,robust,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2885,1,['robust'],['robust']
Availability,"@helgridly commented on [Thu Apr 13 2017](https://github.com/broadinstitute/wdl4s/issues/103). I'm loading a WDL in wdl4s using `WdlNamespaceWithWorkflow.load(my_wdl, Seq())` -- i.e. passing no import resolvers. If the WDL contains imports, I'd expect it to fail and complain that it can't resolve the imports because I haven't specified any resolvers. (In the current code, that'd be [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/WdlNamespace.scala#L198).). However, what actually happens is that wdl4s tries to turn the import into a `File` object [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/Import.scala#L18). If that's not a valid kind of file path (perhaps because of a custom URI scheme, or because you're running Windows and colons aren't allowed in filenames), wdl4s blows up with some java-native exception. (In the Windows case, that's `java.nio.file.InvalidPathException`.). TLDR: wdl4s should throw a useful error if your WDL contains imports but you haven't specified resolvers. It probably shouldn't attempt to load the imports outside the context of a resolver, either. ---. @helgridly commented on [Thu Apr 13 2017](https://github.com/broadinstitute/wdl4s/issues/103#issuecomment-293936559). I discussed this with @Horneth and he's provided me with a workaround: I can check for the existence of imports in my code by loading the AST directly using something like `Option(ast).map(_.getAttribute(""imports"")).toSeq` (lifted from [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/WdlNamespace.scala#L185)). So consider this a non-urgent enhancement rather than a cloud of fire.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2701:992,error,error,992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2701,1,['error'],['error']
Availability,@hjfbynara has started testing the migration from 19 -> 25 and he noticed restart_and_recover_migration.xml seems to be throwing an unhelpful liquibase error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2108:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108,1,['error'],['error']
Availability,"@kshakir commented on [Mon Feb 27 2017](https://github.com/broadinstitute/wdl4s/issues/92). The following wdl currently parses in wdl4s without error, later causing an error within cromwell. ```; workflow undeclared_scatter_variable {; scatter (i in undeclared) {}; }; ```. See https://github.com/broadinstitute/cromwell/issues/2020 for more info of cromwell patches, including a centaur test, that may be updated or removed if this is fixed in wdl4s.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2699:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2699,2,['error'],['error']
Availability,"@kshakir commented on [Mon Feb 27 2017](https://github.com/broadinstitute/wdl4s/issues/93). The following wdl currently parses in wdl4s without error, later causing an error within cromwell. ```; task x {; Int i; command { echo $i }; runtime { docker: ""ubuntu"" }; output {; Int out_but_intentionally_misnamed = i; Boolean validOutput = i % 2 == 0; }; }. workflow missing_optional_output {; Array[Int] arr = [0,1,2,3]; scatter (i in arr) {; call x { input: i = i }; if (x.validOutput) {; Int x_out = x.out_except_undeclared; }; }. Array[Int?] x_out_maybes = x_out; }; ```. See https://github.com/broadinstitute/cromwell/issues/2020 for more info of cromwell patches, including a centaur test, that may be updated or removed if this is fixed in wdl4s.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2700:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2700,3,"['echo', 'error']","['echo', 'error']"
Availability,"@mcovarr commented on [Fri Aug 05 2016](https://github.com/broadinstitute/centaur/issues/96). ---. @cjllanwarne commented on [Tue Dec 20 2016](https://github.com/broadinstitute/centaur/issues/96#issuecomment-268376059). @mcovarr what's the context of this ticket?. ---. @ruchim commented on [Tue Dec 20 2016](https://github.com/broadinstitute/centaur/issues/96#issuecomment-268390892). I believe he's referring to the test in Centaur? I notice today the test was set to ignored, even though we offer this wf failure mode. Perhaps its just outdated, I'll try un-ignoring the test. ---. @mcovarr commented on [Mon Jan 30 2017](https://github.com/broadinstitute/centaur/issues/96#issuecomment-276110656). Only seeing this now! Yes I was referring to the test in Centaur, and I just confirmed it still fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2889:508,failure,failure,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2889,1,['failure'],['failure']
Availability,"@mcovarr commented on [Thu Aug 04 2016](https://github.com/broadinstitute/centaur/issues/95). It looks like most of the shards are successful, but one fails and might not be retried:. ```; 2016-08-03 15:20:01,502 cromwell-system-akka.dispatchers.backend-dispatcher-107 INFO - $a [UUID(eaeaa32d)DeliciousFileSpam.StringSpam:215:1]: JesAsyncBackendJobExecutionActor [UUID(eaeaa32d):DeliciousFileSpam.StringSpam:215:1] Status change from Running to Success; 2016-08-03 15:20:01,923 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: Job DeliciousFileSpam.StringSpam:215:1 succeeded!; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionFailedState. Shutting down.; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] done. Shutting down.; 2016-08-03 15:20:03,593 cromwell-system-akka.dispatchers.engine-dispatcher-85 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transitioning from ExecutingWorkflowState to FinalizingWorkflowState; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationPendingState to FinalizationInProgressState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationInProgressState to FinalizationSucceededState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatche",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2887:966,down,down,966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2887,1,['down'],['down']
Availability,"@mcovarr commented on [Thu Sep 07 2017](https://github.com/broadinstitute/wdltool/issues/48). Per the link below, enhance wdltool to be able to detect malformed expressions. Expressions that can't be evaluated are okay and expected due to values not being available, but malformed expressions are not okay. https://gatkforums.broadinstitute.org/wdl/discussion/10311/error-evaluating-output-files-that-serve-as-input-files-for-following-step#latest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2869:256,avail,available,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2869,2,"['avail', 'error']","['available', 'error-evaluating-output-files-that-serve-as-input-files-for-following-step']"
Availability,@mcovarr commented on [Tue Nov 14 2017](https://github.com/broadinstitute/wdltool/issues/51). This is for the benefit of womtool which is going to have to resort to forging inputs as a [stopgap measure](https://github.com/broadinstitute/cromwell/pull/2857). WdlNamespace should continue to make the current API available for production code.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2867:311,avail,available,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2867,1,['avail'],['available']
Availability,"@meganshand commented on [Fri Apr 29 2016](https://github.com/broadinstitute/wdltool/issues/9). The following task results in an uninformative error message when using `validate`: . ```; task printReads {; File bam; File ref_fasta; File ref_fasta_index; File ref_dict. command {; java -jar /usr/gitc/GenomeAnalysisTK.jar \; -T PrintReads \; -I ${bam} \; -o smaller.bam \; -L chr1 \; -R ${ref_fasta}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud:1.1010_with_gatk3.5""; disks: ""local-disk 400 SSD""; memory: ""10 GB""; }; output {; File smaller = smaller.bam; }; }; ```. results in this error message:. ```; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```. The problem is that there aren't quotes around ""smaller.bam"" in the outputs of the task. It would be great if the error message could tell you which line or object was causing the problem. The error message from cromwell is different, but also uninformative and it would be great if the error message could be clearer there as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2875:143,error,error,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2875,5,['error'],['error']
Availability,"@prefix"" ""["" <EMPTY_BLANK_NODE> <FULLIRI> <NODEID> <PNAME_LN> <PNAME_NS> org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser.parse(TurtleOntologyParser.java:58) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) Encountered unexpected token: ""<"" <ERROR> at line 1, column 1. Was expecting one of: ""("" ""@base"" ""@prefix"" ""["" <EMPTY_BLANK_NODE> <FULLIRI> <NODEID> <PNAME_LN> <PNAME_NS> org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.generateParseException(TurtleParser.java:1034) org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.jj_consume_token(TurtleParser.java:902) org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.parseDocument(TurtleParser.java:165) org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser.parse(TurtleOntologyParser.java:54) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:3216,ERROR,ERROR,3216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['ERROR'],['ERROR']
Availability,"@ruchim commented on [Thu Aug 24 2017](https://github.com/broadinstitute/wdltool/issues/46). When validating the workflow below with wdltool-0.14.jar, the response is simply ""null"". . ```; workflow w {; String s = ""test"". output {; String o =; }; }; ```; It would be great if a more comprehensive error is returned in the case of invalid workflow outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2883:297,error,error,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2883,1,['error'],['error']
Availability,"@ruchim commented on [Wed May 24 2017](https://github.com/broadinstitute/wdltool/issues/29). Given a task:. task myTask {; File f; command {; touch ${f.bam.bai}; }; }. A workflow with such a task validates in wdltool-0.10 but when run on cromwell-26, it fails with an error: java.lang.UnsupportedOperationException: Could not evaluate expression:.... Given a slightly altered version of that previous task:. task myTask {; File f; command {; touch ${f%%.bam.bai}; }; }. This task also validates but fails before the Workflow is about to run with the error: scala.MatchError: null. ---. @geoffjentry commented on [Wed May 24 2017](https://github.com/broadinstitute/wdltool/issues/29#issuecomment-303819742). Is this an artifact of wdltool being out of synch? it happens way too often :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2873:268,error,error,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2873,2,['error'],['error']
Availability,"@tmdefreitas commented on [Tue Mar 22 2016](https://github.com/broadinstitute/wdltool/issues/7). I came across this while helping a colleague debug her WDL file. When this WDL file is validated, wdltool claims an ""Error: finished parsing without consuming all tokens"", even though the error is commented out:. ```; task comment_bug {; #String an_input. command {; echo ""Alternate command""; # The following line has a WDL syntax error, but in a comment!; #echo {an_input} ; }. output {. }; }. workflow test {; call comment_bug; }; ```. EDIT: error message, for completeness:. ```; $ java -jar wdltool.jar validate comment_bug.wdl; ERROR: Finished parsing without consuming all tokens. output {; ^; ```. I can get rid of the error by changing the comment line to `#echo ${an_input}`. ; I think errors in comment lines should probably be ignored by the validator. As an aside, is there a more helpful error message for this? The message sounds like an unused input variable or something, not that the bracket syntax was off, so it was hard to diagnose (The above is a toy example, the real task had a much more complicated command). ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200416975). This is happening because the the `command {...}` section is parsed differently than the rest of WDL. The parser thinks that the closing brace in `#echo {an_input}` is actually trying to close the `command` section. If you use the alternative delimiters (`command <<< ... >>>`) this is another way to get around it. We parse the command as ""opaque strings intermixed with `${...}` blocks"". That means that the `#`-style comments inside a command section are not interpreted as WDL comments, but instead as part of the command. More thought would have to go into figuring out what the right thing to do here is. ---. @tmdefreitas commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200442613). Admittedly",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:214,Error,Error,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,11,"['ERROR', 'Error', 'echo', 'error']","['ERROR', 'Error', 'echo', 'error', 'errors']"
Availability,"@tmdefreitas commented on [Tue Mar 29 2016](https://github.com/broadinstitute/wdltool/issues/8). In the following WDL, **GSEA_v_1_0_fwer_p_val_threshold** was not declared as an input, but the validator didn't raise an error. Cromwell choked when trying to run the task. Is there a reason wdltool shouldn't throw an error here?. ```; task tool_gsea_mrnaseq_subtypes {; String outputprefix; String pheno_from_aggregate_molecular_subtype_clusters; String pheno_name; File tcga_pheno_FileName; File tcga_exp_FileName; File gs_db; String GSEA_v_1_0_reshuffling_type; String GSEA_v_1_0_nperm; String GSEA_v_1_0_weighted_score_type; String GSEA_v_1_0_nom_p_val_threshold; String GSEA_v_1_0_topgs; String GSEA_v_1_0_adjust_FDR_q_val; String GSEA_v_1_0_gs_size_threshold_min; String GSEA_v_1_0_gs_size_threshold_max; String GSEA_v_1_0_reverse_sign; String GSEA_v_1_0_perm_type. command {; /R/RunR.sh -f main /src/Pathway_GSEA.R --libdir/src --disease_type${outputprefix} --pheno.from.Aggregate_Molecular_Subtype_Clusters${pheno_from_aggregate_molecular_subtype_clusters} --pheno.name${pheno_name} --tcga.pheno.FileName${tcga_pheno_FileName} --tcga.exp.FileName${tcga_exp_FileName} --gs.db${gs_db} --GSEA.v.1.0.reshuffling.type${GSEA_v_1_0_reshuffling_type} --GSEA.v.1.0.nperm${GSEA_v_1_0_nperm} --GSEA.v.1.0.weighted.score.type${GSEA_v_1_0_weighted_score_type} --GSEA.v.1.0.nom.p.val.threshold${GSEA_v_1_0_nom_p_val_threshold} --GSEA.v.1.0.fwer.p.val.threshold${GSEA_v_1_0_fwer_p_val_threshold} --GSEA.v.1.0.fdr.q.val.threshold${GSEA_v_1_0_fdr_q_val_threshold} --GSEA.v.1.0.topgs${GSEA_v_1_0_topgs} --GSEA.v.1.0.adjust.FDR.q.val${GSEA_v_1_0_adjust_FDR_q_val} --GSEA.v.1.0.gs.size.threshold.min${GSEA_v_1_0_gs_size_threshold_min} --GSEA.v.1.0.gs.size.threshold.max${GSEA_v_1_0_gs_size_threshold_max} --GSEA.v.1.0.reverse.sign${GSEA_v_1_0_reverse_sign} --GSEA.v.1.0.perm.type${GSEA_v_1_0_perm_type}. zip -r ${outputprefix}.pathway_gsea_mrnaseq_subtypes.zip . ; }. output {; File zip_results=""${outputprefix}.pat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2874:219,error,error,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2874,2,['error'],['error']
Availability,"@vdauwera commented on [Sun Jan 08 2017](https://github.com/broadinstitute/wdltool/issues/20). I had a few issues that were fixed by building the latest from source. There's a good chance that will also be the case for the other issues that have been reported. Would be good to cut a release to make it available without requiring people to build from source. . ---. @knoblett commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281030248). Are we responsible for building a new release now that we own this repo? Or am I misremembering?. ---. @ruchim commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281054777). The wdltool release is a part of the Cromwell release process . ---. @knoblett commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281096658). Ah, thank you, that makes sense. @vdauwera do we still need a release? The most recent release was done 3 days prior to the creation of this ticket. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281097178). Does the release process include making a packaged jar? I think my problem was that there wasn't a jar that corresponded to the latest releases version available for download, come to think of it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2872:303,avail,available,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2872,3,"['avail', 'down']","['available', 'download']"
Availability,"A 503 StorageException seems to have failed one of the centaur JES jobs, and hence the workflow. Via the [cromwell.log](https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/travis/centaur_workflow/0310fa51-e985-4c54-8cdb-5058155f452e/call-centaur/cromwell_root/logs/). ```java; 2017-08-25 05:43:25,399 cromwell-system-akka.dispatchers.engine-dispatcher-51 ERROR - WorkflowManagerActor Workflow dabddbe7-a385-4df4-be97-c1ef7b884823 failed (during ExecutingWorkflowState): Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); java.lang.RuntimeException: Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:190); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:189); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:182); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.stan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:390,ERROR,ERROR,390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,3,"['ERROR', 'Failure', 'recover']","['ERROR', 'Failure', 'recoverWith']"
Availability,A bad file input should create an error on the BCS backend but is currently succeeding. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3522:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3522,1,['error'],['error']
Availability,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3675:15,down,down,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675,6,"['down', 'heartbeat']","['down', 'heartbeat', 'heartbeats']"
Availability,"A couple of separate ""belt-and-braces"" fixes to BW-478 which allowed errors in job preparation engine execution to leave workflows stuck in a zombie/Running/""call Starting"" state:. * Stop the engine function itself from throwing an exception; * Put a safety catch in the Job Execution actor to catch any other exceptions thrown by engine functions; * Put a safety catch in the `ErrorOr` `flatMap` function to automatically catch any exception thrown in `for`-comprehension chains.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6161:69,error,errors,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6161,2,"['Error', 'error']","['ErrorOr', 'errors']"
Availability,A couple of tactical centaur reliability tune-ups [BW-484],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6156:29,reliab,reliability,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6156,1,['reliab'],['reliability']
Availability,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654:348,error,error,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654,2,['error'],['error']
Availability,"A key feature for job manager is an ability to 'archive' a job so that it no longer shows in the UI when you don't want to see it anymore. Say it was a failure and you have dealt with the failure. You don't need to see it every time you look at recently failed jobs if you've dealt with it. The idea for implementing this was to have a label key called ""flag"" and have a value ""archived"" that we could apply to jobs a user wants to archive. The part we are struggling with is how to filter those out of view in the UI. If it's not difficult, the best way would be to be able to do queries where we say ""show me all the jobs that match this query and do not have the label `flag:archived` attached to them"". Is this something that would be possible?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3608:152,failure,failure,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3608,2,['failure'],['failure']
Availability,"A partial implementation of the WES 1.0 standard directly embedded in the Cromwell server (modulo auth, but that's another story altogether). Why partial? Because I wanted to keep PR sizes down and didn't want to be rebasing every 3 days. Also this lays the basic infrastructure, so might as well get commentary on that. It's not hurting anything to have a partial implementation. Why status & abort? Because they were the easiest to do.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425:189,down,down,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425,1,['down'],['down']
Availability,"A problem (#4364) existed with aliased tasks not being correctly separated on the AWS instance. . The test case relied on a file on the EC2 instance being written to by both tasks and failed if this happened. Unfortunately, the test case only works (ie fails correctly) if the two aliased tasks *happen to* run on the same EC2 instance. Come up with a way to make this test reliably fail if the aliasing is broken. Maybe by forcing centaur tests to always run on exactly one instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4848:374,reliab,reliably,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848,1,['reliab'],['reliably']
Availability,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057,2,['failure'],"['failure', 'failures']"
Availability,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5228:324,echo,echo,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228,5,['echo'],['echo']
Availability,"A significant amount of GotC failures are due to out of memory / disk errors.; Design a mechanism that allow to specify custom retry strategies that can modify runtime parameters based on failure modes. For example, â€œRetry on return code X with double the amount of memory and / or diskâ€. Thoughts:; - `currentAttempt()` wdl function to be used as a variable in a memory / disk formula; - monitor the job (monitoring script ?) to detect disk / memory overflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1847:29,failure,failures,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1847,3,"['error', 'failure']","['errors', 'failure', 'failures']"
Availability,"A simple rewrite of a CKTS test in Centaur, I'm curious to hear what the waterfowl think before going too far down this path. Some other CTKS tests I've looked at would require adding a minor feature or two to Centaur (e.g., assertions for exclusivity of outputs) but so far nothing major.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4503:110,down,down,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4503,1,['down'],['down']
Availability,"A task in a workflow failed because of the issue:; ```; Gsutil failed: Could not capture docker logs: Unable to capture docker logs exit status 1; ```; We could find all of the expected output files for the failed task in the cromwell-execution bucket, except `stdout.log` and `stderr.log` files. And we can make sure that this task only used nearly 2% of the disk space. Only 1 out of 8000 workflows ran into this issue, and re-run the failed workflow with call caching on didn't run into the same error, so it seems like an intermittent problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3615:499,error,error,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615,1,['error'],['error']
Availability,"A user (@helgridly) was flummoxed when they tried to use `read_json` to read a result back into WDL after running a python task. They had designed a method around the idea of being able to serialize and deserialize to/from JSON and had a bad experience at the last minute when these functions turned out to not be implemented. The less-than-ideal error message did not help matters. Although the general-purpose `read_json` might be dangerous, we could at least support something like `read_json_array`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1825:347,error,error,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825,1,['error'],['error']
Availability,"A user has a workflow where the output of a task is localized to a path ""/some/path/./blah/blah.txt"". A task downstream that goes to utilize that file path as input fails silently because it's trying at some point to convert that path to a RealPath (http://googlecloudplatform.github.io/google-cloud-java/0.7.0/apidocs/com/google/cloud/storage/contrib/nio/CloudStoragePath.html method:`toRealPath`) and it fails to do so as dot-dirs are not allowed. Something throws an exception `IllegalArgumentException - if path contains extra slashes or dot-dirs when permitEmptyPathComponents is false, or if the resulting path is empty.` and it's present in the server logs only and the workflow remains running and the task hangs in notstarted state. It would be great if dot-dirs were handled or atleast reported back as not-supported and fail the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2506:109,down,downstream,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2506,1,['down'],['downstream']
Availability,"A user requested this [here](https://gatkforums.broadinstitute.org/firecloud/discussion/10273/error-messages-should-include-the-problematic-input-whenever-possible-disk-strings). This user ran into an error message which stated he had an improperly formatted disk string, but with no further information on how to find out which disk string was incorrect. It would be better if the error message specifically referenced which disk string was improperly formatted, either through a line number, or by showing the specific incorrect disk string referenced. In addition, his improper formatting was `local-disk 0 HDD`, but the error message stated that the format must be `local-disk SIZE TYPE`. If you could add that `SIZE` must be a non-zero value, that would improve the error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2739:94,error,error-messages-should-include-the-problematic-input-whenever-possible-disk-strings,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2739,5,['error'],"['error', 'error-messages-should-include-the-problematic-input-whenever-possible-disk-strings']"
Availability,"A user requested this [here](https://gatkforums.broadinstitute.org/firecloud/discussion/10319/incorrect-error-message-when-task-fails-to-complete). . When a script fails to run, the error that is eventually returned is that Cromwell failed to delocalize the output file, due to the fact that the file is missing because the script failed to run. They would like the error message to be clearer as to the true reason of the failure, that it is due to an invalid return code from the script and (secondarily) a failure to delocalize the missing file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2740:104,error,error-message-when-task-fails-to-complete,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2740,5,"['error', 'failure']","['error', 'error-message-when-task-fails-to-complete', 'failure']"
Availability,"A very simple WDL:; ```; version development. task sfx {; input {; Array[String] x; String docker = ""ubuntu:20.04""; }. command <<<; echo ~{sep="" "" x}; >>>. output {; Array[String] x_sfx = suffix("".sfx"", x); }. runtime {; docker: docker; }; }. workflow sfx {; input {; Array[String] x; }. call sfx {; input:; x = x; }. output {; Array[String] x_sfx = sfx.x_sfx; }; }; ```; Will fail when run with the latest version of Cromwell:; ```; java -jar cromwell-51.jar run sfx.wdl -i sfx.json; ```; With the following error:; ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'suffix'; ```; Yes, the Array[String] [suffix](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#arraystring-suffixstring-arrayx)(String, Array[X]) function is part of the WDL specification. It seems like this should not be difficult to implement as the [prefix](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#arraystring-prefixstring-arrayx) function is already implemented.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5549:132,echo,echo,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5549,2,"['echo', 'error']","['echo', 'error']"
Availability,"A while back we changed things to return gzipped content in the metadata endpoint by default, despite what the client says it can handle. generally the client should be telling the server what it can expect and the server either handles it appropriately or returns a 415. I think that we should switch this to encoding on request. The downside is that there might be users who would be much better off getting a gzipped response but not realizing they have to do something extra (depending on their client).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2419:335,down,downside,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2419,1,['down'],['downside']
Availability,"A while back we had a fairly serious performance bug due to the usage of `mapValues`. It'd gone away for a while but it seems to be used somewhat frequently again. Currently 22 times in Cromwell. A quick skim of them made at least a few of them look to be pretty suspicious looking. Remove these, or at least prove that any remaining invocation can't possibly be a big deal down the road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2434:374,down,down,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434,1,['down'],['down']
Availability,"A workflow was submitted and the inputs looked like:; ```; ""inputs"": {; ""HaplotypeCallerGvcfScatterWf.ref_dict"": ""\""gs://gatk-legacy-bundles/b37/human_g1k_v37_decoy.dict\"""",; ""HaplotypeCallerGvcfScatterWf.MergeVCFs.disk_size"": 10,; ""HaplotypeCallerGvcfScatterWf.HaplotypeCaller.mem_size"": ""4 GB"",; ""HaplotypeCallerGvcfScatterWf.ref_fasta"": ""\""gs://gatk-legacy-bundles/b37/human_g1k_v37_decoy.fasta\"""",; ""HaplotypeCallerGvcfScatterWf.ref_fasta_index"": ""\""gs://gatk-legacy-bundles/b37/human_g1k_v37_decoy.fasta.fai\"""",; ""HaplotypeCallerGvcfScatterWf.input_bam_index"": ""gs://gatk-tutorials/workshop_1702/variant_discovery/data/bams/father.bai"",; ""HaplotypeCallerGvcfScatterWf.HaplotypeCaller.interval_padding"": 100,; ""HaplotypeCallerGvcfScatterWf.MergeVCFs.mem_size"": ""2 GB"",; ""HaplotypeCallerGvcfScatterWf.scattered_calling_intervals_list"": ""gs://gatk-test-data/intervals/b37_wgs_scattered_calling_intervals.txt"",; ""HaplotypeCallerGvcfScatterWf.input_bam"": ""gs://gatk-tutorials/workshop_1702/variant_discovery/data/bams/father.bam"",; ""HaplotypeCallerGvcfScatterWf.HaplotypeCaller.disk_size"": 100; },; ```; Cromwell started jobs but they don't have operations IDs and the jobs/workflow remain in the ""Running"" state forever. Ideally the workflow should fail, and fail gracefully with an error message that makes sense. This ticket can be easily reproduced on 25_hotfix by corrupting a file input path to look like:; ```; ""HaplotypeCallerGvcfScatterWf.ref_fasta_index"": ""\""gs://gatk-legacy-bundles/b37/human_g1k_v37_decoy.fasta.fai\"""",; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2178:1284,error,error,1284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2178,1,['error'],['error']
Availability,"AC: We require a unit test that recreates a deadlock occurs when workflow heartbeats are de-serialized (#4239). This is important because this is a problem we've seen before in production (prior to the serialization of heartbeats) and that's the *main* scenario we have to solve for as a requirement to be able to scale Cromwell horizontally. Since we've only really seen deadlock behavior in production, it seems running a unit test at high scale could be one possible way to reproduce the deadlocking error. For example:. Start 10K workflows (that just sleep) and configure the heartbeat-interval rate to be the shortest duration possible (or whatever is reasonable). One would have to check the Cromwell server logs to see if there's a SQL exception",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4414:74,heartbeat,heartbeats,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4414,4,"['error', 'heartbeat']","['error', 'heartbeat-interval', 'heartbeats']"
Availability,"AC:; Something like this should run:. JSON:; {; ""Test.testMe"":{""left"":""Left"",""right"":""Right""}; }. WDL:; workflow Test {; 	Pair[String,String] testMe; call echoPair{input: pair=testMe}; }. task echoPair{; Pair[String,String] pair; command{; echo (${pair.left}, ${pair.right); }; output {; String out = read_string(stdout()); }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2260:155,echo,echoPair,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2260,3,['echo'],"['echo', 'echoPair']"
Availability,"ASTDB=~{blastdb} ; blastn \; -query ~{fasta} -db nt -num_threads 24 -evalue 1 -outfmt '6' -out ~{out_file}; >>>; output { File out = out_file }; runtime { docker: ""ncbi/blast:2.10.1"" }; }; ```. **confiuration snippet - localization only:**; ```; filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; # Call caching strategies; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""md5""; check-sibling-md5: false; }; }; }; ```. **logs:**; ```; [2020-08-08 19:20:00,49] [error] Failed to hash ""../../data/blast/blastdb"": Is a directory; [2020-08-08 19:20:00,49] [warn] Localization via hard link has failed: /workflows/cromwell-executions/good_donor_good_recipient/f7947643-2729-483f-b987-44ef932f88bd/call-blaster/main/6e4fa8a1-0d72-486e-a9ae-254319c4915d/call-blaster/shard-20/inputs/2058596876/blastdb -> /data/blast/blastdb: Operation not permitted; [2020-08-08 19:20:00,49] [error] 6e4fa8a1:main.blaster:46:1: Hash error (Is a directory), disabling call caching for this job.; ```. **contents of the BLASTDB directory:**; ```; /data/blast/blastdb$ ls; nt.00.nhd nt.01.nhd nt.02.nhd nt.03.nhd nt.04.nhd nt.05.nhd nt.06.nhd nt.07.nhd nt.08.nhd nt.09.nhd nt.10.nhd nt.11.nhd nt.12.nhd nt.13.nhd nt.14.nhd nt.15.nhd nt.16.nhd nt.17.nhd nt.18.nhd nt.19.nhd nt.20.nhd nt.21.nhd nt.22.nhd nt.23.nhd nt.24.nhd nt.nal nt.00.nhi nt.01.nhi nt.02.nhi nt.03.nhi nt.04.nhi nt.05.nhi nt.06.nhi nt.07.nhi nt.08.nhi nt.09.nhi nt.10.nhi nt.11.nhi nt.12.nhi nt.13.nhi nt.14.nhi nt.15.nhi nt.16.nhi nt.17.nhi nt.18.nhi nt.19.nhi nt.20.nhi nt.21.nhi nt.22.nhi nt.23.nhi nt.24.nhi nt.ndb nt.00.nhr nt.01.nhr nt.02.nhr nt.03.nhr nt.04.nhr nt.05.nhr nt.06.nhr nt.07.nhr nt.08.nhr nt.09.nhr nt.10.nhr nt.11.nhr nt.12.nhr nt.13.nhr nt.14.nhr nt.15.nhr nt.16.nhr nt.17.nhr nt.18.nhr nt.19.nhr nt.20.nhr nt.21.nhr nt.22.nhr nt.23.nhr nt.24.nhr nt.nos nt.00.nin nt.01.nin nt.02.nin nt.03.nin nt.04.nin nt.05.nin nt.06.nin nt.07.nin nt.08.nin nt.09.nin nt.10.nin nt.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737:2073,error,error,2073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737,1,['error'],['error']
Availability,AWS backend: sleep 60 secs and retry 5 times if download or upload fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4749:48,down,download,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4749,1,['down'],['download']
Availability,AWS hello output parsing failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4373:25,failure,failure,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4373,1,['failure'],['failure']
Availability,AWS: Error building path,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6233:5,Error,Error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6233,1,['Error'],['Error']
Availability,AWS: Error creating reconfigured-script,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6106:5,Error,Error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106,1,['Error'],['Error']
Availability,"Accepts yaml input files during submission.; Whether the input is in yaml or json, it will be written in **json** in the database, as well as metadata.; Also `cats`ify things a little to get better error reporting on incorrect submit request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2832:198,error,error,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2832,1,['error'],['error']
Availability,"Accommodate ""enhanced"" Requester Pays error messages [CROM-6820]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556,1,['error'],['error']
Availability,"According to [the docs](https://dev.mysql.com/downloads/connector/j/) it is ""highly recommended"" that we upgrade to the latest version, even if our MySQL is behind - `5.6.36-google-log` in our case. Reason I started looking into this was https://github.com/broadinstitute/cromwell/issues/4689",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4690:46,down,downloads,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690,1,['down'],['downloads']
Availability,"According to https://cromwell.readthedocs.io/en/stable/backends/Google/; The format for the json file should be; ```; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. I followed this format but got this error; ```; [2022-11-20 18:17:16,88] [warn] Failed to build PipelinesApiConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:438,error,error,438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,3,"['Down', 'Error', 'error']","['DownField', 'Errors', 'error']"
Availability,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4724:372,echo,echo,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724,3,['echo'],['echo']
Availability,Actor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:749); 	 at scala.util.Try$.apply(Try.scala:213); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1139); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1131); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	 at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at akka.actor.Actor.aroundReceive(Actor.scala:539); 	 at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:7974,robust,robustExecuteOrRecover,7974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['robust'],['robustExecuteOrRecover']
Availability,Actor.scala:312); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.commandScriptContents(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:132) ; at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4279:2459,robust,robustExecuteOrRecover,2459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4279,1,['robust'],['robustExecuteOrRecover']
Availability,Add Recover to Backend Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/663:4,Recover,Recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/663,1,['Recover'],['Recover']
Availability,Add a case for 504 error in GCS IoActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344,1,['error'],['error']
Availability,Add a new API endpoint to CromIAM which will allow a user to update the collection name for one or more workflows in a one to many fashion - i.e. one collection name will be applied to 1+ workflows. If Sam says that the user does not have access to this collection name it will be an error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2839:284,error,error,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2839,1,['error'],['error']
Availability,Add a workflow with at least 3 calls. For the call which triggers the Cromwell restart make sure that it involves a long sleep or something like that so that we know the job is still considered to be running by Cromwell when we shut down the server.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2353:233,down,down,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2353,1,['down'],['down']
Availability,Add config option to shutdown cromwell when unable to write heartbeats.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4785:60,heartbeat,heartbeats,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785,1,['heartbeat'],['heartbeats']
Availability,Add details in failure when failing to upload auth file for JES,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1453:15,failure,failure,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1453,1,['failure'],['failure']
Availability,Add metadata for WorkflowMaterialiserActor Failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/966:43,Failure,Failure,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/966,1,['Failure'],['Failure']
Availability,Add new retryable case: JES error code 2. Message: Instance failed to start due to preemption.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970,1,['error'],['error']
Availability,Add non-preemption retryable JES error codes into JES wiring,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1926:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1926,1,['error'],['error']
Availability,Add recovery / abort to HtCondorBackend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/885:4,recover,recovery,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/885,1,['recover'],['recovery']
Availability,Add test to recreate MySQL heartbeat deadlock.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4415:27,heartbeat,heartbeat,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4415,1,['heartbeat'],['heartbeat']
Availability,Add the return code to JES error messages.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2686:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2686,1,['error'],['error']
Availability,Add workflow option to not run new job upon certain call cache errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2787:63,error,errors,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2787,1,['error'],['errors']
Availability,"Added a test to our CI. It will build docker images for `cromwell`, `womtool`, `cromiam`, and `cromwell-drs-localizer` in a way that is very similar to how we do it in our release process (`chart_update_on_merge.yml`). Build errors should now be caught earlier in the development process.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7151:225,error,errors,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7151,1,['error'],['errors']
Availability,Added heartbeats to swr and refactor kills BA-5983,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5165:6,heartbeat,heartbeats,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5165,1,['heartbeat'],['heartbeats']
Availability,Added index and attempt number to failure messages. Closes #1479,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1580:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1580,1,['failure'],['failure']
Availability,"Added methods for retrieving cwl File/Directory parts like basename.; Added a directory listing stub close to the existing glob listing code.; Updated CWL types to reuse more Polys and to parse ecmascripts with embedded newlines and extra whitespace.; Minor cleanup around throw/ErrorOr/Try conversions especially around Javascript processing.; JsUtil encoding now encodes WomMap/WomArray into instances of JsObject instead of java.util.Map/java.lang.Array.; Hacked JsUtil to support reading in ""structs"" of mixed types.; Replaced all usages of .stripMargin.replaceAll with .stripMargin.replace so that the replacements aren't processed like regexs.; Removed docs/.Rhistory and ./Running empty files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3124:279,Error,ErrorOr,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3124,1,['Error'],['ErrorOr']
Availability,"Adding `setThrowExceptionOnExecuteError(false)` call causes execution flow to go into the path we want that inspects the error more precisely in `handleGoogleError`. I tested this by deliberately breaking the request so it always gets a `400` back. With the existing code, the log looks just like what we see in prod:. ```; 2023-11-01 19:54:12 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - PAPI request worker had 2 failures making 5 requests: ; 400 Bad Request; POST https://lifesciences.googleapis.com/v2beta/projects/1005074806481/locations/us-central1/operations/6175597626605185257:cancel; {; ""code"": 400,; ""errors"": [; {; ""domain"": ""global"",; ""message"": ""Invalid JSON payload received. Unexpected token.\nasdf\n^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle."",; ""reason"": ""parseError""; }; ],; ""message"": ""Invalid JSON payload received. Unexpected token.\nasdf\n^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle."",; ""status"": ""INVALID_ARGUMENT""; }; ```. <img width=""1238"" alt=""Screenshot 2023-11-01 at 15 43 59"" src=""https://github.com/broadinstitute/cromwell/assets/1087943/63e4e788-517f-4f1e-a4ff-4075cec3e6d3"">. ---. Changing `throwExceptionOnExecuteError` to `false`, we see that we no longer throw an exception and we get the expected ""no longer running"" message in the log!. ```; 2023-11-01 19:57:48 cromwell-system-akka.dispatchers.backend-dispatcher-162 INFO - PAPI declined to abort job projects/1005074806481/locations/us-central1/operations/5250112889402522122 in workflow b70eafc9-66a7-4b22-b9bc-621c22b5a4ed, most likely because it is no longer running. Marking as finished. Message: Invalid JSON payload received. Unexpected token.; asdf; ^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle.; ```. <img width=""1239"" alt=""Screenshot 2023-11-01 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7245:121,error,error,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7245,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,Adding retries and proper failures to the creation of JES runs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/227:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/227,1,['failure'],['failures']
Availability,Additional Failure Metadata Inconstency,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2201:11,Failure,Failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201,1,['Failure'],['Failure']
Availability,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1091:82,down,down,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091,1,['down'],['down']
Availability,Address DB write failures for WorkflowStore and JobStore,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1223:17,failure,failures,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1223,1,['failure'],['failures']
Availability,"Addresses [WX-1282](https://broadworkbench.atlassian.net/browse/WX-1282). PR replaces the INNER JOIN statement against `pg_largeobject` with a `lo_get` statement to avoid ""Permission Denied"" errors that comes from scanning the `pg_largeobject` table (which enforces owner/role permissions for each row which needs to be taken in consideration for the new Workflows App ecosystem). [WX-1282]: https://broadworkbench.atlassian.net/browse/WX-1282?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228:191,error,errors,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228,1,['error'],['errors']
Availability,"Addresses [WX-1306](https://broadworkbench.atlassian.net/browse/WX-1306), [WX-1307](https://broadworkbench.atlassian.net/browse/WX-1307), [WX-1308](https://broadworkbench.atlassian.net/browse/WX-1308), [WX-983](https://broadworkbench.atlassian.net/browse/WX-983). PR creates a GHA (currently runs on dispatch, can be updated to run on schedule of choice) that creates a billing project and BEE, attaches the BEE to a static landing zone, creates a workspace and provisions an app within the BEE, submits a workflow (basic hello world) to Cromwell, and performs app/workspace/billing project cleanup afterwards. BEE template is flagged by Janitor for post workflow cleanup to ensure no lingering resources. Workspace deletion and billing project deletion are finicky due to invariable timing of the deletion itself (can be either extremely short or longer than 12 minutes), so those two steps are handled by either an exception block (workspace deletion) or `continue-on-error` (billing project) to ensure that failures there do not reflect a failure on the test against Cromwell. Workspace provisioning and app creation are necessary for running tests against Cromwell, so a failure there will be reported as a failure on the Cromwell test. (As an aside, this could be rectified by having a static testing app that's always running in a dedicated testing environment. Test could be updated to run submissions against it so as long as that app is kept up to date. [WX-1306]: https://broadworkbench.atlassian.net/browse/WX-1306?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1307]: https://broadworkbench.atlassian.net/browse/WX-1307?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1308]: https://broadworkbench.atlassian.net/browse/WX-1308?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-983]: https://broadworkbench.atlassian.net/browse/WX-983?atlOri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7236:970,error,error,970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7236,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,Adds a cron trigger for the centaur integration tests that cause the integration tests to run at midnight each weekday and post failures to slack at #cromwell-integration-action,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7089:128,failure,failures,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7089,1,['failure'],['failures']
Availability,Adds an event handler for a backend job that failed with a retry able failure. The engine attempts to restart the failed job again on the same backend for a `max` number of times (where that number is configurable and controlled by the engine),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/795:70,failure,failure,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/795,1,['failure'],['failure']
Availability,"Adds capability to. * Download metadata from GCS using the IoActor; * Fetch workflow labels from the database; * Combine the two into a single response. Currently tested using mock IoActor and ServiceRegistry, and not ""created"" by the main Cromwell process",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5100:22,Down,Download,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5100,1,['Down'],['Download']
Availability,Adjust expected failures for new CWL conformance tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3213:16,failure,failures,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3213,1,['failure'],['failures']
Availability,Adjust the error message for the Call Caching diff endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2406:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2406,1,['error'],['error']
Availability,"After running a job that completed (or so it seems by the files generated ... Cromwell labels it as failed), I tried to retrieve the metadata but I got this error message instead:; ```; {; ""status"": ""error"",; ""message"": ""Metadata for workflow xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx exists in database but cannot be served because row count of 1249471 exceeds configured limit of 1000000.""; }; ```; I have tried to understand what configuration variable holds this 1000000 row limits, but I could not figure it out. :-( I think it would save users valuable time if they were directed to what to do when these type of very specific errors are recognized. I know this is a little bit more work on the developer side but I would certainly be very grateful ... and I would also stop asking questions. :-)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6236:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236,3,['error'],"['error', 'errors']"
Availability,"After running a large workflow on GCS with ~2,500 tasks, rather than the workflow transitioning from running to success, I received the following error:; ```; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Workflow is making no progress but has the following unstarted job keys: \nScatterCollectorKey_PortBasedGraphOutputNode_xxx.yyy:-1:1\nConditionalCollectorKey_PortBasedGraphOutputNode_xxx.yyy:-1:1"",; ""causedBy"": []; }; ],; ```. The `xxx.yyy` output variable is from a task being scattered and defined as follows:; ```; task xxx {; ...; output {; ...; File? yyy = if defined(zzz) then ... else None; }; }; ```; With `zzz` not defined. Despite the error, the job seemed to have completed successfully. However the files were not moved into the `final_workflow_outputs_dir` as they were supposed to, causing an unwelcome inconvenience. This [problem](https://support.terra.bio/hc/en-us/community/posts/360073398892-Workflow-failure-Workflow-is-making-no-progress-but-has-the-following-unstarted-job-keys-) has also been reported about six months ago in the Terra forum. The job run with CallCaching activated but no entries in the cache were present before the job started. The only event of notice was that at some point Cromwell crashed due to high memory demand (while trying to retrieve the metadata for the workflow) but, after I restarted it, the workflow proceeded without issues. The workflow is a `version development` WDL, as can be evinced from the use of the `None` keyword.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6238:146,error,error,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6238,4,"['error', 'failure']","['error', 'failure-Workflow-is-making-no-progress-but-has-the-following-unstarted-job-keys', 'failures']"
Availability,"After this morning's discussion of Cromwell's memory usage I poked around the Travis docs and found the container infrastructure on which we were running gave us only 4 GB:. https://docs.travis-ci.com/user/ci-environment. Also on this page is mentioned the new Trusty Tahr beta environment which offers 7.5 GB. I've tried this and have seen no intermittent SlickDataAccess or other failures. As a bonus this is a real VM (not a container) and can run all of our Docker tests. The one gotcha I've found is that these builds don't start as quickly as the container builds, but if this delay remains reasonable it may be worth trading some lag for stability and Docker coverage. There were some additional changes required to the Travis YAML to add MySQL as that's not baked into Trusty, and I also had to pull the ubuntu:latest image in advance to keep the first Docker test from timing out. There was also a bit of weirdness with files not globbing in alphabetical order which broke a test.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/319:382,failure,failures,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/319,1,['failure'],['failures']
Availability,"After workflow succesffuly ran this failed with the below error when trying to copy the final outputs. ```; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(88b21d2d)]: failed to create call actor for PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log.; java.util.NoSuchElementException: None.get; at scala.None$.get(Option.scala:347) ~[cromwell.jar:0.19]; at scala.None$.get(Option.scala:345) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1$$anonfun$apply$26.apply(CallMetadataBuilder.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1$$anonfun$apply$26.apply(CallMetadataBuilder.scala:155) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:728) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:727) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1.apply(CallMetadataBuilder.scala:155) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1.apply(CallMetadataBuilder.scala:153) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$15.apply(CallMetadataBuilder.scala:233) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$15.apply(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Allow for the fact that errorMessage might be null,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/408:24,error,errorMessage,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/408,1,['error'],['errorMessage']
Availability,Allow to ask for retries on JES error 13,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1849:32,error,error,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1849,1,['error'],['error']
Availability,Allows PAPI/GLS jobs to run a background action to upload a checkpoint file to GCS at regular intervals. Notes from demo:. - [x] Remove checkpoint file when task completes; - [x] Change upload interval to 10m,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6137:60,checkpoint,checkpoint,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6137,2,['checkpoint'],['checkpoint']
Availability,Also cherry pick a couple of Khalid's CI fixes for unrelated failures.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5035:61,failure,failures,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5035,1,['failure'],['failures']
Availability,Also creates #852. Following things need to be done (separate stories?) :; - [ ] Add docker support (#884); - [ ] Add recovery and abort (#885); - [x] Add continueOnErrorCode support; - [ ] Find a way to add condor specific runtime attributes to Condor ClassAds (#886). Anything more to support?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/861:118,recover,recovery,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/861,1,['recover'],['recovery']
Availability,Also removes large chunks of the WDL writer that were only used for very specific error messages.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7382:82,error,error,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7382,1,['error'],['error']
Availability,Alternative title: kicking the SDAS can down the road until 0.21 (when these methods will make sense),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1080:40,down,down,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1080,1,['down'],['down']
Availability,"Alternative to #5588 which completely removes this redundant queuing mechanism which doesn't seem to be doing what it thinks it was doing, and is worse in any case than the existing token distribution safety mechanisms.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5590:51,redundant,redundant,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5590,1,['redundant'],['redundant']
Availability,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:284,recover,recover,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,1,['recover'],['recover']
Availability,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4013:220,avail,available,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013,2,"['avail', 'down']","['available', 'down']"
Availability,"An example from a couple of errors that failed two PapiV2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$trans",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:28,error,errors,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,4,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:364,ERROR,ERROR,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,An issue to address the user report: http://gatkforums.broadinstitute.org/gatk/discussion/8873/file-not-found-errors-when-using-call-caching/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1929:110,error,errors-when-using-call-caching,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1929,1,['error'],['errors-when-using-call-caching']
Availability,And another possible bug: why are we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:155,ERROR,ERROR,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,2,"['ERROR', 'recover']","['ERROR', 'recoverWith']"
Availability,Another CTKS test down.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4516:18,down,down,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4516,1,['down'],['down']
Availability,"Another in a long line of @geoffjentry written placeholder issues. While exploring WDL->CWL conversions as well as the potential to replace a strictly WDL object model with something more generic in the Cromwell engine I keep coming back to how to handle WDL expressions. Another ""problem"" (in quotes as it hasn't actually bitten us .... yet) is that expression evaluation is currently happening in an uncontrolled fashion within the engine - it is conceivable that if enough of these triggered at once that it could cause a lot of problems. This has been a nagging concern in the back of my mind for a long time now. Idea: Replace how we evaluate expressions by replacing them with a task upstream of the expression-ed task. These expression tasks shall run on the local backend allowing it to both be fast but also managed by our process throttling. As an example, a JES based `read_string` could involve a `gsutil` call in its `command` block to suck down the file or something similar for a `size` expression. This seems like it'd imply that there needs to be some universal primitives in Cromwell to effectively `read_XYZ` from the local filesystem. I'll admit to not having thought this all the way through. IMO this will improve the robustness/stability of the engine while making a big stride towards de-WDLing the underbelly of the engine. @kcibul note that I'm not asking for this to happen tomorrow but I do think it's an idea we should kick the tires on in the near term to see if it makes enough sense to pursue more deeply at a later point.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1618:954,down,down,954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618,2,"['down', 'robust']","['down', 'robustness']"
Availability,"Another tweak to logging (de)localization errors in PAPI v2.; As you'll notice we blindly retry everything (even a missing file), but that's another issue.. Before:. ```; Attempt 1; Attempt 1; Attempt 1; 2018/11/30 15:53:22 Starting container setup.; gsutil command failed; gsutil command failed; 2018/11/30 15:53:24 Done container setup.; 2018/11/30 15:53:25 Starting localization.; 2018/11/30 15:53:26 Localizing input gs://tjeandet-cromwell-execs/this/does/not/exist -> /cromwell_root/tjeandet-cromwell-execs/this/does/not/exist; Attempt 1; Attempt 2; Attempt 2; gsutil command failed; CommandException: No URLs matched: gs://tjeandet-cromwell-execs/this/does/not/exist; gsutil command failed; gsutil command failed; Attempt 2; Attempt 3; Attempt 3; gsutil command failed; CommandException: No URLs matched: gs://tjeandet-cromwell-execs/this/does/not/exist; gsutil command failed; gsutil command failed; Attempt 3; gsutil command failed; CommandException: No URLs matched: gs://tjeandet-cromwell-execs/this/does/not/exist; 2018/11/30 15:53:47 Delocalizing output /cromwell_root/does/not/exist/either -> gs://tjeandet-cromwell-execs/w/bb90765b-c0d0-41f7-ae75-c03ed30a6a4b/call-t/does/not/exist/either; Attempt 1; gsutil command failed; CommandException: No URLs matched: /cromwell_root/does/not/exist/either; Attempt 1; Attempt 2; gsutil command failed; CommandException: No URLs matched: /cromwell_root/does/not/exist/either; Attempt 3; gsutil command failed; CommandException: No URLs matched: /cromwell_root/does/not/exist/either; 2018/11/30 15:54:07 Delocalizing output /cromwell_root/stdout -> gs://tjeandet-cromwell-execs/w/bb90765b-c0d0-41f7-ae75-c03ed30a6a4b/call-t/stdout; Attempt 1; gsutil command failed; CommandException: No URLs matched: /cromwell_root/stdout; Attempt 2; gsutil command failed; CommandException: No URLs matched: /cromwell_root/stdout; Attempt 3; gsutil command failed; CommandException: No URLs matched: /cromwell_root/stdout; Attempt 1; 2018/11/30 15:54:26 Delocalizi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4445:42,error,errors,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4445,1,['error'],['errors']
Availability,"Any details I provide are only definitely true w/ the jes backend, haven't looked at others but I'm assuming they're similar. Also glob processing is backend-specific which leads to further problems I'll also address. Even if other backends aren't similar I think the same problems will manifest in other ways so there should still be similarity of solution. When a glob returns there's a FOFN on JES and that path is stored in a WdlGlobFile. In `JesAsyncBackendJobExecutionActor.postProcess` the function `evaluateOutputs` is called which effectively decomposes the `WdlGlobFile` into a `WdlArray[WdlSingleFile]` and then this array is carried around in memory in perpetuity (for the workflow). Much CPU and memory are spent for this conversion both at creation time and trying to stuff these things into the metadata service (see the very patriotic #1776). . I'm wondering if we could do something sneaky here and maintain the `WdlGlobFile` as-is and evaluate only what's necessary when necessary. Some thoughts as examples, some are contradictory I'm sure. - Allow that `Array[WdlFile]` to have both `WdlGlobFile` and `WdlSingleFile` with the former being dynamically expanded; - When we need the full list of files for downstream tasks perhaps we can stream them somehow instead of holding in memory; - In an example like a scatter/gather perhaps we could perform the collection on the glob files themselves via merging the stored glob file; - Perhaps we could *always* store an Array[WdlFile] as a FOFN on disk?; - Presumably we'll want to always expand to the full list of single files stored in the metadata for output reporting purposes. Could we stream the strings directly (and then remove) into the DB instead of all of the multiple boxings/unwrappings/etc?. Beyond ""this is all a bunch of work"" some issues that pop out:. - As stated not all backends might be handling globs the same. ; - What if Backend 1 generates a `WdlGlobFile` but that gets handed to Backend 2?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777:1223,down,downstream,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777,1,['down'],['downstream']
Availability,ApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); cromwell_1 | at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); cromwell_1 | at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | at akka.dispatch.forkjoin.ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2818,recover,recoverWith,2818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,2,['recover'],['recoverWith']
Availability,"Apologies if this is the wrong space for this. I've got a workflow written up and running locally in WDL, and I'm getting started with the JES backend. I've done a fair amount of work in google genomics before this, but this is my first use of cromwell/WDL. Some details first. I first noticed this error on 0.19.2, but went back to check 0.19 and HEAD of the develop branch. where it occurs as well. For completeness, here's my WDL file:. ```; cat ~/workflows/hello-jes.wdl ; task jes_task {; command {; echo ""Hello JES!""; }; runtime {; docker: ""ubuntu:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }; workflow jes_workflow {; call jes_task; }; ```. and the console output:. ```; [2016-04-28 15:35:51,218] [info] JesBackend [1cb9c1d2:jes_task]: echo ""Hello JES!""; Apr 28, 2016 3:35:51 PM com.google.api.client.googleapis.services.AbstractGoogleClient <init>; WARNING: Application name is not set. Call Builder#setApplicationName.; [2016-04-28 15:35:51,646] [info] JES Pipeline [1cb9c1d2:jes_task]: Inputs:; exec -> disk:local-disk relpath:exec.sh; [2016-04-28 15:35:51,647] [info] JES Pipeline [1cb9c1d2:jes_task]: Outputs:; jes_task-rc.txt -> disk:local-disk relpath:jes_task-rc.txt; [2016-04-28 15:35:51,648] [info] JES Pipeline [1cb9c1d2:jes_task]: Mounts:; c98942d68bf4c33728f1adef1bfd9ccc -> /mnt/mnt1 (3GB PERSISTENT_SSD); 4fd1d1e01455dfdd4eabcf02c1abaf55 -> /mnt/mnt2 (500GB PERSISTENT_HDD); local-disk -> /cromwell_root (10GB PERSISTENT_SSD); [2016-04-28 15:35:51,728] [warn] JesBackend [1cb9c1d2:jes_task]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names mus",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:299,error,error,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,3,"['echo', 'error']","['echo', 'error']"
Availability,"Apologies, but I can not currently access jira. First time user question:. # The problem. When running `cromwell` locally, I get an excessive number of messages (thousands) from `liquibase`. Here is an example:. ```; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: Custom SQL executed; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: ChangeSet metadata_changesets/remove_non_summarizable_metadata_from_queue.xml::delete_non_summarizable_metadata_from_queue::mcovarr ran successfully in 1ms; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_MAS dropped from table WORKFLOW_METADATA_SUMMARY_ENTRY; ```. Is it possible to control these from cromwell? Is this a liquibase issue? ; The standard `-DLOG_LEVEL=WARN` does not seem to effect log messages. - Version: cromwell-74.jar; - Backend: local ; - Java version: 17.0.2+8 (azul). ## Workflow:; ```wdl; version 1.0. task say_hello {; input {; String name; }. command {; set -euxo pipefail; echo ""Hello ~{name}""; echo ""Hello ~{name}"" > greeting.txt; }. output {; File greeting = ""greeting.txt""; }. runtime {; docker: ""debian:bullseye-slim""; }; }. workflow hello {; input {; String name; }. call say_hello {; input: ; name = name; }. output {; File greeting = say_hello.greeting; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6664:985,echo,echo,985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6664,2,['echo'],['echo']
Availability,"Apparently there weren't any ""real"" workflow names > 100 chars, only one artificially long one created for test purposes, and Cloud SQL did not take kindly to the attempt to have a 500-char field as part of an index. The test data was downsized and the embiggening commit reverted and the migration ran successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1721:235,down,downsized,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1721,1,['down'],['downsized']
Availability,"As a Site Reliability Engineer (SRE) , I would like to have Cromwell support Sentry (https://sentry.io) to capture and report exceptions. This will allow me to better support our runtime operations and know when the system is functioning properly. This could be as simple as a document detailing how to deploy cromwell with the appropriate configuration, or it may involve code changes. @ansingh7115 is working on this in workbench at the moment and should be able to provide more information. @davidbernick can also provide configuration details.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2120:10,Reliab,Reliability,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2120,1,['Reliab'],['Reliability']
Availability,"As a user I would like:. - [ ] an endpoint (/version) which returns metadata about the current version of cromwell. Currently just the version of the server, but if/when we have WDL versioning it should include that as well; - [ ] the same information available from a command line option (version). --- Original -- ; That returns the versions of Cromwell / WDL / etc. Ultimately we want to bubble this up to users in FC.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1694:252,avail,available,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1694,1,['avail'],['available']
Availability,"As a user who runs cromwell in a production setting (like @ktibbett), I need to be able to manage the lifecycle of workflows in the system. After running many workflows, they consume a lot of space on disk and even within the cromwell environment. I woul to be able to delete them through a REST endpoint. Add a new endpoint at DELETE/workflows/{version}/{id} which effectively removes this workflow from the system. This should include; - removing all output files for the workflows and calls; - removing all metadata from the metadata service; - removing all workflows/calls from the call caching service. attempting to remove a workflow in a non-terminal state should result in an error (it should either finish or be aborted first). --; In detail specification:. https://docs.google.com/document/d/1aJn5HzvDgYbvBlEG4z0KO8oZgaQ3lFu2hE8QzRC0_18/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292:684,error,error,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292,1,['error'],['error']
Availability,"As a user, I have a workflow that runs a scatter over 10 shards with a task that produce a file, then uses writelines to write out the gathered array of files (the fofn), which is used as an input to a downstream step. That downstream step will never call cache because the fofn is different every time because the file in it, while each having the same md5, produce a different fofn because call caching copies the data to new paths on each cache hit. This is painful because I love call caching, and now I have to recompute this step (and all steps downstream of it) every time. @cjllanwarne @jmthibault79 any more details or ideas for addressing this; @katevoss this is the issue we talked about on the phone yesterday. It's slowing down JG, but depending on how hard it is to fix may be too late for this use case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309:202,down,downstream,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309,4,['down'],"['down', 'downstream']"
Availability,"As a user, like @yfarjoun, if I run a workflow against the JES backend and an input file for a task doesn't exist (e.g. gs://foo/bar/baz.txt does not exist). I would like to get back a clear error message that this is why the task failed. This is important because currently ""the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had an inkling that it's a missing file problem. so [you] have to resort to divide and conquer in order to identify the missing file...and that's a pain.""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137:191,error,error,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137,2,['error'],['error']
Availability,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3637:142,error,error,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637,7,"['Error', 'error', 'failure']","['Error', 'error', 'failure']"
Availability,"As discussed in https://github.com/broadinstitute/cromwell/issues/6235, developers of workflows for GCP who store their images in Google Container Repositories can be exposed to large Google GCS egress charges when users attempt to run workflows in different continental regions, resulting in many trans-continental container pulls. There currently does not seem to be a satisfactory way to guard against this:. - We can't make our image repositories private because we want to make the workflows available to the public via Terra.; - We can't make the repositories requester-pays because the pipelines API does not support pulling images from requester-pays repositories.; - We can mirror our repositories to different regions, but we are still dependent on our users to configure their workflows to point to the right region and take good-faith extra steps to help us avoid these charges. Some possible ideas were suggested by @freeseek in https://github.com/broadinstitute/cromwell/issues/6235:. - Convince Google to support requester-pays buckets for container pulls in PAPI.; - Modify some combination of Cromwell/PAPI to cache images rather than pulling them for each task that is run.; - Develop infrastructure within Cromwell to know what region the workflow is running in and automatically select the right GCR mirror to pull from.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442:497,avail,available,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442,1,['avail'],['available']
Availability,"As far as I can tell, the timeline is:. - Shutdown signal received; - The job is aborted in JES but not removed from the JobStore; - On restart, the job is recovered because it remains in the JobStore, but in JES it's already been aborted; - On the console, a ""Job Failed"" message appears.; - The EJEA actor dies in an unexpected way (this concerns me *most*. Why isn't the failure cause recorded!). Not tested on SFS backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050:156,recover,recovered,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050,2,"['failure', 'recover']","['failure', 'recovered']"
Availability,"As far as I remember, in the Past it was possible to reference global workflow variables inside a task. But now I get wdl validation errors like this:; ```; ERROR: Variable genome does not reference any declaration in the task (line 36, col 27):. curl -z ${folder}""/""${genome} --max-time 10 --retry 3 --retry-delay 1 ${genomeURL}; ^. Task defined here (line 26, col 6):. task download_genome {; ```; Here is the wdl; ```wdl; workflow indexes {. File genomesFolder; String version #release version; String species #species and also name of the index/. String releaseURL #path to releseas. String transcriptome #relative file name (.fa.gz); String genome #relative file name (.fa.gz); String annotation #relative annotation file name (.gtf). call download_genome {; input:; genomeURL = releaseURL + ""/"" + genome,; transcriptomeURL = releaseURL + ""/"" + transcriptome,; annotationURL = releaseURL + ""/"" + annotation,; folder = genomesFolder + ""/"" + species + ""/"" + version; }. }. task download_genome {. String genomeURL; String transcriptomeURL; String annotationURL; String folder. command {; mkdir -p ${folder}; curl -z ${folder}""/""${genome} --max-time 10 --retry 3 --retry-delay 1 ${genomeURL}; curl -z ${folder}""/""${transcriptome} --max-time 10 --retry 3 --retry-delay 1 ${transcriptomeURL}; curl -z ${folder}""/""${annotation} --max-time 10 --retry 3 --retry-delay 1 ${annotationURL}; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2504:133,error,errors,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2504,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"As mentioned in [this forum post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/39791), there appears to be a race between cromwell checking stderr and it actually being written/flushed to disk. > WDL seemed to fail with a file not found error always in regard to the stderr file, but when I look up the file manually the file was always there, and the specific task also finished with rc=0, but the main cromwell process failed with return code of 1 already due to the file not found error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378:253,error,error,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378,2,['error'],['error']
Availability,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4069:147,failure,failure,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069,2,['failure'],['failure']
Availability,"As of right now, when using preemptible instances, Google has two types of error messages: 13 & 14. We want to to be able to retry when receiving Error Code 13 in the same way we currently retry for Error Code 14.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744,3,"['Error', 'error']","['Error', 'error']"
Availability,As part of auditing our Codecov secrets leak I am trying to trim down the number of items we maintain in Vault. This changeset allows us to delete `secret/dsde/cromwell/common/cromwell-refresh-token`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6331:65,down,down,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6331,1,['down'],['down']
Availability,"As part of my expression evaluation in `InitialWorkDirRequirement` I happened to accidentally write this:; ```yml; listing:; - entryname: $(script_name); ```; Instead of this:; ```yml; listing:; - entryname: $(inputs.script_name); ```. Instead of the expected runtime workflow failure (static checking of JS is too much), the workflow ran forever, repeatedly printing out the same expression evaluation error. Alas!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3088:277,failure,failure,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3088,2,"['error', 'failure']","['error', 'failure']"
Availability,As you see on the screenshot all pairs are highlighted as errors in Intellij while wdltool validates everything without an issue.; ![pair_highlightning_error](https://cloud.githubusercontent.com/assets/842436/25742086/783c9136-3196-11e7-8649-9fd5e1403b70.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246:58,error,errors,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246,1,['error'],['errors']
Availability,Asynced the standard backend execute/recover.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1947:37,recover,recover,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1947,1,['recover'],['recover']
Availability,"At least at the level of outputs, in AWS backend the isolation between separate calls of the same task is broken: e.g. running the following wf:; ```; workflow testCase {; call t as t1 { input: str=""t1"" }; call t as t2 { input: str=""t2"" }; output {; Array[String] t1out = t1.out; Array[String] t2out = t2.out; }; }; task t {; String str; command {; echo ${str} >> outfile; }; output {; Array[String] out = read_lines(""outfile""); }; runtime {; docker: ""amazonlinux:latest""; }; }; ```; Results in the output:; ```; ""outputs"": {; ""testCase.t1out"": [; ""t2"",; ""t1""; ],; ""testCase.t2out"": [; ""t2"",; ""t1""; ]; }; ```; i.e. the same file `outfile` is being written to by the two different task calls. I believe this could only be happening (when the same host is used for both containers) because the output file path is being specified by the workflow and task name rather than workflow and task alias.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4364:349,echo,echo,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4364,1,['echo'],['echo']
Availability,"At the moment we see slick errors from excessive database activity. This is working as intended on the part of slick, so now we need to gracefully handle this. There are temporary measures people can enact, but this ticket is about rewiring things to be brave in the face of danger. I see this as requiring heavy tech talk/design discussion prior to being shovel ready. . Some initial thoughts:. - My assumption is via a backpressure mechanism but I don't want to mandate this. But it seems natural to me that the answer to ""it hurts when I do this"" is ""stop doing that for a while"". ; - One area where this gets tricky is the metadata service. The solution should not overfit to the default (currently only) metadata service implementation (i.e. MySQL tied to the same database connection as the rest of the system).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466:27,error,errors,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466,1,['error'],['errors']
Availability,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1355:243,recover,recovering,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355,1,['recover'],['recovering']
Availability,At what interval does check-alive get called?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877:28,alive,alive,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877,1,['alive'],['alive']
Availability,Atlas 2.1.0 -503 error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6133:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6133,1,['error'],['error']
Availability,Attempt to fix the repeated 403 auth error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/296:37,error,error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/296,1,['error'],['error']
Availability,Attempts to unflakify the abort tests by making them fail reliably if they fail > 20% of the time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3321:58,reliab,reliably,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3321,1,['reliab'],['reliably']
Availability,Auth before downloading from docker.io then always try to login BT-139 BT-143,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6226:12,down,downloading,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6226,1,['down'],['downloading']
Availability,Automatically increase JES VM boot disk size when docker download fails due to full storage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1449:57,down,download,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449,1,['down'],['download']
Availability,Available system variables accessible from Cromwell configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6005:0,Avail,Available,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6005,1,['Avail'],['Available']
Availability,"B""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; }. task gatk_haplotypecaller_task {; String gatk_path = ""/humgen/gsa-hpprojects/GATK/bin/GenomeAnalysisTK-3.7-93-ge9d8068/GenomeAnalysisTK.jar""; String interval; File in_bam; File in_bam_index; String sample_name = ""fumoz_12""; File ? bqsr_table; String ? ploidy; String ? erc; String ? extra_hc_params. File reference_tgz. String out_gvcf_fn = ""${sample_name}.gvcf"". String output_disk_gb; String boot_disk_gb = ""10""; String ram_gb = ""60""; String cpu_cores = ""1""; String preemptible = ""0""; String debug_dump_flag. command {; set -euo pipefail; ln -sT `pwd` /opt/execution; ln -sT `pwd`/../inputs /opt/inputs. /opt/src/algutil/monitor_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:2429,echo,echo,2429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,1,['echo'],['echo']
Availability,BA-5800: Downgrade log level of WorkflowFailedResponse Event to INFO,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065:9,Down,Downgrade,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065,1,['Down'],['Downgrade']
Availability,BCS: A bad file inputs should create an error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3522:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3522,1,['error'],['error']
Availability,BCS: Delocalizing a file that doesn't exist should error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3523:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3523,1,['error'],['error']
Availability,BT-271 Do not cache to calls that are successes by Cromwell standards but failures by Centaur standards.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6335:74,failure,failures,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6335,1,['failure'],['failures']
Availability,BW-1320 Retry unexpectedly transient 400 error that vexes users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6807:41,error,error,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6807,1,['error'],['error']
Availability,Backend Store performing recovery closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1241:25,recover,recovery,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1241,1,['recover'],['recovery']
Availability,Backend should not have any dependency from Engine.; This means that all utils implementation should be moved to backend or removed.; It may require a break down in sub-tasks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/554:157,down,down,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/554,1,['down'],['down']
Availability,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563:366,error,error,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563,5,"['avail', 'down', 'error', 'failure']","['available', 'download', 'error', 'failure']"
Availability,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162:570,error,error,570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162,1,['error'],['error']
Availability,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357:156,Error,Error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357,3,"['Error', 'error']","['Error', 'error']"
Availability,Bad errors reported validating null.wdl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2703:4,error,errors,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2703,1,['error'],['errors']
Availability,Band aid robustification for HealthServiceMonitorActorSpec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3402:9,robust,robustification,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3402,1,['robust'],['robustification']
Availability,"Based on a conversation with the Epigenomics group that is using Cromwell 34 on PAPI v2 --; it seems that a task can fail, not delocalize an expected output and the workflow will continue to start a downstream task but fail at running the command (as an expected input is missing). It's been confirmed that Cromwell was running in a Fail fast mode. There seem to be two main requirements here:; 1. Cromwell should be evaluating if all expected outputs for a task exist before marking the task as a success.; 2. Cromwell should fail a job if it failed localize a specific input file. ; 3. Cromwell should fail a job if it failed to delocalize any expected output file. AC: Ensure that the PAPI v2 backend fulfills requirements #1-3.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4140:199,down,downstream,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4140,1,['down'],['downstream']
Availability,"Based on a post I made on the openWDL slack; was asked to make a ticket here. ## quick summary; Cromwell handles `/` in strings inconsistently. In some cases, it is dropped without throwing an error, in other cases it will cause an error immediately. If the string is in the WDL file itself, womtool does not detect any issues with it but it will not be handled as expected as runtime. ## use case and how to reproduce; [goleft indexcov ](https://github.com/brentp/goleft/tree/master/indexcov#indexcov) defaults to this value for --excludePattern:; `""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""`. So I set `String excludePattern = ""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""` in my WDL. That passes miniwdl check and womtool. But... * Terra will accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$ `as a variable default or as hardcoded variable, but will handle it incorrectly -- it will not error, but it will be changed into `^chrEBV$|^NC|_random$|Un_|^HLA-|_alt$|hapd$`; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable via JSON; it will fail to import; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable if entered manually; it will throw token recognition error in the workflow menu and not allow you to submit; * Terra will accept the escaped version `^chrEBV$|^NC|_random$|Un_|^HLA\\-|_alt$|hap\\d$` as an input if entered manually or hardcoded, and will interpret it as `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$`. Only tested via Terra-Cromwell, as I was previously told local-Cromwell is a lower development priority. ## expected behavior; 1. A user inputting a string as a variable vs that exact same string being a hardcoded default should be handled the same way.; 2. If Cromwell is supposed to handle `/` by requiring they be escaped as `//`, that should be documented if it isn't already.; 3. womtool should throw a warning when it sees a hardcoded variable/default with a `/` inside of it, and that wa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7167:193,error,error,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7167,2,['error'],['error']
Availability,"Based on a report from @curoli . I can't immediately reason out what the correct behavior is (post-vacation fuzziness...) so this is just a a log of stuff that looked suspicious to me. ---. A workflow like; ```; version 1.0. workflow test {; ; Map[String, String] m = {""a"": ""a"", ""b"": ""b""}; String s = ""string"". output {; File write_attempt = write_json({""m"": m, ""s"": s}); }; }; ```; validates in Womtool but fails at runtime with error; ```; WorkflowManagerActor Workflow 2a3db889-e126-467b-be60-6abb815ea46e failed (during ExecutingWorkflowState):; java.lang.UnsupportedOperationException:; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types:; Map(; WomString(m) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(a) -> WomString(a), WomString(b) -> WomString(b))),; WomString(s) -> WomString(string); ); ```. ---. The problem is not so simple as heterogeneous types in the map values; the workflow; ```; version 1.0. workflow test {; ; String s = ""string""; Float f = 0.1; File file = ""asdf"". output {; File write_attempt = write_json({""s"": s, ""f"": f, ""file"": file}); }. }; ```; works just fine:; ```; {""s"":""string"",""f"":""0.1"",""file"":""asdf""}; ```. ---. Interestingly, if we take out `String s = ""string""` we do get an error in Womtool, but it's a confusing one - why would we say a map value has to be an `Object` when we clearly used `String`, `Float`, and `File` right above?; ```; version 1.0. workflow test {; ; Map[String, String] m = {""a"": ""a"", ""b"": ""b""}. output {; File write_attempt = write_json({""m"": m}); }; }; ```; yields; ```; womtool validate any_map.wdl ; Failed to process workflow definition 'test' (reason 1 of 1):; Failed to process declaration 'File write_attempt = write_json({ ""m"": m })' (reason 1 of 1):; Failed to process expression 'write_json({ ""m"": m })' (reason 1 of 1):; Invalid parameter 'MapLiteral(Map(StringLiteral(m) -> IdentifierLookup(m)))'. Expected 'Object' but got 'Map[String, Map[String, String]]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4512:430,error,error,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4512,2,['error'],['error']
Availability,"Based on my understanding of the bash, it seems like we might have been retrying with the requester pays flag project regardless of the error. . The change is to enable localization with the project flag only if the requester pays error exists.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3946:136,error,error,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3946,2,['error'],['error']
Availability,"Based on the documentation https://cromwell.readthedocs.io/en/stable/Imports/, we should have the ability to import from any public HTTPS link. I am getting error on import: ; ```; womtool validate wf_test.wdl; Failed to import 'https://github.com/broadinstitute/cromwell/blob/master/engine/src/main/resources/3step.wdl' (reason 1 of 1): Unrecognized token on line 8, column 1:. <!DOCTYPE html>; ^; ```. Using cromwell 78, and running local with version development (also tried with 1.0). Has this feature been disabled? . Running a very simple WDL workflow: ; ```; version development. import ""https://github.com/broadinstitute/cromwell/blob/master/engine/src/main/resources/3step.wdl"" as http_import2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6788:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6788,1,['error'],['error']
Availability,"Based on the travis test failures -- it seems the `write_lines` centaur tests has a draft3 and a draft2 version and they can end up caching to each other. AC: Either change the test cases so they won't cache to each other, or ensure that `read_from_cache()` is set to false.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4058:25,failure,failures,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4058,1,['failure'],['failures']
Availability,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4960:20,error,error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960,1,['error'],['error']
Availability,Based on this report on the forums: https://gatkforums.broadinstitute.org/wdl/discussion/12878/exception-in-thread-main-scala-matcherror-null-validating-my-wdl. In this case the mistake was using `if (is_exome !=) {` instead of `if (!is_exome)` - but that should be nicely turned into a reportable error... rather than throwing up some obtuse scala-internals error message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4081:298,error,error,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4081,2,['error'],['error']
Availability,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) â†’ mid-way to completion, Upgrade â†’ ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows â†’ Run them again to see they successfully cached once â†’ upgrade â†’ run them again to ensure theyâ€™re still caching. â€œUpgradeâ€ consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldnâ€™t break:; Log names (detritus files) donâ€™t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4101:657,error,error,657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101,3,['error'],['error']
Availability,"Basic Expectations:; Run a bunch of workflows (call caching turned off) â†’ mid-way to completion, Upgrade â†’; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows â†’ Run them again to see they successfully cached once â†’ upgrade â†’ run them again to ensure theyâ€™re still caching. â€œUpgradeâ€ consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldnâ€™t break:; - ~Log names (detritus files) donâ€™t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4099:686,error,error,686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099,3,['error'],['error']
Availability,"Basically changed the sbt assembly from ""--error"" to ""early(error)"" as they changed the flag. The other stuff is just 1.X cleanup",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3597:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3597,2,['error'],['error']
Availability,Be resilient if jobs fail half-way through a very large scatter [BA-6517],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595:3,resilien,resilient,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595,1,['resilien'],['resilient']
Availability,Be resilient if the db has WaitingForQueueSpace statuses [BW-387 fixup],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6131:3,resilien,resilient,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6131,1,['resilien'],['resilient']
Availability,Because sometimes things other than cromwell can cancel jobs. Also might make restarts after aborts a little more resilient in case of unexpected race conditions (not a guarantee TM),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2503:114,resilien,resilient,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2503,1,['resilien'],['resilient']
Availability,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3074:912,recover,recover,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074,2,"['failure', 'recover']","['failures', 'recover']"
Availability,"Before the break I was playing around trying to async-ify Cromwell's IO a bit more. It's not complete and needs clean up / refinements / tests, but considering that one of the goals is reliability/scalability, I thought I'd make a PR out of it since it might provide a base for discussion. This branch has an IO Actor that handles *some* of the IO that has to be done both on the engine and the backend side. Specifically the script.sh upload, rc file reading, stderr file size reading, call cache copying (on JES), workflow outputs copying is done using this mechanism.; The actor is under the service registry umbrella, that was to be able to test it more rapidly (as the service registry is already wired up pretty much everywhere), but it should probably be it's own top level actor. Due to the Future-based approach we took in the backend interface, the IO messages (copy, read, write, delete file...) are declined into 2 different flavors:; - A classic Command -> Response; - A Promise based version, that takes a promise in the command message itself to be completed when the operation finishes. This allow for the actor to integrate with parts of the code that can't (easily) handle the response as a message. The underlying implementation of the IO Actor is a router, but could be swapped for something else. Each worker tries to perform the operation, and once it's complete (successfully or not) either sends a message back or completes the promise depending on the command flavor.; Retries are handled by keeping an exponential backoff object in the command itself. If the failure is retryable, the worker sends the command message back to the router after waiting for the appropriate backoff time. The message will then be rerouted when a worker is available.; Note that the actual time before the command is picked up again by another worker could be longer than intended if all workers are busy and the command spends time in the mailbox. ; A command will be retried as many times as po",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831:185,reliab,reliability,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831,1,['reliab'],['reliability']
Availability,"Better ""no metadata found"" errors for call cache diffs [BA-6106]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260:27,error,errors,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260,1,['error'],['errors']
Availability,Better JES errors + RC Closes #1848,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1856:11,error,errors,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1856,1,['error'],['errors']
Availability,"Better error message for ""Upgrade Config from C26""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2186:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2186,1,['error'],['error']
Availability,Better error message for string member accesses,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3867:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3867,1,['error'],['error']
Availability,Better error messages for cyclic dependencies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4548:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4548,1,['error'],['error']
Availability,Better error messages for incomplete expressions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4175:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4175,1,['error'],['error']
Availability,"Better error messages, esp missing workflow outputs expressions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2907:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2907,1,['error'],['error']
Availability,Better error reporting if an EJEA crashes unexpectedly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1685:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1685,1,['error'],['error']
Availability,Better error reporting when a cromwell task is canceled by google due to 6 day limit,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496,1,['error'],['error']
Availability,Better import failure messages and importLocalFilesystem resolver set,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3987:14,failure,failure,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3987,1,['failure'],['failure']
Availability,"Binding"": {; ""prefix"": ""-somatic_min_total""; },; ""default"": 300,; ""id"": ""#purple-2.44.cwl/somatic_min_total""; },; {; ""type"": [; ""null"",; ""float""; ],; ""doc"": ""Proportion of somatic deviation to include in fitted purity score. Default 1.\n"",; ""inputBinding"": {; ""prefix"": ""-somatic_penalty_weight""; },; ""default"": 1,; ""id"": ""#purple-2.44.cwl/somatic_penalty_weight""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\nSample name must match tumor parameter. GZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-somatic_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/somatic_vcf""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of structural variant vcf for more accurate segmentation.\nGZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-structural_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/structural_vcf""; },; {; ""type"": ""File"",; ""doc"": ""Optional location of failing structural variants that may be recovered.\nGZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-sv_recovery_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/sv_recovery_vcf""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads\n"",; ""inputBinding"": {; ""prefix"": ""-threads""; },; ""default"": 2,; ""id"": ""#purple-2.44.cwl/threads""; },; {; ""type"": ""string"",; ""doc"": ""Name of the tumor sample. This should correspond to the value used in AMBER and COBALT.\n"",; ""inputBinding"": {; ""prefix"": ""-tumor""; },; ""id"": ""#purple-2.44.cwl/tumor""; },; {; ""type"": [; ""null"",; ""boolean""; ],; ""doc"": ""Tumor only mode. Disables somatic fitting.\n"",; ""inputBinding"": {; ""prefix"": ""-tumor_only""; },; ""default"": false,; ""id"": ""#purple-2.44.cwl/tumor_only""; }; ],; ""outputs"": [; {; ""type"": ""Directory"",; ""outputBinding"": {; ""glob"": ""$(inputs.output_dir)/""; },; ""id"": ""#purple-2.44.cwl/outdir""; }; ],; ""id"": ""#purple-2.44.cwl""; },; {; ""class"": ""Workflow"",; ""id"": ""#main"",; ""l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:36318,recover,recovered,36318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Availability,Bonus: Robust METADATA_VALUE embiggening.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1648:7,Robust,Robust,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1648,1,['Robust'],['Robust']
Availability,Bring in lots of Scott's wdl4s goodness. This should include; - [x] Individuals spend 1-2 hours reviewing the PR as homework; - [x] Team gathers for group discussion of the code; - [ ] Rebase wdl4s; - [ ] Test cromwell 0.21 with wdl4s (unit tests & centaur); - [ ] Fix test failures from above; - [ ] Merge!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1443:274,failure,failures,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1443,1,['failure'],['failures']
Availability,"Brought this up with Dion, suggested we have a backoff for transient issues like this on their end. Seems very transient, but needed to have it documented. Looks like this:. ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; { ; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }"". timestamp:""2016-05-06T22:39:17.321Z""; jobId:""operations/EOiv78DIKhjQhqv9q_TfliEgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903:287,error,errors,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903,3,"['Error', 'error']","['Error', 'errors']"
Availability,Bug Fix: Response error codes in releaseHold endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3918:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3918,1,['error'],['error']
Availability,"Building docker with `docker build .` generates warnings like. `; [warn] Error extracting zip entry 'scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class' to '/cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class': java.io.FileNotFoundException: /cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class (File name too long); `. It appears this is because the max filename under docker is ~242 characters, but the sbt default for max generated class name is ~254/255. See https://github.com/docker/docker/issues/1413. The fix is to reduce this as described here . http://stackoverflow.com/questions/28565837/filename-too-long-sbt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1428:73,Error,Error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1428,1,['Error'],['Error']
Availability,Bump file read timeout to maybe reduce test failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,1,['failure'],['failures']
Availability,Bump heterodon to version with http download / zlib fix.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4485:36,down,download,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4485,1,['down'],['download']
Availability,"Bypass Mockito error ""Cannot cast to primitive type"" on classes also modified due to use of Refined.; Dependencies still reported out of date via `sbt dependencyUpdates`:; - com.aliyun:aliyun-java-sdk-core : 3.6.0 -> 3.7.1 -> 4.0.8; Waiting for https://github.com/aliyun/aliyun-openapi-java-sdk/issues/54; - com.github.pathikrit:better-files : 2.17.1 -> 3.6.0; Unstable API would probably require multiple changes; - com.google.apis:google-api-services-cloudkms : v1-rev63-1.25.0 -> InvalidVersion(v1beta1-rev6-1.22.0); False positive due to version not being SemVer; - com.google.apis:google-api-services-genomics : v2alpha1-rev31-1.25.0 -> InvalidVersion(v2alpha1-rev9-1.23.0); False positive due to version not being SemVer; - mysql:mysql-connector-java : 5.1.47 -> 8.0.12; See notes in Dependencies.scala on changes that would be required by users.; - org.broadinstitute.dsde.workbench:workbench-google : 0.15-2fc79a3 -> 0.15-ff73de5-SNAP -> 0.100-f9bd914-SNAP -> 1.0-e8e6ff0-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-model : 0.10-6800f3a -> 0.10-ff73de5-SNAP -> 0.12-e24d5a6-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-util : 0.3-f3ce961 -> 0.3-ff937c4-SNAP; Need more research to know what changed; - org.liquibase:liquibase-core : 3.5.5 -> 3.6.2; Waiting for https://liquibase.jira.com/browse/CORE-3311; - org.webjars:swagger-ui : 3.2.2 -> 3.18.2; Unstable API would probably require multiple changes; - software.amazon.awssdk:aws-sdk-java : 2.0.0-preview-9 -> 2.0.1; Waiting for https://github.com/broadinstitute/cromwell/issues/3909",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4078:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4078,1,['error'],['error']
Availability,"CI clone of ""Implement recoverAsync for AWS backend"" #5216",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5291:23,recover,recoverAsync,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5291,1,['recover'],['recoverAsync']
Availability,CPU on Cromwell machines pegged and unable to recover without restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093:46,recover,recover,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093,1,['recover'],['recover']
Availability,CROM-6920 Add option to retry only known errors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7456:41,error,errors,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7456,1,['error'],['errors']
Availability,CWL + AWS Batch + resource requirements gives error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['error'],['error']
Availability,CWL Parsing Error for large multi-step workflow with no logs to assist debug,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:12,Error,Error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['Error'],['Error']
Availability,"CWL was treating output glob strings as if they were filenames, and thus was not returning the filename that Cromwell expects, namely `glob-${md5(fileName)}.list`. The implementation boils down to `OutputEvaluator` trying to detect whether the output of the expression is a glob. If it is _is_ a glob, it changes the output to be the filename as listed above.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2828:189,down,down,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2828,1,['down'],['down']
Availability,CaaS: Don't try to deserialize SAM HTML errors as JSON,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3622:40,error,errors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622,1,['error'],['errors']
Availability,Cache Hit copy failures with RP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4771:15,failure,failures,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771,1,['failure'],['failures']
Availability,Cache output copy failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4091:18,failure,failures,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091,1,['failure'],['failures']
Availability,Call Caching hit 503 Service Unavailable error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1185:41,error,error,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185,1,['error'],['error']
Availability,Call cache no copy fixups and centaur error improvements,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4951:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951,1,['error'],['error']
Availability,Call caching output copy failures handling. Closes #1510,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1559:25,failure,failures,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1559,1,['failure'],['failures']
Availability,"Call caching works sometimes for me but not all the time. I find it especially strange when working on a scatter job and some of the scatter jobs get a cache hit but others get a cache miss. . I have queried the METADATA_ENTRY table for the two workflows and all the call cache entries look identical. . Here is my process:. 1. I queried METADATA_ENTRY with this WHERE condition: `(WORKFLOW_EXECUTION_UUID ='29791b64-b47a-44ba-aff0-7ab48bc10677' or WORKFLOW_EXECUTION_UUID ='5de042e3-7a03-4c77-8972-f0e4cd010e4b') and CALL_FQN = 'sampleLevelWorkflow_WGS.align' and JOB_SCATTER_INDEX =0`; 2. I sort by METADATA_KEY; 3. Then I go down the list and compare the hashes for the two workflows for each METADATA_KEY. Here is a case where workflow 29791b64 is a restart of 5de042e3. (Workflow 5de042e3 is itself a restart but I don't think that is important here.) I have shown below all the records from METADATA_ENTRY that start with ""callCaching"" and they all look identical, yet it clearly says it is a ""Cache Miss"". **Is there anywhere I can see a log message stating exactly which hashes resulted in the cache miss?** I have tried to enable LOG_LEVEL=DEBUG but couldn't see it there. Thanks in advance for your help!. |WORKFLOW_EXECUTION_UUID|METADATA_KEY|METADATA_VALUE|; |-----------------------|------------|--------------|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:result|Cache Miss|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:result|Cache Miss|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:628,down,down,628,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['down'],['down']
Availability,"Callcaching fails on GCPBATCH but not on PAPIv2 when using a private docker image in gcr.io. ; Is this a missing feature or a bug? The documentation on the subject could go either way, depending on whether GCPBATCH is part of the other backends or a subset of the pipelines backend (https://cromwell.readthedocs.io/en/latest/cromwell_features/CallCaching/). ; I do not think this is a configuration error, since the same config works with PAPIv2 backend, but if it is, what configuration options would be necessary for configuring gcr.io authentication when using GCPBATCH?. Errors from cromwell logs when task is being callcached:; ```; cromwell_1 | 2024-01-11 11:09:38 pool-9-thread-9 INFO - Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithoutHash(Some(eu.gcr.io),Some(project),image_name,tag); cromwell_1 | cromwell.docker.registryv2.DockerRegistryV2Abstract$Unauthorized: 401 Unauthorized {""errors"":[{""code"":""UNAUTHORIZED"",""message"":""You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.scala:246); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$6(CompileScope.scala:245); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$4(CompileScope.scala:244); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$2(CompileScope.scala:242); cromwell_1 | 	at flatMap",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:1508,error,errors,1508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['error'],['errors']
Availability,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998:481,failure,failure-mode,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998,1,['failure'],['failure-mode']
Availability,Carbonite JSON parsing error checking [BA-6081],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5257:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5257,1,['error'],['error']
Availability,Catch and log workflow log copy failures [BA-4916],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5022:32,failure,failures,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5022,1,['failure'],['failures']
Availability,Centaur errors to BigQuery.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4072:8,error,errors,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4072,1,['error'],['errors']
Availability,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3874:1099,heartbeat,heartbeat,1099,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874,1,['heartbeat'],['heartbeat']
Availability,Centaur should be more robust to missing files,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2885:23,robust,robust,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2885,1,['robust'],['robust']
Availability,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4816:511,Fault,FaultHandling,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816,1,['Fault'],['FaultHandling']
Availability,Centaur tests should reliably pass even when they don't have a `sleep 2` suffixed to the command block.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1894:21,reliab,reliably,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1894,1,['reliab'],['reliably']
Availability,"Certain error messages that Cromwell receives are longer than the default limit, which is a big pain when debugging. Going from 64 to 1024 characters (1kb) doesn't seem unreasonable and solves this issue. For context, the error message below is 364 characters. . [Relevant Akka Doc](https://doc.akka.io/docs/akka-http/10.0/configuration.html). . Before:; ```; 2024-04-12 14:58:18 cromwell-system-akka.actor.default-dispatcher-26 ERROR - Error in stage [akka.http.impl.engine.client.OutgoingConnectionBlueprint$PrepareResponse@71a2a20e]: Response reason phrase exceeds the configured limit of 64 characters; akka.http.scaladsl.model.IllegalResponseException: Response reason phrase exceeds the configured limit of 64 characters; 	at akka.http.impl.engine.client.OutgoingConnectionBlueprint$PrepareResponse$$anon$3.onPush(OutgoingConnectionBlueprint.scala:191); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:523); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:409); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:606); 	at akka.stream.impl.fusing.ActorGraphInterpreter$SimpleBoundaryEvent.execute(ActorGraphInterpreter.scala:47); 	at akka.stream.impl.fusing.ActorGraphInterpreter$SimpleBoundaryEvent.execute$(ActorGraphInterpreter.scala:43); 	at akka.stream.impl.fusing.ActorGraphInterpreter$BatchingActorInputBoundary$OnNext.execute(ActorGraphInterpreter.scala:85); 	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:581); 	at ; ...; ```. After: ; ```; <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">; <html><head>; <title>401 Unauthorized</title>; </head><body>; <h1>Unauthorized</h1>; <p>This server could not verify that you; are authorized to access the document; requested. Either you supplied the wrong; credentials (e.g., bad password), or your; browser doesn't understand how to supply; the credentials required.</p>; </body></html>; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7406:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7406,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,Changing directories in the command block causes workflow failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3116:58,failure,failure,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3116,1,['failure'],['failure']
Availability,Chaos monkey Cromwell. Put up a Firewall and deny access to Cromwell. Tear down. See what happened. Block ports. Tables. ; Putting up a block. . The real Chaos Monkey. Much hectic. Simian army. . Henry knows how to spin up a Cromwell test environment. Much magic.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2113:75,down,down,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2113,1,['down'],['down']
Availability,Check RC first and report that status instead of delocalization errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4160:64,error,errors,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4160,1,['error'],['errors']
Availability,Checkpoint update to wes2cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3932:0,Checkpoint,Checkpoint,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3932,1,['Checkpoint'],['Checkpoint']
Availability,Checksum S3 signed URL downloads during localization [BT-257],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6485:23,down,downloads,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6485,1,['down'],['downloads']
Availability,Clarify PAPI Error Code 10 Message 14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855:13,Error,Error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855,1,['Error'],['Error']
Availability,"Cleans up a little the error reporting for PAPI2 failure and add some context.; It's not perfect and can be improved but is better than what's there now and will hopefully make it a bit easier to debug failures going forward.; e.g:. ```; WorkflowManagerActor Workflow 0c939095-9e15-4a20-9d35-9b3a7494304c failed (during ExecutingWorkflowState): java.lang.Exception: Task my_workflow.my_task:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 2: unexpected exit status 1 was not ignored; [Localization] Input name: my_input - Unexpected exit status 1 while running ""gsutil cp gs://my_bucket/input.txt /cromwell_root/my_bucket/input.txt"": CommandException: No URLs matched: gs://my_bucket/input.txt. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stdout"": CommandException: No URLs matched: /cromwell_root/stdout. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stderr"": CommandException: No URLs matched: /cromwell_root/stderr. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/rc gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/rc"": CommandException: No URLs matched: /cromwell_root/rc; ```. I thought about omitting the delocalization failures if there was a localization failure (as the task did not run so obviously there won't be any stdout/stderr/rc), but it seemed a bit too magical. Can always be done later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3722:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722,6,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,Clear cache from optional associated workflow when rerunning after Centaur error [CROM-6807],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654,1,['error'],['error']
Availability,Clearer error message for improperly formatted disk strings,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2739:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2739,1,['error'],['error']
Availability,"Close script done; [2023-02-08 16:24:31,65] [info] dataFileCache commit start; (...); ```. And at the end:; ```; 2023-02-08 16:32:11,54] [info] checkpointClose synched; [2023-02-08 16:32:11,57] [info] checkpointClose script done; [2023-02-08 16:32:11,57] [info] dataFileCache commit start; [2023-02-08 16:32:11,57] [info] dataFileCache commit end; [2023-02-08 16:32:11,69] [info] checkpointClose end; [2023-02-08 16:32:11,69] [info] Checkpoint end - txts: 5342; [2023-02-08 16:32:21,70] [info] Checkpoint start; [2023-02-08 16:32:21,70] [info] checkpointClose start; [2023-02-08 16:32:21,70] [info] checkpointClose synched; [2023-02-08 16:32:21,74] [info] checkpointClose script done; [2023-02-08 16:32:21,74] [info] dataFileCache commit start; [2023-02-08 16:32:21,76] [info] dataFileCache commit end; [2023-02-08 16:32:21,82] [info] checkpointClose end; [2023-02-08 16:32:21,82] [info] Checkpoint end - txts: 5348; [2023-02-08 16:32:21,89] [error] Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.LockException: Could not acquire change log lock. Currently locked by fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1 (fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1) since 2/8/23, 4:23 PM; 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:270); 	at liquibase.Liquibase.lambda$update$1(Liquibase.java:214); 	at liquibase.Scope.lambda$child$0(Scope.java:180); 	at liquibase.Scope.child(Scope.java:189); 	at liquibase.Scope.child(Scope.java:179); 	at liquibase.Scope.child(Scope.java:158); 	at liquibase.Liquibase.runInScope(Liquibase.java:2405); 	at liquibase.Liquibase.update(Liquibase.java:211); 	at liquibase.Liquibase.update(Liquibase.java:197); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:74); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:46); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:3361,down,down,3361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,1,['down'],['down']
Availability,Closes #1649 Closes #1754 Support gcs files with spaces + be robust to workflow log deletion faâ€¦,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1764:61,robust,robust,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1764,1,['robust'],['robust']
Availability,"Closes #3214. Top level view:; - Totally split `draft2` from a fresh copy of WDL code in `draft3`.; - They now inhabit separate and unconnected sbt projects.; - Any existing code references to `wdl.` are now references to `wdl.draft2.`; - Please review my `build.sbt` file, the rest is just moving, shuffling and fixing up intellij's failures to refactor packages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3229:334,failure,failures,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3229,1,['failure'],['failures']
Availability,"Closes #3790 . I went down the ""allow WomAnyType"" values route for now, to solve the general case of:; ```; Object o = read_json(some_file); scatter (x in o.blah) {; ...; }; ```. Ideally directing people to `struct`s will mean this problem goes away, but in the mean time, we'll need to handle scatters over `WomAnyType`s (at least in the static analysis).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3794:22,down,down,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3794,1,['down'],['down']
Availability,Closes #3990. ~Awaiting accompanying OpenWDL PR with the underlying grammar change (after lunch).~. OpenWDL PR: https://github.com/openwdl/wdl/pull/253. ---. Reviewers: is it worth a changelog note to tell people `version draft-3` went away?. I think the change itself is OK because it seems like anyone who signed up to use a draft version is pretty breakage-tolerant.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4135:360,toler,tolerant,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4135,1,['toler'],['tolerant']
Availability,Closes #4023 . I hijacked one of the steps in `cwl_cache_between_workflows.cwl` to test `Long` alongside `Float` - the second commit [fails](https://api.travis-ci.org/v3/job/419242749/log.txt) with the error the user saw.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4033:202,error,error,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4033,1,['error'],['error']
Availability,Closes #4051 . Also checks for and produces clear errors when tabs and spaces are mixed in a cmd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4065:50,error,errors,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4065,1,['error'],['errors']
Availability,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567:898,avail,available,898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567,2,['avail'],['available']
Availability,"Closes #4824 . I didn't do; >a sort of input_errors map with input names as keys and error(s) as values. because; 1. The underlying WOM creation returns pre-formatted strings (e.g. `Required workflow input 'wf_hello.hello.addressee' not specified`) and changing that interface would be a huge undertaking; 2. There's no neat way to handle extraneous inputs, since they obviously would not match any input name key in the map",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4940:85,error,error,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4940,1,['error'],['error']
Availability,"Closes #5460 . Cromwell isn't correctly cascading `string + optional + string` behaviour. It seems that `string + optional` is evaluating to an empty string. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. That is, if `str` resolves to `None`, then this should resolve to `echo `. Instead this is currently resolving to `echo ""` (single quote).; ```; echo ~{'""--prefix"" ""' + str + '""'}; ```. ## Example. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. Without value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl ; # Job quotetest:NA:1 exited with return code -1; # STDERR: unexpected EOF while looking for matching '""'; ```. With value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl -i quotestest-inp.json ; # ""outputs"": {; # ""quotetest.out"": ""--prefix Hello""; # }; ```. ## This PR. Addresses the spec, by:. - Changing type checker: OptionalType<T> + T => Optional<T> (within the current rules).; - Optional + String => String else None",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5464:705,echo,echo,705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464,4,['echo'],['echo']
Availability,Colon ':' in output filename causes workflow failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2919:45,failure,failure,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2919,1,['failure'],['failure']
Availability,Combination of read_lines and list of filenames causes erroneous file not found errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632:80,error,errors,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632,1,['error'],['errors']
Availability,Come up with a way to test aliases reliably,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4848:35,reliab,reliably,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848,1,['reliab'],['reliably']
Availability,Command field cannot have empty strings errors from Centaur,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3737:40,error,errors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3737,1,['error'],['errors']
Availability,Command variable expansion masks bash variable interpretation with ${...},MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3111:27,mask,masks,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3111,1,['mask'],['masks']
Availability,Command-line tool hangs after error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4061:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4061,1,['error'],['error']
Availability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl > /dev/null; ```; This produces no output. Running the same command without redirecting STDOUT results in an error message. This message should likely be reported on STDERR, not STDOUT, to be consistent with common standards around command-line tools.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4063:184,error,error,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4063,1,['error'],['error']
Availability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; Output:; ```; [2018-08-30 17:36:02,67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our oth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4060:727,ERROR,ERROR,727,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060,1,['ERROR'],['ERROR']
Availability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; The output shows a stacktrace and then hangs. It should likely exit with a non-zero status, following the convention of other command-line tools and allowing for failure detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4061:243,failure,failure,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4061,1,['failure'],['failure']
Availability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:715,heartbeat,heartbeat,715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"CommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 375 SSD""; noAddress: true; preemptible: 1; maxRetries: 3; system.memory-retry-error-keys = [""OutOfMemory"", """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1647,error,error-keys,1647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,"['Error', 'error']","['Error', 'error-keys']"
Availability,Communications link Failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742:20,Failure,Failure,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742,1,['Failure'],['Failure']
Availability,"Concoct an SQL table schema which can handle efficiently looking up any information currently provided by the Cromwell endpoints which read from the metadata service. This might get hairy with query and/or metadata, but see what can be done. This schema will be realized in a CloudSQL database. NB: Google PubSub does not guarantee ordering of events. We should already be robust to this via the CRDT structures and such, but take that into account w/ this table structure. I could imagine there being situations where e.g. â€œtimestamp from the payload of last event I processed which led to this stateâ€ might also be good to track or something like that. If you find yourself looking for ideas on how to handle ordering issues, [this Google doc](https://cloud.google.com/pubsub/docs/ordering) provides some examples for all of the situations we have",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3242:373,robust,robust,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3242,1,['robust'],['robust']
Availability,Configurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Stand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:7835,recover,recover,7835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,"Consider the following WDL using the [if then else](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#if-then-else) construct:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if x == 1 then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 20, col 15) when parsing 'e'. Expected then, got """". echo ~{if x == 1 then 1 else 0}; ^. $e = :if $e :then $e :else $e -> TernaryIf( cond=$1, iftrue=$3, iffalse=$5 ); ```. The following equivalent WDL instead:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. Int y	= if x == 1 then 1 else 0; command <<<; echo ~{y}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; Success!; ```. Similarly this equivalent WDL:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if !(x != 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; Success!; ```; It seems like the parser does not accept the `==` operator in the condition of the `TernaryIf` for some reasons, but only in the case it is included in a `command <<< >>>` section.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602:254,echo,echo,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602,5,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,Consistent failure to delocalize file referenced by variable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4901:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901,1,['failure'],['failure']
Availability,Console error includes debugging information,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4060:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060,1,['error'],['error']
Availability,Copy workflow logs even upon failure closes #1621,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1720:29,failure,failure,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1720,1,['failure'],['failure']
Availability,Correct HPC tutorial config error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3488:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3488,1,['error'],['error']
Availability,"Could not download toolchain archive from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020.06.25.065836.tar.xz, giving up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:10,down,download,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['down'],['download']
Availability,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3244:425,error,errors,425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244,1,['error'],['errors']
Availability,"Created in collaboration with: @TMiguelT. The OpenWDL spec states when interpolating a string in the command block:; > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. Ie:; - `string + null + string -> null`:. That is, if `str` is not defined (`null`), the following should resolve to null and empty:; ```; ~{'""--prefix"" ""' + str + '""'}; ```. Currently, it's resolving to `""` (a single double-quote). Eg: In the task:. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. A fix to this would reduce our usages of `if defined(name) then """" else """"`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5460:743,echo,echo,743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5460,1,['echo'],['echo']
Availability,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1290:365,failure,failure,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290,1,['failure'],['failure']
Availability,CromIAM is already more than an IAM service and that will continue. This has already caused some confusion. Perhaps `caas`? . If moved to monorepo I'm thinking a `workbench` subproject would be ideal as we start down this path. . either way these changes will likely need some interaction w/ devops for hte docker magic,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3217:212,down,down,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3217,1,['down'],['down']
Availability,Cromiam error with gzip encoding,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4708:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708,1,['error'],['error']
Availability,"Cromwell (38, in this case) is saturating the available connections to our managed MySQL database. Our DBAs increased the limit and Cromwell proceeded to fill up these slots. How can we limit the number of concurrent connections to a MySQL database? There doesn't seem to be any configuration option for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4777:46,avail,available,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777,1,['avail'],['available']
Availability,Cromwell 21 fails retrying non-preemptible jobs when getting a 429 error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763,1,['error'],['error']
Availability,"Cromwell 34; Backend: local. `cc28a122-6605-4d53-83d3-245a88f8ad96` is the current workflow and I did not open the log file.; ```; [2018-10-01 10:02:13,69] [error] Failed to delete workflow log; java.nio.file.FileSystemException: /media/sf_broad_oncotator_configs/test_m2_small/cromwell-workflow-logs/workflow.cc28a122-6605-4d53-83d3-245a88f8ad96.log: Text file busy; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); 	at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); 	at java.nio.file.Files.delete(Files.java:1126); 	at better.files.File.delete(File.scala:619); 	at cromwell.core.path.BetterFileMethods.delete(BetterFileMethods.scala:413); 	at cromwell.core.path.BetterFileMethods.delete$(BetterFileMethods.scala:412); 	at cromwell.core.path.DefaultPath.delete(DefaultPathBuilder.scala:55); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186,1,['error'],['error']
Availability,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:12,error,errors,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,4,"['Error', 'error', 'recover']","['Error', 'error', 'errors', 'recoverWith']"
Availability,Cromwell 503 Service Unavailable Error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903:33,Error,Error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903,1,['Error'],['Error']
Availability,Cromwell 55 expected to handle 504 error in GCS but instead WDL pipeline failed,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:35,error,error,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,1,['error'],['error']
Availability,Cromwell 59 download broken,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6500:12,down,download,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6500,1,['down'],['download']
Availability,Cromwell GCP error - The referenced network resource cannot be found,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477,1,['error'],['error']
Availability,Cromwell Metadata essentially empty; Error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/801:37,Error,Error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/801,1,['Error'],['Error']
Availability,Cromwell Out of Memory Error Joint Genotyping Workflow Large Array of Arrays,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1051:23,Error,Error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1051,1,['Error'],['Error']
Availability,"Cromwell Version: 77. Hi Guys,. Im not sure if this is a bug but I recently noticed this behaviour. Im trying to write a task that registers task S3 outputs at the end of a workflow in a third-party system and copy it to another S3 bucket. This task itself does not generate any files locally but at the end of its execution I expect a new S3 object to appear in the destination bucket. The inputs and outputs are marshalled using a struct with a ""File"" variable for the S3 path of the objects. After the task executes Cromwell throws an error scala.MatchError - it recognises that the output is a `cromwell.filesystems.s3.S3Path` but dosn't appear to know what to do with it. Im wondering if it is because the ""file"" is created outside of the workflow workspace?. My work around is to use the `String` type in place of `File` type for ""path"" and cast back to `File` later in the workflow but this feels inelegant. Input:. {; ""main.bucket"" : ""my_bucket_2"" ,; ""main.file_list"":[; { ""path"": ""s3://my_bucket_1/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html""}; ]; }. Expected Output:. [; {; ""id"": ""123""; ""path"": ""s3://my_bucket_2/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html""; }; ]. Code:. struct file_thing {; String? id; File path; }. task copy_file_list{; input{; String bucket; Array[file_thing] file_list; }. command <<<; copy_files \; --bucket ~{bucket} \; --json_in ~{write_json(file_list)} \; --json_out outputs.json; >>>. output {; Array[file_thing] outputs = read_json(""outputs.json""); }. runtime {; docker: ""my_copy_tool:latest""; }; }. Error:. WorkflowManagerActor: Workflow e7a60e4b-8dc4-471b-aec6-b8cc1481f889 failed (during ExecutingWorkflowState): ; scala.MatchError: s3://my_bucket_2/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html (of class ; cromwell.filesystems.s3.S3Path); 	at cromwell.backend.sfs.SharedFileSystem.hostAbsoluteFilePath(SharedFileSystem.scala:239); 	at cromwell.backend.sfs.SharedFileSystem.hostAbsoluteFilePath$(SharedFileSystem.scala:237); 	at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6716:538,error,error,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6716,1,['error'],['error']
Availability,"Cromwell Version: `""cromwell"": ""33-e90c4de""`. Problem: The workflow has scattered tasks, a few of the shards finished without any errors, but when looking into the actual results of the task, we can only see files with `0B` size. Example workflow: workflow `42e173c6-7fc3-4a3e-93c7-c9d95836f6a5 ` in `https://cromwell.mint-dev.broadinstitute.org/`, specifically, the task: `call-sc/shard-98/SmartSeq2SingleCell/b4ac422c-e5b1-42ed-8dcf-cca51394e08c/call-RSEMExpression`, shard-98. @jishuxu has run into this issue for several times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006:130,error,errors,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006,1,['error'],['errors']
Availability,"Cromwell cyclic dependency error in 30.1, fine in 29",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['error'],['error']
Availability,"Cromwell deforms the path to JNI library while running wdl workflow. Printout of the **cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/script** file:; `... -Djava.library.path=/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux ...`. Path that is specified in the json file:; `""case_gatk_acnv_workflow.jni_lib"": ""/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux/`. JSON file path: **/dsde/working/asmirnov/TestCromwell/downsampled-bams.json**; Cromwell output directory: **/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1140:697,down,downsampled-bams,697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140,1,['down'],['downsampled-bams']
Availability,"Cromwell doesn't seem to retry on *some* 503 errors, but does an others.; Might be related to https://github.com/GoogleCloudPlatform/google-cloud-java/issues/1545; The `code` attribute in the exception seems to be 503 in both cases so we could check that explicitly instead of relying on the `isRetryable` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2207:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2207,1,['error'],['errors']
Availability,"Cromwell doesn't support images from ghcr.io. Not sure if there's a workaround besides personally rehosting the image somewhere else. Seeing as a lot of bioinformatics images are hosted there I'd like to see it be supported. ## error returned; [2022-08-11 12:40:55,22] [warn] BackendPreparationActor_for_e48a0b67:Minos.minos_adjudicate:-1:1 [e48a0b67]: Docker lookup failed; java.lang.Exception: Registry ghcr.io is not supported. ## backend; Running Cromwell on a local machine, eventually will be running in Terra. My local machine has Docker installed and already has the required Docker image pulled. ## relevant workflow task; ```; task minos_adjudicate {; 	input {; 		File ref; 		File reads; 		File vcf1; 		File vcf2. 		# runtime attributes; 		Int addldisk = 1; 		Int cpu = 4; 		Int retries = 1; 		Int memory = 8; 		Int preempt = 2; 	}; 	# Estimate disk size required; 	Int ref_size = ceil(size(ref, ""GB"")); 	Int finalDiskSize = 2*ref_size + addldisk. 	String ref_basename = basename(ref). 	command <<<; 		# softlinks don't seem to cut it here; 		set -eux -o pipefail; 		cp ~{ref} .; 		minos adjudicate --reads ~{reads} outdir ~{ref} ~{vcf1} ~{vcf2}; 	>>>; 	; 	runtime {; 		cpu: cpu; 		docker: ""ghcr.io/iqbal-lab-org/minos""; 		disks: ""local-disk "" + finalDiskSize + "" HDD""; 		maxRetries: ""${retries}""; 		memory: ""${memory} GB""; 		preemptibles: ""${preempt}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6827:228,error,error,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827,1,['error'],['error']
Availability,Cromwell gave mysterious error of missing files during workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,1,['error'],['error']
Availability,Cromwell gives wrong error message with unclosed brace,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['error'],['error']
Availability,"Cromwell has a system for adjusting boot disk sizes that is intended to account for large Docker images that would otherwise overflow the boot disk at its default size. However it seems this system is often not preventing boot disks from overflowing: we have received a number of reports recently of users having to explicitly set runtime attributes for boot disk size because this automatic system did not sufficiently increase the boot disk size. Furthermore a review of the ""compression factor"" used for this boot disk size adjustment system revealed that the default compression factor appears to be much larger than actual compression factors seen in Docker images commonly used in Cromwell (cloud-sdk, gatk, etc). i.e. the current model of boot disk size adjustment does not appear to line up well with actual observed boot disk size behavior. This PR changes the way boot disk size adjustment is done to use a more realistic compression factor (3.0 vs 6.0), and instead *adds* the estimated uncompressed Docker image sizes to the default boot disk size. This should yield consistently larger and less failure prone boot disk sizes with scaling behavior that should hopefully better align with real world Docker images.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5790:1108,failure,failure,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5790,1,['failure'],['failure']
Availability,"Cromwell keeps track of the status of the DAG with an internal map of `Scope`s.; This map should probably now reference `GraphNodes` which are a subset of the `Scope`s and represent more accurately a node in the graph, with associated traversal methods (upstream, downstreams etc...). `Declaration`s are now `Scope`s (and even (`GraphNode`s). This should allow WDL like. ```; ...; workflow wf {; call A; String dec = A.out; call B { input: b_input = dec }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1577:264,down,downstreams,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1577,1,['down'],['downstreams']
Availability,Cromwell metadata servers in FC prod have been brought down by repeated requests for the same workflow metadata before a response has been returned for the first request. It should be fairly easy to implement a requesters map that would allow Cromwell to only assemble metadata once and respond to all requests with that same metadata. The implementation here could be very similar to (though simpler than) the [file hash caching](https://github.com/broadinstitute/cromwell/pull/4143/files) system.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226:55,down,down,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226,1,['down'],['down']
Availability,Cromwell out of memory error large array,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['error'],['error']
Availability,Cromwell parallel to https://github.com/DataBiosphere/cbas/pull/194. #7190 is an example of a cromwell PR that didn't make it to terra-helmfile - [corresponding job failure](https://github.com/broadinstitute/cromwell/actions/runs/5870702268/job/15918276016),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7206:165,failure,failure,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7206,1,['failure'],['failure']
Availability,Cromwell preemption and error retry interaction with GCP Batch built-in retry,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7476:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7476,1,['error'],['error']
Availability,"Cromwell production goes down, ""too many files open""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3716:25,down,down,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3716,1,['down'],['down']
Availability,"Cromwell release 36.1 has a docker image available for the release, but nowhere in the docs is the cromwell docker container documented (as far as I can tell).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682:41,avail,available,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682,1,['avail'],['available']
Availability,Cromwell retry file download,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4710:20,down,download,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710,1,['down'],['download']
Availability,"Cromwell seems to erroneously treat private/internal variables with File type as input files and attempts to localize them:. ```; version 1.0. task mytask {; 	input {; 		String mystr = ""Hello World!""; 	}. 	File myfile = ""myfile"". 	command <<<; 		echo ~{mystr} > ~{myfile}; 	>>>. 	output {; 		String file_contents = read_string(myfile); 	}; }. workflow wf {; 	call mytask; }; ```. ```; Could not localize myfile -> /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/cromwell-executions/wf/51d2f863-0e91-45b6-9e7b-f2365c259144/call-mytask/inputs/1979661608/myfile:; 	myfile doesn't exist; 	File not found /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/cromwell-executions/wf/51d2f863-0e91-45b6-9e7b-f2365c259144/call-mytask/inputs/1979661608/myfile -> /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/myfile; 	File not found myfile; 	File not found /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/myfile; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:94); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:90); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:669); 	... 35 common frames omitted; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6562:246,echo,echo,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6562,1,['echo'],['echo']
Availability,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109:899,down,download,899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109,3,['down'],"['down', 'download']"
Availability,Cromwell should be resilient to Docker host outages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4056:19,resilien,resilient,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056,2,"['outage', 'resilien']","['outages', 'resilient']"
Availability,"Cromwell should prevent itself from dying on attempt to read too large workflow metadata, and return read failure instead [BA-6447]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5519:106,failure,failure,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5519,1,['failure'],['failure']
Availability,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:208,recover,recover,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,4,"['ERROR', 'Error', 'Recover', 'recover']","['ERROR', 'Error', 'Recover', 'recover']"
Availability,"Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6070:154,reliab,reliable,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6070,1,['reliab'],['reliable']
Availability,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855:16,Error,Error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855,8,"['Error', 'error', 'failure']","['Error', 'error', 'failure']"
Availability,Cromwell tries to define undefined variables causing bizarre errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:61,error,errors,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,1,['error'],['errors']
Availability,"Cromwell version 0.21; Running on Openstack + ubuntu 16.04 instance; Local backend with a docker container. I was working on creating a simple WDL for this workflow: https://github.com/ICGC-TCGA-PanCancer/pcawg_delly_workflow. Which looks a little like this:. ```; workflow PcawgDelly {; call SeqwareWorkflow; }. task SeqwareWorkflow {. String run_id; File reference_gc; File tumor_bam; File normal_bam; File reference_gz; String delly_id = ""embl-delly_1-3-0-preFilter.20150318"". command {; perl /usr/bin/run_seqware_workflow.pl \; --run-id ${run_id} \; --reference-gc ${reference_gc} \; --tumor-bam ${tumor_bam} \; --normal-bam ${normal_bam} \; --reference-gz ${reference_gz}; }. runtime {; docker: ""delly-docker-root""; }; }; ```. and I'm leaving out some details, but you get the idea, it's very simple. I would frequently get a failed Cromwell workflow, with an error in the logs like:. ```; mv: cannot stat/root/PcawgDelly/e173fd52-3c15-4b87-bfec-087c7cf0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently wit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556:865,error,error,865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556,1,['error'],['error']
Availability,"Cromwell version: 30-9a7de06. Minimized from one of our WDLs:; ```wdl; workflow Test {; Boolean do; Int n. if (do) {; call Optional; }. scatter (i in range(n)) {; call Scattered; }. call Gather {; input:; # HERE: select_first returns String, and Scattered.out is an Array[String]; ins = if defined(Optional.out) then select_first([Optional.out]) else Scattered.out; }; output {; Gather.out; }; }. task Optional {; command {; echo ""Hey!""; }; output {; String out = read_string(stdout()); }; }. task Scattered {; command {; echo ""Hello!""; }; output {; String out = read_string(stdout()); }; }. task Gather {; Array[String] ins. command {; cat ${write_lines(ins)}; }; output {; String out = read_string(stdout()); }; }; ```. This WDL runs successfully, but in code review I noticed the weird type mismatch between the branches. I asked @cjllanwarne about it and he thought it was an old ""feature"" that had been purged to avoid bugs / confusion. I'd expect something like this to be rejected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3478:425,echo,echo,425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3478,2,['echo'],['echo']
Availability,"Cromwell version: 34; Backend: local. Fails in cromwell 34. Worked fine in cromwell 32. Task M2 is scattered. Then FuncotateMaf takes the output value from the first shard. M2 was has not been run by the time this error message appears. Error:; ```; [2018-10-01 10:02:13,68] [error] WorkflowManagerActor Workflow cc28a122-6605-4d53-83d3-245a88f8ad96 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'Mutect2.FuncotateMaf.case_id' (reason 1 of 1): Evaluating M2.tumor_sample[0] failed: Failed to find index Success(WomInteger(0)) on array:. Success([]). 0; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:483); 	at scala.util.Either.flatMap(Either.scala:338); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:480); 	at scala.util.Either.flatMap(Either.scala:338); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:479); 	at scala.util.Either.flatMap(Either.scala:338); ....; java.lang.RuntimeException: Failed to evaluate 'Mutect2.FuncotateMaf.control_id' (reason 1 of 1): Evaluating M2.normal_sample[0] failed: Failed to find index Success(WomInteger(0)) on array:. Success([]). 0; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:483); 	at scala.util.Either.flatMap(Either.scala:338); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4185:214,error,error,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4185,3,"['Error', 'error']","['Error', 'error']"
Availability,Cromwell's failure is now complete (develop edition). Closes #2201,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2203:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2203,1,['failure'],['failure']
Availability,Cromwell's failure is now complete (hotfix edition),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2205:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2205,1,['failure'],['failure']
Availability,"Cromwell's requester pays logic works by trying to perform GCS operations without specifying a project to bill. If the operation is successful, great, all done. If the operation is not successful and the error message looks like a requester pays error, the operation is retried with the project to bill specified. IIRC this system is in place because always specifying the project to bill resulted in the project being billed even if the bucket was not requester pays. It's unfortunate this logic needs to be so clunky when GCS does have the concept of [provisional user projects](https://developers.google.com/resources/api-libraries/documentation/storage/v1/java/latest/com/google/api/services/storage/Storage.Buckets.GetIamPolicy.html#setProvisionalUserProject-java.lang.String-) but this concept is not supported in the Google Storage API used by the GCS filesystem. Anyway the ""is this requester pays"" logic used to look for exact matches to an error message string, i.e. exactly this:; ```; Bucket is requester pays bucket but no user project provided.; ```; However with increasing probability (the `requester_pays_engine_functions` Centaur test fails about 50% of the time with the baseline Cromwell code) we are seeing error messages that actually look like this:; ```; 400 Bad Request; POST https://storage.googleapis.com/upload/storage/v1/b/cromwell_bucket_with_requester_pays/o?projection=full&uploadType=multipart; {; ""error"": {; ""code"": 400,; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""errors"": [; {; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""domain"": ""global"",; ""reason"": ""required""; }; ]; }; }; ```. The changes here accommodate either version of the error message with a `null`-safe `contains` check courtesy Apache StringUtils.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556:204,error,error,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556,7,['error'],"['error', 'errors']"
Availability,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:598,alive,alive,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,2,"['alive', 'error']","['alive', 'error']"
Availability,"Cromwell: 36; Backend: Google Cloud. When running a CWL snpEff tool using Cromwell ([CWL is here](https://github.com/broadinstitute/cromwell/files/2743866/snpEff.cwl.txt)), I get the following errors: ([full error log](https://github.com/broadinstitute/cromwell/files/2743887/snpeff_indels-stderr.log)). ```; xargs: invalid option -- 'I'; find: unrecognized: -empty; ```. In other words, Cromwell is generating a script that uses `xargs -I` and `find -empty`, flags which are not compatible with Busybox ([full Cromwell-generated script](https://github.com/broadinstitute/cromwell/files/2743877/script.txt)). The reason this matters is that all [Biocontainers](https://biocontainers.pro/) are based on Busybox, and I would say they represent a majority of the bioinformatics containers, so ensuring compatibility is in everyone's interest. Might it be possible to edit the Cromwell script to use a smaller subset of these command line flags?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4536:193,error,errors,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536,2,['error'],"['error', 'errors']"
Availability,"Cromwell: 36; Backend: PAPI/Google Cloud. If an input file to a WDL pipeline contains a space, the script Cromwell generates to run on the pipelines API will fail. For instance, I have an input file like this (truncated):; ```json; {; ""best_practise.ref_fasta"": ""gs://genovic-test-data/hg19/Reference Genome/default_unzipped/ucsc.hg19.fasta"",; }; ```; This make Cromwell generate a PAPI script containing this line:; ```bash; bash_ref_fasta=/cromwell_root/genovic-test-data/hg19/Reference Genome/default_unzipped/ucsc.hg19.fasta; ```. This then causes the following error in the script:; ```; /cromwell_root/script: line 26: Genome/default_unzipped/ucsc.hg19.fasta: No such file or directory; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4393:566,error,error,566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4393,1,['error'],['error']
Availability,"Cromwell: 36; Mode: Server; Backend: Google Cloud; ***; I've noticed a strange error that only happens on the Google Cloud backend. Cromwell runs the WDL tasks with the correct command line, but somehow the arguments after the initial command aren't being picked up by the binary inside the docker container. It seems like only the first argument is actually being used. This isn't an issue with my python script, because I can run it directly and everything works fine. Cromwell showing the command line:; ```; cromwell_1 | 2018-11-12 06:57:56,451 cromwell-system-akka.dispatchers.backend-dispatcher-40 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(5d4c4459)germline_variant_calling.fastqc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:79,error,error,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,2,['error'],['error']
Availability,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:290,error,error,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Crossposting here as I was not able to get an answer on WDL forum. Post is here:; https://gatkforums.broadinstitute.org/wdl/discussion/13540/unable-to-do-docker-lookup#latest. Trying to get hello world working on AWS Batch and cromwell. I am able to spin up the servers however it fails in pulling the docker image with the following error message in the cromwell logs:. `; 2018-10-30 15:43:14,845 cromwell-system-akka.dispatchers.engine-dispatcher-9 WARN - BackendPreparationActor_for_7d0c30ad:wf_hello.hello:-1:1 [UUID(7d0c30ad)]: Docker lookup failed java.lang.Exception: Failed to get docker hash for ubuntu:latest Docker hash lookup failed with code 503. The server is currently unavailable (because it is overloaded or down for maintenance). at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:188); `. Here is my wdl:. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell on AWS""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. This is originally from tutorial by @wleepang found here:; https://www.youtube.com/watch?v=jcC3pz_K4gI. Any idea on how to diagnose? I was not able to find a similar issue. Thanks so much for the help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345:334,error,error,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345,4,"['down', 'echo', 'error', 'mainten']","['down', 'echo', 'error', 'maintenance']"
Availability,Cryptic error when missing refresh token,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,1,['error'],['error']
Availability,Ctrl-C: Cromwell Doesn't Shut Down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1007:30,Down,Down,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1007,1,['Down'],['Down']
Availability,"Current copy simply says, ""wdlDependencies"" ... proposed change to ""wdlDependencies (zip)"". Since uploading a single wdl file in this box will cause an ""invalid zip file error"", but does not specify that the error was in the wdlDependencies field.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1971:170,error,error,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1971,2,['error'],['error']
Availability,"Current known issues (feel free to add/remove/edit):; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. Before running the nightly cron test another script should auto sweep old jobs so they don't have to be manually cleaned up with `bcs dj`. Some mix of bcs/bash/jq/sed/awk/python should work and are all available on Travis.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3555:364,avail,available,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3555,1,['avail'],['available']
Availability,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3554:144,down,down,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554,5,"['avail', 'down', 'failure']","['available', 'down', 'failures']"
Availability,"Current scheme:; - If a workflow option is specified, use that; - If a runtime attribute is specified, use that; - If neither are specified, use the default backend (silently, without error). There are a few things wrong with this.; - If a user specifies a particular backend we should _not_ be silently giving them some other backend. IMO we shouldn't be giving them another backend period.; - The order of workflow option & runtime attribute makes sense in a single backend workflow but in a multi-backend workflow (which apparently _does_ work, at least in some cases) it's wrong. I think the best thing would be to expand workflow option to allow per-call backend specification or something like that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312:184,error,error,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312,1,['error'],['error']
Availability,"Currently -- the Cromwell (and other?) service logs on the alpha env are around for upto 5 days. . It would be great to have their availability extended to a longer life line, if feasible. . Being investigated by David Bernick.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3894:131,avail,availability,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3894,1,['avail'],['availability']
Availability,"Currently Workflow outputs can be copied at the end of a workflow, and this is done by then engine WorkflowFinalizationActor.; All the information this actor has is file paths as `String`s. To be able to copy those files out it needs to create a `Path` from them with the right filesystem / auth, which it currently can't do reliably since it doesn't have any information about which backend produced this output or with which auth. A possible fix to that would be to make `wdl4s.WdlFile` wrap `java.nio.File` instead of `String`, so the filesystem / auth information is not lost.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1684:325,reliab,reliably,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1684,1,['reliab'],['reliably']
Availability,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1504:134,down,down,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504,1,['down'],['down']
Availability,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881:168,avail,available,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881,2,"['avail', 'down']","['available', 'downstream']"
Availability,"Currently it barfs on aliased call commands yielding the following error:; ```; Exception in thread ""main"" wdl4s.parser.WdlParser$SyntaxError: ERROR: Expression references output on call that doesn't exist (line 1572, col 5):. ValidateCram.*; ^; ; 	at wdl4s.Workflow.$anonfun$expandedWildcardOutputs$5(Workflow.scala:166); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241); 	at scala.collection.immutable.List.foreach(List.scala:378); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:241); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:238); 	at scala.collection.immutable.List.flatMap(List.scala:341); ```. FYI: `ValidateCram` was called as such in the file:; ```; call ValidateSamFile as ValidateCram {; input:; input_bam = ConvertToCram.output_cram,; input_bam_index = ConvertToCram.output_cram_index,; report_filename = sample_name + "".cram.validation_report"",; .....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Currently the attributes of `WorkflowInputParameter` (secondaryFiles, doc, format etc...) are being ignored and only the type / name are used to create an `ExternalGraphInputNode` that will then get its value from the input yaml file.; Those attributes might contain information that we want to use and should be made available somehow for processing once the value is assigned to the input.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3441:318,avail,available,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3441,1,['avail'],['available']
Availability,Currently the only trace of a call end status after the workflow has completed is in the metadata.; This makes it impossible for other endpoints (e.g: call caching diff endpoint) to relay this information without reading from metadata. We might want to investigate storing this information somewhere permanent: it is currently available in the jobstore but only for the duration of the workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338:327,avail,available,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338,1,['avail'],['available']
Availability,"Currently when a user uses a subworkflow id for anything which takes a workflow id it will fail. This is because CromIAM queries SAM first and SAM only knows about root workflow IDs. . It seems like the right thing to do here will be to get the root workflow ID from Cromwell and then ping Sam, but feel free to do something edifferent if it makes sense to do so",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3459:285,ping,ping,285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3459,1,['ping'],['ping']
Availability,"Currently, after a backend job completes successfully, if something goes wrong trying to read the RC file and / or the stderr file size, the previous ""handle"" is sent back which triggers a new status poll and it goes down the same code path again, trying to re-read the RC file / stderr.; If for some reason one or both of those files is permanently damaged and can never be read, the actor will be stuck in an infinite retry loop.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1846:217,down,down,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1846,1,['down'],['down']
Availability,"Currently, as far as I can tell, the paging functionality is pretty hard to use since there is no notation of the total records (given filters) available. Returning page and pagesize or at least starting record would be helpful. Finally, being able to do this relies some kind of reproducible ordering of records; I assume that ordering is deterministic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1793:144,avail,available,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1793,1,['avail'],['available']
Availability,"Currently, it seems that aggregated failures have an identical key within metadata, and thus when metadata is reported with that key, only one of the multiple aggregated errors is reported. . AC: Fix this behavior such that, going forward, all aggregated errors are reported by metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2046:36,failure,failures,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2046,3,"['error', 'failure']","['errors', 'failures']"
Availability,"Currently, the file systems available to the engine for functions like read_\* are statically defined. GCS, Local, etc. This issue is to make that driven by the config file. The reason this is important is because if you are running a cromwell server and can not disable the ""Local Shared Filesystem"" from the engine... someone could write a WDL that does a read_ on any file that the cromwell server has access to (e.g. read_lines(""./cromwell.conf"")... which is bad",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/821:28,avail,available,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/821,1,['avail'],['available']
Availability,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1354:672,avail,available,672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354,2,['avail'],['available']
Availability,"Currently, there is no library function to flatten an array of array of files (`Array[Array[File]]`). A scatter, where each task call produces an array of files, is a natural way of ending up with such a structure. In order to flatten this array, you can write a task that takes the it as an argument, and manipulate it with python code. However, this task will also download all the files, taking significant time and disk space. To work around this, you can coerce the files into strings (their paths), and manipulate the paths. . You can see an example [here](https://github.com/HumanCellAtlas/skylab/blob/master/10x/count/count.wdl#L195). The `chunk_reads_join` task flattens the `fastq_chunks` file array, which is coerced into an `Array[Array[String]]`. In order to avoid this circuitous implementation, this pull requests implements a generic flatten operation for ragged array types.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2825:367,down,download,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2825,1,['down'],['download']
Availability,"Currently, this is set to default to 128000. This is too small for most practical use, especially given that gs URLs can get quite long. Can the new limit be much higher? We'll need 5GB (no joke!) for some our larger analyses. Or at least a workflow option to temporarily override?. Otherwise, we get an error such as:; ```""Workflow has invalid declarations: Could not evaluate workflow declarations:\nSingleSampleGenotyping.gvcfs_list:\n\tUse of WdlSingleFile(gs://broad-dsde-methods/gauthier/Finnish_FE_WGS.1000samples.gvcf_list) failed because the file was too big (174730 bytes when only files of up to 128000 bytes are permissible""```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768:304,error,error,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768,1,['error'],['error']
Availability,"Currently, to process a glob on JES, Cromwell does an `ls` of the google cloud storage location. The problem with this is that ls is eventually consistent, which leads to bugs like #843 . JES has added a feature (#28858407) where they now return the number of files that matched the glob as part of their metadata. These appear as events of the form. `{Description: ""copied 3 file(s) to \""gs://my-bucket/out/\"""",; StartTime: {Seconds: 1470063955,; Nanos: 748725437}},`. In Cromwell, when processing these globs, we should poll (with adjustable maximum timeout) for this number of files to appear via the ls. If they do not appear after the timeout, the task should fail with an appropriate error message. If we are processing globs on GCS and NOT using JES, the best we can do is just grab and go via the ls (as we are doing currently for JES).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1395:690,error,error,690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395,1,['error'],['error']
Availability,Custom scaladoc settings must be appended (++=) instead of overwriting (:=).; Fixed unmoored doc warnings/errors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2989:106,error,errors,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2989,1,['error'],['errors']
Availability,Cwl Expressions Support Checkpoint #3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2737:24,Checkpoint,Checkpoint,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2737,1,['Checkpoint'],['Checkpoint']
Availability,"D, referencedTableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,321 INFO - ALTER TABLE ""public"".""WORKFLOW_STORE_ENTRY"" ADD ""HOG_GROUP"" VARCHAR(100); 2019-07-21 23:07:19,322 INFO - Columns HOG_GROUP(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2019-07-21 23:07:19,331 INFO - ChangeSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.vis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:34499,ERROR,ERROR,34499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['ERROR'],['ERROR']
Availability,"DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initializ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2266,down,down,2266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['down'],['down']
Availability,"DL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:3020,Error,Error,3020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Error'],['Error']
Availability,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085:1627,echo,echo-job,1627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085,8,['echo'],['echo-job']
Availability,"DO NOT MERGE YET!. Hopefully a final-ish version of the DataAccess API to unblock its implementation in Slick. . RIP StoreActor, SymbolStore, and ExecutionStore. The spirit of ExecutionStore lives on in WorkflowActor, and maybe SymbolStore might find a place there too to rid us of some Await.result()s. StoreActor is dead for real. . Known issues:. FIXED ~~1) The Docker test is broken because the backend-specific initialization was accidentally refactored away. Shouldn't be a big deal to restore that.~~; 2) All other tests pass, but that might just be because the restart test stinks. There are worrisome messages being emitted from that test which need to be tracked down and have assertions put on them. It might also be a good idea to test restarting a workflow more complex than ""Hello World"".; 3) No persistence of BackendInfo yet, shouldn't be tough but that needs a bit of discussion.; 4) More Await.results() in WorkflowActor than I would like or are probably necessary. This could be a separate Tech Debt issue. At least DataAccess is fully async.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/64:673,down,down,673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/64,1,['down'],['down']
Availability,DRS ammonite localizer script is gone!! This PR replaces it with a executable jar in docker where the dependencies are no longer complied and downloaded each time you want to localizer DRS input!. JIRA [ticket](https://broadworkbench.atlassian.net/browse/BA-5821),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5120:142,down,downloaded,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5120,1,['down'],['downloaded']
Availability,"DSDEEPB-2026 Fixes Akka logging. The invocation of the error method flipped the arguments such that the exception was thrown away, the other methods don't accept an exception which also effectively threw away the exception. Below is what it looks like on current develop when an exception is thrown away. The Akka formatter realizes it has more substitution arguments than it is able to substitute. WorkflowActor [UUID(8ae8f57a)]: Failed to transition workflow status from Submitted to Failed WARNING arguments left: 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/294:55,error,error,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/294,1,['error'],['error']
Availability,Data migration for restart/recover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1119:27,recover,recover,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1119,1,['recover'],['recover']
Availability,Database communications link failures in prod,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4689:29,failure,failures,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689,1,['failure'],['failures']
Availability,"DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; â€‚â€‚at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; â€‚â€‚at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-05-10 11:38:08,737 cromwell-system-akka.actor.default-dispatcher-3 INFOâ€‚â€‚- WorkflowActor [UUID(972b838f)]: persisting status of CollectUnsortedReadgroupBamQualityMetrics:10 to Failed.; 2016-05-10 11:38:08,738 cromwell-system-akka.actor.default-dispatcher-3 ERROR - WorkflowActor [UUID(972b838f)]: Read timed out",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:8753,ERROR,ERROR,8753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,1,['ERROR'],['ERROR']
Availability,De-serialize the workflow starter and heartbeat writes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4239:38,heartbeat,heartbeat,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4239,1,['heartbeat'],['heartbeat']
Availability,"Dear Cromwell Team,; I am trying to run a workflow written in WDL using Cromwell v.65. The workflow reports the following error in the stdout:; ```[2023-08-11 14:21:11,58] [error] SingleWorkflowRunnerActor received Failure message: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.```; This is after having edited the `cromwell.conf` as suggested in [this thread](https://github.com/broadinstitute/cromwell/issues/2519). The configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:122,error,error,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,3,"['Failure', 'error']","['Failure', 'error']"
Availability,"Dear Cromwell developers,. I am working on SQLite support at the moment. I have been successful in instantiating the database with all the correct tables. In other words the Liquibase migration seems to function fine. Unfortunately, the LiquibaseComparisonSpec keeps failing with a rather non-descript error:; ```; java.lang.NullPointerException was thrown.; java.lang.NullPointerException; 	at liquibase.structure.core.Index.setColumns(Index.java:118); 	at liquibase.snapshot.jvm.PrimaryKeySnapshotGenerator.snapshotObject(PrimaryKeySnapshotGenerator.java:80); 	at liquibase.snapshot.jvm.JdbcSnapshotGenerator.snapshot(JdbcSnapshotGenerator.java:66); ...; ```; If I write the database to a file, all the correct tables seem to be present (I might have missed one, but the database is certainly populated with a schema). Now I have been doing some digging and this is the database object that is created:; ```; database = {SQLiteDatabase@5510} ""null @ jdbc:sqlite::memory:""; systemTables = {HashSet@5517} size = 2; reservedWords = {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `def",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5453:302,error,error,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453,1,['error'],['error']
Availability,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5305:283,error,error,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305,1,['error'],['error']
Availability,"Dear all,. **Issue**; It appears that it is currently not possible to provide task variables in the input json when they are called in a subworkflow. In the example below the goal to overwrite SubTask optional value: overwrite_given_value. **Example**; _For this examples I use womtool 84 and WDL 1.0. I also tried with womtool 53.1 and 83 which showed the same issue_; The following files are used to show case the issue.; The main workflow:; ```; version 1.0. import ""SubWorkflow.wdl"" as SubWorkflow. workflow MainWorkflow {; input {; String value_2_give = ""default value""; }; call MainTask {; input:; given_value = value_2_give; }. call SubWorkflow.SubWorkflow {; input:; value_2_give = value_2_give; }; }. task MainTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. The subworkflow:; ```; version 1.0. workflow SubWorkflow {; input {; String value_2_give = ""default value""; String? overwrite_value_2_give; }; call SubTask {; input:; given_value = select_first([overwrite_value_2_give, value_2_give]); }; }. task SubTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. To be sure I ran the validation mode of womtools:; ```; $ java -jar womtool-84.jar validate MainWorkflow.wdl; Success!; $ java -jar womtool-84.jar validate SubWorkflow.wdl; Success!; ```. After creating these files, I ran womtool with the ""inputs"" option getting the following output:; ```; $ java -jar womtool-84.jar inputs MainWorkflow.wdl; {; ""MainWorkflow.SubWorkflow.overwrite_value_2_give"": ""String? (optional)"",; ""MainWorkflow.MainTask.overwrite_given_value"": ""String? (optional)"",; ""MainWorkflow.value_2_give"": ""String (optional, default = \""default value\"")""; }; ```; This output json shows which variables you can (or must) provide in order to be able to run in this case the main workflow. here we see that",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6841:803,echo,echo,803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6841,1,['echo'],['echo']
Availability,"Dear cromwell team,. We are trying to setup a test environment with GATK4 and cromwell for our local users. I tested the helloHaplotypeCaller.wdl in the data bundle but it is giving the following error:. ```; java -jar $cromwell run storage/WDLdata/WDLscripts/helloHaplotypeCaller.wdl -i storage/WDLdata/WDLscripts/helloHaplotypeCaller_inputs.json; [2017-10-04 06:06:48,43] [info] Running with database db.url = jdbc:hsqldb:mem:2812db5e-e9cc-48b0-bc67-62fd2c7887f9;shutdown=false;hsqldb.tx=mvcc; [2017-10-04 06:06:53,48] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-10-04 06:06:53,49] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-10-04 06:06:53,83] [info] Slf4jLogger started; [2017-10-04 06:06:54,01] [info] Metadata summary refreshing every 2 seconds.; [2017-10-04 06:06:54,02] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-10-04 06:06:54,32] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-10-04 06:06:55,29] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-10-04 06:06:55,36] [info] Workflow bf90a37b-6ffa-4122-a12c-24aced32f3b6 submitted.; [2017-10-04 06:06:55,36] [info] SingleWorkflowRunnerActor: Workflow submitted bf90a37b-6ffa-4122-a12c-24aced32f3b6; [2017-10-04 06:06:55,36] [info] 1 new workflows fetched; [2017-10-04 06:06:55,37] [info] WorkflowManagerActor Starting workflow bf90a37b-6ffa-4122-a12c-24aced32f3b6; [2017-10-04 06:06:55,37] [info] WorkflowManagerActor Successfully started WorkflowActor-bf90a37b-6ffa-4122-a12c-24aced32f3b6; [2017-10-04 06:06:55,37] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-10-04 06:06:55,63] [info] MaterializeWorkflowDescriptorActor [bf90a37b]: Call-to-Backend assignments: helloHaplotypeCaller.haplotypeCaller -> Local; [2017-10-04 06:06:56,98] [info] WorkflowExecutionActor-bf90a37b-6ffa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2673:196,error,error,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2673,1,['error'],['error']
Availability,"Dear developers,. During testing I ran into the problem that the `HashPathStrategy` does not include the last modified date of the file. It assumes: ""if the path is there, it is the same file"". This is not necessarily the case. Files can be modified or replaced.Therefore the current `HashPathStrategy` is a big liability when trying to get reproducible results. By adding a ""last modified date"" to the `HashPathStrategy` this will ensure that nothing has happened to the file from the user or system side. This of course is not as safe as the `HashFileStrategy` since it does not protect against filesystem or hardware errors, but it provides a lot more safety compared to the current `HashPathStrategy`. ; This is also how Snakemake checks if files are the same and it works quite well. Alternatively there could be an option in the Configfile that allows you to set this behaviour. Please let me know what you think of this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4405:620,error,errors,620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4405,1,['error'],['errors']
Availability,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3999:335,echo,echo,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999,8,['echo'],['echo']
Availability,Defining struct inline with boolean or float value causes no coercion defined from wom value error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414:93,error,error,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414,1,['error'],['error']
Availability,Delocalizing a file that doesn't exist should create an error on the BCS backend but is currently succeeding. This may or may not require changes to the embedded python worker. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3523:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3523,1,['error'],['error']
Availability,Deprecation errors for DB configs. Closes #2186,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2200:12,error,errors,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2200,1,['error'],['errors']
Availability,Detect error 500 in JES backend and retry job,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1450:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450,1,['error'],['error']
Availability,Determine HTTP error codes to retry,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1913:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1913,1,['error'],['error']
Availability,"Directory structure:. ```; WDLTesting; -src; --wdl; ---Workflow.wdl; ---WriteTask.wdl; ---Child; ----ChildWF.wdl; ----ChildTask.wdl; ```. Under draft 2 (which is to say, no version specified), Workflow.wdl has the following import statements:; ```; import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write; import ""WDLTesting/src/wdl/Child/ChildWF.wdl"" as Child; ```; ChildWF.wdl has the following import statement:; `import ""WDLTesting/src/wdl/Child/ChildTask.wdl"" as Child`. This works correctly. it works letting things be access from the file system, and it works if we zip up WDLTesting and pass it to Cromwell as --imports. Under version development, this doesn't work. Workflow.wdl's imports still work, but ChildWF.wdl's do not. I must trim it down to ; `import ""Child/ChildTask.wdl"" as Child`; Before it will work. This completely breaks our pipeline, especially since workflows calling files up and down the tree, and even in other projects that start at the same level as ours. Why was this horrible breaking change made, and how do we specify an import that works with respect to the --imports .zip?. Thank you",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6441:746,down,down,746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441,2,['down'],['down']
Availability,Disabled languages produce bad error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3921:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3921,1,['error'],['error']
Availability,"Disaggregate ""Unexpected terminal status"" errors in Sentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4003:42,error,errors,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4003,1,['error'],['errors']
Availability,Display Workflow failure message from /metadata #603,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4875:17,failure,failure,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4875,1,['failure'],['failure']
Availability,Do not log programmer error when Carboniting failed with `TooLargeToArchive` result [BA-6471],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5535:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5535,2,['error'],['error']
Availability,"Do the following:. - [x] Rename the `Preempted` `ExecutionStatus` to be `RetryableFailure`.; - [x] Change the `WorkflowExecutionActor` to blindly honor `JobFailedRetryableResponse`s. Currently it is deciding whether or not to retry when it receives these. Now it will assume the determination was already made.; - [x] Remove the `system.max-retries` config option; - [x] Change the JABJEA to track both preemptions and non-preemption retries via the KV store; - [x] For any failure from JES determine if the job is to be retried, either using the preemption count supplied by the user or a max non-preemption retry count of 2. If the job is retryable ensure a `JobFailedRetryableResponse` finds its way back to the WEA; - [x] Modify `BackendStatus` such that we can see that the job was either a) preempted or b) retried due to ReasonX",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1925:474,failure,failure,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1925,1,['failure'],['failure']
Availability,Docs HPCSlurmWithLocalScratch.md redundant?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:33,redundant,redundant,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['redundant'],['redundant']
Availability,Docs and Error messaging for Pair access,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2300:9,Error,Error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2300,1,['Error'],['Error']
Availability,"Docs here: http://doc.akka.io/docs/akka-http/current/scala.html. NB for this ticket these endpoints wouldn't need to do anything useful, just return some hard-coded value (500 server error perhaps?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2147:183,error,error,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2147,1,['error'],['error']
Availability,"Docs:; - Fixed a bunch of broken links; - I think the Scala Steward updates gave us a new version of doc generation that is more strict; - There are more broken links than just these, did not attempt to be comprehensive; - IntelliJ's markdown validation is helpful:. ![Screen Shot 2020-08-19 at 12 15 43 PM](https://user-images.githubusercontent.com/1087943/90661978-d2613d00-e215-11ea-8c1d-5ae4c842213e.png). Error messages:; - Attempted to make them more concise and consistent; - Sometimes we didn't make it obvious that a limit is configurable; - Did not attempt to be comprehensive",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5779:410,Error,Error,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5779,1,['Error'],['Error']
Availability,Document a general-purpose recovery process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:27,recover,recovery,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,1,['recover'],['recovery']
Availability,Document docker stop command to gracefully shut down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2562:48,down,down,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2562,1,['down'],['down']
Availability,Document that concurrent-job-limit is available on all backends,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1751:38,avail,available,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751,1,['avail'],['available']
Availability,Documentation error? womtool graph,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5126:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126,1,['error'],['error']
Availability,Don't log akka debug messages to error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3237:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3237,1,['error'],['error']
Availability,Don't log request failures in conformance tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3645:18,failure,failures,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3645,1,['failure'],['failures']
Availability,Don't log stack traces for known failures Closes #1817,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1824:33,failure,failures,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1824,1,['failure'],['failures']
Availability,Downgrade liquibase version for unknown mariadb AIOOBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4619:0,Down,Downgrade,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4619,1,['Down'],['Downgrade']
Availability,"Due to ambiguity in the explanation of how to pose questions rather than issues I will post this question (with the same text) here too and see whether it will get removed or answered. This question is the same as [the one I posted a week ago on biostars](https://www.biostars.org/p/448662/). I would like to apologize in advance for any ignorance regarding the documentation that I might have missed. It is not my intention to ask for what I would have known if I had read the documentation better, I am merely trying to grasp the concepts that are abstracted in the Cromwell metadata as described by [the paragraph about metadata in the Cromwell docs](https://cromwell.readthedocs.io/en/stable/SubWorkflows/). When executing a workflow written in WDL and executed with Cromwell (the scientific workflow engine) one can extract metadata out of the Cromwell database. Within this metadata, the following ""executionEvents"" are available for each ""workflow.task"" in the ""calls"" objects. Pending; Requesting ExecutionToken; WaitingFor ValueStore; PreparingJob; CallCache Reading; RunningJob; Updating CallCache; Updating JobStore. From the documentation:; [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. The main purpose of the [Job Store table](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores/#job-store-job_store_entry) is to support resuming execution of a workflow when Cromwell is restarted by recovering the outputs of completed jobs. I couldn't find a description of the Execution Token nor of the [Value Store](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/jobKeyValueStore/) in [the docs](https://cromwell.readthedocs.io/en/develop/developers/Arch). My questions are the following:. What is the engine waiting on when a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5579:926,avail,available,926,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5579,1,['avail'],['available']
Availability,"During CromIAM Perf testing, strange behavior occurred where when querying metadata for a workflow right after aborting it yielded a non empty value in the `failures` field, which later disappeared.; Below is an example metadata with the failure:. ```; {; ""calls"": {; ""wf_hello.hello"": [; {; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:157,failure,failures,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,2,['failure'],"['failure', 'failures']"
Availability,"During code review for #1836 @cjllanwarne noted that `processSource` in what is currently named `WorkflowStoreActor` and most likely `WorfklowStoreSubmitActor` by the time this is acted upon looked suspicious as we had (we think) intended json validation to not happen until later and a workflow ID would always be handed back to the user. Further, the failed Future doesn't appear to be getting handed back to the API at all (I think), which would lead to a timeout response. Further since the sources are being processed monadically it is possible for a user to have multiple borked files but only the first will be reported (if we were reporting). Check into what's up here - either don't perform this check on submission or ensure that appropriate error messages are handed back",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882:752,error,error,752,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882,1,['error'],['error']
Availability,During our Tech Talk today Thibault pointed out it could be quite bad if one Cromwell instance unexpectedly loses its connection to the database and other Cromwells pick up its workflows and start running them. This sparked a discussion that heartbeat TTL expiration is something that should be highly visible to make sure it isn't happening when not expected. A/C; Log when a workflow is claimed under `INFO` visibility,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4451:242,heartbeat,heartbeat,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4451,1,['heartbeat'],['heartbeat']
Availability,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1344:396,down,down,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344,1,['down'],['down']
Availability,"During the testing hackathon, we discovered a number of problems caused by the eventual consistency of the metadata service. One specific case of this is the granularity of the events. . On one side, we have a publisher who has a whole collection of events that they would like to push. They push them one event at a time to the MD service. Because even things like array elements are pushed one update at a time because of MD format, we run into the situation where a consumer can see half of an array. Taken to the extreme, we could push every char of a string as a separate event. The fundamental problem with these partial updates is that a downstream consumer can not tell if an update is complete. Do they wait? How long? Can they check if the data is done?. While this touches on a larger problem in distributed computing, I think we are shooting ourselves in the foot by making every piece of a single update an async, isolated event. Taken to the extreme, we could push every char of a string as a separate event. The proposal is to extend the PutMetadataAction to take in a Seq/Varargs of MetadataEvents with the contract that these will be made available atomically (e.g. in a single Slick transaction for our implementation). Then in places where we basically unrolling a bundle of events to publish, we should use this API (e.g. WorkflowExecutionActor) to do that atomically. . In theory, this should also help with the scalability as the MD service can persist things with batchinserts in single transaction. For larger workflows, currently this would be hundreds or thousands of transactions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/930:645,down,downstream,645,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/930,2,"['avail', 'down']","['available', 'downstream']"
Availability,"E, file = 93DAD89F707FA490E2A46FFAC924DFFF.; [2023-03-29 12:35:42,07] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,08] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1-20000000024 [b303ae23expanse_figures.CBL_hom_not_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,08] [info] BT-322 b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:2006,failure,failures,2006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['failure'],['failures']
Availability,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:154,error,error,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,2,['error'],['error']
Availability,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1461:427,ERROR,ERROR,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,ERROR - Scopes not configured for service account,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:0,ERROR,ERROR,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,1,['ERROR'],['ERROR']
Availability,ERROR: Finished parsing without consuming all tokens.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2296:0,ERROR,ERROR,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2296,1,['ERROR'],['ERROR']
Availability,"Edit (by @cjllanwarne) in light of #4806:. Following #4806 we will be able to read Google project metadata to specify a VPC network and subnet. Therefore what will remain for ***this*** ticket is making the same functionality available on a per-workflow basis... eg an ability to supply the same network/subnetwork information via workflow-options?. ---. ### Original issue text:. https://cloud.google.com/vpc/docs/vpc -- for a primer on GCP Subnets. Users should be able to tell Cromwell to launch nodes into a subnet. For environments like Firecloud, we should have some mechanism (like maybe SAM) to make sure the user actually has the right to use a particular subnet. . The main reason to do this is https://cloud.google.com/vpc/docs/using-flow-logs -- we want to be able to monitor traffic in and out of the network for more significant audited environments. So the driver is ultimately ""compliance"". But it's probably a good idea anyhow. After this is done, please work with FC team to make sure they can take advantage of this. I'm not sure who to tag to make sure this cross-team work is done.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070:226,avail,available,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070,1,['avail'],['available']
Availability,Emit 'rows deleted' metric on success as well as failure [BW-707],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6358:49,failure,failure,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6358,1,['failure'],['failure']
Availability,Enable/disable checking of task exit code to indicate task failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/185:59,failure,failure,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/185,1,['failure'],['failure']
Availability,Enhance Cromwell reporting of Martha errors [WA-340],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5845:37,error,errors,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5845,1,['error'],['errors']
Availability,Enhance map errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4661:12,error,errors,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661,1,['error'],['errors']
Availability,"Ensure GCS file systems use custom configuration.; When an exception/timeout occurs during asyncHashing, report it as a failure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2512:120,failure,failure,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2512,1,['failure'],['failure']
Availability,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4026:559,failure,failures,559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026,1,['failure'],['failures']
Availability,Erroneous error message creating womgraph,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3284:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3284,1,['error'],['error']
Availability,Error 500 in local backend when trying to localize from GCS,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,1,['Error'],['Error']
Availability,Error about trying to head an empty list,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1615:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615,1,['Error'],['Error']
Availability,Error code 10 take 2 [BA-5932],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5129:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5129,1,['Error'],['Error']
Availability,Error compiling under docker (Class name too long),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1428:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1428,1,['Error'],['Error']
Availability,Error handling when Service Registry fails to initialize (e.g. bad config),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/896:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/896,1,['Error'],['Error']
Availability,Error in Cromwell 24 on method that works in Cromwell 21,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937,1,['Error'],['Error']
Availability,Error message about reclaiming execution tokens might be important...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['Error'],['Error']
Availability,Error message despite Cromwell completing all its tasks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618,1,['Error'],['Error']
Availability,Error message for incorrect output string,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2875:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2875,1,['Error'],['Error']
Availability,Error message for inputs pointing to wrong project,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/589:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/589,1,['Error'],['Error']
Availability,Error message should specify which file has the error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933,2,"['Error', 'error']","['Error', 'error']"
Availability,"Error message unclear for invalid options file, Cromwell 24",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2041:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2041,1,['Error'],['Error']
Availability,"Error message: A timeout occurred waiting for a future to complete. Queried 100 times, sleeping 100 milliseconds between each query. tc: ServicesStore should not deadlock. https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/566/. Update 10/22; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/708/. Update 10/28:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/831/. Update 11/03:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/1003/. Update 11/06:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1076/. Update 11/09:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1166/. Further:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1337; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1422; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1445; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1489; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1525; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1590",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328,1,['Error'],['Error']
Availability,"Error message: The code passed to eventually never returned normally. Attempted 28 times over 20.881773345 seconds. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp). tc: HealthMonitor should fail if any subsystems fail but correctly bin them. Occurred at these times:; 2018-10-10 22:25:49 UTC; 2018-10-12 06:25:28 UTC; 2018-10-13 06:24:31 UTC; 2018-10-13 14:25:16 UTC; 2018-10-12 04:23:43 UTC; 2018-10-15 10:25:31 UTC; 2018-10-13 21:27:58 UTC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259,2,"['Error', 'failure']","['Error', 'failure']"
Availability,Error pulling docker image (tag regex fails),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2589:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2589,1,['Error'],['Error']
Availability,Error reading existing file containing float,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/928:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/928,1,['Error'],['Error']
Availability,"Error running ""docker pull"" when local look-up is used with SHA digest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['Error'],['Error']
Availability,Error running the GATK WDL best practices pipeline,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2064:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2064,1,['Error'],['Error']
Availability,Error when POST and GET are done too close together,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2671:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2671,1,['Error'],['Error']
Availability,Error with missing files in input JSON,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703,1,['Error'],['Error']
Availability,Error: Could not load UVM kernel module. Is nvidia-modprobe installed?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,1,['Error'],['Error']
Availability,Escalate service instantiation failures Closes #1266,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1772:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1772,1,['failure'],['failures']
Availability,"Example 1 (valid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[""1""], [""2""]]); }; }; ```; It will validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Success!; ```; And it will run successfully:; ```; $ java -jar cromwell-55.jar run main.wdl; ...; ""main.x"": [""1"", ""2""]; ...; ```. Example 2 (invalid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[""1""], [[""2""]]]); }; }; ```; It will validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Success!; ```; But it will error out:; ```; $ java -jar cromwell-55.jar run main.wdl; ...; Failed to evaluate 'main.x' (reason 1 of 1): Evaluating flatten([[""1""], [[""2""]]]) failed: No coercion defined from wom value(s) '[""2""]' of type 'Array[String]' to 'String'.; ...; ```. Example 3 (invalid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[[""1""]], [[""2""]]]); }; }; ```; It will not validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'Array[String] x = flatten([[[""1""]], [[""2""]]])' (reason 1 of 1): Cannot coerce expression of type 'Array[Array[String]+]+' to 'Array[String]'; ```. It is not a big deal, as the offending construct in Example 2 is not widely used, but it seems like a missed opportunity for flagging as an issue during the validation process. And why is it that womtool can flag Example 3 but not Example 2?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6185:559,error,error,559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6185,1,['error'],['error']
Availability,"Example WDL taken directly from the spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#workflow-level-resolution; ```; version 1.0. workflow wf {; input {; String s = ""wf_s""; String t = ""t""; }; call my_task {; String s = ""my_task_s""; input: in0 = s+""-suffix"", in1 = t+""-suffix""; }; }; ```; results in; ```; ERROR: Unexpected symbol (line 9, col 5) when parsing 'call_body'. Expected input, got ""String"". String s = ""my_task_s""; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4048:326,ERROR,ERROR,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4048,1,['ERROR'],['ERROR']
Availability,"ExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:144); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validateCredential(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.RefreshTokenMode.validateCredential(GoogleAuthMode.scala:127); at cromwell.filesystems.gcs.auth.RefreshTokenMode.credential(GoogleAuthMode.scala:147); at cromwell.filesystems.gcs.GcsPathBuilder.<init>(GcsPathBuilder.scala:57); at cromwell.filesystems.gcs.GcsPathBuilderFactory.withOptions(GcsPathBuilderFactory.scala:37); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:25); at cromwell.backend.impl.jes.JesWorkflowPaths.copy(JesWorkflowPaths.scala:19); at cromwell.backend.impl.jes.JesWorkflowPaths.withDescriptor(JesWorkflowPaths.scala:54); at cromwell.backend.io.WorkflowPaths$class.toJobPaths(WorkflowPaths.scala:51); at cromwell.backend.impl.jes.JesWorkflowPaths.toJobPaths(JesWorkflowPaths.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:1851,Error,Error,1851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,2,"['Error', 'error']","['Error', 'error']"
Availability,Exit with error code 1 on malformed CLI invocation [BA-3094],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5239:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5239,1,['error'],['error']
Availability,"Explicitly add `503` as a retryable error code on `StorageException`s because the `isRetryable` method seems to not be completely reliable.; Also retry SSL Exception (both top level and as a cause for `StorageException`s, I've seen both happen)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2208:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2208,2,"['error', 'reliab']","['error', 'reliable']"
Availability,Extra failure info for Job Manager in attempt metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4894:6,failure,failure,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4894,1,['failure'],['failure']
Availability,FC user have seen their job fail with this error message: https://gatkforums.broadinstitute.org/firecloud/discussion/comment/41300. Could be transient in which case retrying it could be the way to go,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2535:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2535,1,['error'],['error']
Availability,"FYI @cjllanwarne . FireCloud production is seeing a lot of these errors on a regular basis:; ```; message:2019-02-26 18:12:58 [cromwell-system-akka.actor.default-dispatcher-1160] ; ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: <snip>-EngineJobExecutionActor-CHSworkflow.GenerateAndRunScript:; ```. You can see more of these errors in production Kibana with a simple query on ""Job execution token returned from incorrect"". Current cromwell [version](https://api.firecloud.org/#!/Version/executionEngineVersion):; ```; {; ""cromwell"": ""36-7de344f""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4681:65,error,errors,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681,3,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"F_PREFIX,; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. output {; File sample = reads_extraction_and_merging.fastq_file; File genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; command <<<; cat ~{in_forward_fastq} ~{in_reverse_fastq} | pigz -dcp ~{in_cores} > ~{in_label}.fastq; >>>; output {; File fastq_file = ""~{in_label}.fastq""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; }; }. task genome_inference {; input {; String in_container_pangenie; File in_reference_genome; File in_pangenome_vcf; String in_executable; File in_fastq_file; String prefix_vcf; Int in_cores; Int in_disk; Int in_mem; }; command <<<; echo ""vcf: ~{in_pangenome_vcf}"" > /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""reference: ~{in_reference_genome}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo $'reads:\n sample: ~{in_fastq_file}' >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""pangenie: ~{in_executable}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""outdir: /app/pangenie"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; cd /app/pangenie/pipelines/run-from-callset; snakemake --cores ~{in_cores}; >>>; output {; File vcf_file = ""~{prefix_vcf}.vcf""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; preemptible: 1 # can be useful for tools which execute sequential steps in a pipeline generating intermediate outputs; }; }; ```; **_Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL:_**; ![Screenshot from 2022-12-09 10-52-16](https://user-images.githubusercontent.com/98895614/206773588-2e8dbf89-03a9-4021-9495-42f2bc0b801d.png). Please help me out on how to set",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:3054,echo,echo,3054,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,1,['echo'],['echo']
Availability,Fail workflow on localization failure Closes #922,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1800:30,failure,failure,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1800,1,['failure'],['failure']
Availability,Failure Messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/475:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/475,1,['Failure'],['Failure']
Availability,Failure in crc32cHash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738,1,['Failure'],['Failure']
Availability,Failure message for failed workflow should specify the shard number for scattered jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1479:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479,1,['Failure'],['Failure']
Availability,Failure messages include unwanted Some,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1893:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1893,1,['Failure'],['Failure']
Availability,Failure metadata migration. Closes #2037. Closes #2039,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2059:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2059,1,['Failure'],['Failure']
Availability,Failure metadata redux,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2055:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2055,1,['Failure'],['Failure']
Availability,"Failure metadata will look something like:. ```; ""failures"": [{; ""failure"": ""Task 874ad75a-e19e-4b53-8225-746df1436c51:ValidateAggregatedSamFile was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3).\nError code 10. Message: Some(14: VM ggp-15150083938845849899 stopped unexpectedly.)"",; ""timestamp"": ""2016-12-18T05:54:07.296Z""; }],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1893:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1893,3,"['Failure', 'failure']","['Failure', 'failure', 'failures']"
Availability,Failure mode 3 from #1253,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1410:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1410,1,['Failure'],['Failure']
Availability,Failure to find imported WDLs using relative paths,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4515:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4515,1,['Failure'],['Failure']
Availability,Failure to localize should fail job immediately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4809:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809,1,['Failure'],['Failure']
Availability,Failure to recognize variable declared in task call,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4048:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4048,1,['Failure'],['Failure']
Availability,Failure to transfer String output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2115:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2115,1,['Failure'],['Failure']
Availability,Failure using compound types in runtime attributes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685,1,['Failure'],['Failure']
Availability,"Failure writing to call cache, right truncation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['Failure'],['Failure']
Availability,Failures during localization,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1071:0,Failure,Failures,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071,1,['Failure'],['Failures']
Availability,False alarm error in runtime block w/ coercion,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2643:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2643,1,['error'],['error']
Availability,Feature request: Dirt simple error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3226:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226,2,['error'],['error']
Availability,"Figure out how it is possible that travis can not compile the build but returns 0. Incident report:; [travis build](https://travis-ci.org/broadinstitute/cromwell/jobs/292596774); [raw log, excerpted below](https://s3.amazonaws.com/archive.travis-ci.org/jobs/292596774/log.txt?X-Amz-Expires=30&X-Amz-Date=20171025T234339Z&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20171025/us-east-1/s3/aws4_request&X-Amz-SignedHeaders=host&X-Amz-Signature=71b729c64e9fbce0fb7e520c77d2d3da8876b62e8e4f5f87b5084f594b439ee0). When compiling 2.11 `cwl` package, compilation reported an error, yet still returned 0:; ```; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0mtwo errors found[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2788:594,error,error,594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2788,1,['error'],['error']
Availability,FileSystems available to the Engine for expression evaluation should be configurable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/821:12,avail,available,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/821,1,['avail'],['available']
Availability,"Filling in the missing meta and parameter_meta types. This makes Cromwell/WOM support the spec with JSON like structures available for the meta sections. . ```; $parameter_meta = 'parameter_meta' $ws* '{' ($ws* $parameter_meta_kv $ws*)* '}'; $parameter_meta_kv = $identifier $ws* ':' $ws* $meta_value; $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array; $meta_object = '{}' | '{' $parameter_meta_kv (, $parameter_meta_kv)* '}'; $meta_array = '[]' | '[' $meta_value (, $meta_value)* ']'; ```. There is a part I didn't know how to properly code, which is `WorkflowDescription` support ( services/src/main/scala/cromwell/services/womtool/models/MetaValueElementJsonSupport.scala). I could use help there. . Thank you, ; Ohad.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5053:121,avail,available,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053,1,['avail'],['available']
Availability,"Find the original post [here](http://gatkforums.broadinstitute.org/wdl/discussion/7690/running-in-server-mode-jobs-that-have-localization-error-become-immortal#latest). ---. **User Report**. Running cromwell in server mode, with default configuration in each case, I can reproduce the following behaviour in 0.18, 0.19 and 0.19_hotfix (HEAD):. Submit a workflow that has non-existent file as input to a task, e.g.:. ```; task BillyBob {; File bbInput; command { echo ""done"" }; }; workflow badLocalization {; call BillyBob { input: bbInput=""/foo/bar/baz"" }; }; ```. The server log shows ""Failures during localization"" error (below) - as expected initially, I guess - but then _repeats_ the error every 30 seconds or so, forever, and hitting the API `<workflowId>/status` endpoint shows the job in a ""Running"" state, forever. I would expect this error to cause the task, and then the workflow, to die. example of a single block of the server log error: . ```; 2016-05-27 11:08:57,269 cromwell-system-akka.actor.default-dispatcher-5 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:138,error,error-become-immortal,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,7,"['Failure', 'echo', 'error']","['Failures', 'echo', 'error', 'error-become-immortal']"
Availability,"Fix CWL ""failed with null"" error message",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4190:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4190,1,['error'],['error']
Availability,"Fix broken doc links, update error messages [no JIRA]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5779:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5779,1,['error'],['error']
Availability,Fix coercion failure handling,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/83:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/83,1,['failure'],['failure']
Availability,Fix copy/paste error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7005:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7005,1,['error'],['error']
Availability,Fix flakey test: RobustClientHelperSpec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:17,Robust,RobustClientHelperSpec,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,Fix not-so-intermittent failures with WorkflowManagerActorSpec.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/224:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/224,1,['failure'],['failures']
Availability,Fix occasional errors in WomtoolServiceInCromwellActorSpec due to newâ€¦,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5471:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471,1,['error'],['errors']
Availability,"Fix order of args for error method, stringify stack trace for others.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/294:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/294,1,['error'],['error']
Availability,Fix recover on restart [WX-927],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:4,recover,recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,1,['recover'],['recover']
Availability,"Fix the ""non-infinite"" retry bug on transient errors",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/407:46,error,errors,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/407,1,['error'],['errors']
Availability,Fix transient errors on retry + add more tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/407:14,error,errors,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/407,1,['error'],['errors']
Availability,"Fix/update for [WX-1210](https://broadworkbench.atlassian.net/browse/WX-1210). Turns out that the `head_commit` attribute is not available on `pull_request` actions, which is why the JIRA ID check kept failing. I'm opting to use `github.event.pull_request.title` which is accessible on `pull_request`. [WX-1210]: https://broadworkbench.atlassian.net/browse/WX-1210?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7184:129,avail,available,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7184,1,['avail'],['available']
Availability,"Fixed Symbol doubly qualifying the symbol names. (thx mcovarr/scottfrazer); Added `WdlType.fromRawString`, with test against respective `WdlValue.toRawString`.; `DummyDataAccess` replaced with using `DataAccess` instances, with cleanup of connections.; When creating in memory databases will create unique `DataAccess` instances, just like Dummy.; TestSlickDatabase now prints a warning, instead of an error, when unable to connect to MySql.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/84:402,error,error,402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/84,1,['error'],['error']
Availability,Fixed `programmer error` messages related to race conditions in RootWorkflowFileHashCacheActor [BA-6503],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5580:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5580,1,['error'],['error']
Availability,Fixed a collector failure bug and a slow and unnecessary GCS glob,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/831:18,failure,failure,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/831,1,['failure'],['failure']
Availability,Fixed error messages for MetadataTooLargeException exceptions and suppressed stacktraces [BA-6456],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5536:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5536,1,['error'],['error']
Availability,Fixed error on attempt to get metadata for workflow with archived status TooLargeToArchive [BA-6475],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5541:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5541,1,['error'],['error']
Availability,Fixed timeout error during mysql testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398,1,['error'],['error']
Availability,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3048:99,error,error,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048,5,['error'],['error']
Availability,"Fixes #4084 ; For singularity users it is nice that `dockerRoot` can be set, as they do not have control over which paths are available in the image and `/cromwell-executions` cannot be automatically created. Other paths could be used, BioContainers for example have a `/data` folder meant specifically for these use cases. But this requires dockerRoot to be configurable. The fallback value is still `/cromwell-executions` so nothing changes if the value is not specified in the config.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088:126,avail,available,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088,1,['avail'],['available']
Availability,Fixes broken builds:; ```; [error] (wdlDraft3LanguageFactory/*:dockerPush) Failed to push; [error] (wdlDraft2LanguageFactory/*:dockerPush) Failed to push; [error] (cwlV1_0LanguageFactory/*:dockerPush) Failed to push; [error] (languageFactoryCore/*:dockerPush) Failed to push; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3352:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3352,4,['error'],['error']
Availability,Fixes runtime warning introduced in https://github.com/broadinstitute/cromwell/pull/5565. Intentional stowaway: error message improvement for @barkasn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5570:112,error,error,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5570,1,['error'],['error']
Availability,Fixing '[error] File name too long' on sbt assembly in Docker,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/226:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/226,1,['error'],['error']
Availability,Fixing https://github.com/broadinstitute/cromwell/issues/4050. While doing this I notice that this check alive command is not used at all. First did a restructure of the statuses and now there is also a status Running. - [x] Add timeout on `WaitingForReturnCode` step; - [x] Make timeout a config value. Meanwhile please give already feed on this of course ;),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112:105,alive,alive,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112,1,['alive'],['alive']
Availability,Fixing two bugs in syntax error detection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/109:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/109,1,['error'],['error']
Availability,FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.Error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:3674,Robust,RobustClientHelperSpec,3674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,Flatten Failure Metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2038:8,Failure,Failure,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2038,1,['Failure'],['Failure']
Availability,"Follow up to [WX-1410](https://github.com/broadinstitute/cromwell/pull/7414) which introduced a bug converted metadata values to strings before database insertion. In addition to fixing the bug, this PR introduces more robust unit testing to confirm that metadata values that should not be modified remain unmodified. [WX-1410]: https://broadworkbench.atlassian.net/browse/WX-1410?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7427:219,robust,robust,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7427,1,['robust'],['robust']
Availability,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1702:238,echo,echo,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Following up on https://github.com/broadinstitute/cromwell/pull/5321, this is another case when GCS IoActor fails with a retryable error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344,1,['error'],['error']
Availability,"For FC provenance tracking we need to keep an eye on the file hashes of workflow inputs. My understanding is that call-caching stores the CRC32c of each _call_ input, but it's difficult to trace those hashes back to workflow inputs. Could you store the hashes of workflow inputs at job submit time and throw them in the workflow metadata?. Ping @abaumann for prio but I don't think it's super urgent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629:340,Ping,Ping,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629,1,['Ping'],['Ping']
Availability,"For Workflow ID: 5541d851-10bb-455d-a6bc-051e85574b74. We're getting nothing back when we try to look at timing or metadata, the metadata curl call returns:. {; ""status"": ""error"",; ""message"": ""None.get""; }. No clue what's going on here, the cromwell logs don't indicate anything unusual for this workflow. Seemingly it ran normally, just some typical preemption messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/801:172,error,error,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/801,1,['error'],['error']
Availability,"For example, test 3 represents:; ```; E1; caused By A1; - E2; caused by E3; - E4; - E5; ```. Being converted into this:; ```; failures: [{; message: ""E1"",; causedBy: [{; message: ""A1"",; causedBy: [{; message: ""E2"",; causedBy: [{; message: ""E3"",; causedBy: []; }]; }, {; message: ""E4"",; causedBy: []; }, {; message: ""E5"",; causedBy: []; }]; }]; }]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2055:126,failure,failures,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2055,1,['failure'],['failures']
Availability,"For instance, someone used ""docker.io/<their image>"" path in their runtime block and this led to a failure to call cache since call caching doesn't support docker.io urls. . Right now, since this failed to call cache it reran the job, however this was due to user error and they assumed caching would work. Additionally, there are cases such as when there's a failure to communicate with dockerhub that would also lead to failure to call cache. This request is for a workflow option that worked in conjunction with read_from_cache that set this behavior more strictly - if there is call cache miss, then run a new job, but if there is any other type of failure to call cache (transient, bad url, etc) then fail the job for the reason that the user requested to call cache and call caching failed to determine caching correctly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2787:99,failure,failure,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2787,5,"['error', 'failure']","['error', 'failure']"
Availability,"For large output arrays this can get really bogged down as it's processing single threaded. At first glance it looks like it should be monoid-y, perhaps we can farm this out to multiple workers?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1776:51,down,down,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776,1,['down'],['down']
Availability,"For some reason, the expression `| cat` was breaking pushing with tags. As a result, the `gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer` repo was filling up with tagless images. When we went to delete-by-tag, the operation failed. Probably you have seen this message on your PRs:. ```; Please wait, building cromwell-drs-localizer into gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-8281552534-papiâ€¦; The push refers to repository [gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer]; [...]; ERROR: (gcloud.container.images.delete) [gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-8281552534-papi] is not found or is not a valid name. Expected tag in the form ""base:tag"" or ""tag"" or digest in the form ""sha256:<digest>""; ```. Fortunately, some time since 2019 Docker added the `--quiet` tag so we can adopt that instead. Once I fixed that problem, we had a new problem where `centaurPapiV2beta`, `centaurPapiV2betaRestart`, and `centaurHoricromtalPapiV2beta` jobs were all pushing images tagged `github-[build number]-papi`. This caused the most recent image to have the tag while the other two were untagged and leaked. I solved it by salting the tag with the job name:. ![Screenshot 2024-03-18 at 20 46 25](https://github.com/broadinstitute/cromwell/assets/1087943/53f2573d-7b42-4470-b3fa-e41f31cb9179). Now we can get rid of the 2.3 TB of old images and they won't come back, saving about $60 per month at current GCS list prices. ![Screenshot 2024-03-18 at 17 43 15](https://github.com/broadinstitute/cromwell/assets/1087943/9276c4c1-9eea-4781-9e97-94000319d658)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7390:517,ERROR,ERROR,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7390,1,['ERROR'],['ERROR']
Availability,"For the following task ; ```; task star_index {. File genomeDir; File genomeFasta; Int threads; Int binBits; Int max_memory = 100000000000. command {; /usr/local/bin/STAR \; --runThreadN ${threads} \; --runMode genomeGenerate \; --genomeDir ${genomeDir} \; --genomeFastaFiles ${genomeFasta} \; --genomeChrBinNbits ${binBits} \; --limitGenomeGenerateRAM=${max_memory}; }. runtime {; docker: ""quay.io/biocontainers/star@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b""; }. output {; File out = genomeDir; }. }; ```; I get ; ```; ""Unable to load namespace from workflow: For input string: \""100000000000\""""; ```; error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2744:634,error,error,634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2744,1,['error'],['error']
Availability,"For the new /describe endpoint being added for the purposes of Womtool-as-a-Service, implement the ability to validate a given workflow source file. AC: Given a WDL workflow source file (not http url), return whether its a valid WDL workflow, and provide errors if its invalid. ```; {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4433:255,error,errors,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4433,4,['error'],['errors']
Availability,"For the new /describe endpoint developed for the purposes of Womtool-as-a-Service, implement the ability to return inputs for a given workflow source file. AC: Given a WDL workflow source file (not http url), return the optional & required inputs available for this workflow, similar to what would be returned if using the `inputs` function with womtool:. {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ]; ""inputs"": [ ; { ; ""name"": ""inFile"",; ""valueType"": ""File"",; ""optional"": false; }, {...}; ]; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4452:247,avail,available,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4452,4,"['avail', 'error']","['available', 'errors']"
Availability,"For the new /describe endpoint developed for the purposes of Womtool-as-a-Service, implement the ability to return outputs for a given workflow source file. AC: Given a WDL workflow source file (not http url), return the outputs declared for this workflow:. {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ]; ""outputs"": [; {; ""name"": ""inFile"",; ""valueType"": ""File"",; ""optional"": false; }, {...}; ]; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4461:278,error,errors,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4461,3,['error'],['errors']
Availability,"For users just getting started with Cromwell, the documentation here:. https://github.com/broadinstitute/cromwell#getting-started-with-wdl. Points the user here:. https://github.com/broadinstitute/wdl#getting-started-with-wdl. and when you run your first workflow, you'll see in the output:. ```; [2016-04-14 16:21:12,82] [warn] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme(GoogleCredentialFactory.scala:61); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$$anonfun$cromwellAuthenticated$1.apply(StorageFactory.scala:20); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$$anonfun$cromwellAuthenticated$1.apply(StorageFactory.scala:20); at scala.util.Try$.apply(Try.scala:192); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated$lzycompute(StorageFactory.scala:20); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated(StorageFactory.scala:18); at cromwell.engine.backend.local.LocalBac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:441,avail,available,441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,2,['avail'],['available']
Availability,For whatever reason I get a different `failure`/`causedBy` stack when I run these locally vs in travis. I'm probably being stupid ðŸ¤·â€â™‚ï¸. . Reinstate these centaur tests:; - [x] `invalid_runtime_attributes`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2904:39,failure,failure,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2904,1,['failure'],['failure']
Availability,For workflow-success and workflow-failure type tests:. * Wait for the query results for the workflow to indicate carbonite complete; * Ensure that the metadata is as valid afterwards as it was beforehand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237,1,['failure'],['failure']
Availability,Format API Error reporting to common json structure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368:11,Error,Error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368,1,['Error'],['Error']
Availability,"Found during workshop - error formatter caret points to the wrong thing. Looks like a tabs vs. spaces thing.; ```; workflow HelloWorld {. 	call WriteGreetings; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; ERROR: Call references a task (WriteGreetings) that doesn't exist (line 3, col 7). 	call WriteGreetings; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4041:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041,3,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"Found during workshop - incomprehensible error output.; ```; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = asdf(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; wdl.draft2.model.expression.WdlStandardLibraryFunctionsType.asdf(scala.collection.Seq); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4042:41,error,error,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4042,2,"['echo', 'error']","['echo', 'error']"
Availability,"Frequent ""Job execution token returned from incorrect actor"" error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4681:61,error,error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681,1,['error'],['error']
Availability,"From @yfarjoun . > I've been playing around with some wdl on the methods cromwell that has caching turned on. I have some results I do not understand. I modified the bwa step in a small way for input that has already run all the way to HaplotypeCaller. Re-running that wdl shows that the upstream steps are fast (as expected), the BWA step is re-done (again, as expected), but the downstream steps have been used from cache...this seems wrong to me, after all, if the BWA has been re-run, all the downstream steps need to be re-run as well...could you explain what's going on?. For example, the ""PairedEndSingleSampleWorkflow.MergeBamAlignment"" step should have been recomputed as it is downstream of the BWA step. Here's a [timing diagram](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/timing) and [metadata](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/metadata): . metadata also attached here. Although we don't have call caching in 0.20+ we should understand this problem (or clarify why it's not a problem) so that we don't replicate the bug (if that's what it is)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1150:381,down,downstream,381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1150,3,['down'],['downstream']
Availability,"From Henry --. My comment is a little different than the commenter above. So I will try to provide some context in order to differentiate. If one looks at Cromwell as being part of a larger application infrastructure/service such as a Genomics Pipeline where sequenced data is constantly being processed through the system or providing a service where users on the Internet can launch workflows ""as a Service"" - whenever they way. In these systems, uptime and availability are critical - at all times. Below are several scenarios where I could see running multiple Cromwells would be very beneficial. High availability:; Having multiple Cromwells running where jobs are ""load balanced"" between them would allow us to continue operate when there are systems issues or failures with one of the Cromwell instances. Zero-downtime deployments:; Supporting multiple Cromwells running different versions of the code, could provide the ability to upgrade Cromwell with little or no user impact. Essentially the new version is deployed to a new server, it is started up and at an appropriate time traffic (via a load balancer or proxy maybe) is directed away from the ""old version"" Cromwell to the new. Similar to item 2: being able to introduce infrastructure changes (host OS, security patches, host resizing,..) more seamlessly:; If I can support multiple instances of Cromwell, I can build a new instance of the host with all the updates and changes I require - deploy cromwell to the new node and cut over. A corollary to this request is also the ability to ""move"" workflows from one Cromwell instance to another. Maybe this is just workflows not in flight or active - but waiting to run. This capability could make it easier to retire older cromwell instances (once multiple cromwell instances support is in place). Some workflows may take days to run, being able to ""relocate"" these workflows to the ""new"" cromwell - allows us to decommission ""old"" cromwells faster.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/691:460,avail,availability,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/691,4,"['avail', 'downtime', 'failure']","['availability', 'downtime', 'failures']"
Availability,"From [a user's forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/12577/stdout-stderr-output-in-bucket-does-not-seem-to-be-updated-while-task-is-running#latest):. 1. `stderr` and `stdout` do not appear to show up until a workflow finishes. ; 2. In some cases `stderr` and `stdout` are of type `application/octet-stream` rather than `text/plain`, not allowing the content to be viewable without downloading. . I was able to see these by running a [five-dollar-genome-analysis-pipeline](https://portal.firecloud.org/#workspaces/fccredits-silver-pumpkin-7172/five-dollar-genome-analysis-pipeline_copy) as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3967:414,down,downloading,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3967,1,['down'],['downloading']
Availability,"From `IntToIntArray` in the JG workflow, this was working on 23 but not 24. I chatted with @Horneth and he said it wasn't intentional so let's fix it. Presumably it's a coercion issue but unclear. error was:. `to evaluate outputs.: RuntimeException: Could not evaluate IntToIntArray.array = read_lines(stdout()); wdl4s.util.AggregatedException: Failed to evaluate outputs.: RuntimeException: Could not evaluate IntToIntArray.array = read_lines(stdout())`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1761:197,error,error,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1761,1,['error'],['error']
Availability,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:295,ERROR,ERROR,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['ERROR'],['ERROR']
Availability,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916:494,ERROR,ERROR,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916,5,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"From looking at the behaviour of cromwell, I think it traverses the execution graph (is that what it's called, the order in which tasks are run?) breadth-first, (e.g. first perform trimming for all samples, then mapping for all samples, then genotyping, etc). For most workflows, different tasks will have different performance characteristics (trimming and mapping is read/write heavy, genotyping is mostly read/CPU intensive). Would it then not make sense to do a depth first traversal of the execution graph? That way, we will have the most diversity in performance characteristics for all running tasks, which should speed up the overall runtime (e.g. no fighting over harddisk time between two trimming tasks). As a secondary bonus, depth first will mean that all different tasks are run as soon as possible, so when there is an error in one of the later tasks this is revealed to the user much more quickly.; The drawback is that cromwell will then also stop running the early tasks that do not give an error, but IIRC that behaviour is configurable in the settings file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736:834,error,error,834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736,2,['error'],['error']
Availability,"Fully caches Genome in a Bottle chm to run in ~2 hours. Still waiting on results from GIAB Joint because that takes much longer and has to be restarted for sporadic failures, but all the calls so far will cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3541:165,failure,failures,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3541,1,['failure'],['failures']
Availability,GCP Batch backend not recovering workflows on restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:22,recover,recovering,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recovering']
Availability,GCP: Ignore error if ssh-server fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771,1,['error'],['error']
Availability,GCPBATCH: accessing private gcr.io docker for callcaching raises error: unauthorized,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:65,error,error,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['error'],['error']
Availability,GCR Docker-Content-Digest v2 not available for all images,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:33,avail,available,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,1,['avail'],['available']
Availability,GCS storage appears to be created redundantly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1437:34,redundant,redundantly,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437,1,['redundant'],['redundantly']
Availability,"GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 4; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4988,ERROR,ERROR,4988,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['ERROR'],['ERROR']
Availability,"GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""eu-west4-a"",""eu-west4-b"",""eu-west4-c""]; }. include ""papi_v2_reference_image_manifest.conf""; }; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14634,ERROR,ERROR,14634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['ERROR'],['ERROR']
Availability,"GOTC keeps track of very workflow failure in a spreadsheet, and summarizes. @bradtaylor has shown it to me before, so he might be the place to start. The task here is to review that spreadsheet, provide a summary of the top failure modes and schedule a tech/design meeting to define the tickets that would have prevented these failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820,3,['failure'],"['failure', 'failures']"
Availability,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763:158,error,error,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763,4,['error'],"['error', 'errors']"
Availability,Gather reliability / workflow failure causes from GOTC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820:7,reliab,reliability,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820,2,"['failure', 'reliab']","['failure', 'reliability']"
Availability,"GitHub ""helpfully"" collapses `JsonEditorSpec` due to the scope of the changes, but actually that does need to be reviewed. ðŸ™‚ . Does:. * Fix `exclude` to only examine workflows and calls; * Support `:` syntax in excludes; * Add `ErrorOr` validation to method signatures; * ""Adjust"" `JsonEditorSpec` to not actively test for incorrect behavior. Does not:. * Fix `include` to only examine workflows and calls; * Support `:` syntax for include; * Add as many real-world or rigorous tests as I would like, mostly because the aforementioned things are still broken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5309:228,Error,ErrorOr,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5309,1,['Error'],['ErrorOr']
Availability,Give cromwell the opportunity to shut down gracefully in Caas,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3674:38,down,down,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3674,1,['down'],['down']
Availability,"Given the eventually consistent scheme in place for deriving workflow status, it's likely that a status check soon after workflow submission will fail with a 404 response. From a UX perspective a bogus 404 error due to eventual consistency feels wrong; a lag in status update is one thing, but Cromwell probably shouldn't disavow the existence of the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/962:206,error,error,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/962,1,['error'],['error']
Availability,"Given this workflow:; ```; workflow w {; 	call t {}; 	output { t.* }; }. task t {; 	; 	command {; 		touch file.txt; 		echo ""file.txt"" > filename.txt; 	}. 	output {; 		File f1 = ""filename.txt""; 		File f2 = read_string(f1)		; 	}. 	runtime {; 	 docker: ""ubuntu""; 	}; }; ```; One would expect 2 files to be marked for delocalization for `call t`, yet only ""filename.txt"" is properly localized, but not ""file.txt"". This behavior is only observed when running against the JES backend, and works as expected on the SharedFileSystem backend. This is a regression introduced in Cromwell v30. When checking Google's operation metadata, the outputs are listed as:; ```; outputs:; filename.txt: gs://rm-dev/w/6b29d52a-d665-4ce9-b042-8cd4e4374473/call-t/filename.txt; t-rc.txt: gs://rm-dev/w/6b29d52a-d665-4ce9-b042-8cd4e4374473/call-t/t-rc.txt; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3065:118,echo,echo,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3065,1,['echo'],['echo']
Availability,"Glob only appears to work if the globbed files are in the same file system cromwell's exec directory. Example:; ```; task doGlob {; String globExpr. command {; echo ""noop"" > /dev/null; }; output {; Array[File] files = glob(globExpr); }; runtime {; backend: ""Local""; }; }; ```. If `globExpr` points to a different file system from where the `exec` dir is, this fails. The reason seems to be that `glob` only attempts **hard-linking**, and if that fails, it does not attempt symbolic linking. Here's an extract of the generated `script` file:. ```; ( ln -L /tmp/my/path/* /my/cromwell/exec/myWf/.../call-doGlob/execution/glob-64b5dd2db11682a95849e477695e878c 2> /dev/null ) || \; ( ln /tmp/my/path/* /my/cromwell/exec/myWf/.../call-doGlob/execution/glob-64b5dd2db11682a95849e477695e878c ); ```. This part fails with `Invalid cross-device link`. . However, the script itself does not fail - it goes on to list the contents of (empty) `glob-xxxx` directory, and returns an empty array. Running on local backend but I assume a HPC backend would get the same behaviour. For context, the reason why I need a task like this is because putting globs in arguments is still not supported.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4395:160,echo,echo,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4395,1,['echo'],['echo']
Availability,Google Cloud backend v2beta - Hello World documentation outdated + failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:67,failure,failure,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['failure'],['failure']
Availability,"Google Genomics fails when transferring the String output from a task to a bucket (Cromwell 25, using the wdl_runner pipeline). Here is the offending WDL file:; ```yaml; task Print_String {; command {; echo hello; }; runtime {; docker: ""ubuntu:14.04""; cpu: ""1""; memory: ""1 GB""; disks: ""local-disk "" + 10 + "" HDD""; }; output {; String out_string = read_string(stdout()); }; }. workflow My_Workflow {; call Print_String {; }; }; ```. This results in the error log:; ```; 2017-03-31 03:39:42,716 wdl_runner INFO: Workflow output files = [u'hello']; 2017-03-31 03:39:42,716 file_util INFO: Copying [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/; 2017-03-31 03:39:45,722 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 0; 2017-03-31 03:39:48,695 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 1; 2017-03-31 03:39:51,643 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 2; ERROR: copying files from [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: CommandException: No URLs matched: hello. (exit status 1); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2115:202,echo,echo,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2115,3,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"Got a centaur failure with `Failed to upload auth file` caused by the following exception, not considered retryable:. ```; 017-04-18 21:11:49,413 cromwell-system-akka.dispatchers.engine-dispatcher-64 ERROR - WorkflowManagerActor Workflow 6e23463e-3fc6-4b18-aeb0-fc7c920cd758 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:63); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:62); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageExc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:14,failure,failure,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,4,"['ERROR', 'failure', 'recover']","['ERROR', 'failure', 'recoverWith']"
Availability,Graceful shut down is expecting a `Ctrl-C` or SIGINT (possibly SIGKILL too?). Docker stop just turns off the lights. We should document how to stop a container in order to take advantage of the graceful shutdown process.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2562:14,down,down,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2562,1,['down'],['down']
Availability,Graceful shutdown of cromwell wrt workflow state and heartbeats,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4242:53,heartbeat,heartbeats,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242,1,['heartbeat'],['heartbeats']
Availability,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4641:26,error,errors,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641,2,"['error', 'failure']","['errors', 'failures']"
Availability,GreenTeam saw 3 406 errors when polling our workflows for status. These occurred on 5/3/2016. Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406; Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406; Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775:20,error,errors,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775,4,"['Error', 'error']","['Error', 'errors']"
Availability,"HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6638,alive,alive,6638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,['alive'],['alive']
Availability,Handle 'insufficient data written' errors from the JES API Query Manager,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2535:35,error,errors,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2535,1,['error'],['errors']
Availability,Handle invalid inputs parsing error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4375:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4375,1,['error'],['error']
Availability,"Have not not found anywhere a complete example showing it working, including what should be int eh .conf file. If such an example exists, please point us to it,. Command:; nohup java -Dconfig.file=My.conf -jar cromwell-87-5448b85-SNAP-pre-edits.jar run ~/MemoryRetryTest.wdl 2>&1 > nohup.out. MemoryRetryTest.wdl:; workflow MemoryRetryTest {; 	String message = ""Killed""; 	; 	call TestOutOfMemoryRetry {}; 	call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; 	command <<<; 		free -h; 		df -h; 		cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1023,error,error-keys,1023,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,"['Error', 'error']","['Error', 'error-keys']"
Availability,"Having default runtime attributes in jes config caused faulty WARN messages about ""Unrecognized configuration key(s) for Jes"". This PR should fix those.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3662:55,fault,faulty,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3662,1,['fault'],['faulty']
Availability,"Hello Cromwell People, . Currently I believe Cromwell has retry logic for I/O issues or preemptible VMs issues for Google Cloud,. However, when Cromwell jobs that are executed via GridEngine dispatcher will fail with no re-try if the return code is deemed as a error code,. I am submitting a potential patch where Cromwell can retry failed jobs running on GridEngine with user specified retries (""backend.max-job-retries""), . I'm not sure how the configurations should be organized but here is my starting point; let me know what you guys think. Thanks,; Paul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176:261,error,error,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176,1,['error'],['error']
Availability,"Hello Cromwell Team, . Our bioinformatics team have been reporting a single retry after preemptible attempts have been exhausted. They've added logic in the task itself that introspects the vm in the event the job ends up on a non-preemptible VM and promptly exists. This isn't ideal as starting a VM still incurs cost. . I've made the follow changes in:; ```diff; --- a/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala; +++ b/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala; @@ -882,8 +882,11 @@ class PipelinesApiAsyncBackendJobExecutionActor(override val standardParams: Sta; else {; val msg = s""$baseMsg The maximum number of preemptible attempts ($maxPreemption) has been reached. The "" +; s""call will be restarted with a non-preemptible VM. Error code $errorCode.$prettyPrintedError)""; - FailedRetryableExecutionHandle(StandardException(; - errorCode, msg, jobTag, jobReturnCode, standardPaths.error), jobReturnCode, kvPairsToSave = Option(preemptionAndUnexpectedRetryCountsKvPairs)); + FailedNonRetryableExecutionHandle(; + StandardException(errorCode, msg, jobTag, jobReturnCode, standardPaths.error),; + jobReturnCode,; + kvPairsToSave = Option(preemptionAndUnexpectedRetryCountsKvPairs); + ); }; ```. and tested with a trivial WDL and tasks such as (trying out multiple premptible / maxRetries):. ```wdl; task crash {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" && sleep infinity ; }; output {; String message = read_string(stdout()); }; runtime {; preemptible: 3; maxRetries: 0; docker: ""ubuntu:latest""; }; }. workflow wf_preempt {; call crash. output {; crash.message; }; }. ```. Let me know if I'm going in the right direction for a pull request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666:942,Error,Error,942,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666,7,"['Error', 'echo', 'error']","['Error', 'echo', 'error', 'errorCode']"
Availability,"Hello I am trying to re-use an existing workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; â””â”€â”€ Mutect2; â””â”€â”€ e5769b79-5e02-44a5-a4f8-38745e152beb; â”œâ”€â”€ call-M2; â”‚ â””â”€â”€ shard-0; â”‚ â”œâ”€â”€ execution; â”‚ â””â”€â”€ inputs; â”‚ â”œâ”€â”€ -1816294717; â”‚ â”œâ”€â”€ 1855713868; â”‚ â”‚ â””â”€â”€ run_cromwell_only.tmp; â”‚ â”‚ â””â”€â”€ cromwell-executions; â”‚ â”‚ â””â”€â”€ Mutect2; â”‚ â”‚ â””â”€â”€ e5769b79-5e02-44a5-a4f8-38745e152beb; â”‚ â”œâ”€â”€ 2035192126; â”‚ â””â”€â”€ 891763929; â””â”€â”€ call-SplitIntervals; â”œâ”€â”€ execution; â”‚ â”œâ”€â”€ glob-0fc990c5ca95eebc97c4c204e3e303e1; â”‚ â””â”€â”€ interval-files; â”œâ”€â”€ inputs; â”‚ â””â”€â”€ -1816294717; â””â”€â”€ tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/981352",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:61,avail,available,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,2,['avail'],['available']
Availability,"Hello cromwell dev team,. I'm currently working with the reference-disks option using GCPBatch as my backend. I executed the create_images.sh script from the documentation (https://cromwell.readthedocs.io/en/develop/backends/GCPBatch/) to generate my reference-disk-localization-manifests. I'm also using Cromwell v87, as specified in the same documentation. I also tested with the current `develop` build. While the manifest is correctly configured in the Cromwell config, and the reference disk appears to be mounting successfully, Iâ€™m encountering a failure with the umount command. Iâ€™m not sure why this command is being invoked in the first place. Mount Image; <img width=""573"" alt=""Screenshot 2024-09-26 at 16 07 46"" src=""https://github.com/user-attachments/assets/ce43be4a-132b-4c87-a27d-718a376171f7"">. **Error Message:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. Below is an example of what I have included in my Cromwell configuration (cromwell.conf). . ```; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/private-test-cromwell/global/images/omics-reference-disk-image"",; ""diskSizeGb"" : 10,; ""files"" : [ ; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.dict"",; ""crc32c"" : 2158779318; },; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta"",; ""crc32c"" : 420322484; },; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai"",; ""crc32c"" : 1970999569; }; ]; }; ]; ```. @mcovarr and @aednichols, I have seen issue [#7502](https://github.com/broadinstitute/cromwell/pull/7502)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7548:553,failure,failure,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7548,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"Hello guys. I met some problem when I import a wdl in another one. The following code is part of the wf2.wdl:. import ""wf1.wdl"" as wf1. workflow wf2{. input{; String token; }; ; call wf1.t1{; input:; token = token; }; output { ; File final_file = t1.file; }; }. The error message told me:. `Workflow input processing failed:; Failed to import 'wf1.wdl' (reason 1 of 1): Failed to resolve 'wf1.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path`. wf1.wdl and wf2.wdl are in the same path, accounting to official documentationï¼Œ"" import 'wf1.wdl' as wf1"" is valid. ; WOMtool also gives the same view. Furthermore, I have tried relative and absolute path. However both of them did not work. Therefore, any ideas about it? Thank you for your comment!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7101:266,error,error,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7101,1,['error'],['error']
Availability,"Hello! I am trying to create a submit-docker block for use with docker containers, according to instructions from [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/). This is the how my config block looks:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 1; Int requested_memory_mb_per_core = 8000; Int memory_mb = 4000; String queue = ""short""; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""/bin/bash ${script}""; """"""; ; submit-docker = """"""; docker pull ${docker}. sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""docker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; ```; But, I get an error from each node:. /bin/bash: /data/og/wgs/cromwell-executions/HaplotypeCallerGvcf_GATK4/98c2fe18-92b7-49d4-b490-623ed61e3dfc/call-HaplotypeCaller/shard-41/execution/script: No such file or directory. As far as I can tell, it is trying to reach the script local on the main machine, and not in the docker container. Is this expected behavior, or am I missing something? I know I can replace ${script} with ${docker_cwd}/execution/script but I am unsure why I need this change that is not according to your documentation. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5768:1016,alive,alive,1016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5768,2,"['alive', 'error']","['alive', 'error']"
Availability,"Hello! I used this snippet of code successfully on the Cromwell with Google backend:. ```; task fastqc {; input {; File f1; File? f2; Int? cpu=1; Int? machine_mem_gb; Int? preemptible_attempts; String mode. Float f2size = if (mode == ""PE"") then size(f2, ""GB"") else 0.0; Int disk_space_gb = ceil(size(f1, ""GB"") + f2size) + 20; }. command {; fastqc -t ~{cpu} --outdir $PWD ~{f1} ~{f2}; }. output {; Array[File] html = glob(""*html""); Array[File] zip = glob(""*zip""); }. runtime {; docker: ""biocontainers/fastqc:v0.11.5_cv2""; memory: select_first([machine_mem_gb, 4]) + "" GB""; cpu: cpu; disks: ""local-disk "" + disk_space_gb + "" HDD""; preemptible: select_first([preemptible_attempts, 3]); }; }; ```; On local backend I got this error:; _Failed to evaluate input '__disk_space_gb' (reason 1 of 1): Sorry! Operation + is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value._. I tried following the advice (even though I don't see what is missing); Float f2size = select_first([if (mode == ""PE"") then size(f2, ""GB"") else 0.0]); And then I got an even more confusing error:; _Failed to process task definition 'fastqc' (reason 1 of 1): Failed to process expression 'select_first([f2size, select_first([if (mode == ""PE"") then size(f2, ""GB"") else 0.0])])' (reason 1 of 1): Invalid parameter 'ArrayLiteral(Vector(TernaryIf(Equals(IdentifierLookup(mode),StringLiteral(PE)),Size(IdentifierLookup(f2),Some(StringLiteral(GB))),PrimitiveLiteralExpressionElement(WomFloat(0.0)))))'. Expected an array of optional values (eg 'Array[X?]') but got 'Array[Float]+'_. Can you help me understand? It all passes Womtools validation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5694:722,error,error,722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5694,2,['error'],['error']
Availability,"Hello!. I am running cromwell with docker on macOS (Big Sur v 11.2.3). When executing the following command from a simple WDL:. ```. docker run --cidfile /cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller/execution/docker_cid -i --entrypoint /bin/bash -v /cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller:/cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller:delegated pegi3s/gatk-4@sha256:8415200fe87b4d4eb295e9decb207727e9f75aad73c860a8d0ffe3385a8eaa7a /cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller/execution/script; ```. I get the error:Â . ```. /bin/bash: /cromwell-executions/VarCallAndTable/5a729547-b927-431f-9793-8a2ac48035ea/call-HaplotypeCaller/execution/script: No such file or directory. ```. After some digging, my guess is that the `docker run` command is not mounting the contents of `/cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller`.; For that reason the execution script is not found. Has anyone experieced this? ; Are there any settings I should edit for it to work?. Really appreciate the help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353:713,error,error,713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353,1,['error'],['error']
Availability,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3910:1231,error,errors,1231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910,2,"['alive', 'error']","['alive', 'errors']"
Availability,"Hello, I executed the following workflow using chromwell behind a proxy . $ java \; -Djava.net.useSystemProxies=true \; -Dhttp.proxyHost=202.241.78.237 -Dhttp.proxyPort=8080 \; -Dhttps.proxyHost=202.241.78.237 -Dhttps.proxyPort=8080 \; -jar cromwell-85.jar run public_health_bacterial_genomics/workflows/wf_theiaprok_illumina_pe.wdl -i input.json. but it caused the following error, indicating connection with quay.io/50.17.122.58:443 timed out. 2023-05-11 10:01:42,43] [info] Request threw an exception on attempt #1. Retrying after 596 milliseconds; org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false); at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144); at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139); at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83); at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:172); Caused by: java.net.SocketTimeoutException: An attempt to establish connection with quay.io/50.17.122.58:443 timed out after 10 seconds.; at org.http4s.blaze.channel.nio2.ClientChannelFactory$$anon$1.run(ClientChannelFactory.scala:66); at org.http4s.blaze.util.Execution$$anon$3.execute(Execution.scala:80); at org.http4s.blaze.util.TickWheelExecutor$Node.run(TickWheelExecutor.scala:271); at org.http4s.blaz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136:376,error,error,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hello,. I am having a problem that has been already discussed but I haven't been able to solve it using the suggestions. Basically, In the wdl workflow, I have two tasks (at the moment). The first works fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcod",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:616,down,downloaded,616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['down'],['downloaded']
Availability,"Hello,. I am trying to use our on premise cromwell instance to submit jobs to GCP via the genomics api. I seem to have everything working except the VPC network name. We have a naming convention for the VPC network names configured in our GCP project and don't have the flexibility to change the name. However, cromwell seems to expect the VPC network name to be ""default"" and this does not seem to be configurable. Here is the error I am receiving when trying to submit the workflow:. [error] WorkflowManagerActor Workflow 8f55bf4d-9389-40e6-a469-dbd434394dd8 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Invalid value for field 'resource.networkInterfaces[0].network': 'https://www.googleapis.com/compute/v1/projects/projectxyz/global/networks/default'. The referenced network resource cannot be found. Is there a way for me to pass a different network name? If not, where can I request this feature be added to a future version?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4005:428,error,error,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005,3,['error'],['error']
Availability,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5434:283,down,downloaded,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434,1,['down'],['downloaded']
Availability,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4732:170,error,errors,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732,2,['error'],"['error', 'errors']"
Availability,"Hello,. I'am running a wdl worflow that I have created and I want to make a conditional statement. This because according to which sequencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:722,error,error,722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['error'],['error']
Availability,"Hello,. I'm seeing this error with glob. Looks like the glob-xxx.list file is not create for some reason. The file it's looking for has a couple commas (,) in them, could that be the issue?. wdl: /humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/medically-relevant-coverage.wdl.copy; json: /humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/medically-relevant-coverage.json; cromwell server: gsa5:8003 (configured for SGE); metadata: http://gsa5:8003/api/workflows/v2/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/metadata?expandSubWorkflows=false. ""message"": ""Failed to evaluate outputs.: Could not evaluate GetHSMetrics.per_target_coverage = glob(\""*.per_target_coverage\"")[0]\n\tFile not found /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution/glob-66c7ddc0219653b72a61bd2dae8bb454.list""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1980:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1980,1,['error'],['error']
Availability,"Hello,. I'm using Cromwell with Google Pipelines as backend and sometimes (maybe when more than 30 analysis running at same time) I'm getting workflow error (~2 of the 30). When inspecting the metadata for the workflow I can see a error message that contains ""ServiceException: 401 Requester pays bucket access requires authentication."". **Edit:** Using Cromwell 35. Has anyone had a similar problem? Here are the WDL task that are affected (from Broad's five dollar genome workflow):. ```wdl; task BaseRecalibrator {; File input_bam; File input_bam_index; String recalibration_report_filename; Array[String] sequence_group_interval; File dbSNP_vcf; File dbSNP_vcf_index; Array[File] known_indels_sites_VCFs; Array[File] known_indels_sites_indices; File ref_dict; File ref_fasta; File ref_fasta_index; Int disk_size; Int preemptible_tries. command {; /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \; -Xloggc:gc_log.log -Xms4000m"" \; BaseRecalibrator \; -R ${ref_fasta} \; -I ${input_bam} \; --useOriginalQualities \; -O ${recalibration_report_filename} \; -knownSites ${dbSNP_vcf} \; -knownSites ${sep="" -knownSites "" known_indels_sites_VCFs} \; -L ${sep="" -L "" sequence_group_interval}; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135""; memory: ""6 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File recalibration_report = ""${recalibration_report_filename}""; }; }; ```. And here is my cromwell server config:. ```scala; include required(classpath(""application"")). webservice {; port = 8000; }. system {; workflow-restart = true; }. engine {; filesystems {. gcs {; auth = ""service-account""; }. http {}. local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }. backend {; default = ""Local""; providers {. Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336:151,error,error,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336,2,['error'],['error']
Availability,"Hello,. I'm working on making our cromwell server work with GCP Batch and running in our private VPC network. However, after following [this tutorial](https://cromwell.readthedocs.io/en/develop/backends/GCPBatch/), I encounter the following error:. ```; com.google.api.gax.rpc.InvalidArgumentException: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: network field is invalid. network: projects/${project_id}/global/networks/${network_id}/ is not matching the expected format: global/networks/([a-z]([-a-z0-9]*[a-z0-9])?)$; 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:92); 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:41); 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:86); 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:66); 	at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97); 	at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:84); 	at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1133); 	at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31); 	at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1277); 	at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038); 	at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:808); 	at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:574); 	at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544); 	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39); 	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23); 	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500:241,error,error,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500,1,['error'],['error']
Availability,"Hello,. when I try to run my workflow using a config file using. `$ java -Dconfig.file=../config/LSF.conf cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json`. I get the following message; ```; Error: Could not find or load main class cromwell.jar; Caused by: java.lang.ClassNotFoundException: cromwell.jar; ```. Without specifying a config file, the pipeline runs without any problems. ; I installed Cromwell (version 79) using conda. I also tried the following:. `$ java -Dconfig.file=../config/LSF.conf cromwell-79.jar run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. ```; Error: Could not find or load main class cromwell-79.jar; Caused by: java.lang.ClassNotFoundException: cromwell-79.jar; ```. I also checked where the cromwell.jar file is saved in my conda environment and tried the following:. `; java -Dconfig.file=./LSF.conf /path/to/env/share/cromwell/cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. This is the config file LSF.config:; ```; include required(classpath(""application"")). backend {. # Override the default backend.; default = LSF. # The list of providers. Copy paste the contents of a backend provider in this section; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; 	submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; # Second backend provider would be copy pasted here!. }; }; ```. I have not much experienced with cromwell and would be very grateful for help. Thank you,; Johannes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6796:235,Error,Error,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796,3,"['Error', 'alive']","['Error', 'alive']"
Availability,"Hello. Here is the simple example of workflow with optional input: . ```; workflow wf {; String? inn; call tsk_grep { input: input_381012639=inn }; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. When i do not specify any input for workflow(json file below):. ```; {; }; ```. i get an the error:; `wdl4s.WdlExpressionException: Could not resolve inn as a scatter variable, namespace, call, or declaration`. But when my workflow looks like this:. ```; workflow wf {; call tsk_grep; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. Cromwell works as expected executing `ps aux | grep j`.; Is this a bug or i can only use optional variables inside of **task scope**?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1176:348,error,error,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1176,1,['error'],['error']
Availability,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1016:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016,2,['error'],['error']
Availability,"Helloï¼Œ Could someone tell me how to fix this problem? Thanks!!!. **task hello** {; String name. command {; echo 'Hello ${name}!' > /Users/00.wdl/QC_new/QC/hello.out1; }; output {; File out1 = ""/Users/00.wdl/QC_new/QC/hello.out1""; }; }. **task hello2** {; 	File in1. 	command {; 		cat ${in1}; 	}. 	output {; 		File	response2 = stdout(); 	}. }. **workflow test** {; call hello {input name=""World"" }; call hello2 {input in1=hello.out1 }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5059:107,echo,echo,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5059,1,['echo'],['echo']
Availability,"Here is a very simple WDL (test.wdl):; ```; version 1.0. workflow test {; input {; Array[File] list; }. call test {; input:; list = list; }; }. task test {; input {; Array[File] list; String docker = ""ubuntu:20.04""; }. File lines = write_lines(list). command <<<; cat ~{lines}; cat ~{write_lines(list)}; echo -e ""~{sep=""\\n"" list}""; >>>. runtime {; docker: docker; }; }; ```. Here is a basic input for the WDL (test.json):; ```; {; ""test.list"": [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }; ```. When I run the workflow on my laptop with the following command:; ```; java -jar cromwell-51.jar run test.wdl -i test.json; ```; I get the following stdout output:; ```; /tmp/1; /tmp/2; /tmp/3; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/1; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/2; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/3; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/1; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/2; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/3; ```; Which shows that the absolute paths of the files passed to the test workflow have been exposed. I cannot imagine this being the expected behavior. How do I get to have `write_lines(...)` behave more like `~{sep="" "" ...}` even when it is not run inside a command <<< ... >>> instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540:304,echo,echo,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540,1,['echo'],['echo']
Availability,"Here is an example error for a task failing since there is less space than needed:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file - Unexpected exit status 1 while running \""/bin/sh -c python -c 'import base64; print(base64.b64decode(\\\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIGdzdXRpbCAgY3AgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCA+IGdzdXRpbF9vdXRwdXQudHh0IDI+JjEKIyBSZWNvcmQgdGhlIGV4aXQgY29kZSBvZiB0aGUgZ3N1dGlsIGNvbW1hbmQgd2l0aG91dCBwcm9qZWN0IGZsYWcKUkNfR1NVVElMPSQ/CmlmIFsgIiRSQ19HU1VUSUwiICE9ICIwIiBdOyB0aGVuCiAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgZ3N1dGlsXCBcIGNwXCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0XCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dFwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiBjcCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0IC9jcm9td2VsbF9yb290L3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0CiAgZWxzZQogICAgZXhpdCAiJFJDX0dTVVRJTCIKICBmaQplbHNlCiAgZXhpdCAwCmZpCiAgKQogIFJDPSQ/CiAgaWYgWyAiJFJDIiA9ICIwIiBdOyB0aGVuCiAgICBicmVhawogIGZpCiAgaWYgWyAkaSAtbHQgMyBdOyB0aGVuCiAgICBwcmludGYgJyVzICVzXG4nICIkKGRhdGUgLXUgJyslWS8lbS8lZCAlSDolTTolUycpIiBXYWl0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4603:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603,3,"['error', 'failure']","['error', 'failures']"
Availability,"Here is the current situation from Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:486,Error,Error,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,2,"['Error', 'error']","['Error', 'error']"
Availability,"Here's the error message:; ```; ""Unknown status"" did not start with ""Timed out"" (HealthMonitorServiceActorSpec.scala:76; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3391:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3391,1,['error'],['error']
Availability,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:155,error,errors,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,2,"['Error', 'error']","['Error', 'errors']"
Availability,"Hi ,; Im running the GATK [warp joint genotyping pipeline ](https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/joint_genotyping/JointGenotyping.wdl)cromwell on GCP backend . The pipeline fails because cromwell cannot localize files with certain data types like ` Array[Array[String]]`. This issue was reported on the [terra website](https://support.terra.bio/hc/en-us/community/posts/4409388371611-How-do-I-pass-an-array-array-file-to-another-task-) too and the workaround was to write a task to read file into an array, I have performed the workaround but are there plans to fix this issue in any upcoming releases. **Cromwell version tested :** 85. **Are you seeing something that looks like a bug? Please attach as much information as possible.** ; `""Failed to evaluate 'sample_name_map_lines' (reason 1 of 1): Evaluating read_tsv(sample_name_map) failed: Failed to read_tsv(\""gs://wgs/test/sample_map.txt\"") (reason 1 of 1): java.lang.IllegalArgumentException: Could not build the path \""gs://wgs/test/sample_map.txt\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: \nHTTP: gs://wgs/test/sample_map.txt does not have an http or https scheme (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from gs://wgs/test/sample_map.txt (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""`. **Which backend are you running?** ; GCP. **Link to the workflow if possible**; https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/joint_genotyping/JointGenotyping.wdl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7364:1181,Failure,Failures,1181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7364,1,['Failure'],['Failures']
Availability,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4710:231,failure,failure,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710,7,"['Error', 'down', 'error', 'failure']","['Error', 'download', 'error', 'failure']"
Availability,"Hi All,. Im running into an issue with my deployment of Cromwell 65. I am running scripts connecting to a local MySQL(also tested on MariaDB). Upon running a reasonably complex pipeline I am receiving a number of database errors:. ```; java.sql.BatchUpdateException: Data truncation: Data too long for column 'METADATA_KEY' at row 6; 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:78); 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499); 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480); 	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192); 	at com.mysql.cj.util.Util.getInstance(Util.java:167); 	at com.mysql.cj.util.Util.getInstance(Util.java:174); 	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchedInserts(ClientPreparedStatement.java:755); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:426); 	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:796); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$18(JdbcActionComponent.scala:542); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:425); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:420); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:489); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6545:222,error,errors,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6545,1,['error'],['errors']
Availability,"Hi Atlas Team,. . I have installed the Atlas(apache-atlas-sources-2.1.0) in; our server by following in the link; ""https://atlas.apache.org/2.0.0/InstallationSteps.html. After all the setup; have been done, we ran the atlas-start.py and the atlas is running on port; 21000.When we are accessing the atlas, we are facing 503 Service Unavailable; Error. We checked the logs from application.log file. We got below issue . . 2020-12-13 06:29:02,309 WARN - [main:] ~ JMX is not enabled to receive; remote connections. Please see cassandra-env.sh for more info.; (CassandraDaemon:81). 2020-12-13 06:29:02,310 ERROR - [main:] ~ cassandra.jmx.local.port missing; from cassandra-env.sh, unable to start local JMX service.null; (CassandraDaemon:87). . We see there is no ""cassandra-env.sh"" file in atlas and we; tried other ways and didn't find any solution for the above error. . Could you please help us in rectify these problem so that it; will be very helpful to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6133:345,Error,Error,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6133,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because Iâ€™m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldnâ€™t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. Iâ€™m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, Iâ€™m wondering if thereâ€™s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4688:1194,error,error,1194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688,1,['error'],['error']
Availability,"Hi Cromwell team,. I've heard rumors that lately you've been plagued with the same intermittent 503 errors using the GCS NIO library that have plagued the GATK. We have what we believe is a definitive fix for this issue up at https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281. To take advantage of the fix, client code needs to set the library-level retry/reopen settings to be sufficiently aggressive. The settings GATK uses (which have cleared up our 503 issues at scale completely) can be seen in the `setGlobalNIODefaultOptions()` method in this class:. https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/gcs/BucketUtils.java. Mentioning @LeeTL1220 on this ticket by request",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495:100,error,errors,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495,1,['error'],['errors']
Availability,"Hi I did try to build DAG for a task from WARP pipeline, and wom failed with an error; https://github.com/broadinstitute/warp/issues/438; Turns out it it no a problem of a pipeline, but possibly a bug in a wom. Best, Eugene",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6499:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6499,1,['error'],['error']
Availability,"Hi Want to run my pipeline in gcp with nvidia-tesla-a100. Get errors for insert machine. Trake into the code since cromwell set n2-custom machine meanwhile a100 require a2-highgpu-1g. I guess it is not a big change but can extend capbilities. runtime {bootDiskSizeGb: 100; disks: ""/mnt 3000 HDD""; gpuType: ""nvidia-tesla-a100""; gpuCount: 1; nvidiaDriverVersion: ""418.87.00""; zones: [""us-central1-c""]; } (edited) . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6558:62,error,errors,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6558,1,['error'],['errors']
Availability,"Hi all, . I try to modify & deploy my cloud function. . But I get this error:. ![image](https://user-images.githubusercontent.com/8334979/70418119-bcec1e80-1a9d-11ea-8f13-d6809efe63e7.png). Then I [check my bucket information](https://cloud.google.com/storage/docs/using-requester-pays#enable), the ""requester pays"" is disabled. Also, I try to create a new bucket & deploy a new cloud function. The error remains. So, I am wondering is there any other possibilities that could cause this situation?; (maybe MIS team accidentally change some related settings); I can not deploy any cloud function(s) successfully since early today. Please give me some suggestion, thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5311:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311,2,['error'],['error']
Availability,"Hi all, . I'm encountering an issue when using the AWS Batch backend. I'm using the EFS (local) file system for the backend, not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:158,down,downloads,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,2,['down'],"['download', 'downloads']"
Availability,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4080:592,error,error,592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080,3,"['avail', 'error']","['available', 'error', 'errors']"
Availability,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,3,"['Failure', 'error']","['Failure', 'error']"
Availability,"Hi all;; In testing release 35 with CWL inputs I've also been looking at supporting remote URL references. This is working correctly for GS URLs but not for http URLs. I've put together a test case that demonstrates the problem:. https://github.com/bcbio/test_bcbio_cwl/tree/master/gcp. The `somatic-workflow-http` CWL workflow uses http URLs and doesn't work, while the comparable `somatic-workflow` CWL workflow uses GS URLs referencing the same data and does work. The workflow fails with:; ```; java.io.FileNotFoundException: Cannot hash file https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genom; es/hg19/seq/hg19.fa; ```; when running tasks. The files get downloaded to the input directories but get numerical values instead of the original file names so never seem to sync over and get translated correctly to the workflow; ```; ls -lh cromwell_work/cromwell-executions/main-somatic.cwl/eaa632df-52a8-4aae-826f-647a42fa7145/call-prep_samples_to_rec/inputs/1515144/; total 136K; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 225050424226294657; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 2612405277530248055; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 503001634356675169; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 5802330287039666628; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 5809676514510180826; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6090832304768530540; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6105514522473810611; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 6807576659333162957; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 6853384576121493061; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7483350933664987331; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7538690575330349970; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 7691692211431528147; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7783203266940950463; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 8389565043859020157; -rw------- 2 chapmanb chapmanb 43 Sep 26",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184:679,down,downloaded,679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184,1,['down'],['downloaded']
Availability,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3185:47,error,errors,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185,2,"['alive', 'error']","['alive', 'errors']"
Availability,"Hi everyone, this is as far as I can get on my own trying to introduce SQLite support into Cromwell. I have highlighted reasons to support Sqlite on Cromwell elsewhere. This is as far as I could get without help:. + Migration scheme correctly implemented. All the necessary tables with all the correct constraints (foreign key, unique, primary key) are created on startup.; + Updated upstream liquibase in order to allow unique constraints to be defined properly.; + Made sure all the types were converted in SQLite types (TEXT, INTEGER, BLOB etc.); + Updated the testing to understand SQLite types properly. So far so good. Unfortunately the testing does not recognize the foreign key, primary key or unique constraints, even though they are defined (clearly visible in the sqlitebrowser). . Since the testing is just testing, I also decided to run cromwell with a workflow, but that does not work: ; ```; [ERROR] [07/20/2020 14:01:02.134] [cromwell-system-akka.dispatchers.engine-dispatcher-50] [akka://cromwell-system/user/cromwell-service/WorkflowStoreActor/WorkflowStoreEngineActor] Error trying to fetch new workflows; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:1010); at org.sqlite.core.DB.newSQLException(DB.java:1022); at org.sqlite.core.DB.throwex(DB.java:987); at org.sqlite.core.NativeDB.prepare_utf8(Native Method); at org.sqlite.core.NativeDB.prepare(NativeDB.java:134); at org.sqlite.core.DB.prepare(DB.java:264); at org.sqlite.core.CorePreparedStatement.<init>(CorePreparedStatement.java:45); at org.sqlite.jdbc3.JDBC3PreparedStatement.<init>(JDBC3PreparedStatement.java:30); at org.sqlite.jdbc4.JDBC4PreparedStatement.<init>(JDBC4PreparedStatement.java:19); at org.sqlite.jdbc4.JDBC4Connection.prepareStatement(JDBC4Connection.java:35); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Connection.java:241); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Conne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5582:908,ERROR,ERROR,908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5582,1,['ERROR'],['ERROR']
Availability,"Hi folks,; I am running cromwell 36 with AWS batch. Doing the hello world example from the following:. https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/. I am able to submit from the swagger UI and am getting the following erro:. `2018-10-30 00:39:25,929 INFO - jobQueueArn: arn:aws:batch:us-east-2:365166883642:job-queue/GenomicsHighPriorityQue-0c2108973103ca2; 2018-10-30 00:39:25,929 INFO - taskId: wf_hello.hello-None-1; 2018-10-30 00:39:25,929 INFO - hostpath root: wf_hello/hello/bcc91ab0-fd91-41a8-b3e6-cbf091cb511d/None/1; 2018-10-30 00:39:25,965 cromwell-system-akka.dispatchers.backend-dispatcher-229 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(bcc91ab0)wf_hello.hello:NA:1]: Error attempting to Execute; software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: batch.default.amazonaws.com: Name or service not known`. Any idea the source of this error?; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334:629,ERROR,ERROR,629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Hi team,. I try to configure cromwell to run ExomeGermlineSingleSample_v3.1.9.wdl on Slurm, and I follow your guide, but I have an error that ${docker_script} : No such file or directory; /cromwell-executions/ExomeGermlineSingleSample/118135f5-ce0e-437b-9fd2-332dd614bded/call-GenerateSubsettedContaminationResources/execution/script : No such file or directory; I attached the run file; #!/bin/bash; #SBATCH --nodes=1; #SBATCH --time=2:00:00. module load jdk. java -Dconfig.file=/mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-slurm_5.config \; -jar /mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-85.jar \; run /mainfs/wrgl/broadinstitute_warp_development/warp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['error'],['error']
Availability,"Hi there!. I'm new to both WDL and Cromwell. I'm trying to run a workflow and the main input is an Illumina run folder, which could be up to a Tb in size. So I don't want it to be copied or hard-linked (which is almost the same in time) but to be soft-linked. I changed the localization option in the backend but then I'm getting the following error:. `Cannot localize directory with symbolic links`. So, isn't this possible? Will I always have to hard-linked the directories? This is not an option in my case due to performance issues. Everything is in the same shared file system, so I don't see the point in hard-linking. Also, if I call the same directory in different tasks, will it be hard-linked every time?. Thank you very much in advance!. Best,; Santiago",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3614:344,error,error,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3614,1,['error'],['error']
Availability,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:254,avail,available,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,1,['avail'],['available']
Availability,"Hi there,. There is an issue posting a cromwell job, getting the id from the response, and then trying to run a GET call too soon after the initial post. Here is a simple example. `curl -v ""localhost:8000/api/workflows/v1"" -F workflowSource=@simple.wdl -F workflowInputs=@simple.json | python -c 'import json,sys,time;obj=json.load(sys.stdin);print obj[""id""];' | xargs -I {} curl ""http://localhost:8000/api/workflows/v1/{}/status""`. The get response was. `{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: 054f9462-3775-4745-8ca6-5a63c9c4492f""; }` . Even though the 054f9462 ID was gotten directly from the original response. The simple solution is to add a sleep statement between the post and get. `curl -v ""localhost:8000/api/workflows/v1"" -F workflowSource=@simple.wdl -F workflowInputs=@simple.json | python -c 'import json,sys,time;obj=json.load(sys.stdin);time.sleep(10);print obj[""id""];' | xargs -I {} curl ""http://localhost:8000/api/workflows/v1/{}/status""`. The response was . `{""status"":""Submitted"",""id"":""897e2f30-2e4d-4cf0-9bb3-d926ec42dc6a""}`. Is there a way to know for sure the job is ready for a GET post? Or is this a bug, and the response JSON should be returned after GETs are available?. Thanks for your time!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2671:1206,avail,available,1206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2671,1,['avail'],['available']
Availability,"Hi! cromwell. When I try to use the cromwell on AWS, I got the error in below. $ java -Dconfig.file=aws.conf -jar cromwell-latest.jar run hello.wdl -i hello.inputs; (Ref. https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). [2018-10-23 04:06:40,39] [error] AwsBatchAsyncBackendJobExecutionActor [6d66572bwf_hello.hello:NA:1]: Error attempting to Execute. Please refer to attached image.; ![2018-10-23 13 07 21](https://user-images.githubusercontent.com/4966343/47335191-01173680-d6c5-11e8-8c51-97142bbf0d02.png). I guess this issue related to aws batch or setting.; How can I resolve this issue?. Thanks in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4294:63,error,error,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4294,3,"['Error', 'error']","['Error', 'error']"
Availability,"Hi!. I am attempting to run a tool that requires a directory structure as an input. One issue is that the tool will error out if there are files in the directory that are not the expected type. In our infrastructure we occasionally will have `.md5` files next to the important files which leads to the error in running the tool. To avoid this issue when launching Cromwell workflows I am attempting to list the good files in the listing attribute of the Directory input data type. The files I list are being staged in the inputs folder for the step but they are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670:116,error,error,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670,2,['error'],['error']
Availability,"Hi!. I have been trying to make memory retry work on our system without sucess. ; Read all docs and previous issues I could find, but it still doesn't work for us. I have written a test wdl with two tasks, both write ""Killed"" to stderr, and supposed to get retried with more memory. The first task, **TestBadCommandRetry** is designed to fail regularly with rc 127, due to a bad command.; The purpose of this task is to prove the memory-retry mechanism is configured correctly in our system. Result of TestBadCommandRetry:; The memory-error-key is caught and memory is increased as defined in memory-retry-multiplier.; I also see this failure message in metadata.json:; _""message"": ""stderr for job `MemoryRetryTest.TestBadCommandRetry:NA:1` contained one of the `memory-retry-error-keys: [Killed]` specified in the Cromwell config. Job might have run out of memory.""_. Grepping metadata for memory of this job, I see the expected behaviour:; ""memory"": ""1 GB"",; ""memory"": ""2 GB"",. The second task, **TestOutOfMemoryRetry** is designed to fail do to real out of memory error.; The purpose of this task is to shoe that memory-retry mechanism is not working when a task runs out of memory, even if ""Killed"" is written to stderr. Result of TestOutOfMemoryRetry:; When this task is run, it fails but **the job is retried with the same amount of memory**.; This time I see the following failure message:; _""message"": ""Task MemoryRetryTest.TestOutOfMemoryRetry:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running \""/cromwell_root/script\"": unexpected exit status 137 was not ignored\n[UserAction] Unexpected exit status 137 while running \""/cromwell_root/script\"": Killed\n"",_. Grepping metadata for memory of this job, I see the memory expension is not working:; ""memory"": ""1 GB"",; ""memory"": ""1 GB"",; ; I have verified ""Killed"" is written correctly to stderr :; ```; gsutil cat gs://<out_bucket>/cromwell-execution/Me",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205:535,error,error-key,535,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205,3,"['error', 'failure']","['error-key', 'error-keys', 'failure']"
Availability,"Hi!. I'm having some trouble request s3 objects that are outside my current region (I get a Status Code: 301). . Backend: AWS Batch; Filesystem: S3; Region : `ap-southeast-2`. I'm attempting to run a small genomics pipeline that is trying to request some of the [`broad-reference` open data set](; https://registry.opendata.aws/broad-references) on AWS S3. I can see that open data set exists in `us-east-1`. . Specifically, I'm requesting (`s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta`) and I'm receiving the same error 5 times.; ```; [2019-03-12 11:27:21,50] [error] WorkflowManagerActor Workflow 434834fb-cb24-4bd2-ba44-8a1c929b11f5 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null). 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731:531,error,error,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731,2,['error'],['error']
Availability,"Hi!. I've followed the instructions at https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/ and I've run into the following errors during 'sbt assembly': . [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:111:57: type mismatch; ; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; ja",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:149,error,errors,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,13,['error'],"['error', 'errors']"
Availability,"Hi, . I have been getting PoolTime out error (below) for all human jobs. Smaller ones e.g mouse genomes are a success. There are multiple steps in the wdl with outputs at every stage. Each output is copied from the S3//temp//cromwell_executions folder to S3://output folder successfully except the largest one i.e. .bam file. The .bam file is successfully copied from EC2 instance temp folder to S3://temp folder but it does copy from S3://temp/cromwell_executions to S3://output.; ; Both core environment and workflows have been set up using the templates provided by AWS genomics workflow. AWS batch jobs show a success notification, however it is only Cromwell that sends a status of ""failure"".. . Cromwell metadata : ; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""Timeout waiting for connection from pool"",; ""causedBy"": []; }; ],; ""message"": ""Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ""message"": ""software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ""message"": ""[Attempted 1 time(s)] - CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ```. Options.json; ```; ""final_workflow_outputs_dir"": ""s3://singleronbio-de-tmp/output/2023/Aug/2023-00578"",; ""final_call_logs_dir"": ""s3://singleronbio-de-tmp/log/2023/Aug/2023-00578/230719005"",; ""final_workflow_log_dir"": ""s3://singleronbio-de-tmp/workflow_log/2023/Aug/2023-00578/230719005"",; ""backend"": ""AWSBATCH"",; ""base_url"": ""XXXX"",; ""route_submit"": ""/api/workflows/v1"",; ""route_valid"": ""/api/womtool/v1/describe"",; ""route_status"": ""/api/workflows/v1/{id}/status"",; ""route_outputs"": ""/api/workflows/v1/{id}/outputs"",; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. Thank you, ; Lakshmi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7200:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7200,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"Hi, . I'm reporting an issue that seem to suggest File localization issues.; The execution bucket is [here](https://console.cloud.google.com/storage/browser/broad-dsde-methods/cromwell-execution-34/PindelSmallVariants/?project=broad-dsde-methods&organizationId=548622027621) using the DSDE-methods Cromwell server V.34. The errors I'm experiencing is described below. Resubmitting the job fixes the problem. ## failure for job ID ; * `2ebeed9a-8f42-418b-8569-8d80f5654d50` (shard-40), ; * `1cc79dda-9c6e-41b4-ac99-e1a422258039` (shard-20). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; /bin/bash: /cromwell_root/script: No such file or directory; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `4c5c8530-b79d-465b-8050-f7ba7368c057` (shard-26), ; * `5cbb2124-c526-4ca0-978e-4154ff4501cd` (shard-0). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Loading reference genome done.; Initializing parameters done.; Please use samtools to index your reference file.; .fai is missing. sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `7f219a29-69c1-4116-9c54-5d4656ee0124`. ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Error: fasta line starts with  instead of '>'. Aborting.; sh: -q: unknown operand;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148:324,error,errors,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148,3,"['error', 'failure']","['errors', 'failure']"
Availability,"Hi, ; I have a CWL workflow which works well when no output is specified but fails with the following when output is added:. ```; [2019-01-09 17:56:27,87] [error] WorkflowManagerActor Workflow b240bd3e-cdfd-45c8-be1e-046794929e90 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Workflow output parameters such as WorkflowOutputParameter(file:///var/folders/34/kzv1pzl57s92dbz54rvr1px00000gn/T/b240bd3e-cdfd-45c8-be1e-046794929e90.temp.4180597054306144316/b240bd3e-cdfd-45c8-be1e-046794929e90.cwl#outputwf,None,None,None,None,None,None,Some(Inr(Inl([Ljava.lang.String;@2ff95103))),None,Some(Inl(Inl(File)))) are not supported.; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4538:156,error,error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538,1,['error'],['error']
Availability,"Hi, ; I understand cromwell that call caching will copy whatever it finished last time to current run if I keep the script unchanged. However, it seems to be not that case to me. I have run Mutect2, one of the GATK tool on fireclouds. one of the major step is it splits to 50 jobs based on some genomic interval and runs separately on different VM. Most jobs are successful done but a few fail because of some transient error. Once I relanuch the job, it splits to 50 jobs based on the same interval, but it is still running all 50 jobs simultaneously without knowing some of the jobs have run successfully last time (of course, I enabled call caching). So I ends up spending more and more money and time on it. Could you please advise on this ?. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740:420,error,error,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740,1,['error'],['error']
Availability,"Hi, ; To use Cromwell in AWS, may I have my own mount point?; If I use the following in my WDL file, I will got two mount points. But the problem is I can't provide my own Source Path. Cromwell will generate one for test1. If we can provide own source path, then we can use our reference files in EFS instead of downloading them each time from S3.; runtime {; docker: ""ubuntu:latest""; disks: ""local-disk, test1""; }. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579:312,down,downloading,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579,1,['down'],['downloading']
Availability,"Hi, ; can we define Directory in workflow input section? I met error ""Cannot coerce expression of type 'String' to 'Directory'"".; my wdl file, cromwell 59 is used: ; ```; version development. workflow pipeline {; input {; Directory index_dir = ""/home/danny.gu/PycharmProjects/nestcmd/tests/testdata/index/""; }. call getFastqInfo{}. scatter (each in keys(getFastqInfo.fastq_info)) { ; String sample = each; File read1 = getFastqInfo.fastq_info[each][0][0]; File read2 = getFastqInfo.fastq_info[each][1][0]. call fastp {; input: ; read1 = read1,; read2 = read2; }. call salmon {; input: ; indexDir = index_dir,; read1 = fastp.out1,; read2 = fastp.out2; }. }. call MergeTranscriptTPM {; input: ; quants = salmon.outDir; }. call MergeTranscriptCount {; input: ; quants = salmon.outDir; }. meta {; name: ""PipelineExample""; desc: ""This is a simple pipeline for fast gene/transcript quantification. workflow = [fastq -> Fastp -> Salmon]""; author: ""unknown""; source: ""source URL for the tool""; version: ""unknown""; }. output{; Array[File] fastp_out1 = fastp.out1; Array[File] fastp_out2 = fastp.out2; Array[File] salmon_transcript = salmon.transcript; Array[Directory] salmon_outDir = salmon.outDir; File MergeTranscriptTPM_result = MergeTranscriptTPM.result; File MergeTranscriptCount_result = MergeTranscriptCount.result; }. }. task getFastqInfo{; input {; Array[Directory]? fastq_dirs; Array[File]? fastq_files; String r1_name = '(.*).read1.fastq.gz'; String r2_name = '(.*).read2.fastq.gz'; String docker = 'gudeqing/getfastqinfo:1.0'; }. command <<<; set -e; python /get_fastq_info.py \; ~{if defined(fastq_dirs) then ""-fastq_dirs "" else """"}~{sep="" "" fastq_dirs} \; ~{if defined(fastq_files) then ""-fastq_files "" else """"}~{sep="" "" fastq_files} \; -r1_name '~{r1_name}' \; -r2_name '~{r2_name}' \; -out fastq.info.json; >>>. output {; Map[String, Array[Array[File]]] fastq_info = read_json(""fastq.info.json""); File fastq_info_json = ""fastq.info.json""; }. runtime {; docker: docker; }. ; }; ; task fastp{; i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501:63,error,error,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501,1,['error'],['error']
Availability,"Hi, I am using cromwell-59.jar, and run in local backend mode. ; Used command:; `java -Dconfig.file=cromwell.examples.conf -jar ~/softwares/cromwell-59.jar run example.wdl -i input_detail.json`. however, when I try to re-run a successed workflow to validate the cache calling function, I got the following information that confused me. very much. Would you be kind to give me some ideas on what's going on? . `a588e03e-a4fc-4809-b5f5-bb540cac9ca3-EngineJobExecutionActor-rnaseq_pipeline.fastp:NA:1 [UUID(a588e03e)]: Could not copy a suitable cache hit for a588e03e:rnaseq_pipeline.fastp:-1:1. No copy attempts were made.`. following is the source code related I fetched, but still cannot understand it. `if (data.cacheHitFailureCount > 0) {; val totalHits = data.cacheHitFailureCount; val copyFails = data.failedCopyAttempts; val blacklisted = totalHits - copyFails; workflowLogger.info(; s""Could not copy a suitable cache hit for $jobTag. "" +; s""EJEA attempted to copy $totalHits cache hits before failing. "" +; s""Of these $copyFails failed to copy and $blacklisted were already blacklisted from previous attempts). "" +; s""Falling back to running job.""; ); val template = s""BT-322 {} cache hit copying failure: {} failed copy attempts of maximum {} with {}.""; log.info(template, jobTag, data.failedCopyAttempts, callCachingParameters.maxFailedCopyAttempts, data.aggregatedHashString); } ; else {; log.info(s""BT-322 {} cache hit copying nomatch: could not find a suitable cache hit."", jobTag); workflowLogger.info(""Could not copy a suitable cache hit for {}. No copy attempts were made."", jobTag); }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6484:1203,failure,failure,1203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484,1,['failure'],['failure']
Availability,"Hi, I was trying to have a VCF related workflow, which involves gatk4, picard tools. As an example, lets say I want to call gatk4 first to get some VCF files, and use picard to sort them. if i have `gatk4.cwl` output as; ```; outputs:; vcf_list:; type: File[]; outputBinding:; glob: '*.vcf.gz'; secondaryFiles: [.tbi]; ```; and next `picard sort` has input array (w/ or w/o `secondaryFiles` here doesnâ€™t matter from my tests. Neither works and will have the same error); ```; inputs:; vcf:; type:; type: array; items: File; inputBinding:; prefix: I=; separate: false; ```; After gatk4 finishes, the `execution` dir will look like; ``` ; drwx------ 3 root root 4.0K Jan 14 19:16 genomicsdb-0; -rw-r--r-- 3 root root 5.7K Jan 14 20:17 genomicsdb-0.vcf.gz; -rw-r--r-- 2 root root 105 Jan 14 20:17 genomicsdb-0.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:17 genomicsdb-1; -rw-r--r-- 3 root root 927K Jan 14 20:32 genomicsdb-1.vcf.gz; -rw-r--r-- 2 root root 7.6K Jan 14 20:32 genomicsdb-1.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:29 genomicsdb-2; -rw-r--r-- 3 root root 554K Jan 14 20:31 genomicsdb-2.vcf.gz; -rw-r--r-- 2 root root 11K Jan 14 20:31 genomicsdb-2.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:41 genomicsdb-3; -rw-r--r-- 3 root root 813K Jan 14 20:30 genomicsdb-3.vcf.gz; -rw-r--r-- 2 root root 11K Jan 14 20:30 genomicsdb-3.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:52 genomicsdb-4; -rw-r--r-- 3 root root 620K Jan 14 20:32 genomicsdb-4.vcf.gz; -rw-r--r-- 2 root root 12K Jan 14 20:32 genomicsdb-4.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 20:04 genomicsdb-5; -rw-r--r-- 3 root root 50K Jan 14 20:17 genomicsdb-5.vcf.gz; -rw-r--r-- 2 root root 746 Jan 14 20:17 genomicsdb-5.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 20:05 genomicsdb-6; -rw-r--r-- 3 root root 673K Jan 14 20:31 genomicsdb-6.vcf.gz; -rw-r--r-- 2 root root 13K Jan 14 20:31 genomicsdb-6.vcf.gz.tbi; drwxr-xr-x 2 root root 4.0K Jan 14 20:32 glob-330eecb06b4c0ad6b45febf0c8001b04; -rw-r--r--",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4546:463,error,error,463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4546,1,['error'],['error']
Availability,"Hi,. I am a novice BI trying to implement cromwell via GATK exomesinglesample.wdl on a slurm HPC.; I notice the Haplotype Caller stage tries to use all available memory on the system -1024 with:; available_memory_mb=$(free -m | awk '/^Mem/ {print $2}'); let java_memory_size_mb=available_memory_mb-1024. which is causing problems.; Is there a way of setting the maximum memory that this rule can grab in the config file?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7335:152,avail,available,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7335,1,['avail'],['available']
Availability,"Hi,. I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch; Cromwell version: 51; Error log:. Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; nmerged.bam)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977:292,Error,Error,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"Hi,. I have built a WDL workflow which works well with SLURM but now I am trying to get it to be able to be run on a standalone server. . I have Slurm as my provider and have created one for Local. ` Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. run-in-background = true; exit-code-timeout-seconds = 300; workflow-reset = true; read_from_cache = true; write_to_cache = true; system.file-hash-cache=true; concurrent-job-limit = 2. runtime-attributes = """"""; String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg""; """""". submit = ""singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; } ## end local; } ## end file systems; } ## end config; } ## End Local`. Oddly, when running the workflow I get a submit docker error. ie. as per below. I have no idea why it's looking for docker as I'm not knowingly using it. I'm not using docker in my run time parameters. I have been able to get standalone working on another workflow by passing a singularity container to each task command output but I was wondering if there was a more elegant solution I could use such as just changing to a pre-made provider. I have searched Google and through here but not found anything. I did find one issue here but they seemed to want to use docker where as I don't. . Thanks for the help!. `task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_scri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:934,error,error,934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['error'],['error']
Availability,"Hi,. I recently ran a workflow on our dev server which was running cromwell 37. Right around that time dev-ops reverted it to 36. I wanted to confirm that it ran on 37, but couldn't find anything in the metadata to indicate the version of cromwell that was running when the workflow executed. I think it would be great to have that information available. Thanks. FYI - here's the metadata for the workflow in question:; https://cromwell.gotc-dev.broadinstitute.org/api/workflows/v1/0ac19869-db74-45ba-a785-9e3eebe4103d/metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4668:344,avail,available,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668,1,['avail'],['available']
Availability,"Hi,. I was wondering if there's any interest to support a new backend for submitting jobs to a Hashicorp Nomad cluster?; I suppose this will be fairly similar to the AWS batch system. Thanks; Matthias. ping @tomiles",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6001:202,ping,ping,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6001,1,['ping'],['ping']
Availability,"Hi,. I'm trying to run through the Hello World example. I'm running Cromwell on OS X with Java 8. Any ideas on how to troubleshoot the below error?. Thanks!. ```; â€º cromwell run hello.wdl hello.json; [2015-12-18 08:43:13,222] [info] Slf4jLogger started; [2015-12-18 08:43:13,335] [info] Default backend: LOCAL; [2015-12-18 08:43:13,335] [info] RUN sub-command; [2015-12-18 08:43:13,336] [info] WDL file: hello.wdl; [2015-12-18 08:43:13,439] [info] Inputs: hello.json; [2015-12-18 08:43:13,560] [info] input: test.hello.name => ""world""; [2015-12-18 08:43:13,776] [info] SingleWorkflowRunnerActor: launching workflow; [2015-12-18 08:43:15,936] [info] Running with database db.url = jdbc:hsqldb:mem:86473284-494c-43d2-94fd-d00107a2a787;shutdown=false;hsqldb.tx=mvcc; [2015-12-18 08:43:17,516] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = e67af113-c3a7-41f4-9178-6640c1c652e9; [2015-12-18 08:43:17,592] [info] WorkflowManagerActor Found no workflows to restart.; [2015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:141,error,error,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['error'],['error']
Availability,"Hi,. I've noticed that the glob() function from WDL doesn't work on the AWS. Here is a sample workflow:. ```; workflow glob_test; {; call GlobTest; {; input:; m1 = ""msg1"",; m2 = ""msg2""; }; }. task GlobTest; {; String m1; String m2; command; <<<; echo ${m1} > foo.1.txt; echo ${m2} > bar.1.txt; echo ${m2} > foo.2.txt; >>>; runtime; {; cpu: 1; memory: ""128 MB""; docker: ""ubuntu:latest""; }; output; {; Array[File] o1 = glob(""*.1.txt""); Array[File] o2 = glob(""*.2.txt""); }; }; ```. The workflow finishes successfully, but in the AWS logs I see the following line:; ```; aws s3 cp --no-progress /cromwell_root/glob-6f3569c1e6d4f3f2cfb3469eb61bd5a0/* s3://path/to/s3/; The user-provided path /cromwell_root/glob-6f3569c1e6d4f3f2cfb3469eb61bd5a0/* does not exist. ; ```. I've checked and there is no trace of directory with the name ""glob-*"" anywhere starting from root dir. Command used to run this workflow on aws:; `java -Dconfig.file=aws.conf -jar cromwell-36.jar run test.wdl`. Thank you,; Timur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4353:246,echo,echo,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4353,3,['echo'],['echo']
Availability,"Hi,. Is it possible to use an SQLite database instead of a MySQL server to allow for call-caching?. This post comes close to saying there isn't a way without a lot of work (https://github.com/broadinstitute/cromwell/issues/3786). Is this still the position?. I know you can use a MySQL database, but sometimes the extra resources are not available or difficult to get. An option for a local persistence database would be very useful. Thanks for your help,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271:338,avail,available,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271,1,['avail'],['available']
Availability,"Hi,. Running a workflow on WSL/Ubuntu 20.04 using conda-installed Cromwell:. `cromwell run ngs-ubuntu-20-04/iletisim/warp/pipelines/broad/dna_seq/germline/single_sample/exome/local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl -i ngs-ubuntu-20-04/iletisim/json/S736Nr1.json -o ngs-ubuntu-20-04/iletisim/json/options2.json`. Getting the error:. ```; [2023-02-04 08:55:00,61] [info] Running with database db.url = jdbc:hsqldb:mem:bc9ad7e3-efc7-4f37-aecb-b283b104cbcd;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,54] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2023-02-04 08:55:06,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2023-02-04 08:55:06,64] [info] Running with database db.url = jdbc:hsqldb:mem:a487ea75-b617-4523-a254-d0e694e68ff9;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,92] [info] Slf4jLogger started; [2023-02-04 08:55:07,18] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b625dba"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2023-02-04 08:55:07,22] [info] Metadata summary refreshing every 2 seconds.; [2023-02-04 08:55:07,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2023-02-04 08:55:07,63] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2023-02-04 08:55:07,64] [info] SingleWorkflowRunnerActor: Version 34-unknown-SNAP; [2023-02-04 08:55:07,65] [info] SingleWorkflowRunnerActor: Submitting workflow; [2023-02-04 08:55:07,68] [info] Unspecified type (Unspecified version) workflow 48f62f22-25fe-4f0f-b5fe-21191f035abd submitted; [2023-02-04 08:55:07,72] [info] S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:344,error,error,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['error'],['error']
Availability,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5131:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131,1,['error'],['error']
Availability,"Hi,. Switching from version 51 to 52/53 I get the error:. ```; Task XYZ has an invalid runtime attribute memory = !! NOT FOUND !!; ```. I could not find any docs describing the change in required runtime attributes. I cannot add it to config as it raises another error. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5817:50,error,error,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5817,2,['error'],['error']
Availability,"Hi,. The task doesn't fail when the output file can't be found, hence, cromwell tries to run the next job. Here is a small sample workflow:. ```; workflow should_fail; {; call Task1; {; input:; out_base = ""foo""; }; }. task Task1; {; String out_base. command; <<<; #typo simulation, one extra 't'; echo ""Hello World!"" > ${out_base}.ttxt; >>>. runtime; {; cpu: 1; memory: ""128 MB""; docker: ""ubuntu:latest""; }. output; {; File out = ""${out_base}.txt""; }; }; ```. command used to run the workflow on the aws: ; `java -Dconfig.file=aws.conf -jar cromwell-36.jar run test.wdl`. Thank you,; Timur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4354:297,echo,echo,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4354,1,['echo'],['echo']
Availability,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:493,error,error,493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,2,['error'],['error']
Availability,"Hi,; I am trying to create two task. MarkDuplicates followed up Samtools index of that MarkDuplicates. Both from separate dockers. The issue I think is that when samtools indexes a bam file it does it in the same directory. So if the input to samtools is output from MarkDuplicates then I get this output error:. java.io.FileNotFoundException: Could not process output, file not found: /home/coyote/cromwell/WGS/cromwell-executions/AlignBwaMem/87cfebcc-b103-4e15-8313-0f39a8a959d5/call-md/shard-1/execution/SRR13481471.md.bam.bai. because for some reason both SRR13481471.md.bam and SRR13481471.md.bam.bai and in the folder inputs for samtools to index, i.e. samtools is indexing the input folder bam and the resulting index goes to that inputs folder and not the execution folder. How do we handle this? There must be a way since this must happen alot. Why does cromwell think the output index file should be in the MarkDuplictes folder ""md"" in my case. Here is my calls and tasks:; ```; call md_bqsr.markdupsIndiv as md {; 	input :; 		bam = samsort.outputBam,; 		outputPrefix = s.outputPrefix,; 		runtime_params = standard_runtime_gatk; }. call samtools.index as samindex {; 	input :; 		bam = md.bamMD,; 		runtime_params = standard_runtime_samtools; }. task markdupsIndiv {; 	input {; 		File bam; 		String outputPrefix. 		# runtime; 		RuntimeGATK runtime_params; 	}; 	; 	command {; 		set -e -o pipefail; 		export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. 		gatk --java-options ""-Xmx~{runtime_params.memory}m"" MarkDuplicates \; 			-I ~{bam} \; 			-O ~{outputPrefix}.md.bam \; 			-M ~{outputPrefix}.md.metrics; 	}. 	runtime {; 		maxRetries: runtime_params.max_retries; 		memory: runtime_params.memory + "" MB""; 		cpu: runtime_params.cpu; 		docker: runtime_params.gatk_docker; 	}; 	; 	output {; 		File bamMD = ""~{outputPrefix}.md.bam""; 		File metrics = ""~{outputPrefix}.md.metrics""; 	}	; }. task index {; 	input {; 		File bam; 		; 		# runtime; 		RuntimeSamtools runtime_pa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6182:305,error,error,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6182,1,['error'],['error']
Availability,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:403,error,errors,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,3,['error'],"['error', 'errors']"
Availability,"Hi,; I run cromwell for warp pipeline using slurm HPC and apptainer, and I got errors of localization:. (i) hard-link try to bind the cwd with wrong name, the real one has a suffix (.tmp); (ii) copy and cashe-copy have error: file-name too long (I can see the generated file path has loops, that makes the file name too long). B.W.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7097:79,error,errors,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7097,2,['error'],"['error', 'errors']"
Availability,"Hi,; I'm seeing a number of compilation errors with head dev etc. I've attached the output from debug. Scala version; scala-2.12.0-1.noarch. Java version; java 10.0.1 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode). CentOS Linux release 7.5.1804 (Core). Error output attached. Any thoughts ? Could be something obvious as I'm new to scala.; Thanks. [comp.log](https://github.com/broadinstitute/cromwell/files/2109138/comp.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3791:40,error,errors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791,2,"['Error', 'error']","['Error', 'errors']"
Availability,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4541:566,error,error,566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541,2,['error'],['error']
Availability,"Hi. I have a workflow named `main_wf.wdl` which imports a subworkflow named `sub_wf.wdl` like the following:; ```; import ""sub_wf.wdl"" as sub; ```; I am able to run `main_wf.wdl` in run mode. But when I run a cromwell server and uses the swagger UI to commit the above workflow, I get the following error:. ```; 2018-12-17 11:47:12,214 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-12-17 11:47:12,235 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-12-17 11:47:13,348 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow 3bd35e44-d8a7-41ed-8271-c773949e8c5c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; error in opening zip file; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:214); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:184); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:179); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); ```; I put the `sub_wf.wdl` into a `workflow.zip` file like below:; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509:299,error,error,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi. I have been using WDL/Cromwell/Terra for over a year. It works great for ""simple"" pipeline. It has saved me a lot of time and money. I need to run something more complicated. I want to use the scatter gather pattern. I wrote a simple workflow to figure out how to implement scatter-gather. My scatter task works as expected however, my gather task does not. I have each of my tasks in separate files. This made it easier to debug my task separately. The only way I could iterate over an Array was to set the first line of my wdl file to be 'version 1.0' and use the following syntax. ```; version 1.0; workflow loopWorkflow {; call loopTask; }. task loopTask {; Array[String] csvParts = [""a"", ""b"", ""c""]; Int n = 123. command <<<; set -euxo pipefail. exec 2>&1; echo ""shell: $SHELL""; $SHELL -h. echo ""AEDWIP The value of N should be 123 n = ~{n}"". for part in ~{sep=' ' csvParts}; do; echo ""AEDWIP $part AEDWIP""; done. echo ""\n\n********* create a bashArray""; bashArray=('~{sep=""' '"" csvParts}'); for part in ""${bashArray[@]}""; do; echo ""bashArray loop aedwip $part aedwip""; done. ls -l >results.csv; >>>. output {; File results_csv='results.csv'; }; }; ```. setting the version to 1 prevents me from being able to import my loopTask into my workflow. ```; [2023-03-20 19:23:37,02] [info] MaterializeWorkflowDescriptorActor [c74751ef]: Parsing workflow as WDL draft-2; [2023-03-20 19:23:37,26] [info] WorkflowManagerActor: Workflow c74751ef-5494-46d1-8064-25e7d4334405 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; ERROR: Finished parsing without consuming all tokens. version 1.0; ^; ```. Also when I set the version to 1.0 I am no longer able to use womtool-85 to generate template input. . Also I can not use cromwell-85 run with --inputs with version 1. comments and suggestions appreciated. Kind regards. Andy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7093:765,echo,echo,765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7093,6,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5370:1203,alive,alive,1203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370,1,['alive'],['alive']
Availability,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967:409,error,errors,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967,1,['error'],['errors']
Availability,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023:654,error,error,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023,2,['error'],['error']
Availability,"Hiï¼ŒI have learned that cromwell does not support ã€cpu and memoryã€‘runtime attributes for local backendsï¼ˆsee https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/ï¼‰. Thus, when running a workflow locallyï¼Œ How can we avoid concurent jobs that may crash the workflow by running out of memoryï¼Ÿ I know that maximum job number can be limited, and some jobs can be parallelized wildly for they require only little resources , however, some jobs should not be paralleized for need of large memory. So, we need to check available resource before submit a job. . best wishes!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6458:517,avail,available,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6458,1,['avail'],['available']
Availability,Hopefully addresses issues with missed summarizations by filtering on the client. This appears to have the same 3 PAPI v2 failures that are on develop related to GPUs and slightly less memory than expected in a monitoring log assertion. ðŸ¤·â€â™‚,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125:122,failure,failures,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125,1,['failure'],['failures']
Availability,How does Cromwell deal with rate limits on JES? I have some largish batch jobs that seem to result in triggering JES (google genomics api) rate limits when submitted via Cromwell. Is Cromwell robust to this?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1663:192,robust,robust,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1663,1,['robust'],['robust']
Availability,How does it work? Does Cromwell still have the chance to evaluate failure retries against e.g. exit codes?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7476:66,failure,failure,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7476,1,['failure'],['failure']
Availability,"How to evaluate engine functions in a Pluggable Backends World?. **Acceptance Criteria**:. Use the following WDLs files to test isolated components of engine function implementation:. Read/Write file at a task level when running against the Local Backend:. ```; task t {; Array[String] a = [""a"", ""b"", ""c""]; File x = write_lines(a); String y = read_string(""/foo/bar/some-public-object.txt""); command {; cat ${x}; echo ${y}; }; }. workflow w { call t }; ```. read functions from the task output section when running against the Local Backend:. ```; task t {; command {; echo foo; echo bar; }; output {; Array[String] array = read_lines(stdout()); }; }. workflow w { call t }; ```. **Unanswered Questions**. What do we do when a user wants to call `write_lines` from the workflow level? If multiple filesystems are supported, then it's unclear where this gets written to. Perhaps all of them? If `x` is used by a JES job, then the file _must_ be written to at least GCS. ```; workflow w {; String x = write_lines([""a"", ""b"", ""c""]); }; ```. *\* Decision **; We should make this operation unsupported until we have a concrete use case for it, and likely this will be easier when we have filesystem separated out entirely from backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/654:412,echo,echo,412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/654,3,['echo'],['echo']
Availability,How to troubleshoot the backend failure?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6904:32,failure,failure,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6904,1,['failure'],['failure']
Availability,I accidentally submitted a WDL and inputs to the batch submission endpoint instead of the regular submission endpoint. Cromwell did not respond with an error and eventually just timed out.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1270:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1270,1,['error'],['error']
Availability,"I added a workaround since #891. The output block now makes a File output and then reads from it. Unfortunately, this version still doesn't work!. Here's a slightly different workflow:. ```; task MakeMeAFile {; Int c = 6. command <<<; echo FILECONTENT > output.txt; >>>. runtime {; docker: ""ubuntu:latest""; memory: ""1 GB""; preemptible: 3; }. output {; File outFile = ""output.txt""; String out = read_string(outFile); }; }. task ReadMeAFile {; File infile. command <<<; cat ${infile}; >>>. runtime {; docker: ""ubuntu:latest""; memory: ""1 GB""; preemptible: 3; }. output {; File out = read_string(stdout()); }; }. workflow FileMakeAndRead {; Int a = 5; call MakeMeAFile; call ReadMeAFile { input: infile = MakeMeAFile.out }; }; ```. And the new failure:. ```; ""failures"": [; ""Item not found: cromwell-dev/cromwell_execution/chrisl/FileMakeAndRead/000adc78-7dc7-4073-b5f6-83e499c13706/call-MakeMeAFile/cromwell-dev/cromwell_execution/chrisl/FileMakeAndRead/000adc78-7dc7-4073-b5f6-83e499c13706/call-MakeMeAFile/output.txt""; ],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/892:235,echo,echo,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/892,3,"['echo', 'failure']","['echo', 'failure', 'failures']"
Availability,"I am a developer of one of the Cromwell UI clients ( https://github.com/antonkulaga/cromwell-client ) and I use /api/womtool/{version}/describe call to let user check if there are errors in his code before running the workflow (so if something is wrong it is possible to figure out without bloating cromwell-executions history with failed workflows). However, right now it does not check if files in input.json can be resolved that is checked only when you attempt to run, it would be nice to allow optional parameters to check if the files mentioned in input.json can be resolved in describe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6142:180,error,errors,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6142,1,['error'],['errors']
Availability,"I am aiming to run several thousand samples using Cromwell server on AWS. In testing, when submitting more than a few jobs to batch, I see many errors like:. ```; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: b8d3f02a-deee-11e8-8f12-b5957ed5827f); ```. I think this has to do with AWS batch limits. The recommendation, it appears, is to set the concurrent-job-limit in cromwell config to some number like 16. (Setting to 100 results in the errors above). While that fixes the errors, it seems to limit the scale of what can be done on AWS. So, a few questions:. 1. Does the job submission mechanism (batch vs single) affect this behavior?; 2. Would the use of scatter-gather over large batches of otherwise independent samples help?; 3. Are there limit increases that can or should be requested from AWS to make batch more amenable to large numbers of workflows?; 4. Are the best practices for AWS different than for GCP with respect to workflow authoring and submission?. Thanks for any thoughts and sorry if I missed something obvious in the docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4355:144,error,errors,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4355,3,['error'],['errors']
Availability,"I am brand new to cromwell and sorry if this question looks so basic.; Following the tutorial of ""Server Mode"", I open a webpage `localhost:8000` but I didn't find a button of `choose file` at `workflowSource` or at `workflowInputs`. I did see a red error sign at the bottom showing `ERROR {...}` and there is a message in it `{""schemaValidationMessages"":[{""level"":""error"",""message"":""Can't read from file /swagger/cromwell.yaml""}]}`; Any suggestions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869:250,error,error,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I am constantly having issues when I run docker-compose of cromwell (I use https://github.com/broadinstitute/cromwell/tree/develop/scripts/docker-compose-mysql where I provide my configutation). Even though I do not have docker-user in my application.conf file when I run my pipelines, I get:; ```; cromwell_1 | [ERROR] [05/20/2017 12:57:46.015] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-f7fdd305-d137-4128-bf68-7c39fd6c834b/WorkflowInitializationActor-f7fdd305-d137-4128-bf68-7c39fd6c834b/Local] Error parsing generated wdl:; cromwell_1 | task submit {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromwell_1 | }; cromwell_1 | ; cromwell_1 | ; cromwell_1 | task submit_docker {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | ; cromwell_1 | String docker_cwd; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | ; cromwell_1 | docker run \; cromwell_1 | --rm -i \; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | --entrypoint /bin/bash \; cromwell_1 | -v ${cwd}:${docker_cwd} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | java.lang.RuntimeException: Error parsing generated wdl:; cromwell_1 | task submit {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:313,ERROR,ERROR,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"I am developing a pipeline and want to share google storage buckets for genome data (bowtie2 index tar ball and other big files) with users (Google authenticated) but want users to pay for the network traffic to download genome data. My pipeline works fine if storage bucket for genome data is set as ""owner pays"". But if I set it ""requester pays"" then I get the following error `BadRequestException: 400 Bucket is requester pays bucket but no user project provided`. I googled it and found that [`gsutil` must use JSON API (not CLI)](https://cloud.google.com/storage/docs/requester-pays) for ""requester pays"" buckets. Is there any plan to support ""requester pays"" buckets for JES backend? . ```; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-0/glob-019a547c7b0dda79121d0398158a07d0/ENCFF439VSY.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-1/glob-019a547c7b0dda79121d0398158a07d0/ENCFF463QCX.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: job id: operations/EJSf0Yz9Kxjs8__E9aKWivQBILWN-vrbGyoPcHJvZHVjdGlvblF1ZXVl; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: job id: operations/EJWg0Yz9KxiXpeC4gsSenC4gtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 16:01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:212,down,download,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,2,"['down', 'error']","['download', 'error']"
Availability,"I am encountering call caching issues with images from google artifact registry (gar). . When I use image directly from dockerhub or gcr I have no call caching issues and see this in the logs. > 2024-07-19 14:35:24 cromwell-system-akka.dispatchers.engine-dispatcher-26890 INFO - BT-322 61ba2acc:garTest.simpleLs:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-07-19 14:35:24 cromwell-system-akka.dispatchers.engine-dispatcher-26890 INFO - 61ba2acc-4274-423b-818a-8cf1da67cd44-EngineJobExecutionActor-garTest.simpleLs:NA:1 [UUID(61ba2acc)]: Could not copy a suitable cache hit for 61ba2acc:garTest.simpleLs:-1:1. No copy attempts were made. However, when I copy the same image to my access controlled google artifact registry I get this authentication error. > 2024-07-19 14:31:44 cromwell-system-akka.dispatchers.engine-dispatcher-3006 WARN - BackendPreparationActor_for_f20da4b8:garTest.simpleLs:-1:1 [UUID(f20da4b8)]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for us-central1-docker.pkg.dev/xxx/yyy/aaa Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/xxx/locations/us-central1/repositories/yyy\"" (or it may not exist)""}]}. The workflow completes successfully regardless of this error but call caching doesn't work when a gar image is used.; The service account I am using with the cromwell server has ""Artifact Registry Reader"" IAM role.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7473:778,error,error,778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473,4,"['down', 'error']","['downloadArtifacts', 'error', 'errors']"
Availability,"I am following up on a [report](https://gatkforums.broadinstitute.org/wdl/discussion/23250/wdl-1-0-wont-let-me-call-an-optional-task-with-an-optional-input) (by someone else) filed over a year ago.; Now I have a slightly different need, that is, the task 2 will take the optional input `input_2_opt` for its required input.; So, the following is what I want to achieve. ```; version 1.0. workflow my_workflow {; input {; File input_1; File? input_2_opt; }. call task1 {; input:; input_1 = input_1; }. if (defined(input_2_opt)) {; call task2 {; input:; input_2 = input_2_opt; }; }. output {; File output_1 = task1.output_1; File? output_2 = task2.output_2; }; }. task task1 {; input{; File input_1; }; command {; echo ""Hello, world!"" > hello.txt; }; output {; File output_1 = ""hello.txt""; }; }. task task2 {; input{; File input_2; }; command {; cat ${input_2} > goodbye.txt; }; output {; File output_2 = ""goodbye.txt""; }; }. ```. Running `womtool validate` on this gives. ```; Failed to process workflow definition 'my_workflow' (reason 1 of 1): Failed to process 'call task2' (reason 1 of 1): Failed to supply input input_2 = input_2_opt (reason 1 of 1): Cannot coerce expression of type 'File?' to 'File'; ```. But like in the original post, if I take out the version specification and the `input` braces in the workflow and tasks, womtool thinks the WDL is OK. Can you please explain what is the cause? And is there a solution on my end?. Thanks. ----------------------; ### The Jira interface is way too overwhelming ; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5354:712,echo,echo,712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5354,1,['echo'],['echo']
Availability,I am getting java.lang.ClassNotFoundException: languages.cwl.CwlV1_0LanguageFactory while using Cromwell 85 and 86 jar. ; [cromwell-85.log](https://github.com/broadinstitute/cromwell/files/13271608/cromwell-85.log); PFA for cromwell.log; Cromwell jar build completes without any errors. While using the jar to start Cromwell on Premise system we get the CWL error (command: nohup java -jar -Dconfig.file=/fastdata/02/genomics/cromwell/reference-84.conf /fastdata/02/genomics/cromwell/cromwell-85-f34251c-SNAP-original.jar server 2>&1 >> cromwell.log &).; https://github.com/broadinstitute/cromwell/releases notes for Cromwell 85 says: CWL implementation removed :This release removes the cwl top-level artifact. Some nonfunctional references may remain and will be addressed over time. Looks like some references is causing the issue?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7247:279,error,errors,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7247,2,['error'],"['error', 'errors']"
Availability,"I am reading your tutorial on [HPC](https://cromwell.readthedocs.io/en/stable/backends/HPC/#:~:text=FileSystems-,Shared%20FileSystem,providers.) and I have a question that could be very uneducated. The shared filesystem section talks about the localization strategies for inputs, which is certainly an issue, but the outputs are not mentioned. Let's say I have several nodes in a cluster and a single shared volume between them, either physical or software one (like Lustre). I am using Slurm backend and any node can end up running any task based on internal Slurm scheduling. Ideally I would want each task to copy the inputs from the shared volume to a local folder, create outputs, and then copy outputs to the shared volume. I know one can output final outputs anywhere, but how can one control what happens to intermediate files? The problem would arise if the subsequent tasks in the workflow are done on different nodes, but is enforcing (one node)/(one wf execution) even possible? Even if it is it beats the point of scheduling resources by availability. The solution I can see is running Cromwell FROM the shared volume, but then everything would happen there and tiny inputs and outputs would choke the job and possibly cause wear on hardware. Unless I can set a temp directory while the outputs are written?. I am asking because I am not experienced and would like to know if there are solutions I am missing before I end up doing development on my own. ; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5802:1051,avail,availability,1051,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5802,1,['avail'],['availability']
Availability,"I am running on a local backend (laptop) and have noticed that the number of tasks running simultaneously appears to be unlimited? If this is the case, would it be possible to add config parameters to local backends to limit cpu and memory usage to that available?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365:254,avail,available,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365,1,['avail'],['available']
Availability,"I am running the program [deFuse](https://bitbucket.org/dranew/defuse) version 0.8.1 using a local backend. When I run the progam locally inside a docker container, the program completes successfully. When I run it using Cromwell/WDL, it raises the following error:; ```; Starting defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakp; oints.split.001.fa.cdna.psl missing; Failure for defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a; -ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/96242; 9bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; Reason:; Job command with nonzero return code; Return codes: 139; Job output:; Running on 2ecb3961d54d; Note: /usr/local/bin/gmap.avx2 does not exist. For faster speed, may want to compile package on an AVX2 machine; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.sse42 -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa; Checking compiler assumptions for SSE2: 6B8B4567 327B23C6 xor=59F066A1; Checking compiler assumptions for SSE4.1: -103 -58 max=198 => compiler zero extends; Checking compiler options for SSE4.2: 6B8B4567 __builtin_clz=1 __builtin_ctz=0 _mm_popcnt_u32=17 __builtin_popcount=17 ; Finished checking compiler assumptions; Pre-loading compressed genome (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:259,error,error,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,2,"['Failure', 'error']","['Failure', 'error']"
Availability,"I am struggling to make read_json work on any data that is more complex than a flat json-map. For instance, a json like; ```json; [{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]; ```; cannot be read by ; ```; Array[Map[String,String]] runs = read_json(path_to_json_file); ```; even through it is clearly Array[Map[String, String]]; I get the following failure:; ```; Workflow failed; WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'get_gsm.runs': Failed to read_json(""/data/cromwell-executions/test/f8f591dc-3797-46de-9846-dbd2a902ff65/call-get_gsm/execution/GSM1698568_runs.json"") (reason 1 of 1): No coercion defined from '[{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]' of type 'spray.json.JsArray' to 'Object'.,List()))); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4518:202,down,download,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518,3,"['down', 'failure']","['download', 'failure']"
Availability,"I am trying to adapt a WDL workflow originally developed for DNAnexus, to work in AWS Batch. I am running from ""develop"" branch on Mac, server mode. The workflow seems to run on AWS, but then fails when checking for output logs in S3... inputs: [demux_plus_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/2099495/demux_plus_inputs.json.txt); workflow: [demux_only.wdl.txt](https://github.com/broadinstitute/cromwell/files/2099496/demux_only.wdl.txt); resource file: [workflows.zip](https://github.com/broadinstitute/cromwell/files/2099470/workflows.zip); config file, which shows some attempts to add the local filesystem, since I get an error about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:656,error,error,656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['error'],['error']
Availability,"I am trying to reproduce one of the [examples](https://github.com/broadinstitute/wdl/blob/develop/SPEC.md#example-5-word-count) in the spec. It doesn't work out of the box since `wc` command produces a string with filename, and not just integer. Thus, I tried to substitute all non-digit characters with function `sub`:; ```; output {; Int count = read_int(sub(stdout(), ""\D"", """"); }; ```; However, I get an error:; ```; Unable to load namespace from workflow: Unrecognized token on line 7, column 40:. Int count = read_int(sub(stdout(), ""\D"", """")); ^; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3$$anon$1: Workflow input processing failed:; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1908:408,error,error,408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908,1,['error'],['error']
Availability,"I am trying to run a workflow with a subworkflow that has some workflow level inputs. Somethign along the lines of ; ```; Workflow subWorkflow {. File f; String s; ...; ...; }; ```. ```; import ""file/path/subworkflow.wdl"" as sub. workflow root {; call sub.subWorkflow as aliasSub; }; ```. When I try to pass the values for `File f` and `String s` from the inputs json I get an failure message. To make sure I was giving the workflow the correct inputs json I first ran it with bad inputs on purpose and got expected failures; ```; status: ""Failed"",; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_pac' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.agg_preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_ann' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.wgs_coverage_interval_list' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.unmapped_bam_suffix' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_ud' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_amb' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_sa' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:377,failure,failure,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,3,['failure'],"['failure', 'failures']"
Availability,"I am trying to run the GATK4 workflow on Cromwell using the Spark backend. The WDL is from the Broad's GitHub repo:; https://github.com/broadinstitute/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl. The following is my cromwell conf file where cromwell is running on the namenode of the spark cluster:; ```json; include required(classpath(""application"")). backend {; default = ""Spark""; providers {; Spark {; actor-factory = ""cromwell.backend.impl.spark.SparkBackendFactory""; config {; root: ""/mnt/data/cromwell"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; master: ""yarn""; deployMode: ""cilent""; }; }; }; }; ```. It looks like the shell script cromwell is generating to submit the job to spark using spark-submit has a syntax error in it, so the workflow fails immediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4611:871,error,error,871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611,1,['error'],['error']
Availability,"I am trying to test my wdl and docker image on my local computer before publishing. ```; [2022-02-09 11:16:10,66] [warn] BackendPreparationActor_for_35c8738b:deseq_one_vs_all.one_vs_all:-1:1 [35c8738b]: Docker lookup failed; java.lang.Exception: Unauthorized to get docker hash aedavids/test-1vs-all-2:latest; at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:222); ```. using docker image I see that my image exists. I am able to start the container using docker run. I also tried using the sha instead of the tag name. same error. I run Cromwell as follows; ```; $ java -jar ${WDL_TOOLS}/cromwell-74.jar run --inputs ../1vsAllTask.wdl.inputs.json ../1vsAllTask.wdl; ```. ```; $ cat ../1vsAllTask.wdl.inputs.json ; {; stuff deleted; ""deseq_one_vs_all.one_vs_all.dockerImg"": ""aedavids/test-1vs-all-2""; }; ```. ```; $ docker images |grep aedavids; aedavids/test-1vs-all-2 latest 0d33407a54e3 19 hours ago 6.28GB; ```; Any idea what my issue might be?. Andy. fyi https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team is a bad URL. I not have access to Jira on broadworkbench.atlassian.net.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6674:634,error,error,634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674,1,['error'],['error']
Availability,"I assume this error has to do with my config, but not particularly clear what is going on... Here is the workflow, inputs, and config; I am running from swagger:; [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2077985/myWorkflow_awsbatch.wdl.txt). [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2077989/aws.conf.txt). [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2078033/hello.inputs.txt). ```; 2018-06-06 16:18:30,442 cromwell-system-akka.dispatchers.api-dispatcher-215 INFO - WDL (Unspecified version) workflow 948bf608-f91b-46a7-b892-86454be067fd submitted; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['error'],['error']
Availability,"I cant get the sra filesystem to work. Here is the error:. ```; [2020-08-21 11:08:59,62] [info] WorkflowManagerActor Workflow dbd5cdc0-c79a-42cd-b929-56ddb1115467 failed (during InitializingWorkflowState): common.exception.AggregatedMessageException: Failed to instantiate backend filesystem:; Cannot find a filesystem with name sra in the configuration. Available filesystems: ftp, s3, gcs, oss, drs, http; 	at common.validation.Validation$ValidationChecked$.$anonfun$unsafe$2(Validation.scala:98); 	at cats.syntax.EitherOps$.valueOr$extension(either.scala:66); 	at common.validation.Validation$ValidationChecked$.unsafe$extension(Validation.scala:98); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories$lzycompute(backend.scala:109); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories(backend.scala:108); 	at cromwell.backend.BackendConfigurationDescriptor.pathBuilders(backend.scala:120); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders$lzycompute(StandardInitializationActor.scala:62); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders(StandardInitializationActor.scala:62); 	at cromwell.backend.google.pipelines.common.PipelinesApiInitializationActor.$anonfun$workflowPaths$2(PipelinesApiInitializationActor.scala:137); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(Abst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,2,"['Avail', 'error']","['Available', 'error']"
Availability,"I could not find on [WDL spec](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md) something saying that structs could not be used as outputs... so I decide to report here. This is an example I prepared:. ```wdl; version 1.0. struct Test {; String name; File path; }. struct Collection {; Array[Test] samples; }. task GenerateComplexObject {; input {; Int items; }. command <<<; python <<CODE; import sys; import os; import json; items = []; for item in range(0, ~{items}):; name = f""test{item}.txt""; os.system(f""echo 'some content' > {name}""); items.append({""name"": f""item-{item}"", ""path"": name}). with open(""results.json"", ""w"") as fh:; json.dump({'samples': items}, fh); CODE; >>>. runtime {; docker: ""python:3.8""; memory: ""1 GB""; cpu: 1; preemptible: 3; disks: ""local-disk "" + 10 + "" HDD""; }. output {; Collection results = read_json(""results.json""); }; }. workflow TestStruct {; input {; Int items; }. call GenerateComplexObject {; input:; items=items; }. output {; Collection out = GenerateComplexObject.results; }; }; ```. When using local backend I have no problem, but when using PAPIv2 (`cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory`) the files from `Test` struct (path) do not delocalize. ```bash; gsutil ls gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/GenerateComplexObject.log; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_delocalization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_localization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_transfer.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/rc; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/results.json; gs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5592:524,echo,echo,524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592,1,['echo'],['echo']
Availability,"I do not really know how to give you more relevant information, but IntelliJ is complaining that the WDL plugin has an error ... ```; Cyclic TextAttributesKey dependency found: BAD_CHARACTER->BAD_CHARACTER; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276:119,error,error,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276,1,['error'],['error']
Availability,"I encountered this error in Cromwell v26, while running a large workflow that appears to have failed. I am not sure whether it is the cause of the failure; I have not yet been able to locate what job this corresponds to. ; ```; [ERROR] [05/22/2017 00:14:05.821] [cromwell-system-akka.dispatchers.engine-dispatcher-38] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; 	at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:22); 	at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2286:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2286,3,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"I encountered this error when running a WDL:; ```message: Runtime validation failed; causedBy: ; message: Task hello has an invalid runtime attribute docker = !! NOT FOUND !!; ```. I understand that it requires a docker attribute. The issue is that the error gets found at runtime. This should be caught when validating the WDL. . The risk is that users could ""run half their tasks and only find out mid-workflow that one needs an extra parameter"" (ChrisL's words).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2932:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2932,2,['error'],['error']
Availability,"I gave Cromwell the following JSON when I had to specify the pair workflow:. {; ""CNVSomaticPairWorkflow.common_sites"": ""gs://broad-dsde-methods/mkanaszn/iota_small_exac_grch38.interval_list"",; ""CNVSomaticPairWorkflow.gatk_docker"": ""samuelklee/gatk:sl_wgs_acnv_headers"",; ""CNVSomaticPairWorkflow.intervals"": ""gs://shlee-dev/CNV/intervals/pad_parmY_int_agilent_1kg_exotars_logrch38.tsv"",; ""CNVSomaticPairWorkflow.normal_bam"": ""gs://shlee-dev/hcc/hcc1143_N_clean.bam"",; ""CNVSomaticPairWorkflow.normal_bam_idx"": ""gs://shlee-dev/hcc/hcc1143_N_clean.bai"",; ""CNVSomaticPairWorkflow.read_count_pon"": ""gs://broad-dsde-methods/cromwell-execution-29/CNVSomaticPanelWorkflow/b36a6e5a-d31e-45e1-9935-b44b87690b6e/call-CreateReadCountPanelOfNormals/1kg_tutorial.pon.hdf5"",; ""CNVSomaticPairWorkflow.ref_fasta_dict"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.dict"",; ""CNVSomaticPairWorkflow.ref_fasta_fai"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa.fai"",; ""CNVSomaticPairWorkflow.ref_fasta"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa"",; ""CNVSomaticPairWorkflow.tumor_bam"": ""gs://shlee-dev/hcc/hcc1143_T_clean.bam"",; ""CNVSomaticPairWorkflow.tumor_bam_idx"": ""gs://shlee-dev/hcc/hcc1143_T_clean.bai"",; ""CNVSomaticPanelWorkflow.PreprocessIntervals.bin_length"": ""0"",; ""CNVSomaticPanelWorkflow.PreprocessIntervals.padding"": ""250""; }. As you can see, I accidentally wrote ""CNVSomaticPanelWorkflow"" instead of ""CNVSomaticPairWorkflow"" in the last two entries. I didn't get an error for this, which might have been useful though. This might be a duplicate issue but I didn't find another one related to this problem. Sorry if it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3120:1545,error,error,1545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3120,1,['error'],['error']
Availability,"I get a lot of coercion errors area just when I want to get a file inside the Directory, like dir + ""/"" +""filename"" that makes the Directory totally useless for the pipelines for me.; the error is:; ```; Workflow failed. WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'methylation_extraction.out': IllegalArgumentException: No coercion defined from wom value(s) '""/data/cromwell-executions/bs_map_fast/1ea51f16-2197-4703-be02-ee3e59c448c1/call-methylation_search/execution/output/output_CpG.bedGraph""' of type 'Directory' to 'File'.,List()))); ```; I enclose the pipeline ( main WDL there is bs_map_run_fast.wdl) and the input; [bs-seq.zip](https://github.com/broadinstitute/cromwell/files/3317934/bs-seq.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5041:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041,2,['error'],"['error', 'errors']"
Availability,"I get an `ArrayIndexOutOfBoundsException` error when starting the latest `develop` branch in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend.; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-56b0390-SNAP.jar server; 2019-01-31 18:29:28,169 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:29:32,763 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,786 INFO - CREATE TABLE cromwell.DATABASECHANGELOGLOCK (ID INT NOT NULL, `LOCKED` BIT(1) NOT NULL, LOCKGRANTED datetime NULL, LOCKEDBY VARCHAR(255) NULL, CONSTRAINT PK_DATABASECHANGELOGLOCK PRIMARY KEY (ID)); 2019-01-31 18:29:32,845 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,853 INFO - DELETE FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,854 INFO - INSERT INTO cromwell.DATABASECHANGELOGLOCK (ID, `LOCKED`) VALUES (1, 0); 2019-01-31 18:29:32,869 INFO - SELECT `LOCKED` FROM cromwell.DATABASECHANGELOGLOCK WHERE ID=1; 2019-01-31 18:29:32,888 INFO - Successfully acquired change log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEX",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:42,error,error,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['error'],['error']
Availability,"I get cyclic dependency errors in places where I do not see a reason for them. I enclose the wdl and input.json; [cyclic_error.zip](https://github.com/broadinstitute/cromwell/files/1654348/cyclic_error.zip). Here is also the error log:; ```; Workflow input processing failed; WorkflowFailure(This workflow contains a cyclic dependency on quality_de_novo_with_download.copy_initial_quality_reports,List())WorkflowFailure(wdl.Scope.childGraphNodesSorted(Scope.scala:51),List())WorkflowFailure(wdl.Scope.childGraphNodesSorted$(Scope.scala:42),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3176:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176,2,['error'],"['error', 'errors']"
Availability,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1637:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637,4,['error'],"['error', 'errors']"
Availability,"I had accidently left my `filesystems` stanza to `refresh-token` mode instead of what it was supposed to be: `application-default`. The error was very cryptic and left me guessing. ```; [2016-07-13 10:12:45,36] [info] Slf4jLogger started; [2016-07-13 10:12:45,59] [info] RUN sub-command; [2016-07-13 10:12:45,59] [info] WDL file: 3step.wdl; [2016-07-13 10:12:45,59] [info] Inputs: 3step.inputs; [2016-07-13 10:12:45,59] [info] Workflow Options: 3step.options; [2016-07-13 10:12:45,61] [info] Slf4jLogger started; [2016-07-13 10:12:45,63] [info] SingleWorkflowRunnerActor: Launching workflow; [2016-07-13 10:12:45,63] [info] WorkflowManagerActor Starting workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] SingleWorkflowRunnerActor: Workflow submitted dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] WorkflowManagerActor Successfully started WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,67] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [2016-07-13 10:12:46,07] [info] Running with database db.url = jdbc:hsqldb:mem:937e84db-703a-4f18-8e6d-1a2a18227cf5;shutdown=false;hsqldb.tx=mvcc; [2016-07-13 10:12:46,43] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: Call-to-Backend assignments: three_step.ps -> JES, three_step.cgrep -> JES, three_step.wc -> JES; [2016-07-13 10:12:46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:136,error,error,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,1,['error'],['error']
Availability,"I havd submit a job , but submit failed. The job input.json file size is large, longer than the maximum of 8388608ã€‚. How can i submit the large job successfully ?. The submit error is below:. {; ""status"": ""fail"",; ""message"": ""Request too large: Request was longer than the maximum of 8388608""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7426:175,error,error,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7426,1,['error'],['error']
Availability,"I have a WDL with command block `samtools view -s ${frac} -b ${bampath} -o ${outname}.bam`, where `frac` is a `Float` type. I'm running in FireCloud. Downstream, in the PAPI script, this had turned into `samtools view -s 5.0E-5 -b ...<etc>`, which rather confused samtools. Someone somewhere decided to render `0.00005` as `5.0E-5`, and it wasn't me. Can you help?. (Adam N said to reference WDL draft-2, so I am hereby doing that.). WDL follows:. ```; task downsample {; File bampath; File baipath; File refpath; File refipath; Float frac; String outname; ; Int disk_size = ceil((size(bampath, ""GB"") + size(refpath, ""GB"") + size(refipath, ""GB"")) * 3). command {; samtools view -s ${frac} -b ${bampath} -o ${outname}.bam; }; ; output {; 	File outcram = ""${outname}.bam""; }. runtime {; docker : ""mshand/genomesinthecloud:samtools.1.8""; disks: ""local-disk ${disk_size} HDD""; memory: ""4GB""; }; }. workflow dswf {; call downsample; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4049:150,Down,Downstream,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4049,3,"['Down', 'down']","['Downstream', 'downsample']"
Availability,"I have a command that creates an output file, and prints the name of that file to a known file. EG `./program` puts its output in 1234.txt, and prints '1234' into output_name. I thought I could do this in my task:. ```; command <<<; ./program; >>>; String filename = read_string('output_name'); output {; File output = ""~{filename}.txt""; }; ```. However this fails, because apparently read_string is executed before command. (no such file error, cromwell never calls the backend to execute the command). I tried replacing `File output = ""~{filename}.txt""` with `File output = ""~{read_string('filename')}.txt""`. This does not work as arbitrary expressions are not yet allowed in string interpolation:. `Unable to start job due to: No implementation of FileEvaluator[StringExpression]`. Is there anything I am doing wrong? Can I make read_string execute after command?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4092:439,error,error,439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4092,1,['error'],['error']
Availability,"I have a few cases in Firecloud (using cromwell .19) where it appears like some scatter jobs are not being launched. I have tracked it down to a problem with outputs from the prepare call not being registered as actual outputs. According to the logs and the gcs bucket all the outputs from the prepare step are right, but according to cromwell the output array is missing a couple entries. Looking in [workflow metadata](https://github.com/broadinstitute/cromwell/files/557271/metadata.txt) at the CallingGroup_Workflow.mone_prepare call, the outputs for split_base.06.interval_list and split_base.22.interval_list are missing. According to the [google bucket listing of the glob dir](https://github.com/broadinstitute/cromwell/files/557270/glob_listing.txt), the files are there. The JES logs for the prepare step show the upload happens successfully. To summarize, the call CallingGroup_Workflow.mone_prepare successfully generates and upload 25 interval files but the output section of the metadata shows only 23 (6 and 22 are missing). This means that only 23 downstream calls run which is bad. [workflow log](https://github.com/broadinstitute/cromwell/files/557272/workflow.9b9e6b19-aac4-451f-bf24-a9132c0e5408.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1633:135,down,down,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1633,2,['down'],"['down', 'downstream']"
Availability,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5256:562,error,error,562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I have a job that ended up in a weird state. It's a series of 3 tasks, where each one depends on the output of the previous one. The workflow finished, but the middle job is listed as still running. It's workflow. Seems like an error state. See https://cromwell-v30.dsde-methods.broadinstitute.org/api/workflows/v1/01c7d76f-5b2b-48cd-be08-ce75b923666e/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483:228,error,error,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483,1,['error'],['error']
Availability,"I have a large WDL file. At some point, I write the below WDL and get an error:. ```; Array[File] mybams. if (run_my_bams) {; scatter (i in range(length(mybams))) {; ......; }; }; ```; ```; Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for If '$if_1': Two or more nodes have the same FullyQualifiedName: ^.mybams""; ```. To fix, I randomly tried the below and it worked:; ```; Array[File] mybams; Int num_mybams = length(mybams). if (run_my_bams) {; scatter (i in range(num_mybams)) {; ......; }; }; ```. Given that this is equivalent WDL, there should not have been an error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3284:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3284,2,['error'],['error']
Availability,"I have a scatter block in a WDL (see below). I had one task that called a subworkflow and it was fine (m2.Mutect2). So I decided to add another task to the scatter block and now I get a parsing error. I'm really trying to find the issue here, but can't. Basically, p.left.left works for earlier calls in the same scatter block but not the later ones. ```. import ""dl_ob_training_m2.wdl"" as dl_ob_training_m2; import ""mutect2.wdl"" as m2. workflow dl_testing_m2 {; File tumor_bam_file_list; Array[File] tumor_bam_files = read_lines(tumor_bam_file_list). File tumor_bam_file_index_list; Array[File] tumor_bam_indices = read_lines(tumor_bam_file_index_list). File normal_bam_file_list; Array[File] normal_bam_files = read_lines(normal_bam_file_list). File normal_bam_file_index_list; Array[File] normal_bam_indices = read_lines(normal_bam_file_index_list). # pair.left and pair.right; Array[Pair[File, File]] tumor_bam_pair = zip(tumor_bam_files, tumor_bam_indices); Array[Pair[File, File]] normal_bam_pair = zip(normal_bam_files, normal_bam_indices). Array[Pair[Pair[File, File], Pair[File, File]]] tumor_normal_pairs = zip(tumor_bam_pair, normal_bam_pair). # ...........<snip>......... scatter (p in tumor_normal_pairs) {; # This call works fine; call m2.Mutect2 as m2_tn {; input:; gatk4_jar = ""/root/gatk-protected.jar"",; intervals = mutectIntervals,; ref_fasta = ref_fasta,; ref_fasta_index = ref_fai,; ref_dict = ref_dict,; tumor_bam = p.left.left,; tumor_bam_index = p.left.right,; tumor_sample_name = sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; normal_bam = p.right.left,; normal_bam_index = p.right.right,; normal_sample_name = sub(sub(p.right.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; scatter_count = scatter_count,; dbsnp = dbSNPVCF,; dbsnp_index = dbsnp_index,; cosmic = cosmicVCF,; cosmic_index = cosmic_index,; is_run_orientation_bias_filter = true,; is_run_oncotator = false,; oncotator_docker = ""broadinstitute/oncotator:1.9.2.0-eval-gatk-protected"",; m2_docker = ""broadinstitute/ga",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2334:194,error,error,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2334,1,['error'],['error']
Availability,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5348:481,error,error,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348,2,['error'],['error']
Availability,"I have a task with an input 'File' (WDL concept) that is actually a folder. I did not know what would happen, as I could not find explanation of the concept in either wdl or cromwell docs. . For the 'File's that are actually files, cromwell reports ```Localisation via hard-link has failed ... invalid cross device link``` as expected (different disks), and then seems to successfully soft-link (as expected). For the folder, the same hard link error appears, but then there is no soft link error, and the folder is (recursively) copied. The folder hard link would also fail due to folders not being hard-linkable. But the soft-link should probably succeed. I am running an SGE backend, with no modification of the 'localisation' settings https://cromwell.readthedocs.io/en/latest/backends/HPC/#shared-filesystem. What is the expected behaviour? Should I avoid using folders for WDL 'File's? Any advice would be appreciated. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3785:445,error,error,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3785,2,['error'],['error']
Availability,"I have a wdl task that doesn't handle an optional parameter as it should.; The wdl validates (wdltool-0.8.jar), but when I submit to my server, an exception is thrown over and over again. ```; [ERROR] [01/10/2017 15:07:12.214] [cromwell-system-akka.actor.default-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-98777f84-7e; 14-4408-b31e-9b57db5d813b/WorkflowExecutionActor-98777f84-7e14-4408-b31e-9b57db5d813b/98777f84-7e14-4408-b31e-9b57db5d813b-EngineJobExecutionActor-snps.testHC:NA:1/98777f84-7e1; 4-4408-b31e-9b57db5d813b-BackendJobExecutionActor-98777f84:snps.testHC:-1:1/DispatchedConfigAsyncJobExecutionActor] DispatchedConfigAsyncJobExecutionActor [UUID(98777f84)snps.t; estHC:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlStringType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at scala.util.Try$.apply(Try.scala:192); at wdl4s.Task.instantiateCommand(Task.scala:108); ...; ```. Restarting the service seems to be the only way of stoppin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1830:194,ERROR,ERROR,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:585,error,error,585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I have an similar issue to this [https://github.com/broadinstitute/cromwell/issues/1306](https://github.com/broadinstitute/cromwell/issues/1306), but I don't think OP's solution fits whats going in my case since input files are not part of the issue. Using the hello-world example:. ```{wdl}; task hello {; String name. command {; echo 'Hello ${name}!'; }; output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. with input; ```{json}; {; ""test.hello.name"": ""World""; }; ```; I run `$ cromwell run hello.wdl hello.json hello.out`. I get the error message:. > /Users/jasonweirather/Dropbox (Partners HealthCare)/projects/2017_08_FIRECLOUD/cromwell-executions/test/12ed39b6-cf8f-4ea1-a965-193cd89f99e9/call-hello/execution/stderr.background; -bash: syntax error near unexpected token `('. Seems it may be having troubles being run from a working directory with a space and parentheses. This was done on Mac OS X 10.12.6 in the bash terminal.; Version of cromwell is `cromwell: 28-5fd2237-SNAP`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2518:331,echo,echo,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2518,3,"['echo', 'error']","['echo', 'error']"
Availability,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3417:38,failure,failures,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417,4,['failure'],"['failure', 'failures']"
Availability,"I have been testing AWS Batch with Cromwell 36. Jobs are submitted, etc. However, I keep seeing **disk full errors** with any real workflows. I started looking into how the autoscaling of the filesystem is working. It appears that the autoscaling is set up to scale `/scratch`, but not `/cromwell_root`. I have tested that it is working in my custom AMI (created with the python script here: http://aws-genomics-workflows.s3-website-us-east-1.amazonaws.com/aws-batch/create-custom-ami/#create-a-custom-ami) by ssh'ing into a running container and creating some large files in `/scratch`. Doing the same in `/cromwell_root`, of course, does not result in growing the filesystem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4322:108,error,errors,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4322,1,['error'],['errors']
Availability,"I have caught cromwell red-handed! I did not realize that the cromwell timing was implemented in [TrumpScript](https://github.com/samshadwell/TrumpScript). **TL;DR** The timing diagram showed that my job took > 2 hours to run, even though half of that was spent on overhead in cromwell. ; ## Proof. Here is a snapshot of the timing diagram (see highlighted runtime below -- 2h 6m).; ![cromwell_snapshot_so_slow_lies](https://cloud.githubusercontent.com/assets/2152339/18798566/23db7738-81a1-11e6-8a39-43612a561aa7.png); Yet, when I check that job:. ```; $ head -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout. 17:06:18.839 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/root/gatk-protected/build/libs/gatk-protected-all-24e6bdc-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 17:06:19.327 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; 17:06:19.353 INFO CalculateTargetCoverage - Defaults.BUFFER_SIZE : 131072; 17:06:19.355 INFO CalculateTargetCoverage - Defaults.COMPRESSION_LEVEL : 5. $ tail -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout; 18:10:04.122 INFO CalculateTargetCoverage - Writing counts ...; 18:10:05.250 INFO CalculateTargetCoverage - Writing counts done.; 18:10:05.250 INFO CalculateTargetCoverage - Shutting down engine; ```. The job finished in about 64 minutes (please note that timezones are not concordant between timing and log messages).; This will likely (de facto) be addressed once the md5 issue is resolved in issue #1483",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1484:1478,down,down,1478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1484,1,['down'],['down']
Availability,"I have configured a SLURM backend for Cromwell and have encountered unusual behavior while trying to configure memory as a runtime attribute. . I define a runtime attribute Int with default value in my Cromwell configuration file and attempt to override this in my task WDL. Whether the override succeeds seems to depend on the variable name used! This is very confusing behavior; I expect to either receive a message that a variable name is not allowed, or the override should succeed. . Fails: ""memory_mb""; Succeeds: ""requested_memory_per_core"". I am verifying whether the override succeeds by checking the Cromwell output [task]/execution/script.submit. . ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int bsub_cpu = 1; Int memory_mb = 1000; String queue; """"""; ; submit = """"""; 			sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; 			${""-n "" + bsub_cpu} \; 			--mem-per-cpu=${memory_mb} \; 			--wrap ""/bin/bash ${script}""; 		""""""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. ```; workflow tutorialWorkflow{; 	call task_A { input : in=""testing"" }; 	call task_B { input : in=task_A.out }; 	call task_C { input : in=task_A.out }; }. task task_A{; 	String in. 	command{; 		echo 'This is task A ${in}.'; 	}	; 	output{; 		String out='This is task A ${in}'; 	}; 	runtime{; 		bsub_cpu: 1; 		runtime_minutes: 10; 		memory_mb: 100; 		queue: ""short""; 	}; }. task task_B{; 	String in; 	command{; 		echo 'This is task B ${in}.'; 	}; 	runtime{; 		bsub_cpu: 2; 		runtime_minutes: 15; 		memory_mb: 110; 		queue: ""short""; 	}; }. task task_C{; 	String in; 	command{; 		echo 'This is task C ${in}.'; 	}; 	runtime{; 		runtime_minutes: 25; 		memory_mb: 210; 		queue: ""short""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2068:1123,alive,alive,1123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068,4,"['alive', 'echo']","['alive', 'echo']"
Availability,"I have encountered this issue when running the GATK 4 [Merge BAM Alignment](https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl#L334) task, on Cromwell 36 running on Google Cloud. For some reason, it seems that the docker image `broadinstitute/gatk` is too large for the disk. What I'm not sure about is which disk this is referring to. Is it the machine running Cromwell, or is it the pipelines API worker machine? If it's the latter, how can I stop this error happening?. ```; cromwell_1 | 2018-10-25 06:53:52,612 cromwell-system-akka.dispatchers.engine-dispatcher-440 ERROR - WorkflowManagerActor Workflow 28605745-a8d2-43c4-ab02-70e5c5c032fe failed (during ExecutingWorkflowStat; e): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:0:1 failed. The job was stopped before the command finished. PAPI error code 5. 8: Failed to pull image broadinstitut; e/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: ""docker pull broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71"" failed: exit status 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:515,error,error,515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I have locally configured mariaDB my.cnf as followsï¼š. > [mysqld]; max_connections=5024; thread_cache_size=1000; datadir=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/Data/; socket=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/socket/cromwell.sock; default-time-zone='+8:00'; port=3310; skip-character-set-client-handshake; init_connect='SET collation_connection = utf8mb4_unicode_ci'; init_connect='SET NAMES utf8mb4'; character-set-server=utf8mb4; collation-server=utf8mb4_unicode_ci; #open slow query logging; slow_query_log = ON; slow_query_log_file = /zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/log-files/slow_cromwell.log; long_query_time = 1; [mysqld_safe]; log-error=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/log-files/error.log; pid-file=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/pid/mysqld_cromwell.pid; [client]; default-character-set=utf8mb4; > . Then I configured the flow. The following error was reported when scatter 478 tasks were needed in one stepï¼š. > 2021-12-06 17:03:51,401 cromwell-system-akka.dispatchers.service-dispatcher-9 ERROR - Failed to summarize metadata. java.sql.SQLException: null; at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129); at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); at com.mysql.cj.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:2045); at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:388); at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:511); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:37); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:34); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6583:658,error,error,658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6583,3,['error'],['error']
Availability,"I have tried the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) to run Cromwell on Google Cloud. I did not get very far. I have followed the long set of instructions. I have logged in with my `<google-user-id>`, I have set my own `<google-project-id>`. I have created my own bucket. I have generate my service account key with the command:; ```; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. Then I run the hello.wdl with the command:; ```; GOOGLE_APPLICATION_CREDENTIALS=sa.json; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```. But I get the following error:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:308); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:213); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:210); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.internalCreate(StorageImpl.java:209); 	at com.google.cloud.storage.StorageImpl.create(StorageImpl.java:171); 	at cromwell.filesystems.gcs.GcsPath.request$1(GcsPathBuilder.scala:196); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:658,error,error,658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,3,"['Error', 'error']","['Error', 'error']"
Availability,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3367:202,error,error,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367,1,['error'],['error']
Availability,I haven't seen this happen live but in theory 'take' can throw an exception if the underlying queue changes between checking `available` and running `dequeue`. That might happen if an unluckily timed `abort` removes an actor from the queue between those checks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4165:126,avail,available,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4165,1,['avail'],['available']
Availability,"I know that this is bigger than what the issue says - I'm sorry about that. It just felt like now or never to rework a little bit our communication with GCS and remove some duplicate code in the process. This PR removes the `IoInterface`/`IoManager` and uses `java.nio.FileSystem` implementations instead to provide file system interactions.; The (now removed) `IoInterface` was nothing but a wrapper interface around a `FileSystem`.; The (also removed) `IoManager` was a wrapper of `IoInterface`s that selected the most appropriate interface according to the path to be processed. This functionality is now provided by the `cromwell.engine.backend.io.PathString.toPath` method which takes a list of available `FileSystem`s and tries to create a `java.nio.Path`. If it succeeds (ie if there is a suitable `FileSystem` able to parse the raw `String`), the created `Path` can now be used in an abstract way (with `Files.*`) or `better.files` as long as the corresponding `FileSystemProvider` implements the required methods. . The `WdlStandardLibraryImpl` now has a `List[FileSystem]` at its disposal and uses it to parse raw `String` paths with the `toPath` method. Most of the implementation can then be the same and rely entirely on the `FileSystem`s implementations. Engine functions can always be overridden by their respective backend implementations if a special treatment is needed. The functionalities previously in `GoogleCloudStorage` have been merged into `GcsFileSystemProvider`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/524:700,avail,available,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/524,1,['avail'],['available']
Availability,"I know there have been some issues with read_object/write_object and structs but I haven't been able to find anything mentioning this particular problem. If I define a struct with mixed types and assign a string and int value inline it works as expected (with cromwell-48):. ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""int"": 4; }. command {; echo -e ""hello ~{p.string} ~{p.int}""; }; }. workflow main {; call print_params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414:448,echo,echo,448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414,3,"['echo', 'error']","['echo', 'error']"
Availability,"I know there is a more serious ticket for this issue, but it seems like a number of centaur local tests have been failing again lately due to `expected ""hello"" but got """"` kind of errors.; I found this https://linux.die.net/man/2/sync and have been restarting centaur local on this branch a few times and none of them have failed so far. I don't think it entirely fixes the issue though.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1974:180,error,errors,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1974,1,['error'],['errors']
Availability,"I missed that the value for `client-payload` is in single quotes and does not interpolate. Consequently, `""version"": ""$CROMWELL_VERSION""` was passed as a literal to the `update-service` [action](https://github.com/broadinstitute/terra-helmfile/actions/runs/3190563369/jobs/5205838370). There, it was evaluated in double quotes `VERSION=""$CROMWELL_VERSION""`, found to be empty in that environment, and halted with `Error: empty version string`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6926:414,Error,Error,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6926,1,['Error'],['Error']
Availability,I noticed an error in your tutorial:; https://cromwell.readthedocs.io/en/jg_add_http_doc/tutorials/AwsBatch101/. The aws.config you supply is missing:; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6; from the config portion:. It was generating a confusing error and should be updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4278:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4278,2,['error'],['error']
Availability,I noticed that I got a few failures using the default 1 second timeout here. . So this PR ups that to 10 seconds... ðŸ¤ž,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4385:27,failure,failures,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4385,1,['failure'],['failures']
Availability,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1070:689,ERROR,ERROR,689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070,3,"['ERROR', 'Failure']","['ERROR', 'Failures']"
Availability,"I noticed that cromwell generates exec.sh setting HOME presumably incorrectly.; This is from GATK4's somatic CNV workflow executed by cromwell 31 with JES backend. Thanks!. ```; #!/bin/bash; tmpDir=$(; set -e; cd /cromwell_root; tmpDir=""$(mkdir -p ""/cromwell_root/infant-all-cnv-out/cromwell-execution/CNVSomaticPairWorkflow/aa10c071-5c42-4f6a-a4e0-3ba96cc54283/call-ModelSegmentsTumor/tmp.3efcbfd1"" && echo ""/cromwell_root/infant-all-cnv-out/cromwell-execution/CNVSomaticPairWorkflow/aa10c071-5c42-4f6a-a4e0-3ba96cc54283/call-ModelSegmentsTumor/tmp.3efcbfd1"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME""; (; cd /cromwell_root. ); ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3421:403,echo,echo,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3421,2,['echo'],['echo']
Availability,"I ran 123 workflows as part of a single submission in FireCloud which means, same WDL and identical inputs aside from the BAM and sample name. They all started around the same time. . 3 failed with this failure:. `Google credentials are invalid: connect timed out`. Here is the deep-link to the FireCloud run:. https://api.firecloud.org/api/workspaces/engle-macarthur-ccdd/genomes-reprocessing/submissions/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflows/a714b11b-0162-4585-afa5-abbd7433af51. Here is the full metadata for the failed workflow:. {; ""workflowName"": ""BamToUnmappedBams"",; ""submittedFiles"": {; ""inputs"": ""{\""BamToUnmappedBams.input_bam\"":\""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/batch04/S64-2_Illumina.bam\""}"",; ""workflow"": ""task RevertSam {\n File input_bam\n String revert_bam_name\n Int disk_size\n\n # TODO: why is SORT_ORDER=coordinate set below since we sort it again in the next step?\n # TODO: why did we need this line?\n # OUTPUT_MAP=${output_map} \\\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n RevertSam \\\n INPUT=${input_bam} \\\n OUTPUT=${revert_bam_name} \\\n VALIDATION_STRINGENCY=LENIENT \\\n ATTRIBUTE_TO_CLEAR=FT \\\n ATTRIBUTE_TO_CLEAR=XS \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000 \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File unmapped_bam = \""${revert_bam_name}\""\n }\n}\n\ntask SortSam {\n File input_bam\n String sorted_bam_name\n Int disk_size\n\n # TODO: why not use samtools sort as it is multi-threaded?\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n SortSam \\\n INPUT=${input_bam} \\\n OUTPUT=${sorted_bam_name} \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000\n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File sorted_bam = \""${sorted_bam_name}\""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:203,failure,failure,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['failure'],['failure']
Availability,"I ran a WDL file locally, by first creating a gatk jar and building a Docker image on my computer and then executing the WDL using Cromwell. I ran the following commands:; ./gradlew shadowJar; docker build -t us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:testimage1 --build-arg DRELEASE=false .; java -jar cromwell-30.2.jar run cnv_somatic_pair_workflow.wdl --inputs cnv_somatic_pair_wgs_no-gc_workflow.json. Although all the necessary tasks seem to have finished, I still got the following error message:; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-mergePreferred#1200284127]] terminated abruptly; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-mergePreferred#-1631704537]] terminated abruptly; [2018-05-14 11:34:10,43] [info] Automatic shutdown of the async connection; [2018-05-14 11:34:10,43] [info] Gracefully shutdown sentry threads.; [2018-05-14 11:34:10,43] [info] Shutdown finished.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618:495,error,error,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618,5,['error'],['error']
Availability,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253:468,echo,echoHelloWorld,468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253,3,['echo'],"['echo', 'echoHelloWorld']"
Availability,"I ran into the following problem when upgrading from cromwell 31.1 to 36. ; It looks like in cromwell 31.1, it is possible to pass a single File to a task that expects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550:233,error,error,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550,2,['error'],"['error', 'errors']"
Availability,"I ran it using json ways and after configuring mysql and Call Caching it is still create a new project. here is my commands and config file. ```; java -jar -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/udocker_slum.conf ../cromwell-84.jar run /work/share/ac7m4df1o5/bin/cromwell/1_pipeline/Exome_Germline_Single_Sample/ExomeGermlineSingleSample_v3.1.5.wdl -i D5327.NA12878.json -o ../options.json; ```. conf is. ```; include required(classpath(""application"")). docker {; hash-lookup {; enable = false; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; 	concurrent-job-limit = 5; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -t ${runtime_minutes} \; 	 -p wzhcexclu06 \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; # udocker pull ${docker}. sbatch \; -J ${job_name} \; -D ${cwd} \; -t ${runtime_minutes} \; 	 -p wzhcexclu06 \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. help pleas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6920:1521,alive,alive,1521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6920,1,['alive'],['alive']
Availability,"I ran the WDL at the bottom of this issue using mock-jes. Initially it ran everything but then got stuck in a state with 2 jobs left and the 1 wf still ""running"". I looked and those 2 jobs had had 500 errors associated with them. . At a later point I restarted Cromwell, for a long (> 10 mins) period of time the stats endpoint was unresponsive although other endpoints were fine. After some period of time (< 1.5 hours) I came back and those 2 jobs had completed but the WF was still stuck as Running. task snooze {; command {; sleep 10 && ps > myfile.txt; }; output {; File procs = ""myfile.txt""; }; runtime {; docker: ""ubuntu:14.04""; preemptible: 3; cpu: 10; }; }. workflow one_step {; Array[Int] integers = range(20000); scatter(i in integers) {; call snooze; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662:201,error,errors,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662,1,['error'],['errors']
Availability,"I recently attempted to run a workflow with files in a separate non public google project as inputs. I submitted it using both Swagger and curl but neither gave very informative error messages. Here's the curl command and output:. ```; curl -v https://cromwell.gotc-dev.broadinstitute.org/api/workflows/v1 -F wdlSource=@PairedSingleSampleWf.wdl -F workflowInputs=@ExampleOfIncorrectProject.json; * Trying 104.197.140.34...; * Connected to cromwell.gotc-dev.broadinstitute.org (104.197.140.34) port 443 (#0); * TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384; * Server certificate: cromwell.gotc-dev.broadinstitute.org; * Server certificate: InCommon RSA Server CA; * Server certificate: USERTrust RSA Certification Authority; * Server certificate: AddTrust External CA Root; > POST /api/workflows/v1 HTTP/1.1; > Host: cromwell.gotc-dev.broadinstitute.org; > User-Agent: curl/7.43.0; > Accept: */*; > Content-Length: 38237; > Expect: 100-continue; > Content-Type: multipart/form-data; boundary=------------------------c2956aa00c34148a; > ; < HTTP/1.1 100 Continue; < HTTP/1.1 500 Internal Server Error; < Date: Fri, 18 Mar 2016 17:59:04 GMT; < Server: spray-can/1.3.2; < X-Frame-Options: SAMEORIGIN; < Access-Control-Allow-Origin: *; < Access-Control-Allow-Headers: authorization,content-type,accept,origin; < Access-Control-Allow-Methods: GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD; < Access-Control-Max-Age: 1728000; < Content-Type: text/plain; charset=UTF-8; < Content-Length: 69; < Connection: close; < ; * Closing connection 0; The server was not able to produce a timely response to your request.%; ```. The issue was that I was using the broad-gotc-dev project to run the workflow, but two of my input files were in broad-gp-gotc-pilot, but I had trouble figuring that out from this error message. Happy to provide the wdl and json files as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/589:178,error,error,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/589,3,"['Error', 'error']","['Error', 'error']"
Availability,"I restarted Cromwell with one workflow running and one of my tasks failed with this exception: . 2016-04-07 20:32:28,188 cromwell-system-akka.actor.default-dispatcher-20 ERROR - WorkflowActor [UUID(03db4daf)]: Completion work failed for call CollectQualityYieldMetrics:9.; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '27-PairedEndSingleSampleWorkflow.CollectQualityYieldMetrics-metr' for key 'UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO'. the logs can be found on gotc-staging - 20160407-cromwell.log",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/684:170,ERROR,ERROR,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/684,1,['ERROR'],['ERROR']
Availability,"I run a short bash pipeline like this : for CHROM in `cut -f1 $IN_FILE |grep -Pv '^@'| sort|uniq`; do. But I get this error : sort: cannot create temporary file in â€˜/cromwell_root/tmpâ€™: No such file or directory. when running in firecloud. should I ""mkdir -pv /cromwell_root/tmp"" first?. I see ""export TMPDIR=/cromwell_root/tmp"" in the exec.sh on the 3rd line.... I went ahead and issued before the pipeline starts the command ""mkdir -pv $TMPDIR"" and made the error go away!. Is it normal/expected to need to create TMPDIR ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731:118,error,error,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731,2,['error'],['error']
Availability,"I run cromwell on a SLURM cluster with a postgres database for call caching. Call caching mostly works fine, but there is no way to constrain the size of the cache. Eventually the available space (200GB) for the postgres database fills up, and all cromwell instances using the call cache fail. My workaround is to periodically stop all workflows and truncate the call caching database. . I imagine implementing an option to restrict the number of cache entries should be straightforward for a Guava-based cache; if someone could help roadmap, I may be able to submit a pull request in the future. . For my purposes, I assume that all cromwell instances agree on the size.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7560:180,avail,available,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7560,1,['avail'],['available']
Availability,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:78,failure,failure,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,4,"['ERROR', 'avail', 'error', 'failure']","['ERROR', 'available', 'error', 'failure']"
Availability,"I set up a heavily scattered (~1000x) workflow to run overnight on my local machine (Mac OS Catalina, Intel hardware). The machine is set up to never sleep and was connected to AC power. My Cromwell config is set to only run one task at a time, ie, only one shard of a scattered task runs at a time. The workflow stopped processing on shard 885, which was the 233rd shard to start (shards appear to start in a random order, that's not an issue). It looks like the Docker container in question is getting created, but not used. The container is not running according to Docker Desktop and the Docker CLI tools (see output below). ### Workflow; I've seen this happen with a few workflows, but this time around it's this one (failure is occurring on second task): https://github.com/aofarrel/SRANWRP/blob/bioproject_stuff/workflows/is_this_tuberculosis.wdl. ### Ruled out; * Running tasks concurrently/Cromwell config not being respected: The workflow would have either hung Docker or tasks would have returned 137; * Docker application (not the container, the entire application) hanging, like what happens when trying to run tasks concurrently on a local machine: `docker run -it` works in a new terminal window; * IP getting blocked: This would cause error output, and I can still ping SRA from the same IP without issue; * Loss of internet: This would cause error output (`curl command failed`); * Control-S: Control-Q doesn't unfreeze it; * No more disk space: There's about 50 GB free and each instance of the scattered task uses less than a GB. ### Docker container logs; `docker logs cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528` gives no output. ### Entering the container; `docker exec -it cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 /bin/sh` returns; `Error response from daemon: Container cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 is not running`. ### Docker inspect; ```; >docker inspect cf6f4828adc61eacf06337ce3caf2c110df6cc0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:723,failure,failure,723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,1,['failure'],['failure']
Availability,"I specified a zone of us-east1-a, which is not a legitimate zone. The problem is not handled well and the error message is confusing and unhelpful: . See operations/ELXWv-WdKxis4eaT-cfmx7EBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl which begins with:. ```; done: true; error:; code: 5; message: no zones available; .; .; .; ```. Cromwell loops forever in this pattern (current develop head, 678712acb303a6cce0d35d6bcb963f19407439f5):. ```; [INFO] [01/26/2017 16:30:48.889] [cromwell-system-akka.dispatchers.backend-dispatcher-31] [akka://cromwell-system/user/cromwell-service/$b/$a] The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$b#-1628247913] unexpectedly terminated while conducting 1 polls. Making a new one...; [INFO] [01/26/2017 16:30:48.889] [cromwell-system-akka.dispatchers.backend-dispatcher-31] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$c#1867219466]; [ERROR] [01/26/2017 16:30:58.241] [cromwell-system-akka.dispatchers.backend-dispatcher-31] [akka://cromwell-system/user/cromwell-service/$b/$a/$c] null; java.lang.NullPointerException; 	at cromwell.backend.impl.jes.Run$.ceInfo$lzycompute$1(Run.scala:122); 	at cromwell.backend.impl.jes.Run$.ceInfo$1(Run.scala:122); 	at cromwell.backend.impl.jes.Run$.machineType$lzycompute$1(Run.scala:123); 	at cromwell.backend.impl.jes.Run$.machineType$1(Run.scala:123); 	at cromwell.backend.impl.jes.Run$.interpretOperationStatus(Run.scala:130); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor.interpretOperationStatus(JesPollingActor.scala:86); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anon$1.onSuccess(JesPollingActor.scala:72); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anon$1.onSuccess(JesPollingActor.scala:69); 	at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:197); 	at com.google.api.client.googleapis.batch.BatchUnparsedResponse.pa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915:106,error,error,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915,4,"['ERROR', 'avail', 'error']","['ERROR', 'available', 'error']"
Availability,"I submitted a workflow with `defaultRuntimeOptions` instead of `default_runtime_attributes` and when I then called `/metadata`, I got this:. ```; {; ""calls"": {; },; ""id"": ""93215d4d-1341-4aba-9ded-ce83beaeedce"",; ""submission"": ""2016-06-23T20:11:26.122Z"",; ""status"": ""Failed"",; ""failures"": [""Workflow contains invalid options JSON: Unsupported key/value pair in WorkflowOptions: defaultRuntimeOptions -> {\""zones\"":\""us-central1-b\""}""],; ""end"": ""2016-06-23T20:11:26.509Z"",; ""start"": ""2016-06-23T20:11:26.137Z""; }; ```. Note that the `""inputs""` field is missing, but I didn't ask to exclude it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1066:277,failure,failures,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1066,1,['failure'],['failures']
Availability,"I tested it on a local backend. If you use a non-empty array then it works fine with cromwell-36. But with an empty array it fails. workflow:; ```; workflow test_opt_array {; #Array[File] arr1 = [""0.txt"",""1.txt"",""2.txt"",""3.txt"",""4.txt""]; Array[File] arr1 = []. Boolean go = true; if ( go ) {; scatter( i in range(5) ) {; call t1 {input: i=i}; }; }; Array[File] arr2 = select_first([t1.out, arr1]); }. task t1 {; Int i; command {; echo ${i} > out.txt; }; output {; File out = 'out.txt'; }; }; ```. This workflow worked fine with cromwell-34.; ```; $ java -jar ~/cromwell-34.jar run test_opt_array.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2018-10-25 21:20:58,35] [info] Running with database db.url = jdbc:hsqldb:mem:a975ddc6-f298-4393-b1f0-e93250d3cca8;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:21:07,82] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:21:07,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:21:07,95] [info] Running with database db.url = jdbc:hsqldb:mem:d98689d1-c87b-486c-aa55-626823fb3bb1;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:21:08,32] [info] Slf4jLogger started; [2018-10-25 21:21:08,56] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-fcf9c1d"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:21:08,59] [info] Metadata summary refreshing every 2 seconds.; [2018-10-25 21:21:08,63] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:21:08,64] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-25 21:21:08,64] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:21:09,79] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-25 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:430,echo,echo,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4651:1036,alive,alive,1036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651,1,['alive'],['alive']
Availability,"I tried running a WDL in version 1.1, to use the `as_map(Array[Pair[X,Y]])` function, but it doesn't work. ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; ; ],; ""message"": ""ERROR: Finished parsing without consuming all tokens.\n\nversion 1.1\n^\n ""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ```. According to the [LanguageSupport specifications](https://cromwell.readthedocs.io/en/stable/LanguageSupport/), the only supported versions for WDL are `1.0` and `development`. Is this correct?. Would it be possible to add support for `WDL 1.1`?. [Jira issue](https://broadworkbench.atlassian.net/browse/CROM-6710)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221:113,failure,failures,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221,2,"['ERROR', 'failure']","['ERROR', 'failures']"
Availability,"I tried running a these two glob output commands; `Array[File] gvcf_list = glob(""split_gvcfs/*.gz"")`; `Array[File] gvcf_index_list = glob(""split_gvcfs/*.tbi"")`; Which should have had 3457 elements in each array. When I go to the output directory bucket of the above task, I see each glob directory does indeed have 3457 elements but when I go to the symbols table in the Cromwell db the arrays only have 1000 elements. This causes tasks further down in the workflow to fail because I am expecting an array of 3457 elements but I am only getting 1000.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/699:445,down,down,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/699,1,['down'],['down']
Availability,"I tried to ask this on the broad forums but got an error.""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" . I have been using a . backend.default = SGE. Line in my config file. This has been working until now, with my jobs submitted using my defined SGE backend. Today I changed some other parts of the server, unrelated and numerous. Now SOME jobs are being submitted on the SGE backend, and some on the Local backend. (I did not configure a local backend, but I assume one is built in to cromwell. How is the backend to run on chosen by cromwell? (I know of backend.default, and I beleive there is a wdl.task.runtime.backend parameter also (undocumented from what I can tell)). How can I prevent any task ever running on the local backend? I want this to be a hard error, and not overload my sge login node. . Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3533:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533,2,['error'],['error']
Availability,"I tried to set up AWS Batch with Cromwell 36, and using https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/ to create the AMI, but when I use /cromwell_root as ScratchMountPoint instead of /scratch, the cloud formation failed to create AMI:. 11:43:03 UTC-0500 | CREATE_FAILED | AWS::EC2::Instance | EC2Instance | Received FAILURE signal with UniqueId i-0428e258c971e355e; -- | -- | -- | -- | --; Â  | Physical ID:i-0428e258c971e355e; Â  | Client Request Token:Console-CreateStack-2b86244b-f98c-4c65-9b87-7988dd39a4df. Please advise.; Thanks; Jing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4435:344,FAILURE,FAILURE,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4435,1,['FAILURE'],['FAILURE']
Availability,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4834:669,echo,echo,669,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834,1,['echo'],['echo']
Availability,"I validated `cnv_param_sweep.wdl` and received this error:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'CollectAllelicCountsNormal' in workflow (line 136):. Int model_segments_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(CollectAllelicCountsTumor.allelic_counts, ""GB"")) + ceil(size(CollectAllelicCountsNormal.allelic_counts, ""GB"")) + disk_pad; ```; `cnv_param_sweep.wdl` imports 3 WDLs, and the error was actually in `cnv_somatic_pair_workflow.wdl`. It would be great to know what WDL the error is in, especially if it's not the primary WDL.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3055:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I was just going to ignore this, but then @abaumann mentioned he was confused by the same thing... so why not. Used to be confusing because it could be referring to a value of type Boolean, rather than literally ""Boolean"":; ```; Expected equal, got ""Boolean"". Boolean conditional = incremented != 0; ^; ```; Used to be confusing because integers could suggest you're using the wrong number of identifiers:; ```; Expected identifier, got ""1"". echo ""~{one.1}"" > ~{one.1}.txt; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3725:442,echo,echo,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725,1,['echo'],['echo']
Availability,"I was running Cromwell with a JES backend. I got through about 300 out of 1000 workflows that were submitted as a batch; the rest are still listed as `Running`. Oddly, the genomics API operations report that they are still running though there are no instances still up. I am not sure if this is a Cromwell issue or a Google one, but here is what I am seeing from Cromwell. Cromwell has been working well for smaller batches. Any thoughts? . Thanks!. ```; [ERROR] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665:457,ERROR,ERROR,457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665,1,['ERROR'],['ERROR']
Availability,"I was running a bunch of workflows (same wdl, same method) when we had to restart and redeploy cromwell service and I got Errors with no visible message, when we dug into the cromwell metadata we discovered this message that Cris interpreted as a restart issue.; message"": ""Workflow input processing failed:\nWorkflow contains invalid labels JSON: Unexpected end-of-input at input index 0 (line 1, position 1), expected JSON Value:\n\n^\n""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2067:122,Error,Errors,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2067,1,['Error'],['Errors']
Availability,"I was running through the tutorial, https://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/. I typed the inputs.json file with a tab instead of spaces. ```curl -X POST http://localhost:8000/api/workflows/v1 -F workflowSource=@hello.wdl -F workflowInputs=@inputs.json```. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Input file is not valid yaml nor json: while scanning for the next token\nfound character '\\t(TAB)' that cannot start any token. (Do not use \\t(TAB) for indentation)\n in 'reader', line 2, column 1:\n \t\""test.hello.name\"": \""World\""\n ^\n""; }; ```. However per the JSON spec, tabs, or any whitespace, ""can be inserted between any pair of tokens"" https://www.json.org/. Python:; ```; json.loads(""{\n\t\""valid\"":\""json\""\n}""); ```. Sadly I do not know Java very well or I would just check and or fix whichever parser you are using. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3487:319,Error,Error,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487,1,['Error'],['Error']
Availability,"I was trying to label all GCP batch resources using `-o` in the CLI. (according to [this doc](https://github.com/broadinstitute/cromwell/blob/master/docs/wf_options/Google.md) and [this doc](https://github.com/broadinstitute/cromwell/blob/develop/docs/backends/GCPBatch.md) ); My content of options.json is ; ```; {""google_labels"":{""workflow-run-execution-id"":""97fed6c4-6442-4efe-9e73-7b3592a33480""}} ; ```; and my command is `java -Dconfig.file=config -jar /app/cromwell.jar run wf.wdl -i input.json -o options.json`. The error I am getting:; <img width=""1409"" alt=""Screen Shot 2024-01-04 at 9 58 25 AM"" src=""https://github.com/broadinstitute/cromwell/assets/1992953/e559bccf-dd96-4dea-a48a-6649e448a26f"">. After adding string prefix to my own uuid as label value, the error was gone and my workflow ran smoothly. However, I do need to pass in the uuid so downstream analysis pipeline can still work. . Related code reporting error is [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L65) and error is produce [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) and [this is the regex definition](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L21). I am wondering can the [code](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) be updated to only check the key of a label so it aligns with GCP which does not have this restriction on label values? Or use another regex `""[a-z0-9]([-a-z0-9]*[a-z0-9])?""` for label value check",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7351:523,error,error,523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7351,4,"['down', 'error']","['downstream', 'error']"
Availability,I was trying to restart a cromwell instance on server mode after modified config file. but the log was:; `2023-07-05 08:21:07 cromwell-system-akka.actor.default-dispatcher-25 ERROR - Bind failed for TCP channel on endpoint [/10.10.200.221:8000]`; Is there any way to stop an exist instance or restart it by API or something can automate by script.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7170:175,ERROR,ERROR,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7170,1,['ERROR'],['ERROR']
Availability,"I was trying to run the [PublicPairedSingleSampleWf](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.wdl) using Cromwell 0.19.3 and it was consistently failing with the following:. ```; 2016-10-06 21:32:38,239 cromwell-system-akka.actor.default-dispatcher-41 INFO - JES Run [UUID(65d59b8b):SamToFastqAndBwaMem:1]: Status change from Running to Success; 2016-10-06 21:32:38,586 cromwell-system-akka.actor.default-dispatcher-4 ERROR -WorkflowActor [UUID(65d59b8b)]: Completion work failed for call SamToFastqAndBwaMem:1.; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell.jar:0.19]; <snip>; ```. I had run the previous version of the PublicPairedSingleSampleWf successfully. I finally realized that it had nothing to do with the workflow, but rather because I changed the GCS path for Cromwell's `baseExecutionBucket` to something much longer. Instead of my previous:. ```; gs://my-bucket/gatk-test/; ```. I had changed to:. ```; gs://my-bucket/broad_pipelines/PublicPairedSingleSampleWf_160927/; ```. When I went back and shortened the path, everything worked again. Along the way, I ran a similar test where I changed the task name for my test [vcf_chr_count.wdl](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/workflows/vcf_chr_count/vcf_chr_count.wdl) from:. ```; task vcf_split; ```. to . ```; task vcf_split_what_if_i_have_a_really_long_task_is_that_the_problem; ```. This also generated the same exception:. ```; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1555:486,ERROR,ERROR,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555,1,['ERROR'],['ERROR']
Availability,"I was trying to use an invalid options file for cromwell 24 methods, through swagger, and the error message returned was: . ```; {; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request.""; }; ```; The cromwell logs had more information, I was using the wrong variable format for defaultRuntimeOptions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2041:94,error,error,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2041,2,['error'],['error']
Availability,"I was writing a wdl with this line in the task definition. `Float vaf = .01`. and was getting this error when validating . `Error: Invalid WDL: ERROR: Unexpected symbol (line 28, col 16) when parsing 'e'. Expected identifier, got 01. Float vaf = .01 ^ $e = :identifier <=> :dot :identifier -> MemberAccess( lhs=$0, rhs=$2 )`. if I change it to; ; `Float vaf = 0.01`. it's fine but I think looking at the spec the first iteration should work too - https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. `$float = (([0-9]+)?\.([0-9]+)|[0-9]+\.|[0-9]+)([eE][-+]?[0-9]+)?`. Easily worked around just a little annoying. This was run on a FireCloud method so I assume its one of the more recent versions? (cromwell 34) but ðŸ¤·â€â™‚ï¸",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4089:99,error,error,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4089,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685:257,error,error,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685,2,['error'],['error']
Availability,"I write a new backend. But the hostname verification needs to be canceled due to an internal environment problem. However, after the verification is canceled, the following information is displayed when submitting job:. ```; akka.stream.scaladsl.TcpIdleTimeoutException: TCP idle-timeout encountered on connection to xxxxxxx, no bytes passed in the last 1 minute; java.lang.OutOfMemoryError: GC overhead limit exceeded; ```. If the hostname verification is not canceled, the service is normal. The code is as follows:. ```; val badSslConfig: AkkaSSLConfig = AkkaSSLConfig().mapSettings(s =>; s.withLoose(; s.loose; .withDisableHostnameVerification(true); ); ). val badctx: HttpsConnectionContext = Http().createClientHttpsContext(badSslConfig). private def makeRequest[A](request: HttpRequest)(implicit um: Unmarshaller[ResponseEntity, A]): Future[A] = {. for {; response <- withRetry(() => {. val rsp = if (vkConfiguration.region == ""xxxxxxxxx""){; Await.result({Http().singleRequest(request, badctx)}, Duration.Inf); }else{; Await.result(Http().singleRequest(request), Duration.Inf); }; if (rsp.status.isFailure() && rsp.status.intValue() == 429) {; Future.failed(new RateLimitException(rsp.status.defaultMessage())); } else {; Future.successful(rsp); }; }); data <- if (response.status.isFailure()) {; response.entity.dataBytes.runFold(ByteString(""""))(_ ++ _).map(_.utf8String) flatMap { errorBody =>; Future.failed(new RuntimeException(s""Failed VK request: Code ${response.status.intValue()}, Body = $errorBody"")); }; } else {; Unmarshal(response.entity).to[A]; }; } yield data; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6149:1390,error,errorBody,1390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6149,2,['error'],['errorBody']
Availability,"I'm attempting to run a HaplotypeCaller job that requires that the BAM and BAI file are in the same directory. However, for some reason Cromwell is putting the inputs into separate subdirectories. For example:; ```bash; $ tree cromwell-executions/trio/5cb01e4f-98a2-41d8-8946-a84e4e09291f/call-germline_variant_calling/shard-0/germline_variant_calling/5f7e982d-8f5d-4db4-bb08-9e47532da496/call-haplotype_caller/inputs; cromwell-executions/trio/5cb01e4f-98a2-41d8-8946-a84e4e09291f/call-germline_variant_calling/shard-0/germline_variant_calling/5f7e982d-8f5d-4db4-bb08-9e47532da496/call-haplotype_caller/inputs; â”œâ”€â”€ 1155873852; â”‚Â Â  â””â”€â”€ recal.bam.bai; â”œâ”€â”€ -170302265; â”‚Â Â  â””â”€â”€ recal.bam; â””â”€â”€ 379983236; â”œâ”€â”€ cosmic_test.vcf.gz; â”œâ”€â”€ cosmic_test.vcf.gz.tbi; â”œâ”€â”€ exons.bed; â”œâ”€â”€ GenomeAnalysisTK.jar; â”œâ”€â”€ ucsc.hg19.dict; â”œâ”€â”€ ucsc.hg19.fasta; â”œâ”€â”€ ucsc.hg19.fasta.fai; â””â”€â”€ ucsc.hg19.fasta.gz; ```. Thus, I get the error: `##### ERROR MESSAGE: Invalid command line: Cannot process the provided BAM/CRAM file(s) because they were not indexed. `. The relevant parts of my WDL (simplified for this example) are:. ```wdl; task process_bam {; input {; File bam; File bai; File gatk; File reference; Array[File] reference_indices; Array[File] realigner_knowns; Array[File] realigner_known_indices; Array[File] bqsr_knowns; Array[File] bqsr_known_indices; File intervals; }. command {; OUTPUT_DIR=`pwd`; cd /app; /app/process_bam_docker.py \; --bam ""${bam}"" \; --bai ""${bai}"" \; --gatk ""${gatk}"" \; --ref ""${reference}"" \; ${sep="" "" prefix(""--realigner-known "", realigner_knowns)} \; ${sep="" "" prefix(""--bqsr-known "", bqsr_knowns)} \; --intervals ""${intervals}"" \; --indel-realigner \; --output-dir ""$OUTPUT_DIR""; }. runtime {; docker: ""988908462339.dkr.ecr.ap-southeast-2.amazonaws.com/dx_process_bam:latest""; }. output {; File dedup_bam = glob('*dedup.bam')[0]; File dedup_matrix = glob('*dedup.metrics')[0]; File dedup_recal_bam = glob('*recal.bam')[0]; File dedup_recal_bai = glob('*recal.bam.bai')[0]; File dedup_r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4361:904,error,error,904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4361,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I'm creating a new cromwell server, but running into a migration error with postgres.; ( PostgreSQL v9.2.24 on CentOS7); I can switch to mysql, but just wanted to report the error: . ```; 2019-07-21 23:07:06,634 INFO - Running with database db.url = jdbc:postgresql://localhost:5432/cromwell; 2019-07-21 23:07:13,702 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,734 INFO - CREATE TABLE public.databasechangeloglock (ID INTEGER NOT NULL, LOCKED BOOLEAN NOT NULL, LOCKGRANTED TIMESTAMP WITHOUT TIME ZONE, LOCKEDBY VARCHAR(255), CONSTRAINT DATABASECHANGELOGLOCK_PKEY PRIMARY KEY (ID)); 2019-07-21 23:07:13,751 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,755 INFO - DELETE FROM public.databasechangeloglock; 2019-07-21 23:07:13,762 INFO - INSERT INTO public.databasechangeloglock (ID, LOCKED) VALUES (1, FALSE); 2019-07-21 23:07:13,767 INFO - SELECT LOCKED FROM public.databasechangeloglock WHERE ID=1; 2019-07-21 23:07:13,778 INFO - Successfully acquired change log lock; 2019-07-21 23:07:17,932 INFO - Creating database history table with name: public.databasechangelog; 2019-07-21 23:07:17,934 INFO - CREATE TABLE public.databasechangelog (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED TIMESTAMP WITHOUT TIME ZONE NOT NULL, ORDEREXECUTED INTEGER NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35), DESCRIPTION VARCHAR(255), COMMENTS VARCHAR(255), TAG VARCHAR(255), LIQUIBASE VARCHAR(20), CONTEXTS VARCHAR(255), LABELS VARCHAR(255), DEPLOYMENT_ID VARCHAR(10)); 2019-07-21 23:07:17,983 INFO - SELECT COUNT(*) FROM public.databasechangelog; 2019-07-21 23:07:17,985 INFO - Reading from public.databasechangelog; 2019-07-21 23:07:17,986 INFO - SELECT * FROM public.databasechangelog ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-07-21 23:07:17,987 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:18,146 INFO - CREATE TABLE ""public"".""CALL_CACHIN",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:65,error,error,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,2,['error'],['error']
Availability,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4131:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131,3,['error'],['error']
Availability,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060,3,"['error', 'failure']","['error', 'failures']"
Availability,"I'm having some problems running the AWSBatch examples from the aws opendata page - https://docs.opendata.aws/genomics-workflows/aws-batch/configure-aws-batch-start/ - as far as I can tell I have correctly set the permissions, however when I try running the examples I get the error below. It looks like the output files aren't being written the S3 bucket. . I'm probably missing something obvious, but I can't work out what I'm doing wrong. Would anyone have any suggestions for where might be good to look for errors?. ```[2018-10-31 10:24:09,16] [[38;5;1merror[0m] WorkflowManagerActor Workflow b7e4cdce-ff14-4509-aec3-b226ed31043c failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341:277,error,error,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341,2,['error'],"['error', 'errors']"
Availability,"I'm having trouble getting call caching to work with Singularity and SGE, and I'm wondering if anyone has a working example config or some pointers. My config is below, minus passwords and specific paths/urls, which I've replaced with a label encased in <>. I've tried switching to slower hashing strategies finagling with the command construction to no avail. If there's not an obvious solution, is there an easy way to debug this? There are no network issues preventing connections to dockerhub - pulling images and converting to .sif works fine. It's only call caching that's broken. Even when I see, in the metadata, identical hashes for the docker image and all inputs and outputs, I see a ""Cache Miss"" as the result, every time. . The call caching stanza in my metadata looks like this, for example. Am I missing something? ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""4B2AB7B9EA875BF5290210F27BB9654D"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File output_greeting"": ""DFC652723D8EBD4BB25CAC21431BB6C0""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""2A2AB400D355AC301859E4ABB5432138"",; ""command template"": ""AFAC58B849BD67585A857F538B8E92F6""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ```. ```; # simple sge apptainer conf (modified from the slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:354,avail,avail,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['avail'],['avail']
Availability,"I'm noticing that in the outputs section of a task, that the basename is automatically applied. For example, running the following workflow:. ```wdl; version development. task basenametest {; input {; 	File inp; }. command <<<; echo '~{basename(inp)}'; >>>; ; output {; String outname = inp; String outbasename = basename(inp); String out = read_string(stdout()); }; }; ```. with:. ```bash; echo '{""inp"": ""inputs.json""}' >> inputs.json; java -jar cromwell-52.jar run basenametest.wdl --inputs ""inputs.json""; ```. I receive the output:. ```json; {; ""outputs"": {; ""escapetest.outname"": ""test.bam"",; ""escapetest.out"": ""test.bam"",; ""escapetest.outbasename"": ""test.bam""; },; ""id"": ""<id>""; }; ```. ---. I think this might be logical, but differs from the spec and MiniWDL. I've put a message in the OpenWDL slack for clarification, but through me off a bit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5785:228,echo,echo,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785,2,['echo'],['echo']
Availability,"I'm running a modified version of the [paired-fastq-to-unmapped-bam](https://github.com/gatk-workflows/seq-format-conversion/blob/master/paired-fastq-to-unmapped-bam.wdl) workflow on Cromwell 36 running on AWS. However, it seems to fail at `write_lines()`. Any idea why this might be happening? Is there flaw in the AWS implementation of this function? I can post my modified WDL if needed but all that's different from the official workflow that I have removed the `disks` blocks, since they fail on AWS (see #4274); ```; 2018-10-18 06:51:48,151 cromwell-system-akka.dispatchers.backend-dispatcher-3190 ERROR - Failed command instantiation; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:565); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:500); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.instantiatedCommand$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.instantiatedCommand(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:313); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:312); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.commandScriptContents(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:132); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275:604,ERROR,ERROR,604,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275,1,['ERROR'],['ERROR']
Availability,"I'm running a pipeline and for some of the tasks, sometime I get the following error:. `[E::hts_open_format] Failed to open file ...`. What I find weird is that if I re-run it, it runs successfully. It looks a ""stochastic"" error. Below you can find the full logs for that task and, as you can see, the file was successfully localized. ```; timestamp,message; 1608596940672,*** LOCALIZING INPUTS ***; 1608596942260,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi; 1608596944807,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-ex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:79,error,error,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,3,"['down', 'error']","['download', 'error']"
Availability,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5204:154,ERROR,ERROR,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I'm running gatk-sv workflows with AWS backend and I'm facing some issues on scatter tasks. It seems that for some of the tasks, the reconfigured-script is bad constructed. Below is an excerpt from the script:. ```bash; #!/bin/bash. {; echo '*** LOCALIZING INPUTS ***'; if [ ! -d /tmp/scratch ]; then mkdir /tmp/scratch && chmod 777 /tmp/scratch; fi; cd /tmp/scratch; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz.tbi; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6106:236,echo,echo,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106,1,['echo'],['echo']
Availability,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:288,avail,avail,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,2,"['avail', 'error']","['avail', 'errors']"
Availability,"I'm seeing tons of these info logging messages from the checkpointing system (shown below). Is there a way to turn them off or set them to a 'debug' rather than an 'info' logging level?. [2021-05-06 08:27:27,34] [info] Checkpoint start; [2021-05-06 08:27:27,34] [info] checkpointClose start; [2021-05-06 08:27:27,34] [info] checkpointClose synched; [2021-05-06 08:27:27,34] [info] checkpointClose script done; [2021-05-06 08:27:27,34] [info] dataFileCache commit start; [2021-05-06 08:27:27,34] [info] dataFileCache commit end; [2021-05-06 08:27:27,36] [info] checkpointClose end; [2021-05-06 08:27:27,36] [info] Checkpoint end - txts: 6340; [2021-05-06 08:27:27,36] [info] Checkpoint start; [2021-05-06 08:27:27,36] [info] checkpointClose start; [2021-05-06 08:27:27,36] [info] checkpointClose synched; [2021-05-06 08:27:27,37] [info] checkpointClose script done; [2021-05-06 08:27:27,37] [info] dataFileCache commit start; [2021-05-06 08:27:27,37] [info] dataFileCache commit end; [2021-05-06 08:27:27,39] [info] checkpointClose end; [2021-05-06 08:27:27,39] [info] Checkpoint end - txts: 6347; [2021-05-06 08:27:27,39] [info] Checkpoint start; [2021-05-06 08:27:27,39] [info] checkpointClose start; [2021-05-06 08:27:27,39] [info] checkpointClose synched; [2021-05-06 08:27:27,39] [info] checkpointClose script done; [2021-05-06 08:27:27,39] [info] dataFileCache commit start; [2021-05-06 08:27:27,39] [info] dataFileCache commit end; [2021-05-06 08:27:27,41] [info] checkpointClose end; [2021-05-06 08:27:27,41] [info] Checkpoint end - txts: 6349; [2021-05-06 08:27:27,41] [info] Checkpoint start; [2021-05-06 08:27:27,41] [info] checkpointClose start; [2021-05-06 08:27:27,41] [info] checkpointClose synched; [2021-05-06 08:27:27,41] [info] checkpointClose script done; [2021-05-06 08:27:27,41] [info] dataFileCache commit start; [2021-05-06 08:27:27,41] [info] dataFileCache commit end; [2021-05-06 08:27:27,42] [info] checkpointClose end; [2021-05-06 08:27:27,42] [info] Checkpoint end - txts: ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6337:56,checkpoint,checkpointing,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6337,1,['checkpoint'],['checkpointing']
Availability,"I'm trying to run Cromwell-48 on [Sherlock](https://www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:196,echo,echo,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,2,['echo'],"['echo', 'echoHello']"
Availability,"I'm trying to run a WDL that examines disk sizes using `df -kh`. Unfortunately, read_object is not able to parse the output which appears to match the spec:; The WDL:. ```; task df_kh {; Int bootDiskSizeGb; command { df -kh / }; runtime {; docker: ""ubuntu:latest""; bootDiskSizeGb: bootDiskSizeGb; }; output {; Int size = read_object(stdout()).Size[0]; }; }. workflow someBootDisks {; call df_kh as smallBootDisk { input: bootDiskSizeGb = 10 }; call df_kh as bigBootDisk { input: bootDiskSizeGb = 50 }; }; ```. The console output (example):. ```; Filesystem Size Used Avail Use% Mounted on; /dev/disk1 465G 70G 396G 15% /; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/615:567,Avail,Avail,567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/615,1,['Avail'],['Avail']
Availability,"I'm trying to use `quay.io/biocontainers/platypus-variant:0.8.1.1--htslib1.5_0`, but I'm getting the following error:. ```; [ERROR] [08/31/2017 20:01:21.193] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManage; rActor] WorkflowManagerActor Workflow 73494fe9-ef7b-45e0-b982-b50baf3281d7 failed (during ExecutingWorkflowState): Docker image quay.io/biocontainers/platypus-variant:0.8.1.1--htslib1.5_0 has an invalid syntax.; java.lang.IllegalArgumentException: Docker image quay.io/biocontainers/platypus-variant:0.8.1.1--htslib1.5_0 has an invalid syntax.; at cromwell.docker.DockerImageIdentifier$.fromString(DockerImageIdentifier.scala:60); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor.handleDockerValue$1(JobPreparationActor.scala:112); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor.cromwell$engine$workflow$lifecycle$execution$preparation$JobPreparatio; nActor$$fetchDockerHashesIfNecessary(JobPreparationActor.scala:129); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor$$anonfun$1.applyOrElse(JobPreparationActor.scala:57); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor$$anonfun$1.applyOrElse(JobPreparationActor.scala:54); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:663); at akka.actor.FSM.processEvent$(FSM.scala:660); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor.processEvent(JobPreparationActor.scala:33); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor.aroundReceive(Actor.scala:513); at akka.actor.Actor.aroundReceive$(Actor.scala:511); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor.aroundReceive(JobPreparationActor.scala:33); at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2589:111,error,error,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2589,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911:432,error,error,432,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911,4,['error'],['error']
Availability,"I'm working on mutation calling based on cromwell, the **Failed to summarize metadata** comes out for several shards in the scatter, then the following processes are aborted. How to fixed this error?. ```; [2018-11-17 09:04:45,38] [info] BackgroundConfigAsyncJobExecutionActor [3df56d2bPreProcessingForVariantDiscovery_GATK4.MarkDuplicates:5:1]: job id: 56011; [2018-11-17 09:04:45,48] [info] BackgroundConfigAsyncJobExecutionActor [3df56d2bPreProcessingForVariantDiscovery_GATK4.MarkDuplicates:5:1]: Status change from - to WaitingForReturnCodeFile; [2018-11-17 09:37:07,47] [error] Failed to summarize metadata; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3785ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:14,33] [error] Error summarizing metadata; java.sql.SQLTransientConnectionException: db - Connecti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403:193,error,error,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403,3,"['avail', 'error']","['available', 'error']"
Availability,"I'm writing a workflow that I want to run with and without using docker. The workflow has an optional docker input that is passed to each task for the docker entry in the runtime section. When I don't provide a value for docker, I get the error:; java.lang.IllegalArgumentException: Docker image has an invalid syntax. . when what I want is for the task to run without a docker. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5947:239,error,error,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5947,1,['error'],['error']
Availability,"I've attached two workflows that are essentially the same, one that conforms to draft-2 and the other that conforms to 1.0. The former runs fine (Cromwell v40), the latter one fails. It looks like there's a regression in handling the following situation:. ```wdl; task bar {; input {; Array[Array[String]]? baz; }; command <<<; echo ~{ if defined(baz) then write_tsv(baz) else 'no file' }; >>>; ...; }. Error:. ```java; Failed to process task definition 'bar' (reason 1 of 1): Failed to process expression 'if defined(baz) then write_tsv(baz) else """"' (reason 1 of 1): Invalid parameter 'IdentifierLookup(baz)'. Expected 'Array[Array[String]]' but got 'Array[Array[String]]?'; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981:328,echo,echo,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981,2,"['Error', 'echo']","['Error', 'echo']"
Availability,"I've been getting the following GATK error when running a CWL pipeline on Cromwell:; ```; ##### ERROR MESSAGE: Writing failed because there is no space left on the disk or hard drive. Please make some space or specify a different location for writing output files.; ``` ; To attempt to resolve this, I added the following requirement to the GATK tool in question:; ```yaml; - class: ResourceRequirement; coresMin: 2; ramMin: 8000; tmpdirMin: 100000; outdirMin: 100000; ```. However, this didn't resolve the GATK issue. I looked further into the Cromwell source and I found the following code: https://github.com/broadinstitute/cromwell/blob/44297611175c8a2a92ee71f0fa9c34419b69f0b8/cwl/src/main/scala/cwl/requirement/RequirementToAttributeMap.scala#L46-L57. To me, it looks like these values are being thrown out (I could be wrong). . Should we not convert these into a [`disk` statement](https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#disks) that Cromwell can understand and implement?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4507:37,error,error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4507,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I've been running the 3step workflow on JES. It gets to this point and stops. ```; [2016-07-13 12:52:24,77] [info] WorkflowActor-59abeeeb-eb34-49c6-83d0-c806e6968cb1 [59abeeeb]: transition from FinalizingWorkflowState to WorkflowSucceededState: shutting down; [2016-07-13 12:52:24,77] [info] WorkflowManagerActor Workflow 59abeeeb-eb34-49c6-83d0-c806e6968cb1 succeeded!; [2016-07-13 12:52:24,77] [info] WorkflowManagerActor WorkflowActor-59abeeeb-eb34-49c6-83d0-c806e6968cb1 is in a terminal state: WorkflowSucceededState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1157:254,down,down,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1157,1,['down'],['down']
Availability,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325:5,down,downloaded,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325,1,['down'],['downloaded']
Availability,"I've got a server up and running and I tried out my first WDL. I get the error below. I've pasted my WDL and JSON below the error: . ```; 2016-06-27 08:01:07,766 cromwell-system-akka.actor.default-dispatcher-9 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq -> /cil/shed/apps/internal/cromwell/cromwell-executions/smartseq_amr/21612af1-2c5f-400a-a53f-6ac66ec47674/call-RunSTARAlignment/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq ; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.localizeWdlValue(SgeBackend.scala:60) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:225) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:224) ~[cromwell-0.19.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1071:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071,4,"['ERROR', 'Failure', 'error']","['ERROR', 'Failures', 'error']"
Availability,"INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib] $$ib transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib/WorkflowInitializationActor-fd304efe-2bba-4859-9ffd-6ef7d4ae29d5] State is now terminal. Shutting down.; [WARN] [06/02/2016 19:49:46.381] [test-system-akka.actor.default-dispatcher-3] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$ib]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.495] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$kb] $$kb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$kb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb/WorkflowInitial",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:1822,down,down,1822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,1,['down'],['down']
Availability,"INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:1623,heartbeat,heartbeat,1623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,3,"['failure', 'heartbeat']","['failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,IO logging: print error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989,1,['error'],['error']
Availability,IOException kills workflow on first failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:36,failure,failure,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,1,['failure'],['failure']
Availability,Identify error codes other than 13 (if any) to retry,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1909:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1909,1,['error'],['error']
Availability,"If I run a WDL like the following in JES, the echo'ed path is relative and I would like it to be absolute. This will allow me to reference it from other locations than the current working directory more easily. ```; task echo {; File in_file; command {; echo ${in_file}; }; runtime {; docker: ""ubuntu:14.04""; }; }. workflow abs {; call echo ; }; ```. with this input json:. `{; ""abs.echo.in_file"": ""gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict""; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1332:46,echo,echo,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332,5,['echo'],['echo']
Availability,"If a CWL workflow has `DockerRequirement` specified as a hint, there's no way to run that workflow without Docker enabled currently. Provide a workflow option such that in these circumstances the workflow will be run *without* Docker. This workflow option should only be recognized by the SFS backends (local + HPC). @ruchim @cjllanwarne we might want to consider expanding this more generally to also include cases where `DockerRequirement` is a requirement and for WDL (where there is no hint), IOW a general docker on/off switch for people who know the consequences of their actions. If we did want to go down that path I'd make it a separate option from this one",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3542:608,down,down,608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3542,1,['down'],['down']
Availability,"If a WDL task generates a file with a space in its name, and that file is an output, Cromwell fumbles the outputs and throws an error (at least on GCP-Terra!Cromwell). Additionally, this doesn't seem to be logged clearly. This workflow takes in a bunch of BioSample accessions, downloads their associated run FASTQs, and processes them. https://dockstore.org/workflows/github.com/aofarrel/myco/myco_sra:4.1.2?tab=files. During one run, I accidentally passed in a file of BioSample accessions which had two spaces before each accession, eg; ```; SAMEA104027315; SAMEA104027345; SAMEA104027406; SAMEA104164787; SAMEA104172469; SAMEA104172474; SAMEA104172508; SAMEA104221066; SAMEA104362398; SAMEA104394395; SAMEA104394505; SAMEA104414628; SAMEA104446901; ```. The workflow is scattered per BioSample, so one instance of the scattered task takes in ` SAMEA104027315` as the input `biosample_accession` (type String). The task writes a file like this:. ```; echo ""~{biosample_accession}"" >> ~{biosample_accession}_pull_results.txt; ```; eg ` SAMEA104027315_pull_results.txt`. The workflow output section contains:. ```; String results = read_string(""~{biosample_accession}_pull_results.txt""); ```. eg ` SAMEA104027315_pull_results.txt`, same as what's in the command section. . In the task level logs, I see . ```; 2023/04/18 21:54:34 Starting delocalization.; 2023/04/18 21:54:35 Delocalization script execution started...; 2023/04/18 21:54:35 Delocalizing output /cromwell_root/memory_retry_rc -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/memory_retry_rc; 2023/04/18 21:54:37 Delocalizing output /cromwell_root/rc -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/rc; 2023/04/18 21:54:39 Delocalizing output /cromwell_root/stdout -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:128,error,error,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,3,"['down', 'echo', 'error']","['downloads', 'echo', 'error']"
Availability,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3226:181,failure,failure,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226,6,"['error', 'failure']","['error', 'failure']"
Availability,If at first you fail try try check-failure-retry-count times again,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3334:35,failure,failure-retry-count,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3334,1,['failure'],['failure-retry-count']
Availability,"If cromwell can't bind to it's tcp port, it should shut down, not continue running in zombie mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2645:56,down,down,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2645,1,['down'],['down']
Availability,"If my understanding is correct, this should run call c. ```; task sleeper {; Int seconds; Int output_seconds; Int rc; command {; sleep ${seconds}; echo ${output_seconds} > done. echo ""exit ${rc}"" >> script.sh; chmod +x script.sh; ./script.sh; }; runtime {docker: ""ubuntu:latest""}; output {; Int o = read_int(""done""); }; }. workflow w {; call sleeper as a {input: seconds=1, output_seconds=0, rc=1}; call sleeper as b {input: seconds=300, output_seconds=30, rc=0}; call sleeper as c {input: seconds=b.o, output_seconds=0, rc=0}; }; ```. Options. ```; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1168:147,echo,echo,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1168,2,['echo'],['echo']
Availability,"If some error occurs during release, it'd be nice to kick off the whole job again and have the process adjust accordingly. ## Should we release this project?; Would need to consult that these are correct and current:. * git tags; * release on github; * jar in JFrog; * brew Pr exists (for cromwell). This decision will determine version number for downstream projects.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2573:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2573,2,"['down', 'error']","['downstream', 'error']"
Availability,"If the service registry fails to get instantiated (wrong service classpath in the conf...), an exception is thrown but Cromwell still starts and an ActorRef is available in the `serviceRegistryActor` field, although the actor itself doesn't exist. All messages to the service registry are then lost.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/896:160,avail,available,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/896,1,['avail'],['available']
Availability,"If there is a failure in the `WorkflowMaterialiserActor` phase, there is no external way to debug the error. Allow the `WorkflowMaterialiserActor` to send failure messages to the metadata service in this instance, so that it's possible to debug errors!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/966:14,failure,failure,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/966,4,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"If you have an error in your json file, say a trailing comma on the last line, the error message is:. `Unexpected character '}' at input index 1681 (line 23, position 1), expected '\""':\n}\n^\n`. However it doesn't tell you that the problem is in the json and not the wdl. Changing line 23 in the wdl obviously won't help, so it would be nice to know which line 23 it's referring to.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933,2,['error'],['error']
Availability,"If you have spaces or braces or other symbols which should be escaped in input file names, bash will be sad about this:. > /bin/bash: line 6: syntax error near unexpected token `('. Example input:; {; ""elc.input"": ""/home/vagrant/example (1).file""; }. because in resulting bash file it will be transformed into:. > /home/vagrant/cromwell/cromwell-executions/elc/5646fba9-3bdc-4c63-aeba-16adf80ae7d2/call-tsk_ELC_/home/vagrant/example (1).file. I think Cromwell should just put paths between single quotation marks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1306:149,error,error,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1306,1,['error'],['error']
Availability,"If you use a java or scala client (e.g. CromIAM...) to connect to Cromwell over HTTPS, you'll get an error like:; ```; javax.net.ssl.SSLProtocolException: handshake alert: unrecognized_name; ```; cf. Google, this can be ignored client-side by adding:; ```; System.setProperty(""jsse.enableSNIExtension"", ""false""); ```; However, it would probably be nicer if Cromwell addressed this by adding an appropriate name when running in HTTPS mode. Since CromIAM is happy talking to Sam over HTTPS, they probably know how to do this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2513:101,error,error,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2513,1,['error'],['error']
Availability,"Im testing moving LSAPI runs to Batch with v86, and I keep getting the following error:. textPayload: ""docker: invalid spec: /mnt/disks/cromwell_root:/mnt/disks/cromwell_root:: empty section between colons."". for this command:; Executing runnable container:{image_uri:""gcr.io/google.com/cloudsdktool/cloud-sdk:434.0.0-alpine"" commands:""-c"" commands:""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Starting\\ container\\ setup."" entrypoint:""/bin/sh"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:""} timeout:{seconds:300} labels:{key:""logging"" value:""ContainerSetup""} for Task task/job-49cc8a88-722b-43067ba4-ab34-48bc00-group0-0/0/0 in TaskGroup group0 of Job job-49cc8a88-722b-43067ba4-ab34-48bc00. The docker volumes are defined as:; volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:""}. Shouldn't there be a rw permissions entry after the last colon? As far as I know, there is no way for users to modify the docker launch config to fix this. Is there something I have malformed or missing in my conf file?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7408:81,error,error,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7408,1,['error'],['error']
Availability,"Immutable Docker hash request keys to guard against credential mutations breaking lookups, more graceful handling of actual failure cases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5230:124,failure,failure,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5230,1,['failure'],['failure']
Availability,"Implement a JES PBE, this ticket covers the basics but should NOT include:; - retry support; - call caching support; - recovery support; - metadata support; - abort support. as these are covered in other tickets with the ""JES PBE""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/657:119,recover,recovery,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/657,1,['recover'],['recovery']
Availability,Implement recover in TES and improve abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4197:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4197,1,['recover'],['recover']
Availability,Implement recoverAsync for AWS backend [BA-4857],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216:10,recover,recoverAsync,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216,1,['recover'],['recoverAsync']
Availability,"Implement the [paging protocol](https://github.com/ga4gh/workflow-execution-service-schemas/pull/30) specified by Cromwell. If it makes sense to do so this could be a candidate to update Cromwell itself. To the extent that **that** could require a Cromwell API change, contact @ruchim if going down that path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3843:294,down,down,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3843,1,['down'],['down']
Availability,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3986:776,echo,echo,776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986,1,['echo'],['echo']
Availability,Improve AWS Integration:. - Docker Hub Authentication; - awsBatchRetryAttempts; - ulimits; - Call Caching with ECR private ([geertvandeweyer](https://github.com/geertvandeweyer) and [markjschreiber](https://github.com/markjschreiber/cromwell/)); - revised localization functions to improve stability ([geertvandeweyer](https://github.com/geertvandeweyer)); - Extra failure handling for Batch ([geertvandeweyer](https://github.com/geertvandeweyer)); - AWS/Batch error handling improvements ([geertvandeweyer](https://github.com/geertvandeweyer)); - Correct retry logic for spot kills ([geertvandeweyer](https://github.com/geertvandeweyer)); - handling of very rare early/late job killing ([geertvandeweyer](https://github.com/geertvandeweyer)); - Sychronize multipart uploads between callcache and jobscripts ([geertvandeweyer](https://github.com/geertvandeweyer)),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6835:365,failure,failure,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6835,2,"['error', 'failure']","['error', 'failure']"
Availability,Improve DRS Localizer error logging [WA-373],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5908:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5908,1,['error'],['error']
Availability,Improve Error messaging surrounding Cwl parsing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3022:8,Error,Error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3022,1,['Error'],['Error']
Availability,Improve checkpointing doc [BW-518],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6169:8,checkpoint,checkpointing,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6169,1,['checkpoint'],['checkpointing']
Availability,Improve checkpointing documentation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6165:8,checkpoint,checkpointing,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6165,1,['checkpoint'],['checkpointing']
Availability,"Improve error message for Type error, was ""invalid""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963,2,['error'],['error']
Availability,Improve error message for unrecognized filesystem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3268:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3268,1,['error'],['error']
Availability,Improve error output when parsing of CWL Fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2804:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2804,1,['error'],['error']
Availability,Improve parsing error messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3180:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3180,1,['error'],['error']
Availability,Improve parsing errors for CWL,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3174:16,error,errors,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3174,1,['error'],['errors']
Availability,"Improved ""error 10"" tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5101:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101,1,['error'],['error']
Availability,Improvements to reliability of S3 multipart copies and s3 localization into containers,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5839:16,reliab,reliability,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5839,1,['reliab'],['reliability']
Availability,"In 0.22 you could have something like this in config backend:; ```; runtime_attributes = """"""; String? pbs_email; """"""; ```; And then reference `pbs_email` inside an expression in the `submit` definition like so:; ```; submit = """"""; qsub ${""-me ea -M"" + pbs_email}; """"""; ```; so that if `pbs_email` isn't supplied, neither are the preceding option flags (as documented in `Optional Parameters and Type Constraints -> Prepending a String to an Optional Parameter` at https://software.broadinstitute.org/wdl/devzone.php). This doesn't work anymore in 23:; ```; [ERROR] [12/09/2016 11:09:29.763] [cromwell-system-akka.dispatchers.backend-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-71ccb54a-f88c-49b2-aeee-1af92749e337/WorkflowExecutionActor-71ccb54a-f88c-49b2-aeee-1af92749e337/71ccb54a-f88c-49b2-aeee-1af92749e337-EngineJobExecutionActor-testMe.worker:NA:1/71ccb54a-f88c-49b2-aeee-1af92749e337-BackendJobExecutionActor-71ccb54a:testMe.worker:-1:1/SharedFileSystemAsyncJobExecutionActor] SharedFileSystemAsyncJobExecutionActor [UUID(71ccb54a)testMe.worker:NA:1]: Error attempting to Execute the script; java.lang.UnsupportedOperationException: Could not evaluate expression: ""-m ea -M "" + pbs_email; 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:49); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:107); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:107); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:558,ERROR,ERROR,558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['ERROR'],['ERROR']
Availability,"In Cromwell 29, having a ':' character present in the filename of a non-glob task output results in a workflow failure. It does not matter whether this is hard-coded or simply in the filename. . `WorkflowManagerActor Workflow 1356670a-cea2-45b7-a9c4-e379dc601f9f failed (during ExecutingWorkflowState): Could not evaluate A.out = ""A:out""; java.lang.RuntimeException: Could not evaluate A.out = ""A:out""`. This is reproducible with the following wdl:; ```; workflow colon_filename{; 	call A {}; }. task A{; 	command{; 		echo ""testing"" > A:out; 	}; 	output{; 		File out = ""A:out""; 	}; }. ```; Task A completes normally with return code 0 and creates the file A:out. . I expected this workflow to complete normally without error. . Workaround; -; I have an acceptable workaround where I simply replace all ':' characters with '-'.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2919:111,failure,failure,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2919,3,"['echo', 'error', 'failure']","['echo', 'error', 'failure']"
Availability,"In DSDE methods, there are four backend configurations we would like to support.; - local w/ docker; - local w/o docker; - SGE (req: no docker); - JES (req: docker). But we would like to support these without having to change the WDL file itself. Currently, this is done with complex scripts that choose default options and application configurations. But perhaps there is an easier way... by parameterizing runtime attributes. I hear cromwell already supports that, except for the case where we do not want to run in docker at all (e.g. local w/o docker backends). Assuming that you can specify runtime parameters as part of a workflow. In other words, assuming that the following is valid:. ```wdl. workflow yo {; String msg; String docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```; The above WDL+json should work for JES and ""local w/ docker"" backends. However, to support local w/o docker, we need to be able to specify a ""null"" value, which cromwell will interpret as, ""do not use docker"" or ""docker was never specified"". For example:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": """"; }; ```; My understanding of cromwell is that this json + WDL above will cause a failure in cromwell. If this functionality already exists, please close this issue. This would make our WDLs more complicated, but it would increase flexibility and move runtime specification into the json file (which is easier than juggling default options).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1804:991,echo,echo,991,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804,2,"['echo', 'failure']","['echo', 'failure']"
Availability,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4017:848,error,error,848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017,1,['error'],['error']
Availability,"In JES jobs, retry Error 13 up to 2 times prior to failing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1888:19,Error,Error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1888,1,['Error'],['Error']
Availability,"In JES, the use case where a task exits without producing 1 or more expected output files will end up with the JES job in a failed status (because of non-existing output files failed to be delocalized), and the error message ""Failed to delocalize files"".; This looks like an error from JES where it's actually most likely an error of the task failing to produce some outputs.; If the job fails Cromwell never tries to read the RC file which makes it difficult for a user to know if it's a JES error or a task error. 1) Make sure that JES will delocalize the RC file first before trying to delocalize outputs - so it's available in the bucket.; 2) Have cromwell try to read the file even if the job failed and make sure it's present in the metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1848:211,error,error,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1848,6,"['avail', 'error']","['available', 'error']"
Availability,"In PR #2925, the `no_new_calls` test sometimes generates a cromwell log [message](https://github.com/broadinstitute/cromwell/blob/c6ed64617c51c572863b87d324fa8e68fa085b1a/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L118-L121):. > Cromwell server was restarted while this workflow was running. As part of the restart process, Cromwell attempted to reconnect to this job, however it was never started in the first place. This is a benign failure and not the cause of failure for this workflow, it can be safely ignored. This occurs when cromwell is restarted while `shouldSucceed` is still running. `shouldSucceed` finishes, and then a `Restarting calls: no_new_calls.delayedTask:NA:1` is generated, even though `boundToFail` has already failed and NoNewCalls should be started. The easiest way to reproduce this locally and see the delay is to increase the sleep in the wdl from 100 to something like 300 (five minutes). FYI if cromwell is not restarted, `delayedTask` does not start, does not fail, and does not have a metadata stanza for the call.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2926:488,failure,failure,488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2926,2,['failure'],['failure']
Availability,"In WDL 1.0, variables defined in nested if-statements are ""doubly optional"" (`Int??`) and break `select_first()`. Note that the degree of optionality maxes out at two, regardless of how deep the nesting is. This workflow is fine; ```; workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; but this one fails; ```; version 1.0. workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; with error; ```; Failed to process workflow definition 'Test' (reason 1 of 1):; Failed to process declaration 'Int c = select_first([a, b])' (reason 1 of 1):; Cannot coerce expression of type 'Int??' to 'Int'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3766:528,error,error,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3766,1,['error'],['error']
Availability,"In [CROM-6338](https://broadworkbench.atlassian.net/browse/CROM-6338) Denis reports that Cromwell is unexpectedly failing to retry 503s and provides the following sample error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ]; ```. In https://github.com/broadinstitute/cromwell/issues/6154 @freeseek reports that Cromwell is unexpectedly failing to retry 504s and provides the following sample error:; ```; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=me",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155:170,error,error,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155,6,"['Error', 'error', 'failure']","['Error', 'error', 'failures']"
Availability,"In `CwlCodecs.decodeCwl` , our entry point for parsing CWL, we have poor error messaging. Currently if `Cwl` types fail to parse, the only output is a single `CNil` as it fails to parse either a `Workflow` or a `CommandLineTool` (the inhabitants of the coproduct type `Cwl`). I think we should parse the two types manually (ie. `decode[Workflow]` and `decode[CommandLineTool]`) and think of a nice way to report that neither one was successful. It'd be a nice bonus to figure out which one ""got farther"" before it failed so that it may be more relevant to the caller.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2804:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2804,1,['error'],['error']
Availability,"In a Cromwell configured with multiple backends, be able to dynamically determine which backend to send a job based on where the input files live. . 1. If there are no files, use the default backend; 2. If all of the files are on the same filesystem, use the backend associated w/ that filesystem (see below); 3. If the files are not all on the same filesystem, error. This will require moving backend determination (at least for many tasks) out of materialization and closer to the runtime dispatch. Note that the 2nd stage really only works right now because there's (mostly) a 1-1 mapping of backend to filesystem. If you can find a way to do this slickly in a world where a backend might support multiple filesystems, so much the better. Note also that this relates to #1312 (ideally this is done after that, but shouldn't matter too much either way). The workflow option should always override dynamic dispatch determination.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2584:362,error,error,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2584,1,['error'],['error']
Availability,"In a conversation with @hjfbynara he referenced seeing an issue a few more times which had come up in the past. I remembered that @kshakir posted the following in slack and wanted to capture it for posterity here:. the firecloud thingy is â€œworked aroundâ€. cromwell 1 and 2 were restarted, and both started running jobs again. ruchi and I glanced at the supervision in the CromwellRootActor of the WorkflowStoreActor, and didnâ€™t see anything special that would catch 100% of errors*. Howeverâ€” it was only a hypothesis that the WorkflowStoreActor was dead, as doug and henry reported little to no cpu activity from cromwell if I recall correctly. * The WorkflowStoreActor appears to be supervised for initialization exceptions, but the rest are handled with a â€œdefaultâ€. It looks like default might be equal restart, but as of this second I donâ€™t know what exact state a new instance of a WSA actor/fsm restarts based on the akka spec. Also there was some error logging on db futures within the WSA, but itâ€™s possible an exception was throws before the `Future` got a chance to wrap the exception in a failure? All guesses at this point that could be way off target.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1874:474,error,errors,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1874,3,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"In an issue [BA-5929](https://broadworkbench.atlassian.net/browse/BA-5929) a problem with big numbers, exceeding Int capacity, was introduced. Therefore, I decided to add BigDecimal type support in WDL language.; Changes:; - BigDecimal type introduced in WDL; - Basic binary and unary operations for number types are implemented for BigDecimal. ---; I referred to this list during operations implementation:; https://software.broadinstitute.org/wdl/documentation/spec#types. Manually tested on:; ### inputs.json:; ```json; {; ""my_wf.largenumber"": 7000000000; }; ```. ### workflow; ```; workflow my_wf {; BigDecimal largenumber. call print_number {; input: largenumber=largenumber; }; }; task print_number {; BigDecimal largenumber; BigDecimal largeNumIncr = largenumber + 42; command {; echo SOME_LARGE_NUMBER ${largeNumIncr}; }; output {; String lnum=read_string(stdout()); }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5135:787,echo,echo,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5135,1,['echo'],['echo']
Availability,"In case you don't want to fire up a server on this branch just to see the rendered example in the swagger UI...:. ```json; {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ],; ""name"": ""my_wf"",; ""inputs"": [; {; ""name"": ""my_wf.string_input"",; ""valueType"": {; ""typeName"": ""String""; },; ""optional"": false; },; {; ""name"": ""my_wf.array_input"",; ""valueType"": {; ""typeName"": ""Array"",; ""arrayType"": {; ""typeName"": ""String""; }; },; ""optional"": false; },; {; ""name"": ""my_wf.optional_input"",; ""valueType"": {; ""typeName"": ""Optional"",; ""optionalType"": {; ""typeName"": ""String""; }; },; ""optional"": true,; ""default"": ""hello""; },; {; ""name"": ""my_wf.map_input"",; ""valueType"": {; ""typeName"": ""Map"",; ""mapType"": {; ""keyType"": {; ""typeName"": ""String""; },; ""valueType"": {; ""typeName"": ""Int""; }; }; },; ""optional"": false; },; {; ""name"": ""my_wf.object_input"",; ""valueType"": {; ""typeName"": ""Object"",; ""objectFieldTypes"": [; {; ""fieldName"": ""int_field"",; ""fieldType"": {; ""typeName"": ""Int""; }; },; {; ""fieldName"": ""int_array_field"",; ""fieldType"": {; ""typeName"": ""Array"",; ""arrayType"": {; ""typeName"": ""Int""; }; }; }; ]; },; ""optional"": false; },; {; ""name"": ""my_wf.int_string_pair_input"",; ""valueType"": {; ""typeName"": ""Tuple"",; ""tupleTypes"": [; {; ""typeName"": ""Int""; },; {; ""typeName"": ""String""; }; ]; },; ""optional"": false; }; ],; ""images"": [; [; ""quay.io/seqware/seqware_full/1.1"",; ""ubuntu:latest"",; ""EXPRESSION_BASED_IMAGE""; ]; ],; ""submitted_descriptor_type"": {; ""descriptorType"": ""WDL"",; ""descriptorTypeVersion"": ""1.0""; },; ""imported_descriptor_types"": [; {; ""descriptorType"": ""WDL"",; ""descriptorTypeVersion"": ""1.0""; }; ],; ""meta"": {; ""author"": ""Batman"",; ""copyright"": ""BSD 3-clause""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4432:143,error,errors,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4432,3,['error'],['errors']
Availability,"In cases where call caching is not being used (as is the case for Genomes On the Cloud for the time being), the ability to have workflows ""fail fast"" is desired. Fail fast in this case mean, once a task has failed no new tasks should be started (currently running tasks can finish) and then the workflow should be set to failed. This is important because when running a workflow, and you can't reuse the non-failed work, you want to know the workflow will fail as soon as possible. With a long running workflow, an early failure with few downstream dependencies you have to wait a long time (24 hours for GOTC) to realize it's actually failed. This should be specifyable as a workflow option, e.g. failure-strategy = fast where the current behavior is 'slow'. If it's easy to also have this as a server config setting for the default that would be a nice to have but not critical.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587:521,failure,failure,521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587,3,"['down', 'failure']","['downstream', 'failure', 'failure-strategy']"
Availability,"In certain cases, cromwell/wdltool is unable to resolve call statements in subworkflows (cromwell v29, wdltool v0.14). This may be related to #2753. A simple case to demonstrate this:. main.wdl:; ```; import ""sub.wdl"" as sub; workflow main {; call sub.wf; }; ```. sub.wdl:; ```; workflow wf {; call a; String b = a.out; }. task a {; command {; echo ""hello""; }; output {; String out = read_string(stdout()); }; }; ```; Running the validate command against sub.wdl returns no error messages. Running validate against main.wdl returns:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 4):. String b = a.out; ^; ```; Interestingly, if I add to sub.wdl:; ```; task b {; String msg; command {; echo ${msg}; }; output {; String out = read_string(stdout()); }; }; ```; and replace String b=a.out with:; ```; call b { input: msg=a.out }; ```; it validates ok. Am I doing something wrong with the WDL? Or is there a workaround for this?. My actual use case is a bit more complex - I have a series of mutually exclusive optional cases, so I want to put the File? output from each into an Array, and then use the select_first. But when I try to create the array from the outputs:; ```; if (a_condition) {; call task_a; }; if (b_condition) {; call task_b; }; ...; Array[File?] files = [task_a.out, task_b.out ...]; File file = select_first(files); ```; I see the above manifestation/error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756:344,echo,echo,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756,5,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"In https://github.com/broadinstitute/cromwell/issues/4772 we learned that Google reserves the right to omit keys from the operation metadata map instead of supplying the default values. In https://github.com/broadinstitute/cromwell/pull/4773 we added robustness for when a particular key, `preemptible`, is absent. Look at the `interpretOperationStatus` functions for PAPI v1 and v2, research which keys Google might chose to omit in the future, and make sure they don't break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4774:251,robust,robustness,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774,1,['robust'],['robustness']
Availability,"In order to allow the Private IP flag, it needs to be set both at pipeline creation time and run time. Run time resources override create time resources. However mount point must be set at create time but cannot be re-set at runtime... . ```; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""disks must not have mount points at run time"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""disks must not have mount points at run time"",; ""status"" : ""INVALID_ARGUMENT""; }; ```. This sets mount points to null for run time only. The rest is strictly identical to create time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507:262,error,errors,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507,1,['error'],['errors']
Availability,"In order to discern if a workflow is **really** running instead of having been picked up to be run but abandoned, running workflows should send a heartbeat ping to the workflow store. After some configurable time limit, if a workflow has not sent in a heartbeat, move it to a state (see below) such that it'll again be pick-upable. I think it'd be good to have a new state (e.g. `LostContact`) vs resetting it to `Submitted`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3341:146,heartbeat,heartbeat,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3341,3,"['heartbeat', 'ping']","['heartbeat', 'ping']"
Availability,"In order to improve our scalability, performance, and resilience, we would like to be able to demonstrate measurable improvements. This epic lists the performance tests we would like to run as part of this effort, as well as what to measure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4796:54,resilien,resilience,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4796,1,['resilien'],['resilience']
Availability,"In response to:; 1. the difficulty of debugging https://github.com/broadinstitute/cromwell/issues/4555 and https://github.com/broadinstitute/cromwell/issues/4512 due to excessive, unformatted output; 2. the lack of tests on the error message generation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4661:228,error,error,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661,1,['error'],['error']
Availability,"In the `AsyncBackendJobExecutionActor`, the `isTransient` method is defined as `!isFatal(throwable)` which means any non fatal error will trigger infinite retries which is not great.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2813:127,error,error,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2813,1,['error'],['error']
Availability,"In the cromwell README...; ```wdltool 0.4```. ```; import ""sub_wdl.wdl"" as sub. workflow main_workflow {. call sub.hello_and_goodbye { input: hello_and_goodbye_input = ""sub world"" }. # call myTask { input: hello_and_goodbye.hello_output }. output {; # I believe this will cause a validation failure. This is what is currently in the README...; String main_output = hello_and_goodbye.hello_output. # I believe this will NOT cause a validation failure. ; hello_and_goodbye.hello_output; }; }; ```. My real-world example:. ```; workflow dl_ob_training {. File bam_file; File bam_file_index; File gatk_jar; File ref_fasta; File oncotated_m1; String entity_id; File createOxoGIntervalList. call CollectSequencingArtifactMetrics {; input:; entity_id=entity_id,; bam_file=bam_file,; output_location_prepend=entity_id,; gatk_jar=gatk_jar,; ref_fasta=ref_fasta; }. call CreateObIntervalList {; input:; oncotated_m1=oncotated_m1,; entity_id=entity_id,; createOxoGIntervalList=createOxoGIntervalList; }. call ExtractReadInfo {; input:; bam_file=bam_file,; bam_file_index=bam_file_index,; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; pre_adapter_file=CollectSequencingArtifactMetrics.pre_adapter_detail_metrics,; interval_list=CreateObIntervalList.interval_list,; }. output {; # VALIDATES; ExtractReadInfo.read_infos. # DOES NOT VALIDATE; Array[File] read_infos = ExtractReadInfo.read_infos; }; }; .....snip.... task ExtractReadInfo {; File gatk_jar; File pre_adapter_file; File interval_list; File bam_file; File bam_file_index; File ref_fasta. command {; java -jar ${gatk_jar} ExtractReadInfo \; -P ${pre_adapter_file} \; -L ${interval_list} \; -I ${bam_file} \; -R ${ref_fasta} \; -OP out/; }. output {; Array[File] read_infos = glob(""out/*""); }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837:291,failure,failure,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837,2,['failure'],['failure']
Availability,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3784:91,echo,echo,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784,6,"['echo', 'error']","['echo', 'error', 'errors']"
Availability,"InMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1691,error,error,1691,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['error'],['error']
Availability,Inconsistent failure messages for workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['failure'],['failure']
Availability,Increase # of connections made available by CloudSQL-proxy in firecloud-develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4897:31,avail,available,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4897,1,['avail'],['available']
Availability,Increase the number of characters shown in case of error [NOJIRA],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6132:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6132,1,['error'],['error']
Availability,Indecipherable error message in cromwell (subworkflow WDL issue?),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3039:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3039,1,['error'],['error']
Availability,"Instead of failing fast on database `Exception`s (NOTE: not `Error`s), the actors/fsms should schedule a retry. Example from [`WriteMetadataActor`](https://github.com/broadinstitute/cromwell/blob/9a734c4f36ae122153736004e8d54fc44af8fbde/services/src/main/scala/cromwell/services/metadata/impl/WriteMetadataActor.scala#L45-L52):; ```scala; case Event(FlushBatchToDb, HasEvents(e)) =>; log.debug(""Flushing {} metadata events to the DB"", e.length); addMetadataEvents(e.toVector) onComplete {; case Success(_) => self ! DbWriteComplete; case Failure(regerts) =>; log.error(""Failed to properly flush metadata to database"", regerts); self ! DbWriteComplete; }; stay using NoEvents; ```. On an exception, the `e` events could be rescheduled.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2016:61,Error,Error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016,3,"['Error', 'Failure', 'error']","['Error', 'Failure', 'error']"
Availability,"Instead of the regular `${script}` variable from, this would close #5768. . Pinging @rhpvorderman and @TMiguelT as container / batch system users I know of.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5784:76,Ping,Pinging,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784,1,['Ping'],['Pinging']
Availability,"Intermittent Workflow Failure: ""Google credentials are invalid: connect timed out""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:22,Failure,Failure,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['Failure'],['Failure']
Availability,Intermittent failure in test StandardFileHashingActorSpec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2816:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2816,1,['failure'],['failure']
Availability,"Introduces a `RuntimeEnvironment` type in order to provide CWL expressions the proper values for the `runtime` ECMAscript variable. . See spec for explicit detail: http://www.commonwl.org/v1.0/CommandLineTool.html#Runtime_environment. Also introduce `MinimumRuntimeSettings` as the spec says ; > ""if an implementation can't provide the actual number of reserved cores during the expression evaluation time, it should report back the minimal requested amount."". I've made a few tradeoffs which I intend to document as tickets unless there are objections. To be clear the tradeoff is these compromises for speed, as I'm trying to ""spike"" on 1st-workflow and get it working :. * MinimumRuntimeSettings should come from the config. I see a major dependency tree coming all the way down from `RootCromwellActor` and I'm trying to think of a better way. In this PR I've taken the shortcut of instantiating MinimumRuntimeSettings with default values hardcoded.; * The values of `outdirSize` and `tmpdirSize` are specified in CWL but I haven't yet figured out how to provide those values accurately. I will likely create an issue to do this effectively as I doubt this is regularly used.; * I think we could constrain the types of `RuntimeEnvironment` better than the `String` and `Int` we are using currently, e.g. using `Path` and `MemorySize`. This requires moving these types up to the `wom` package.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2810:777,down,down,777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2810,1,['down'],['down']
Availability,Investigate failure to disconnect FTP client properly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4173:12,failure,failure,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4173,1,['failure'],['failure']
Availability,Investigate workflow failures in FireCloud,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1821:21,failure,failures,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1821,1,['failure'],['failures']
Availability,"Issues addressed:. - Custom-named stdout or stderr files were not being delocalized.; - The wrong stdout and stderr files were being delocalized (user action instead of user command).; - Optional output files that were not produced were being published to metadata as if they had been produced.; - Existence check for optional files was a `-a` test not actually supported by Alpine's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package.; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3668:419,error,error,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3668,1,['error'],['error']
Availability,"Issues addressed:. - ~~Custom-named stdout or stderr files were not being delocalized.~~ Handled in #3695 instead; - ~~The wrong stdout and stderr files were being delocalized (user action instead of user command).~~ Handled in #3695 instead; - ~~Optional output files that were not produced were being published to metadata as if they had been produced.~~ Revert fixes for this due to existence check performance issues; - Existence check for optional files was a `-a` test not actually supported by the delocalization Docker's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - ~~The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.~~ Reverted by reviewer request.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package. Now correctly moved thanks to reviewer input. ðŸ™‚ ; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations. With the existence checks removed from Cromwell proper this has reverted back to ""failing to fail"" which is mistaken for success.; - File literals now actually create a file, needed because the OutputManipulator does existence checks on outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3682:564,error,error,564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682,1,['error'],['error']
Availability,"It appears that the `evaluateFiles` method on `WomExpression` first tries to evaluate the expression, basically calling `evaluateValue`.; In the case of globs, this means trying to read the file containing the list of files that have been globbed, which is; 1) (Almost - see 2)) bound to fail since `evaluateFiles` is called before the task is run to determine the output files that _will_ be generated by an expression, therefore trying to evaluate the glob is pointless and generates unnecessary I/O.; 2) If for some reason the list file _is_ there but is invalid, the result of `evaluateFiles` will be invalid. Why 2) would happen is unknown at the moment, but some of our centaur integration test (dontglobinputs) present very odd failure mode consistent with 2)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4202:735,failure,failure,735,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4202,1,['failure'],['failure']
Availability,It appears that the better file implementation of `nameWithoutExtension` can check for the existence of the file under some circumstances. For GCS files this means an http request which seems unnecessary and and prone to failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3602:221,failure,failures,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3602,1,['failure'],['failures']
Availability,"It is very common to provide folders as inputs to different bioinformatic tools. For instance, STAR index is usually computed once per reference genome and then provided to each STAR-based RNA-Seq task as an input. However, when this is done a common caching failure is reported (because it is a folder):; ```; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Is a directory""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2735:259,failure,failure,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2735,1,['failure'],['failure']
Availability,"It looks like cromwell doesn't properly handle optional variables in tasks. If they are undefined this error occurs: ""Optional value was not set and no 'default' attribute was provided"". Setting a default (either in the input block or like so: `~{default="""" optionalValue}`) will cause the workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(Sha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:103,error,error,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,5,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>Thatâ€™s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>Thatâ€™s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:115,ERROR,ERROR,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'errors']"
Availability,"It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference, Cromwell errors out and refuses to proceed with a similar message in both cases. But it seems like a good move anyway. With `SafeConstructor`:. `java -jar /Users/anichols/Projects/cromwell/server/target/scala-2.12/cromwell-70-1a6c161-SNAP.jar run test3.cwl`; ```; could not determine a constructor for the tag tag:yaml.org,2002:javax.script.ScriptEngineManager; ```. With `Constructor`:. `java -jar cromwell-69.jar run test3.cwl`:; ```; could not determine a constructor for the tag '!!javax.script.ScriptEngineManager'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510:104,error,errors,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510,1,['error'],['errors']
Availability,"It seemed to go like this in FC:. - Cromwell received some workflows in a batch; - [...] some unknown amount of time passed; - `abort` was run on the remaining workflows; - Some workflows aborted fine (but always took at least 2 `abort` calls to abort); - Others returned 404s; - It's unknown whether these also took 2 `abort` calls to abort, and only the second returned 404. Because rawls is retrying on any error, these `abort` calls are now chewing up a lot of Cromwell resource unnecessarily",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497:410,error,error,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497,1,['error'],['error']
Availability,"It seems possible to use the --metadata-output option with the cromwell server command, but when I send the command it returns a syntax error:. ```; java -jar /home/shared/cromwell/cromwell-54.jar server --metadata-output meta.json; Error: Unknown option --metadata-output; Error: Unknown argument 'meta.json'; ```. Notably, it seems to work on ""run"":; ```; java -jar /home/shared/cromwell/cromwell-54.jar run main.wdl --metadata-output meta.json; ```. Ideally, I'd like to send a command like such:; ```; java -Dconfig.file=cromwell.conf -jar /home/shared/cromwell/cromwell-54.jar server --metadata-output meta.json &; ```. I'm basing my usage here on this documentation:; https://cromwell.readthedocs.io/en/stable/CommandLine/. Any help here would be much appreciated.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6076:136,error,error,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6076,3,"['Error', 'error']","['Error', 'error']"
Availability,"It seems that the protocol for setting runtime attributes is to do so within the task, thus allowing expressions based on their values. Say,. ```wdl; task foo {; Int cpus. runtime {; cpus = cpus; }. command {; ./my_binary --threads ${cpus * 2}; }; }; ```. However, a lot of the time, it's not appropriate (in a ""separation of concerns"" sense) to thread the value through the task invocation. For example, you may be setting a Unix group under which all data should be accessed, defining credentials, etc. We're doing this by using the `default_runtime_attributes`, passed in as workflow options. However, these are not visible to the task. This is what we'd like to be able to do, for example:. Workflow Options:; ```json; {; ""default_runtime_attributes"": {; ""AUTH_USER"": ""foo"",; ""AUTH_TOKEN"": ""bar""; }; }; ```. Workflow:; ```wdl; task {; command {; export AUTH_USER=""${AUTH_USER}"" # Taken from default_runtime_attributes; export AUTH_TOKEN=""${AUTH_TOKEN}"" # Taken from default_runtime_attributes; ./my_authenticated_command; }; }; ```. At the moment, this will fail, as `AUTH_USER` and `AUTH_TOKEN` are not defined within the task. Even if you explicitly define it in the task (`String AUTH_USER`, etc.), Cromwell won't automatically seed this from the default options. I can see why it would be useful to define the inputs in the task, for clarity's sake. I'm just thinking out aloud -- so this is very much half-baked -- but perhaps an option would therefore be to have an additional keyword that made it explicit that the task value was to be taken from options:. ```wdl; task {; Int something; runtime String AUTH_USER # ""runtime"" implies this is from the runtime attributes (default or otherwise); runtime String AUTH_TOKEN # Raise an error if undefined or modified within the task. command {; export AUTH_USER=""${AUTH_USER}""; export AUTH_TOKEN=""${AUTH_TOKEN}""; ./my_authenticated_command -n ${something}; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4741:1741,error,error,1741,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741,1,['error'],['error']
Availability,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065,1,['error'],['error']
Availability,"It seems to me that callCaching is not working when a task takes a Directory as input. Take the following example WDL:; ```; version development. workflow main {; call task1 { input: s = ""file"" }; call task2 { input: d = task1.d }; output { String s = task2.s }; }. task task1 {; input {; String s; }. command <<<; set -euo pipefail; mkdir dir; touch ""dir/~{s}""; >>>. output {; Directory d = ""dir""; }. runtime {; docker: ""debian:stable-slim""; }; }. task task2 {; input {; Directory d; }. command <<<; set -euo pipefail; ls ""~{d}""; >>>. output {; String s = read_string(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. On a first `141477ef-e8e6-4fb9-ae58-5c2e8a646088` run, callCaching for `task2` is negative, as it should, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Now though, the directory has been created as a result of the WDL succeeding:; ```; $ gsutil ls gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir; gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir/file; ```. On a second `2690f8a5-4cd4-45e2-a93a-55125a1107f8` run, callCaching for `task2` is negative again though, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509:751,error,error,751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509,1,['error'],['error']
Availability,"It turns out that ""Complete Success"" just wasn't quite right for Cromwell. EDIT: the actual motivation for this is that this was the cause for some of our `dead letter` errors appearing at the end of `cromwell run` commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2218:169,error,errors,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2218,1,['error'],['errors']
Availability,"It would be easier to consume errors if they were wrapped in JSON. I believe 400 errors are already represented this way already, but a 500 response timeout returns with Content-Type:`text/plain`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947:30,error,errors,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947,2,['error'],['errors']
Availability,"It would be great to be able to declare variables based on output from tasks. In this example WDL I want to use an array of all the shards from TaskA as input to TaskB even though TaskB is in the same scatter. ```; task TaskA {; File single_input_file. command {; #Something happens that takes a single file and returns a single file; }; output {; File output_single_file = ""some.file""; }; }. task TaskB {; Array[File] array_of_files; File single_input_file. command {; #Something happens that takes an array of files and a single file; }; }. workflow inputsFromScatter {; Array[File] list_of_files = [""a.file"", ""b.file"", ""c.file""]. scatter(file in list_of_files) {; call TaskA {; input:; single_input_file = file; }. call TaskB {; input:; single_input_file = file,; array_of_files = variable_declared_outside_of_scatter; }; }. Array[File] variable_declared_outside_of_scatter = TaskA.output_single_file; }; ```. This workflow currently results in this error: `Workflow input processing failed.; Workflow has invalid declarations: Could not find a value for TaskA`. I may have oversimplified this example because TaskB could be in a separate scatter. If you'd like a real example with actual tasks I'd be happy to provide it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1513:953,error,error,953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1513,1,['error'],['error']
Availability,"It would be great to have at least some unit tests targeted towards key request flows that CromIAM is expected to perform.; This should include correct order of the requests between SAM <-> CromIAM <-> Cromwell and validity of the responses.; Mocks of SAM and Cromwell could be useful to not depend on real implementation. A/C: This ticket is a good example of something that it would be nice for those unit tests to catch: https://github.com/broadinstitute/cromwell/issues/4284; Other example would be:; - A submit requests first goes to sam to check against the whitelist before going to Cromwell (if whitelisted), or returning an error (if not)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4286:633,error,error,633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4286,1,['error'],['error']
Availability,"It would be nice to have conda recipes for cromwell and its dependencies as part of the [conda-forge channel](https://conda-forge.github.io/). The package would be roughly analogous to the one available via Homebrew, except multi-platform.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1446:193,avail,available,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446,1,['avail'],['available']
Availability,It would be really handy to have a `cromwell.server` script that would make starting and stopping a server cromwell easy. ```; $ cromwell.server start; ... Started!. $ cromwell.server status; running. $ echo $?; 0. $ cromwell.server stop; ... Stopped!. $ cromwell.server status; stopped. $ echo $?; 1; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2623:203,echo,echo,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2623,2,['echo'],['echo']
Availability,"It's a bit of a pet peeve of mine when error messages don't put quotes around raw values. This leads to unnecessarily confusing scenarios like; ```; Your input did not meet our requirements.; ```; for an input of `""""`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4262:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4262,1,['error'],['error']
Availability,"It's possible for Cromwell to not write a `start` time against a call in metadata. ### Impact ; If this happens, Cromwell's metadata ends up with a call with no `start` time but with an `end` time. This causes problems amongst many clients, not least of which is Job Manager which returns a `500` error page and no further details. . ### Cause. #### Why is the start time not set?; This appears to be happening if the `input` section of a call has an expression which fails to evaluate:; Here's a potential example which might cause this:; ```wdl; Array[String] strings = [""0"", ""1""]; call foo { input:; x = strings[2]; }; ```. #### Why is Job Manager Unhappy?; From the logs, the error is being caused by: ; ```; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: File ""/home/vmagent/app/jobs/controllers/jobs_controller.py"", line 126, in get_job; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: sorted_tasks = sorted(tasks, key=lambda t: t.start); May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: TypeError: can't compare datetime.datetime to NoneType; ```. ### What to do about it. This ticket is to address this problem in two ways:; * Cromwell should not be writing calls to metadata with no `start` time (especially if they *do* have an end time. It's hard to prove this 100%, but at least for the above case we should be resilient.; * Job Manager should not return a 500 error code if a `start` time is missing. It should be resilient enough to render the tasks (even if the sorting is awkward). Bonus points for indicating bad metadata somehow so that the user can forward on the problematic metadata to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4978:297,error,error,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4978,5,"['error', 'resilien']","['error', 'resilient']"
Availability,"Iterate at whatever pace makes sense towards running a 10K JG w/ the mock WDL. If there issues (errors and/or things which seem unreasonably pokey) follow this decision tree:. - If the problem seems small, fix it and continue; - If the problem seems large but there's a clear hack/workaround, file a ticket and do the hack/workaround then continue; - If the problem seems large but there is no clear hack/workaround, bring this up with the team to figure out how to proceed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2145:96,error,errors,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2145,1,['error'],['errors']
Availability,JABJEA failure catcher. Closes #1435,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1588:7,failure,failure,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1588,1,['failure'],['failure']
Availability,"JDOM was removed in https://github.com/broadinstitute/cromwell/pull/6785 and this branch is updated from `develop`:. ```; root(aen_bw_1228)> | 81> whatDependsOn org.jdom jdom2; [ ... ]; [error] whatDependsOn org.jdom jdom2; [error] ^; ```. ( This is the normal output when `whatDependsOn` does not find something, see https://github.com/broadinstitute/cromwell/pull/6775 https://github.com/broadinstitute/cromwell/pull/6776 ). For Protobuf, the new MySQL pulls in a safe version â‰¥ 3.16.1:. ```; +-mysql:mysql-connector-java:8.0.29; | +-com.google.protobuf:protobuf-java:3.19.4; ```. which evicts older versions used by other dependencies. ```; +-io.opencensus:opencensus-proto:0.2.0; | +-com.google.protobuf:protobuf-java:3.19.4; | +-com.google.protobuf:protobuf-java:3.5.1 (evicted by: 3.19.4); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6793:187,error,error,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6793,2,['error'],['error']
Availability,"JDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIGdzdXRpbFwgXCBcIGNwXCAvY3JvbXdlbGxfcm9vdC9yY1wgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplL1wgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiAgY3AgL2Nyb213ZWxsX3Jvb3QvcmMgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplLwogIGVsc2UKICAgIGV4aXQgIiRSQ19HU1VUSUwiCiAgZmkKZWxzZQogIGV4aXQgMApmaQogICkKICBSQz0kPwogIGlmIFsgIiRSQyIgPSAiMCIgXTsgdGhlbgogICAgYnJlYWsKICBmaQogIGlmIFsgJGkgLWx0IDMgXTsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgV2FpdGluZ1wgNVwgc2Vjb25kc1wgYW5kXCByZXRyeWluZwogICAgc2xlZXAgNQogIGZpCmRvbmUKZXhpdCAiJFJDIg==\\\""));' > /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && chmod u+x /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && sh /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh\"": ""; }; ]; ```. AC:; Improve this error message by...; 1. Removing all of the message starting from ` Unexpected exit status 1 while running ....`; 2. Replace:; `Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file `; with something similar to:; `Execution failed: action 9: Failed to copy input <input_name> <input_path ie, gs://.....>. Please check the log file for more details: <link to call log>`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4603:8889,error,error,8889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603,1,['error'],['error']
Availability,JES Backend - Add retries to certain JES VM failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1910:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1910,1,['failure'],['failures']
Availability,JES Recover Closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1197:4,Recover,Recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197,1,['Recover'],['Recover']
Availability,JES WF hangs when Disk Full error encountered,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2054:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2054,1,['error'],['error']
Availability,JES backend giving permission error during localization that seems incorrect.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,1,['error'],['error']
Availability,JES error causes cromwell to hang and spew useless (for a user) error messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,2,['error'],['error']
Availability,JES errors are hard to debug and better reporting is helpful,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1637:4,error,errors,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637,1,['error'],['errors']
Availability,JES fails with mysterious error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['error'],['error']
Availability,JES gave an intermittent 404 error when trying to copy outputs from the cached call directory to the current call directory (discovered by Vivek when using Call Caching for GoTC) . AC: Have Cromwell retry Call Caching when upon receipt of the 404 error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/517:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/517,2,['error'],['error']
Availability,"JES has added an additional ""event"" to their metadata. For example:; events: ; - description: copied 1 file(s) to ""gs://some/path/sample.vcf"" ; startTime: '2016-08-15T19:39:49.261235611Z'. gcloud alpha genomics operations describe EJ7K1P3oKhjukpy15ueGiy0gw7vetLsXKg9wcm9kdWN0aW9uUXVldWU (for more details). Cromwell tries to record these execution events to the db, but sometimes the new events exceed the 255 char limit on the 'DESCRIPTION' column. . AC: Filter out these file copying events in 0.19_hotfix so that they don't clog up the execution events table, which can lead to downstream status update failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1299:581,down,downstream,581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1299,2,"['down', 'failure']","['downstream', 'failures']"
Availability,"JES job recovery attempted despite ""abort-jobs-on-terminate"" enabled",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050,1,['recover'],['recovery']
Availability,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1624:606,down,downstream,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624,1,['down'],['downstream']
Availability,"JIRA Issue for ref: [BA-6364](https://broadworkbench.atlassian.net/browse/BA-6364). Hi, I have a simple workflow that import subworkflow. After the subworkflow is run I would like to access an output from particular task from it. The output is not declared as an output of the subworkflow, and subworkflow is not maintained by us so I do not have a (easy) possibility to modify it. Here is the workflow:. ```; import ""subworkflow.wdl"" as sub; workflow hello {; call sub.hello; call show {; input: data = hello.say_hello.out; }; }; task show {; String data; command {; cat ${data}; }; }; ```. And subworkflow.; ```; workflow hello {; String name = ""John""; call say_hello {input: name = name}; }; task say_hello {; String name; command {; which python; python --version; echo ""Hello ${name}!""; }; output {; String out = stdout(); }; }; ```; Of course I have an error: Call â€˜helloâ€™ doesnâ€™t have an output â€˜say_helloâ€™ (line 6, col 29). But according to specs: `If the output {...} section is omitted, then the workflow includes all outputs from all calls in its final output.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5479:769,echo,echo,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5479,2,"['echo', 'error']","['echo', 'error']"
Availability,"JIRA Ticket: https://broadworkbench.atlassian.net/browse/CROM-6867. Hi. . I've been running the GATK-SV pipeline with AWS backend and, sometimes, due to some intermittent errors the tasks are aborted half-way trough. Then Cromwell re-launches the task but some files, generated by the previous run, are already there what makes the pipeline to fail. With this in mind, I'm preparing do a change in cromwell that remove all the files (except for the script which gets created for each run/job) from the task folder before it starts and I would like to ask:; 1. if this makes sense?; 2. if there is any problem on doing this. can the same folder be used twice? or does each task has its own â€œworkspaceâ€? or Will this change impact any other downstream jobs as we will remove everything except â€œscriptâ€ file?. Thanks in advance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6651:171,error,errors,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6651,2,"['down', 'error']","['downstream', 'errors']"
Availability,JIRA: [DSDEEPB-2455](https://broadinstitute.atlassian.net/browse/DSDEEPB-2455); wdl4s PR: https://github.com/broadinstitute/wdl4s/pull/7. This WDL file now runs as expected. ```; task golden_pie {; Float pi = 3.1415926; Float tau = pi + pi. command {; echo 1.6180339887; echo ${tau} 1>&2; }. output {; Float Au = read_float(stdout()); Float tauValue = read_float(stderr()); }; }. workflow wf {; call golden_pie; output {; golden_pie.pi; golden_pie.Au; }; }; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/395:252,echo,echo,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/395,2,['echo'],['echo']
Availability,Jackson-databind: vulnerable version 2.13.2 -> safe version 2.13.2.1+ (used 2.13.3); Nimbus JOSE+JWT: vulnerable version 9.9.3 => safe version 9.22 (used 9.23). ```; root(aen_bw_1227_2)> | 80> whatDependsOn com.fasterxml.jackson.core jackson-databind 2.13.2; [...]; [error] Expected '2.13.3'; [error] whatDependsOn com.fasterxml.jackson.core jackson-databind 2.13.2; [error] ^; ```; ```; root(aen_bw_1227_2)> | 80> whatDependsOn com.nimbusds nimbus-jose-jwt 9.9.3; [...]; [error] Expected '9.23'; [error] whatDependsOn com.nimbusds nimbus-jose-jwt 9.9.3; [error] ^; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6776:267,error,error,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6776,6,['error'],['error']
Availability,Jes flavored abort failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1405:19,failure,failure,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1405,1,['failure'],['failure']
Availability,JesApi run creation failures were being caught by the JesApi status handler and not getting handled correctly.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2251:20,failure,failures,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2251,1,['failure'],['failures']
Availability,"Jira reference: https://broadworkbench.atlassian.net/jira/software/c/projects/BA/issues/BA-6638; Cromwell 51. When using local method in look-up Docker hash together with Docker digest (instead of tag) causes an error because ""@"" symbol is substituted with colon. I have to use tags to be able to get the digest. In ""submit-docker"" section the image is correctly inserted i.e. with ""@"". ```; 2020-10-08 16:08:57,342 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Assigned new job execution tokens to the following groups: 45d03417: 1; 2020-10-08 16:08:57,443 INFO - Attempting to pull python:sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4; 2020-10-08 16:08:57,503 ERROR - Docker pull failed; java.lang.RuntimeException: Error running: docker pull python:sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4; Exit code: 1; invalid reference format. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:366); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:212,error,error,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Job Manager would like to distinguish between ""normal"" and ""exceptional"" failures, e.g. preemption versus something truly wrong. One implementation idea is to use the `causedBy` field but we have not vetted this at all. A/C distinguish between preemption and all other failures in the metadata response for an attempt - i.e. why did this particular attempt for a call fail?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4894:73,failure,failures,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4894,2,['failure'],['failures']
Availability,JobDescriptor merging checkpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/694:22,checkpoint,checkpoint,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/694,1,['checkpoint'],['checkpoint']
Availability,JobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:2223,robust,robustExecuteOrRecover,2223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['robust'],['robustExecuteOrRecover']
Availability,JobStore write failure error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219:15,failure,failure,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219,2,"['error', 'failure']","['error', 'failure']"
Availability,Jobs failed with Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:77,error,error,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,1,['error'],['error']
Availability,Jobs which fail for user error logged with error severity,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3962:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3962,2,['error'],['error']
Availability,"JoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:6441,Fault,FaultHandling,6441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['Fault'],['FaultHandling']
Availability,JoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:7941,recover,recover,7941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,"Just wanted to report a behavior I saw while trying to scale Cromwell horizontally. I haven't had time to take a look to it but is related to metadata and the way that is written in the database. How to reproduce:; 1. Deploy:; 1 VM with Nginx to act as the load balancer; 2 VMs with Cromwell 29; 1 VM with MySQL 5.6.36. 2. Submit Hello World workflow 10000 times. 3. Try to get status for some/all workflows while workflows are being processed. 4. These issues are manifested:. Duplicated entry exception (this one happens repeatedly) =>; ``` ; 2017-07-13 22:07:39,149 cromwell-system-akka.dispatchers.service-dispatcher-243 ERROR - Failed to summarize metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'cromwell-workflow-id-cromwell-6d021019-ac3f-4a28-b034-d58fb92022' for key 'UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:935); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2490); 	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013); 	at com.mysql.jdbc.PreparedSta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:625,ERROR,ERROR,625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,1,['ERROR'],['ERROR']
Availability,Keep the transient google communication errors in the poll queue Closes #1665,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1686:40,error,errors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1686,1,['error'],['errors']
Availability,"Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1574,error,error-keys,1574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,"['Error', 'error']","['Error', 'error-keys']"
Availability,Kills workflow if HPC Scheduler kills job due to out of memory error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107:63,error,error,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107,1,['error'],['error']
Availability,Known issues: . - A `WomMaybeListedDirectory` used as input to an IWDR expression does not have its `listing` attribute populated as expected.; - Even if this directory's `listing` attribute is force-populated the files are not copied to the IWDR.; - Even if the files are force-copied to the IWDR the test runs a `find` command that does not tolerate the presence of detritus.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3575:343,toler,tolerate,343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3575,1,['toler'],['tolerate']
Availability,Known test failures:; - [x] cromwell.SimpleWorkflowActorSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithBadMetadataSpec; - [x] cromwell.ArrayWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataSpec; - [x] cromwell.ArrayOfArrayCoercionSpec; - [x] cromwell.WdlFunctionsAtWorkflowLevelSpec; - [x] cromwell.MapWorkflowSpec; - [x] cromwell.WorkflowOutputsSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataOnFailureSpec; - [x] cromwell.WorkflowFailSlowSpec; - [x] cromwell.FilePassingWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorNormalSpec; - [x] cromwell.engine.WorkflowManagerActorSpec; - [x] cromwell.MultipleFilesWithSameNameWorkflowSpec; - [x] cromwell.CopyWorkflowOutputsSpec; - [x] cromwell.PostfixQuantifierWorkflowSpec; - [x] cromwell.ScatterWorkflowSpec. Ready for review!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1371:11,failure,failures,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1371,1,['failure'],['failures']
Availability,"L doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.erro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:1416,error,errorcode,1416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; 2018-02-12 13:50:46,018 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 4c983def-4f9c-4e62-ae38-4543da9922de failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_dict' not specified; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_dict' not specified; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:203); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3270:11,error,errors,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3270,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; java.lang.Exception: Task Mutect2.oncotate_m2:NA:1 failed. Job exit code 1. Check gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/oncotate_m2-stderr.log for more information. PAPI error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/HCC1143.maf.annotated -> gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/HCC1143.maf.annotated gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated, command failed: CommandException: No URLs matched: /mnt/local-disk/HCC1143.maf.annotated\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3271:11,error,errors,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3271,2,['error'],"['error', 'errors']"
Availability,"Latest ""develop"" branch, simple workflow with single parameter in the inputs (will post here if deemed important), and get this error once I run it the second, third, etc. times:. ```; 2018-06-11 16:10:16,080 cromwell-system-akka.dispatchers.api-dispatcher-630 INFO - Unspecified type (Unspecified version) workflow ab42cf3c-726f-4148-a30f-0f907c843361 submitted; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - 1 new workflows fetched; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Starting workflow UUID(ab42cf3c-726f-4148-a30f-0f907c843361); 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Successfully started WorkflowActor-ab42cf3c-726f-4148-a30f-0f907c843361; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-11 16:10:35,959 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Parsing workflow as WDL draft-2; 2018-06-11 16:10:35,970 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:128,error,error,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['error'],['error']
Availability,"Launch any workflow, let it run long enough to start a job in GCP Batch. Shut down Cromwell, restart Cromwell:. ```; 2024-08-19 14:47:51 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Cromwell 88-e59a6aa-SNAP service started on 0:0:0:0:0:0:0:0:8000...; 2024-08-19 14:47:51 cromwell-system-akka.dispatchers.engine-dispatcher-55 INFO - MaterializeWorkflowDescriptorActor [UUID(119e11a5)]: Call-to-Backend assignments: wf_hello.hello -> GCPBATCH; 2024-08-19 14:47:52 cromwell-system-akka.dispatchers.engine-dispatcher-54 INFO - WorkflowExecutionActor-119e11a5-b981-4510-a6d9-b5c26dfbb4e3 [UUID(119e11a5)]: Restarting wf_hello.hello; 2024-08-19 14:47:53 cromwell-system-akka.dispatchers.engine-dispatcher-53 INFO - Assigned new job restart checking tokens to the following groups: 119e11a5: 1; 2024-08-19 14:47:55 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Not triggering log of restart checking token queue status. Effective log interval = None; 2024-08-19 14:47:55 cromwell-system-akka.dispatchers.engine-dispatcher-41 INFO - Triggering log of execution token queue status. Effective log interval = 300 seconds; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Assigned new job execution tokens to the following groups: 119e11a5: 1; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - 119e11a5-b981-4510-a6d9-b5c26dfbb4e3-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(119e11a5)]: Could not copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:78,down,down,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['down'],['down']
Availability,Lets us trickle declarations and call outputs down into nested workflows. Eg this wdl:; ```wdl; workflow nested_lookups {; Int i = 27; if(true) {; if(true) {; if(true) {; call mirror as m1 { input: i = i}; }; }; }; }; ```. Gives us this womgraph:; ![test](https://user-images.githubusercontent.com/13006282/32580790-0eb26dc8-c4b5-11e7-8852-99d941823a2d.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2841:46,down,down,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2841,1,['down'],['down']
Availability,Like.$anonfun$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:10054,Error,ErrorHandling,10054,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,2,['Error'],['ErrorHandling']
Availability,"Local Cromwell & CromIAM, pointed at Sam dev,. `/describe` endpoint:. Disabled user `anichols@broadinstitute.org`; ```; 403 Forbidden; The supplied authentication is not authorized to access this resource; ```; Enabled user `oednichols@gmail.com`; ```; 200 OK; {; 	""valid"": true,; 	""errors"": [],; 	""validWorkflow"": true,; 	""name"": ""HelloWorld"",; 	""inputs"": [],; 	""outputs"": [],; 	""images"": [],; 	""submittedDescriptorType"": {; 		""descriptorType"": ""WDL"",; 		""descriptorTypeVersion"": ""1.0""; 	},; 	""importedDescriptorTypes"": [],; 	""meta"": {},; 	""parameterMeta"": {},; 	""isRunnableWorkflow"": true; }; ```. `/backends` endpoint:. Disabled user `anichols@broadinstitute.org`; ```; 403 Forbidden; The supplied authentication is not authorized to access this resource; ```; Enabled user `oednichols@gmail.com`; ```; 200 OK; {; 	""defaultBackend"": ""Local"",; 	""supportedBackends"": [; 		""Local"",; 		""LocalBourneShell"",; 		""LocalCacheableRuntimeAttribute"",; 		""LocalDockerSecure"",; 		""LocalNoDocker""; 	]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6826:283,error,errors,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6826,1,['error'],['errors']
Availability,Local PBE should return retryable on failures (which should always be false in this PBE),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/756:37,failure,failures,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/756,1,['failure'],['failures']
Availability,Local backend can use more resources than available on a machine,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1354:42,avail,available,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354,1,['avail'],['available']
Availability,Log access URLs with sensitive parts masked BT-235,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6333:37,mask,masked,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6333,1,['mask'],['masked']
Availability,"Log from broad-dsde-dev here: https://gist.githubusercontent.com/scottfrazer/a0838aa2180e7972da84c4730975b9f5/raw/d0cebdd317489298d6553e5602bfd4b775ab9ea3/gistfile1.txt. Most specifically:. ```; WorkflowActor [UUID(0790bc7e)]: Beginning transition from Running to Aborting.; WorkflowActor [UUID(0790bc7e)]: transitioning from Running to Aborting.; JES Run [UUID(0790bc7e):hello]: Status change from Running to Success; ERROR - 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Precondition check failed."",; ""reason"" : ""failedPrecondition""; } ],; ""message"" : ""Precondition check failed."",; ""status"" : ""FAILED_PRECONDITION""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Precondition check failed."",; ""reason"" : ""failedPrecondition""; } ],; ""message"" : ""Precondition check failed."",; ""status"" : ""FAILED_PRECONDITION""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/700:419,ERROR,ERROR,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/700,3,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,Log on Failures during retry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/256:7,Failure,Failures,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/256,1,['Failure'],['Failures']
Availability,"Logs captured from alpha environment:; ```; November 2nd 2018, 20:16:15.000 | 2018-11-03 00:16:15 [cromwell-system-akka.actor.default-dispatcher-57321] ERROR c.e.w.w.WorkflowStoreSubmitActor - Workflow com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction submit failed.; -- | --. Â  | November 2nd 2018, 20:16:15.000 | 2018-11-03 00:16:15 [cromwell-system-akka.actor.default-dispatcher-57321] ERROR c.e.w.w.WorkflowStoreSubmitActor - Workflow com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction submit failed. Â  | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42970] ERROR c.e.w.w.WorkflowStoreEngineActor - Error trying to fetch new workflows; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 1 milliseconds ago. The last packet sent successfully to the server was 1 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4807); 	at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:152,ERROR,ERROR,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,5,"['ERROR', 'Error', 'failure']","['ERROR', 'Error', 'failure']"
Availability,Look for failure mode option in the right place in the config. Closes #1380,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1381:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1381,1,['failure'],['failure']
Availability,"Looking at status message in the swagger GUI, I see that my workflow failed. However, the job it specifies is scattered and I do not know which shard is the issue.... If there is an easy way to find this, that I missed, then apologies. Example:. ```; ....; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1479:279,failure,failures,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479,1,['failure'],['failures']
Availability,Loosen criteria for retrying PAPI initialization errors [CROM-6808],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6572:49,error,errors,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6572,1,['error'],['errors']
Availability,"Lots of changes to `SingleWorkflowRunnerActor`:; - Clients communicate with SWRA via an ask, giving callers like `Main` a `Future` whose success status can be used to generate an exit code.; - SWRA no longer manages shutting down the actor system, that becomes the responsibility of the caller. This allows tests that want to see certain error messages to shut down the system only after the error messages are seen by an event filter. The previous structure allowed for a race condition where messages that filters wanted to see were produced, but the actor system was torn down before the messages were delivered to the filters.; - SWRA is now an FSM. Also increased the default patience in `CromwellTestkitSpec` for `InputLocalizationWorkflowSpec` and friends. Does _not_ include any changes to address MySQL connection issues sporadically seen in SlickDataAccessSpec; per discussion with Jeff I'll ticket that separately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/311:225,down,down,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/311,5,"['down', 'error']","['down', 'error']"
Availability,"M. Example pipeline:. ```; version 1.0. # WORKFLOW DEFINITION; workflow WholeGenomeGermlineSingleSample {; call SumFloats; output {; Float out = SumFloats.total_size; }; }. task SumFloats {; input {; Array[Float] sizes = [1,2,3,4,5.0]; Int preemptible_tries=3; }. command <<<; python -c ""print ~{sep=""+"" sizes}""; >>>; output {; Float total_size = read_float(stdout()); }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/python:2.7""; preemptible: preemptible_tries; }; }; ```. The error raised with cromwell-53 is:; Failed to read_float(""/data/og/ted/cromwell-executions/WholeGenomeGermlineSingleSample/00090ef9-5211-4f18-9de9-daf3de791408/call-SumFloats/execution/stdout"") (reason 1 of 1): For input string: ""15.0; 15.0""; The stdout file truly contains this. Running with local backend returns no error.; Contents of conf file:. ```; backend {; default = ""SLURM""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); concurrent-job-limit = 30; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 1; Int requested_memory_mb_per_core = 8000; Int memory_mb = 4000; String queue = ""short""; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""/bin/bash ${script}""; """"""; submit-docker = """"""; docker pull ${docker}. sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""docker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_cwd}/execution/script""; """""". kill = ""scancel ${job_id}""; check-alive = ""scontrol show job ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. Any thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5932:1990,alive,alive,1990,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932,1,['alive'],['alive']
Availability,"Made a new ticket for this since it's a new issue. basename() does not work on optional values, but sometimes it seems to think things that aren't optional are optional. This passes miniwdl and Cromwell, ie, is expected behavior:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell, even though it really ought to pass both:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. _Originally posted by @aofarrel in https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6910:435,echo,echo,435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6910,5,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"Made some changes to our Github Actions that should prevent certain slack messages from getting lost. Notes:; - Slack messages are sent to `#cromwell-integration-action`; - Slight change in behavior: We will get a message for any integration test that fails against `develop`, even if that failure didn't happen during a nightly run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7159:290,failure,failure,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7159,1,['failure'],['failure']
Availability,"Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:8848,error,error,8848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['error'],['error']
Availability,Make Cromwell more resilient to GCS IOException,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113:19,resilien,resilient,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113,1,['resilien'],['resilient']
Availability,Make Cromwell more resilient to GCS IOException [BA-5881/#5113 resubmission],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5297:19,resilien,resilient,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297,1,['resilien'],['resilient']
Availability,Make Services library available for publishing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1144:22,avail,available,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1144,1,['avail'],['available']
Availability,Make checkpoint example last long enough to be checkpointed [BW-495],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6164:5,checkpoint,checkpoint,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6164,2,['checkpoint'],"['checkpoint', 'checkpointed']"
Availability,Make inputs available to each other,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3249:12,avail,available,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3249,1,['avail'],['available']
Availability,Make interpretOperationStatus robust to nulls from PAPI,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4774:30,robust,robust,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774,1,['robust'],['robust']
Availability,Make parsing of runtime attributes far more robust via ValidationNELsâ€¦,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163:44,robust,robust,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163,1,['robust'],['robust']
Availability,Make services lib available for publishing. Closes #1144.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1143:18,avail,available,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1143,1,['avail'],['available']
Availability,Makes the IoActor available to the carbonite worker and readers by allowing the service registry to receive and store its reference.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5194:18,avail,available,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5194,1,['avail'],['available']
Availability,"Makes use of the helper traits created in the I/O actor PR to abstract managing of backpressure messages etc.. . Creates a `DockerClientHelper` trait to isolate the timeout management logic (if the docker actor never responds, ensures that we don't hang forever). The changes in `HttpFlowWithRetry` add exponential backoff retries (previously they were simple immediate retries) to HTTP responses, and list explicitly the HTTP codes that are retryable.; More specifically, Http ""failures"", as in ""the http request itself failed"", are not retried, since akka already does that by default under the hood. Only Http responses with a retryable error code are retried asynchronously, following the same model as the I/O actor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2077:479,failure,failures,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077,2,"['error', 'failure']","['error', 'failures']"
Availability,Making errors in tryStartingRunnableCalls() bubble up correctly and aâ€¦,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/243:7,error,errors,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/243,1,['error'],['errors']
Availability,"Many types of common errors could be checked faster, but instead fail when they get to a task. Move as much checking as possible up-front so that submission fails (or allow user to request a dummy run with a no-op backend wherein all these checks can be made). Things that aren't checked immediately: ; 1. input miss-specification; 2. docker image non-availability; 3. missing dependencies",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503:21,error,errors,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503,2,"['avail', 'error']","['availability', 'errors']"
Availability,MaterializeWorkflowDescriptorActor: coercion failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1067:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1067,1,['failure'],['failures']
Availability,"May fix weird test failures. I began getting test failures on my BackendConfiguration branch after I rebased on develop unrelated to anything I had changed. It looks like the workflow from the KV store test is visible to the test that runs after it, which led to more success messages than the assertions expected. Making this a separate PR in case the problem is waiting to bite anyone else.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/706:19,failure,failures,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/706,2,['failure'],['failures']
Availability,"MemoryRetryTest {; 	String message = ""Killed""; 	; 	call TestOutOfMemoryRetry {}; 	call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; 	command <<<; 		free -h; 		df -h; 		cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1245,error,error-keys,1245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,"['Error', 'error']","['Error', 'error-keys']"
Availability,"Message, not throw, failures in WEA.startRunnableScopes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2030:20,failure,failures,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2030,1,['failure'],['failures']
Availability,Metadata entries for early failures. Closes #966,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/976:27,failure,failures,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/976,1,['failure'],['failures']
Availability,"Might not eliminate the ""programmer errors"" just yet, but does improve logging on (unexpected) restarts and eliminates the weird and unintentional parenting of the RootWorkflowFileHashCacheActor by the WorkflowManagerActor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5358:36,error,errors,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5358,1,['error'],['errors']
Availability,Migrate Unflat Failure Metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039:15,Failure,Failure,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039,1,['Failure'],['Failure']
Availability,"Migrate existing failures in the metadata database from the former un-flat failure metadata style:; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. to the new flat style that new workflow metadata will be created with:; ```; ""failures"": [{; ""message"": ""connect timed out""; }, {; ""message"": ""Failed to upload authentication file""; }, {; ""message"": ""Error getting access token for service account: ""; }]; ```. Also note that aggregated exceptions were previously being input using the same random integer for every message part (i.e. every part of the aggregate was overwriting the previous one), so migration could... fix that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039:17,failure,failures,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039,6,"['Error', 'failure']","['Error', 'failure', 'failures']"
Availability,Minor change in order to make services library available for publishing. Formal reviewer: @kshakir.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1143:47,avail,available,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1143,1,['avail'],['available']
Availability,Missing *-version.conf files will no longer cause errors when running from IntelliJ.; Fixed projectName (ex: 'cromwell-engine') vs artifactName (ex: 'cromwell-engine.jar').; Also snuck in sbt 1.x syntax fix missed in a prior commit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2842:50,error,errors,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2842,1,['error'],['errors']
Availability,"More details about recreating the error here: https://gatkforums.broadinstitute.org/firecloud/discussion/10740/error-the-local-copy-message-must-have-path-set. Essentially, if a task looks like ; ```; task t {; 	File x = """"; 	; 	command {; ...; 	}; 	runtime {; ...; 	}; }; ```; The job fails with: ; ```; BackendJobDescriptorKey_CommandCallNode_w.t:-1:1/CCHashingJobActor-b12fef61-w.t:NA:1] Failed to hash ; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: Either exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: Google Cloud Storage. Failures: Google Cloud Storage: does not have a gcs scheme (IllegalArgumentException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:64); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:58); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:30); ```. AC: ; 1. In case that a user has set the value of a required File as an empty string --this error message should instead accommodate for this special case and point out that an empty string isn't valid input for a File object. ; 1b. If easy, it would also be nice to remove the link to the HPC config docs and the rest of the info about supported filesystems. ; [Optional] 2. If this doesn't hurt performance somehow, it would be nice if the error message also included the name of the input that failed to hash, not just the name of the call/value of the file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4158:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4158,6,"['Failure', 'error', 'failure']","['Failures', 'error', 'error-the-local-copy-message-must-have-path-set', 'failure']"
Availability,More informative error message from Cromwell when input files are missing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1691:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691,1,['error'],['error']
Availability,More informative error messages at `/metadata` endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4856:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856,1,['error'],['error']
Availability,Most of this code is just propagating the IOFunction ErrorOr to the edges. `GlobFunctions` used some existing logic to re-create globbing function.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2656:53,Error,ErrorOr,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2656,1,['Error'],['ErrorOr']
Availability,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1326:357,down,downside,357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326,1,['down'],['downside']
Availability,Moved supervision from JES down into Standard trait.; Fixed some vals in the Standard trait.; Fixed non-Travised integration test `JesAttributesSpec`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1781:27,down,down,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1781,1,['down'],['down']
Availability,"Much sad:. ```; 2016-05-24 14:11:46,501 cromwell-system-akka.actor.default-dispatcher-23 ERROR - Workflow d539eabc-3e74-4f06-85ca-0f52ac1f8a2b failed (during ExecutingWorkflowState): java.lang.Throwable: Failed post processing of outputs: ; Workflow d539eabc-3e74-4f06-85ca-0f52ac1f8a2b post processing failedglob function is not supported by this implementation; ```. ```; glob function is not supported by this implementation; ```. ```; 2016-05-24 14:11:46,501 cromwell-system-akka.actor.default-dispatcher-23 ERROR - Workflow d539eabc-3e74-4f06-85ca-0f52ac1f8a2b failed (during ExecutingWorkflowState): ; ```. ```; 2016-05-24 14:11:46,501 cromwell-system-akka.actor.default-dispatcher-23 INFO - WorkflowActor-d539eabc-3e74-4f06-85ca-0f52ac1f8a2b has gone terminal; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/882:89,ERROR,ERROR,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/882,2,['ERROR'],['ERROR']
Availability,Multiple tickets have a root cause of CromwellTestKitSpec and are leading to frequent enough failures in QA Triage to warrant some eyeballs. These tickets include (but are not limited to):. #4457 ; #4454 ; #4453 ; #4418 ; #4469; #4470 ; #4521. --. https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/2438,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4458:93,failure,failures,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4458,1,['failure'],['failures']
Availability,"My WDL pipeline failed to run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API with a long list of errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; I was under the expectation that this had been handled in issue #5344 and that Cromwell would retry to access the files until available (the files do indeed exist at the time of this writing).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:175,error,errors,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,5,"['avail', 'down', 'error']","['available', 'download', 'errors']"
Availability,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:424,error,error,424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,4,"['error', 'failure']","['error', 'failure']"
Availability,"My subworkflow wdl seems to ignore the task-call order. It skips M2 task entirely, which is a subworkflow in `hapmap_sensitivity_no_python.wdl`. I get this error with the latest cromwell master branch. cromwell jar: `/home/unix/tsato/cromwell-jars/cromwell-25-31ae549-SNAP.jar`. wdl: `/humgen/gsa-hpprojects/dev/tsato/mutect/strand-artifact/hapmap_sensitivity_no_python.wdl`; json: `/humgen/gsa-hpprojects/dev/tsato/mutect/strand-artifact/hapmap_sensitivity_10plex.json`; wdl-dependencies: `/humgen/gsa-hpprojects/dev/tsato/mutect/strand-artifact/mutect2.zip`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2117:156,error,error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2117,1,['error'],['error']
Availability,"NOTE: Not a request for immediate work. I'm not even sure that the suggestions are technically sound. This is something I've been mulling over for a while and wanted to get it down as a place holder and also in case anyone else is interested in experimenting. In particular the source and sink I'm not sure about. Currently when a workflow is submitted a record is dropped into the workflow store. At a regular interval the WMA will pull up to N submitted workflows and for each one will create a WorkflowActor which will both create its MaterializeWorkflowDescriptorActor and start running. Instead of a now and then batch system convert this to a streaming system controlled by backpressure. My thinking here is to have the source be the database table of submitted workflows (can a source be a perpetual query?) and then the stream could materialize the descriptor, create a workflow actor, the WA could register itself w/ the WMA and then start. I was picturing using Sink.actorRefWithAck on the WMA to provide back pressure (again, not sure it works like this). . In theory this would allow us to handle submitted workflows as rapidly as the WMA can handle w/o being overwhelmed. . Beyond the two key ""I don't know if it works like that"" points another is if there's another chokepoint which would better serve as the key backpressure signal and if so if it'd still make sense to wire this up as a stream - e.g. would the backpressure signaling point be too deep into the system to be practical?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1705:176,down,down,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1705,1,['down'],['down']
Availability,New known failure list,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3118:10,failure,failure,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3118,1,['failure'],['failure']
Availability,Nicer error message for not-enabled LanguageFactories,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3923:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3923,1,['error'],['error']
Availability,Nix FailureEvent,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/964:4,Failure,FailureEvent,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/964,1,['Failure'],['FailureEvent']
Availability,No error message when the input JSON has incorrect workflow names,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3120:3,error,error,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3120,1,['error'],['error']
Availability,"No issues found with the production code, but a recent CaaS scare led to one additional unit and Centaur test each. The Centaur test would definitely benefit from some ""flock programming"" (h/t @aednichols) for more robust metadata assertions. Right now the only assertion is workflow success, which seems pretty weak except that it's better than what CaaS reported. Ideally I'd like to assert on something like the following: . From ; `; curl --silent -X GET ""http://localhost:8000/api/workflows/v1/<<UUID>>/metadata?expandSubWorkflows=false"" -H ""accept: application/json"" | jq -M '[.calls[""error_10_preemptible.delete_self_if_preemptible""] | .[] | {attempt,preemptible,retryableFailure,executionStatus,backendStatus} | del(.[]| nulls)]'; `. ```; [; {; ""attempt"": 1,; ""preemptible"": true,; ""retryableFailure"": true,; ""executionStatus"": ""RetryableFailure"",; ""backendStatus"": ""Preempted""; },; {; ""attempt"": 2,; ""preemptible"": false,; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success""; }; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5101:215,robust,robust,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101,1,['robust'],['robust']
Availability,"Note how the wdl below attempts to use `output_dir` in its output for `reproducibility_files`. This causes a failure. If I remove the `String output_dir` from the task and update the output to a hardcoded path, `Array[File] reproducibility_files = glob(""reproducibility_output/*.*"")`, it works fine. . ``` wdl; task run_plot_reproducibility {; File file1; File file2; String output_dir; command {; run_plot_reproducibility ${file1} ${file2} ${output_dir} 3.7; }; output {; File reproducibility_table = ""${output_dir}/reproducibility.tsv""; File reproducibility_final_results = ""${output_dir}/final_results.tsv""; File reproducibility_plot = ""${output_dir}/reproducibility_Reproducibility.png""; #### HERE; Array[File] reproducibility_files = glob(""${output_dir}/*.*""); }; runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302:109,failure,failure,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302,1,['failure'],['failure']
Availability,"Note that in order to aggregate files after a scatter step, I must include the python code (see `run_plot_purity_series` task) in the next task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1263:249,error,error,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263,1,['error'],['error']
Availability,"Note to reviewers: this is currently packaged as 4 commits. The ""Upgrade to ScalaTest 3.2.1"" commit was my shot in the dark at fixing a runtime error that turned out not to fix the runtime error but did produce an upgrade to ScalaTest 3.2.1. It's a huge and tedious commit that's not really related to what's happening in the other commits. You'll probably want to ignore the ScalaTest 3.2.1 upgrade commit and focus on the other 3 instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5751:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5751,2,['error'],['error']
Availability,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5075:949,error,error,949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075,1,['error'],['error']
Availability,Note: this is a tactical fix to allow the cache-fetch post-processing to fail and the workflow to continue.; It does nothing to prevent the errors from happening in the first place - for that issue see #3979,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3978:140,error,errors,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3978,1,['error'],['errors']
Availability,"Noticed that cached-copy localization strategy is broken in Cromwell-51. This test works on a cluster where `execution-dir` is some place to do execution and copy the inputs to, and `/different/file/system/` is on a different disk. Small task (`catsmallfile.wdl`):. ```wdl; version development. task CatSmallFile {; input {; File inp; }; command {; cat ${inp}; }; output {; String out = read_string(stdout()); }; }; ```. Config (`cromwell.conf`):; ```hocon; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""<execution-dir>"",; ""filesystems.local.duplication-strategy"": [; ""cached-copy""; ]; }; }; }; }; ```. Command:. ```bash; echo ""Goodbye, call-caching"" >> /different/file/system/inp.txt; echo '{""inp"": ""/different/file/system/inp.txt""}' >> inputs.json. java -Dconfig.file=cromwell.conf -jar cromwell-50.jar run catsmallfile.wdl -i inputs.json; # <execution-dir>/cached-inputs/ is empty. java -Dconfig.file=cromwell.conf -jar cromwell-50.jar run catsmallfile.wdl -i inputs.json; # <execution-dir>/cached-inputs/ is populated; ```. ----. Anecdotally, I've noticed some of the permissions of localised files have changed, I wonder if this is related to that?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533:792,echo,echo,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533,2,['echo'],['echo']
Availability,"Now and then we see jobs get wedged in a state where the following exception kicks in infinitely (as far as we've seen - at least several hours). Restarting cromwell and reattaching those jobs will immediately resolve it - implying that the error wasn't ""real"" at least not any more and that something was bonkers on our side. Standard logging doesn't reveal anything interesting that I've seen. It's not something I've found a way to reproduce reliably - at least not consistently. ```2016-12-09 16:45:21,896 cromwell-system-akka.dispatchers.backend-dispatcher-28 WARN - JesAsyncBackendJobExecutionActor [UUID(50f9b932)JointGenotyping.SplitGlob:72:1]: could not download return code file, retrying:; com.google.cloud.storage.StorageException: Remote host closed connection during handshake; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:322); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:182); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:179); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:179); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:193); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:144); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:53); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:258); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:222); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:315",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782:241,error,error,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782,3,"['down', 'error', 'reliab']","['download', 'error', 'reliably']"
Availability,"Now that we've updated our WDLs to 1.0, we've found that `womtool graph` no longer works. It looks like it only supports draft2 and earlier WDL. `test.wdl`:; ```; version 1.0. workflow Test { }; ```. ```; $ java -jar womtool-35.jar graph /tmp/test.wdl ; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Finished parsing without consuming all tokens. version 1.0; ^; ; 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2330); 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2335); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:266); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:160); 	at scala.util.Try$.apply(Try.scala:209); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:160); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:156); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:571); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:94); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:48); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:18); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```; ; The `womgraph` command still works, but the output from that command is so verbose it's unusable for viewing our workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234:322,ERROR,ERROR,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234,1,['ERROR'],['ERROR']
Availability,"O OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = ""user-via-refresh""; # scheme = ""refresh_token""; # client-id = ""secret_id""; # client-secret = ""secret_secret""; #},; (etc); ```. Do both of these work? If so, is one format preferred to minimize confusion?. What prompted me to write this issue is that setting some defaults isn't yet worki",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4913:1745,failure,failure-mode,1745,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913,1,['failure'],['failure-mode']
Availability,"OM)\n# https://askubuntu.com/a/823798\ntail /dev/zero"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""machineType"": ""custom-1-2048"",; ""googleProject"": ""encode-dcc-1016"",; ""monitoringScript"": ""gs://caper-data/scripts/resource_monitor/resource_monitor.sh"",; ""executionBucket"": ""gs://encode-pipeline-test-runs/caper_out_10"",; ""zone"": ""us-central1-b"",; ""instanceName"": ""google-pipelines-worker-ead27fbad8aa73b157bfc126cd63331f""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""[0,137]"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""1"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2 GB"",; ""memory"": ""2 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""runtime attribute"": {; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327"",; ""docker"": ""A84529F7A095541F1249576699F24AA1"",; ""continueOnReturnCode"": ""614DAABB2D7AAB5D41921614A49E4F92""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""50F66ECBC45488EE5826941BFBC50411"",; ""command template"": ""F41FEBA57D556A16A5F6C4EEF68ED1E0""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; },; ""inputs"": {},; ""backendLabels"": {; ""wdl-task-name"": ""fail-oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""labels"": {; ""wdl-task-name"": ""fail_oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""failures"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""jobId"": ""projects/99884963860/locations/us-central1/operations/1374639517116411519"",; ""monitoringLo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:4956,failure,failures,4956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['failure'],['failures']
Availability,Objectives. confirm:. - deadlocks are not observed; - work is distributed correctly; - abandoned workflows are recovered; - abort workflow is functional,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4241:111,recover,recovered,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4241,1,['recover'],['recovered']
Availability,"Observed in a run from Firecloud: an input File was specified as a string ""gs://....."" where a stray space was carelessly inserted into the name. Three days later (don't ask) when the workflow got to its end, a JES error was thrown on failure to localize the file. I believe we should check for valid file names at the outset of a workflow, and that would include looking for an illegal space character.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2823:215,error,error,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2823,2,"['error', 'failure']","['error', 'failure']"
Availability,Occasionally run `check-alive` in config backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2315:24,alive,alive,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2315,1,['alive'],['alive']
Availability,"Ok, so validation fails because I had a typo in my input variable name, fine. But TELL ME WHICH VARIABLE IT WAS!!!. The rest of this description is an example:. ```; workflow foo {; 	call bar; 	# Oops, bar didn't have an output called b:; 	call baz as bad_output_name { input: b = bar.b }; 	# Oops, baz doesn't have an input called a:; 	call baz as bad_input_name { input: a = bar.a }; }. task bar {; 	command {; 		# noop; 	}; 	output {; 		String a = ""a""; 	}; 	runtime {; 		docker: ""ubuntu:latest""; 	}; }. task baz {; 	String b; 	command {; 		# noop; 	}; 	runtime {; 		docker: ""ubuntu:latest""; 	}; }; ```. The messages I actually get:; ```; Unable to load namespace from workflow: ERROR: Expression references input on call that doesn't exist (line 4, col 47):. 	call baz as bad_output_name { input: b = bar.b }; ^; Unable to load namespace from workflow: ERROR: Call references an input on task 'baz' that doesn't exist (line 6, col 38). 	call baz as bad_input_name { input: a = bar.a }; ^; ```. The message I want:; ```; Unable to load namespace from workflow: ERROR: Cannot use 'bar.b' as an input. That variable was never created. (line 4, col 47):. 	call baz as bad_output_name { input: b = bar.b }; ^; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38). 	call baz as bad_input_name { input: a = bar.a }; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211:681,ERROR,ERROR,681,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211,4,['ERROR'],['ERROR']
Availability,"Okay. So initially, I was passing the `ServiceRegistryActor` reference via props down the chain of actors, and each of the actors needed to follow a pattern of steps to insert into the metadata (e.g. create a `MetadataEvent` with a `MetadataPutAction` within which the `Metadata[Key, Value]` resided, send this message over to the the service registry, and handle failures, if any. It did not look good by any stretch IMO. All the metadata generating entities needed to be aware of the `MetadataService` (the erstwhile dataAcess). **Edit:** Using the above design in-fact now. ~~Currently, what I've done below is create a single instance of the `ServiceRegistryActor` in the `WorkflowManagerActor`and then create a `WorkflowProfilerActor` (one per workflow) which is supposed to handle all the metadata information coming from the engine side for a particular workflow. The way it happens is based on the presumption that almost all the information that we needed was present in the `StateName` and `StateData` of our FSMs. Unfortunately, with Akka's `SubscribeTransitionalCallback` we can only monitor the FSM states, and not the data. So I've created a trait (which the Engine's FSMs can extend from) which provide the semantics of wrapping up the state and data of the FSM in a message, and publish it into Akka's event stream. The ProfilerActor is the listener of these events and handles them appropriately. With this, I was able to make the FSMs unaware of the MetadataServices, and simply publish it's state and data in the event stream while performing any transitions.~~. ~~Let me know if you guys have any (other?) ideas / suggestions.~~. Contents added to metadata with this PR:; - [x] workflowName; - [ ] calls (To come from the backends); - [x] outputs ; - [x] id; - [x] inputs; - [x] submission; - [x] status; - [x] end; - [x] start",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829:81,down,down,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829,2,"['down', 'failure']","['down', 'failures']"
Availability,"Okay. So initially, I was passing the `ServiceRegistryActor` reference via props down the chain of actors, and each of the actors needed to follow a pattern of steps to insert into the metadata (e.g. create a `MetadataEvent` with a `MetadataPutAction` within which the `Metadata[Key, Value]` resided, send this message over to the the service registry, and handle failures, if any. It did not look good by any stretch IMO. All the metadata generating entities needed to be aware of the `MetadataService` (the erstwhile dataAcess). Currently, what I've done below is create a single instance of the `ServiceRegistryActor` in the `WorkflowManagerActor`and then create a `WorkflowProfilerActor` (one per workflow) which is supposed to handle all the metadata information coming from the engine side for a particular workflow. The way it happens is based on the presumption that almost all the information that we needed was present in the `StateName` and `StateData` of our FSMs. Unfortunately, with Akka's `SubscribeTransitionalCallback` we can only monitor the FSM states, and not the data. So I've created a trait (which the Engine's FSMs can extend from) which provide the semantics of wrapping up the state and data of the FSM in a message, and publish it into Akka's event stream. The ProfilerActor is the listener of these events and handles them appropriately. With this, I was able to make the FSMs unaware of the MetadataServices, and simply publish it's state and data in the event stream while performing any transitions. . Let me know if you guys have any (other?) ideas / suggestions. Edit: Names of new actors might be pretty bad IMO. Please suggest better ones if you have any. _P.S. : Currently based out of the Chris's branch since his metadata changes were needed._; _P.P.S. : Still a WIP for improving some stuff._",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/822:81,down,down,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822,2,"['down', 'failure']","['down', 'failures']"
Availability,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3873:173,error,error,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873,3,"['Error', 'error']","['Error', 'error']"
Availability,"On Cromwells which are not our own production instance, people seem to be more frustrated than delighted by having the initial read limits be arbitrarily restrictive. - [x] Will probably need to update `firecloud-develop` to set some values which are still relying on the previous low defaults back down to 128k",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5034:299,down,down,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5034,1,['down'],['down']
Availability,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3776:673,error,error,673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776,1,['error'],['error']
Availability,"On behalf of @leepc12, opening this as a new issue (previously a comment in #2992). Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```wdl; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007:585,echo,echo,585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007,2,"['echo', 'error']","['echo', 'error']"
Availability,"On cromwell version 30.1, when making a request to the query endpoint that includes an `additionalQueryResultFields` parameter, the database will throw an error if the query matches ~1000 results. This seems to be because cromwell is querying the db to get that field's value for each workflow in the result set. Example GET request:; ""https://cromwell.mint-dev.broadinstitute.org/api/workflows/v1/query?additionalQueryResultFields=parentWorkflowId"". Error message: ; ```{; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@349b84c3 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@37b87f04[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 3928972]""; }; ```. This does not occur when paginating the results (as long as the page size is not 1000+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115:155,error,error,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115,2,"['Error', 'error']","['Error', 'error']"
Availability,"Once #1909 and #1925 are complete, populate the set of retryable JES non-preemption errors with the codes from #1909.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1926:84,error,errors,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1926,1,['error'],['errors']
Availability,"Once we decide on a style guide, go through the code and resolve all the issues / errors cropping up because of the check style.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/537:82,error,errors,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/537,1,['error'],['errors']
Availability,"One downside of `grep` approach - result of `grep` is collapsed by default, so one would need to expand it to see actual errors. I think this is tolerable.; ![image](https://user-images.githubusercontent.com/4853242/85418371-b5263000-b53e-11ea-95f3-7bff39b08cff.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5553:4,down,downside,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5553,3,"['down', 'error', 'toler']","['downside', 'errors', 'tolerable']"
Availability,"One of Morgan's input files was missing an md5 in its object metadata. Cromwell was dutifully falling back to our backup option, which is to read every byte of the file into memory and calculate the hash itself. This resulted in extraordinary network and CPU usage that destabilized the instance and caused a continual crash/reboot cycle. We think this is also what Lori ran into with the featured workspaces. Now, we detect & avoid this condition, print a warning, and carry on without call caching:; ```; 41183c60:ImputationBeagle.SubsetVcfToRegion:3:1:; Hash error ([Attempted 1 time(s)] - Exception:; File of type BlobPath requires hash in object metadata, not present for; https://lz8b0d07a4d28c13150a1a12.blob.core.windows.net/sc-94fd136b-4231-4e80-ab0c-76d8a2811066/hg38/inputs/palantir_merged_input_samples.liftedover.vcf.gz),; disabling call caching for this job.; ```. Obviously, we'd like to enhance this in the future so that call caching is still possible for these jobs, but we have to walk before we can run. ---. Visualization eye candy section!. Swiftly downloading a file on the datacenter multi-gigabit LAN:. ![Screenshot 2024-05-02 at 19 24 04](https://github.com/broadinstitute/cromwell/assets/1087943/46484bbd-30e0-4f88-8f6c-05b50649c557). Telltale CPU curve as we chew through one file after another:. ![Screenshot 2024-05-03 at 11 32 13](https://github.com/broadinstitute/cromwell/assets/1087943/7916ce63-8d4c-46f7-a86a-b3313edf0d77). Flame graph showing the smoking gun, `generateMd5FileHashForPath`:. ![Screenshot 2024-05-02 at 14 02 25](https://github.com/broadinstitute/cromwell/assets/1087943/0d06f3ad-8155-4b43-bef7-6d9ccce35132)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7419:325,reboot,reboot,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7419,3,"['down', 'error', 'reboot']","['downloading', 'error', 'reboot']"
Availability,"One representative pair of timings on a stone cold prod clone (restarted between queries) for a single submission ID label with two ORed collection labels:. develop: 31.20 sec; this branch: 0.08 sec. No index changes required. MySQL will use a k/v index on `CUSTOM_LABEL_ENTRY` if it's available but its presence doesn't seem to actually make much difference in execution time. Exclude labels have been folded into the new system, I still need to measure the performance impact of those changes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4610:286,avail,available,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4610,1,['avail'],['available']
Availability,Option to skip (but don't invalidate) call cache hits that fail for permission errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1587:79,error,errors,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587,1,['error'],['errors']
Availability,"Or maybe provide a flag to switch the default?. I get this error when running the hello world example from docs on nixos:. ```; cromwell.core.CromwellFatalException: java.io.IOException: Cannot run program ""/bin/bash"": error=2, No such file or directory; ```; Is it just to change this line?. https://github.com/broadinstitute/cromwell/blob/1b1a56372659b9cb7a168bb1fa2a2296103e1256/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/BackgroundAsyncJobExecutionActor.scala#L22. I could possibly just make my own version building from source then .. It is due to the way nixos is built. A similar case is referenced here :. https://github.com/RcppCore/RcppArmadillo/issues/15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3201:59,error,error,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3201,2,['error'],['error']
Availability,"Order of events:. Start with #3344 and #3342 as those are the requirements for having multiple writer-Cromwell nodes share a database. Next, start #4239 to re-create a deadlocking issue, and address it with a solution:; #4249; #4240 . Generate test cases to make sure Cromwell is able to recover appropriately in case of shutdown:; #4242 ; And test cases to ensure that Cromwell is running/aborting workflows as expected across multiple nodes:; #4241",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4369:288,recover,recover,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4369,1,['recover'],['recover']
Availability,"Order of events:. Start with #3344 and #3342 as those are the requirements for having multiple writer-Cromwell nodes to safely share a database. Next, start #4239 de-serialize workflow heartbeats, and build a test case #4414 to re-create the deadlocking issue we've seen before in production. Follow up that work with a solution that addresses the deadlock. Below are two ideas brainstormed in the past:; #4249; #4240. Generate test cases to make sure Cromwell is able to recover appropriately in case of shutdown:; #4242. And test cases to ensure that Cromwell is running/aborting workflows as expected across multiple nodes:; #4241 . GDoc of plan as of March '19; https://docs.google.com/document/d/10AGE3foZsKOHlgUpq3BE4mkUYphcjYyxMt0miQz4FGk/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4370:185,heartbeat,heartbeats,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4370,2,"['heartbeat', 'recover']","['heartbeats', 'recover']"
Availability,Order of heartbeats database updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240:9,heartbeat,heartbeats,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240,1,['heartbeat'],['heartbeats']
Availability,Original report here: https://gatkforums.broadinstitute.org/firecloud/discussion/10273/error-messages-should-include-the-problematic-input-whenever-possible-disk-strings. AC: ; The failure message returned today looks like `Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE'`. but it should be ; `Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE'. Found 'foo'`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4161:87,error,error-messages-should-include-the-problematic-input-whenever-possible-disk-strings,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4161,2,"['error', 'failure']","['error-messages-should-include-the-problematic-input-whenever-possible-disk-strings', 'failure']"
Availability,"Original report here: https://gatkforums.broadinstitute.org/firecloud/discussion/10319/incorrect-error-message-when-task-fails-to-complete. Today, with PAPI v1 & v2, when a command fails and doesn't produce all expected output files, the failure reported is ""File `x` failed to delocalize..` while the real underlying cause is something about the command that failed and led to a failing return code. AC: ; While this isn't possible with PAPI v1, there may be scope in PAPI v2 to switch the order of checks that occur when a job hits a terminal status to determine success/failure:; 1. first check error code of a command then try to delocalize files. Reason for error should be ""Invalid Error Code"" first, possibly ""Missing File"" second.; 2. show the error code in the failure message, along with links to the stdout/stderr as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4160:97,error,error-message-when-task-fails-to-complete,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4160,8,"['Error', 'error', 'failure']","['Error', 'error', 'error-message-when-task-fails-to-complete', 'failure']"
Availability,"Original report:; https://gatkforums.broadinstitute.org/firecloud/discussion/10434/weird-error-message-encountered-www-googleapis-com. ![screen shot 2018-09-27 at 2 10 26 am](https://user-images.githubusercontent.com/14941133/46126308-bbb53580-c1fa-11e8-9001-2a662b9374dd.png). Here is an example of a workflow that fails with not being able to access www.googleapis.com. Its not implicitly obvious to a user what the consequences of this are and whether it makes sense to re-run their workflow. . In the case above -- it seems like the job that failed with this error had an rc of 0 and the expected outputs were present -- so likely the output evaluation failed, or some other file operation never completed as this is a StorageException. AC: Supplement the existing top-level message with something like ```Please consult this https://status.cloud.google.com/ to ensure that there are no reported outages with the Google Cloud Platform. Otherwise, this is likely a transient error and the workflow should be re-run.```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4159:89,error,error-message-encountered-www-googleapis-com,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4159,4,"['error', 'outage']","['error', 'error-message-encountered-www-googleapis-com', 'outages']"
Availability,"Originally posted this two in the JIRA issue tracker back in August. Reposting here since it didn't get a response over there: https://broadworkbench.atlassian.net/browse/BA-6548. > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl script which runs several subjobs in parallel. I believe the correct parlance is a scatter. I noticed that in some of the jobs of the scatter, some reference files failed to download from S3 even though they existed (Connection Reset by Peer). This failure caused the overall job to fail after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but recently, in June 2020, it appears the AWS Batch backend was majorly overhauled (by @markjschreiber, thanks! Also, tagging you because I suspect you might be the resident expert here :) ), and the previous fix (using the ecs proxy image) was supposedly obsoleted.; > ; > I also see that the s3fs library appears to be vendored into cromwell, and after digging around, it appears that one might be able to set retries via an environment variable(?). But even then, I feel like if that were to work, it would be much nicer if it was configurable through cromwell's config file somehow.; >; > So that brings me to my final question. Is there some configuration that allows me to retry failed downloads some number of times before failing the whole job? Or, perhap there is some alternative configuraiton which I've overlooked and someone could point me to it? Thanks!. In addition, just wondering if perhaps there is a service limit I might be running into?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946:449,down,download,449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946,3,"['down', 'failure']","['download', 'downloads', 'failure']"
Availability,"Originally reported here:; https://gatkforums.broadinstitute.org/gatk/discussion/comment/54964#Comment_54964. Example workflow as the acceptance criteria:; ```; version 1.0; workflow example{; call print{; }; }. task print{; input{; String s = ""\""""; }; command{; echo ""${s}""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4500:263,echo,echo,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4500,1,['echo'],['echo']
Availability,Otherwise we get errors when a subworkflow presents no outputs: ; - [x] `sub_workflow_var_refs`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2937:17,error,errors,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2937,1,['error'],['errors']
Availability,Out of memory error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['error'],['error']
Availability,Outage April 11-12 Post Mortem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3511:0,Outage,Outage,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3511,1,['Outage'],['Outage']
Availability,"P/blob/bioproject_stuff/workflows/is_this_tuberculosis.wdl. ### Ruled out; * Running tasks concurrently/Cromwell config not being respected: The workflow would have either hung Docker or tasks would have returned 137; * Docker application (not the container, the entire application) hanging, like what happens when trying to run tasks concurrently on a local machine: `docker run -it` works in a new terminal window; * IP getting blocked: This would cause error output, and I can still ping SRA from the same IP without issue; * Loss of internet: This would cause error output (`curl command failed`); * Control-S: Control-Q doesn't unfreeze it; * No more disk space: There's about 50 GB free and each instance of the scattered task uses less than a GB. ### Docker container logs; `docker logs cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528` gives no output. ### Entering the container; `docker exec -it cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 /bin/sh` returns; `Error response from daemon: Container cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 is not running`. ### Docker inspect; ```; >docker inspect cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528; [; {; ""Id"": ""cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528"",; ""Created"": ""2022-11-09T05:37:16.872195116Z"",; ""Path"": ""/bin/bash"",; ""Args"": [; ""/bark-bark/IS_THIS_TUBERCULOSIS/7570b5dc-4714-48a7-96b2-9c62245e3618/call-get_organism_names/shard-885/execution/script""; ],; ""State"": {; ""Status"": ""created"",; ""Running"": false,; ""Paused"": false,; ""Restarting"": false,; ""OOMKilled"": false,; ""Dead"": false,; ""Pid"": 0,; ""ExitCode"": 0,; ""Error"": """",; ""StartedAt"": ""0001-01-01T00:00:00Z"",; ""FinishedAt"": ""0001-01-01T00:00:00Z""; },; ""Image"": ""sha256:1c9072d6415cff6481014f64cf7486482dc61620bf09806bafffb1697bf344b1"",; ""ResolvConfPath"": """",; ""HostnamePath"": """",; ""HostsPath"": """",; ""LogPath"": ""/var/lib/docker/containers/cf6f4828adc61eacf06337ce3caf2c110df6cc049375",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:1801,Error,Error,1801,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,1,['Error'],['Error']
Availability,"P2 - to optimize the case of restarting a single cromwell, upon shutdown the heartbeat related information (server, timestamp) should be deleted so the workflow is immediately picked up on restart rather than after the timeout period",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4242:77,heartbeat,heartbeat,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242,1,['heartbeat'],['heartbeat']
Availability,PAPI better error message when localization/delocalization fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4718:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4718,1,['error'],['error']
Availability,PAPI error code 14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306:5,error,error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306,1,['error'],['error']
Availability,"PAPI error code 2. Execution failed: pulling image: docker login: generic::unknown: retry budget exhausted (10 attempts): running docker login: exit status 1 (standard error: ""WARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers). Seen [here](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/170)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438:5,error,error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438,2,['error'],['error']
Availability,PAPI v2 Error logging again,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4445:8,Error,Error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4445,1,['Error'],['Error']
Availability,PAPI v2 private docker failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4553:23,failure,failures,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553,1,['failure'],['failures']
Availability,PAPI v2 vague error message for localization failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4603:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603,2,"['error', 'failure']","['error', 'failure']"
Availability,PAPI2 error reporting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3722:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722,1,['error'],['error']
Availability,"PR 1 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Instead of credentials requiring WorkflowOptions, any String => String will do, including Map[String, String].; Retrieving credentials only requires actorSystem/executionContext when retrying.; Moved logback dependencies from common library over to testing.; Added mockito to all artifact tests.; Fixed akka-stream-testkit dependency appearing in core's main instead of test.; Split confusingly named baseDependencies into configDependencies ++ catsDependencies.; Other dependency cleanup to reduce duplicates and extra transitive dependencies.; Log stderr from centaur'ed cromwell failures.; The total attempt time to connect to cromwell for a test is now longer than the timeout of a cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2938:728,failure,failures,728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2938,1,['failure'],['failures']
Availability,"Pair accessing was never added properly to the docs.; When a pair is accessed incorrectly, it'd be nice to have an error along the lines of `use pair.left and pair.right to access Pair members`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2300:115,error,error,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2300,1,['error'],['error']
Availability,Papi V2: Transient(?) error '[Errno 111] Connection refused.',MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['error'],['error']
Availability,Papi V2: Transient(?) error 'repository ... not found',MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['error'],['error']
Availability,Papi v2 disk error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3776:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776,1,['error'],['error']
Availability,Parse Error when using variables prepended by if*,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6956:6,Error,Error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6956,1,['Error'],['Error']
Availability,"Part 2 of implementing the ""Remove Object"" [PR in OpenWDL](https://github.com/openwdl/wdl/pull/228). Allow us to treat calls as structs of their outputs, for purposes of evaluating downstream expressions, eg:; ```wdl; workflow blah { ; call foo; FooOutputStruct foo_outputs = foo; }. task foo {; # ... output {; Int i = ...; String s = ...; }; }. struct FooOutputStruct {; Int i; String s; }; ```. TODOs:; - [x] Simple use case (eg the example above); - [x] Within `scatter`s and `if`s; - [x] Don't disrupt the `after` logic; - [ ] Make sure WDL draft-2 and 1.0 still can't do this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4535:181,down,downstream,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4535,1,['down'],['downstream']
Availability,"Part 2 of implementing the ""Remove Object"" [PR in OpenWDL](https://github.com/openwdl/wdl/pull/228). Allows us to treat calls as structs of their outputs, for purposes of evaluating downstream expressions, eg:; ```wdl; workflow blah { ; call foo; FooOutputStruct foo_outputs = foo; }. task foo {; # ... output {; Int i = ...; String s = ...; }; }. struct FooOutputStruct {; Int i; String s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4534:182,down,downstream,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4534,1,['down'],['downstream']
Availability,"Part of the #2942 work involved converting the config backend to use the WOM API. The config backend uses WDL to define its commands but it does not have a workflow (it's a WdlNamespace with a bunch of tasks). This conversion mostly went smoothly with the exception of uninitialized task optionals like `docker_user` found in the Local backend's `submit-docker`:. ```; runtime-attributes = """"""; String? docker; String? docker_user; """"""; submit = ""/bin/bash ${script}""; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; ```; Evaluating that `${""--user "" + docker_user}` expression currently blows up with no useful diagnostics in the absence of an explicit `docker_user` input. To hack around this I changed the config backend to force in `none` inputs for all optional declarations in a task, but this would have the effect of clobbering any initialized optionals:. ```; String? docker_user = ""mobydock""; ```. With the #2942 changes `docker_user` would be forced to `none` and `""mobydock""` would be lost (at sea). It's not clear why this is happening when using the WOM API at a task level and not at the workflow level. There may be some `none`-initialization done at the workflow level that should get pushed down to tasks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2946:1343,down,down,1343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2946,1,['down'],['down']
Availability,"PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right plac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:5177,Error,Error,5177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['Error'],['Error']
Availability,Pass through Cromwell & Sam HTTP errors to Cromiam.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4624:33,error,errors,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4624,1,['error'],['errors']
Availability,"Per @jmthibault79 -. ```; failures: [ ""single error string"" ]; ```; Should be:; ```; failures: [{; message: ""single error string"" ; causedBy: []; }]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2201:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201,4,"['error', 'failure']","['error', 'failures']"
Availability,"Per https://cloud.google.com/container-registry/docs/pushing-and-pulling:. >The four options are:; >; > gcr.io hosts the images in the United States, but the location may change in the future; > us.gcr.io hosts the image in the United States, in a separate storage bucket from images hosted by gcr.io; > eu.gcr.io hosts the images in the European Union; > asia.gcr.io hosts the images in Asia. Cromwell today would appear to support the first three but not the last, `asia.gcr.io`. The place in code that seems to be at fault is: https://github.com/broadinstitute/cromwell/blob/develop/dockerHashing/src/main/scala/cromwell/docker/registryv2/flows/gcr/GoogleFlow.scala#L20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4466:520,fault,fault,520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4466,1,['fault'],['fault']
Availability,"Per our test review meeting, we see this is one of the more commonly failing tests. . ![image](https://user-images.githubusercontent.com/165320/46823419-236f9280-cd5c-11e8-8f32-0401c7bdee71.png). (working on a BQ sample of an error for some more details)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4232:226,error,error,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4232,1,['error'],['error']
Availability,"Perhaps a config error... I am using the TES backend, which is configured to utilize my funnel server, that in turn is passing tasks to AWS Batch (nice, right?). This seems to work OK with the simplest workflow possible, but now that I have added some inputs, I am getting an error. Here is my setup and error trace from the server, and I am running on the latest from the develop branch:. [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2081428/hello.inputs.txt); [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081429/my-cromwell.conf.txt); [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081843/myWorkflow_awsbatch.wdl.txt). ```; 2018-06-07 13:09:05,646 cromwell-system-akka.dispatchers.api-dispatcher-119 INFO - Unspecified type (Unspecified version) workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 submitted; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(af282f7a-1e95-4390-8cf7-c3bbd93b10b2); 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,3,['error'],['error']
Availability,Pins the docker image used by the client build script to one which does not error out when we try the build.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6945:76,error,error,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6945,1,['error'],['error']
Availability,"PipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi; 1608597482359,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/script to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/script; 1608597484342,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz; 1608597486185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:149441,down,download,149441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,2,['down'],['download']
Availability,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4677:382,echo,echoHello,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677,3,['echo'],"['echo', 'echoHello']"
Availability,Plumb through submission time and use to sort heartbeat writes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4442:46,heartbeat,heartbeat,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4442,1,['heartbeat'],['heartbeat']
Availability,"Pool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:143); 	at cromwell.engine.workflow.lifecycle.execution.job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:6490,Fault,FaultHandling,6490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['Fault'],['FaultHandling']
Availability,PoolTime Out error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7200:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7200,1,['error'],['error']
Availability,Poor womtool validate output (error formatter),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4041:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041,1,['error'],['error']
Availability,Possibly the first in a series as I trace down two other classes of failures.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3223:42,down,down,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3223,2,"['down', 'failure']","['down', 'failures']"
Availability,Post Mortem of degraded performance May 17-20 2018,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3666:15,degraded,degraded,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666,1,['degraded'],['degraded']
Availability,"Post task success processing failed with a Communications link failure. ```; 2016-04-26 14:06:36,930 cromwell-system-akka.actor.default-dispatcher-8 INFO - JES Run [UUID(143681e1):GatherBqsrReports]: Status change from Running to Success; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(143681e1)]: Completion work failed for call GatherBqsrReports.; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 324 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; at sun.reflect.GeneratedConstructorAccessor102.newInstance(Unknown Source) ~[na:na]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1038) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3434) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3334) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3774) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2447) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2594) ~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2541) ~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4882) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionProxy.setAutoCommit(ConnectionProxy.java:334) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionJavassistProxy.setAutoCommit(ConnectionJavassistProxy.java) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742:63,failure,failure,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742,3,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,Post-Processing slows down to a crawl and fails with large globs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248:22,down,down,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248,1,['down'],['down']
Availability,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909:124,recover,recover,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909,2,"['ERROR', 'recover']","['ERROR', 'recover']"
Availability,Power through non-findable terminals in error messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3048:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048,1,['error'],['error']
Availability,Preempted machines with PAPI error code 10 not handled as preempted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5136:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136,1,['error'],['error']
Availability,Preemptible recovery from a checkpointing file [BW-460],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6137:12,recover,recovery,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6137,2,"['checkpoint', 'recover']","['checkpointing', 'recovery']"
Availability,Preemptible retry behavior for Error Code 13,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744:31,Error,Error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744,1,['Error'],['Error']
Availability,Pretty much what it says. We know we want to add Error 13 to our JES retries. Do any other codes make the cut?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1909:49,Error,Error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1909,1,['Error'],['Error']
Availability,Prints out the gsutil failures when (de)localizing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4396:22,failure,failures,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4396,1,['failure'],['failures']
Availability,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5087:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087,2,['error'],['error']
Availability,Production is hitting an error handler [0] that attempts to stringify too large a value and causes the server to crash before it manages to emit the log. This is a common enough situation that we have a library function for it. [0] https://github.com/broadinstitute/cromwell/blob/aen_wx_892/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/stores/ValueStore.scala#L117,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6981:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6981,1,['error'],['error']
Availability,"Program_required_data/deFuse/defuse-data/gmap/est4/est4.ref153offsets64strm...done (331,380,368 bytes, 38.82 sec); Pre-loading ref positions, kmer 15, interval 3......done (625,955,632 bytes, 54.79 sec); Starting alignment; No paths found for 279; No paths found for 396; No paths found for 1308; No paths found for 1785; No paths found for 1880; No paths found for 1883; No paths found for 1947; No paths found for 1948; No paths found for 2184; No paths found for 2262; No paths found for 2403; No paths found for 2536; No paths found for 2577; No paths found for 2742; No paths found for 2756; No paths found for 2907; No paths found for 2919; No paths found for 2921; No paths found for 3058; No paths found for 3298; No paths found for 3837; No paths found for 3942; No paths found for 4020; No paths found for 4124; No paths found for 4274; Processed 405 queries in 703.84 seconds (0.58 queries/sec); Removed existing memory for shmid 262152; Removed existing memory for shmid 131076; ; real 14m1.724s; user 3m28.053s; sys 0m13.563s; ```. - The WDL I am using: [defuse.wdl.txt](https://github.com/broadinstitute/cromwell/files/2651338/defuse.wdl.txt). The commands I use when running this in the docker container interactively are the same. - I encounter the same error using Cromwell 32 and Cromwell 36. - A similar error was reported [here](https://bitbucket.org/dranew/defuse/issues/36/defuse-with-segmentation-error) for when deFuse was run on a server. I tried running the failing gmap command alone through Cromwell and it succeeded. - The breakpoints files that gmap uses as input for the failing steps differ (deFuse generates these files in earlier steps). - Many gmap steps are run in deFuse; some do succeed when run through Cromwell. - The full log for the Cromwell version: ; [defuse.log](https://github.com/broadinstitute/cromwell/files/2653318/defuse.log); - The full log for the docker version: ; [defuse.log](https://github.com/broadinstitute/cromwell/files/2653319/defuse.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:6450,error,error,6450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,3,['error'],['error']
Availability,Programmer error and logging in PAPI request manager,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671,1,['error'],['error']
Availability,"Proposed high-level flow:. Workflows + inputs + options are submitted. Any grossly malformed submission would quickly generate an error returned to the client immediately. Otherwise save everything to WORKFLOW_EXECUTION and WORKFLOW_EXECUTION_AUX at status Submitted, and return the generated UUID to the client. THATâ€™S IT. Do not validate workflows synchronously to submission, thatâ€™s expensive and difficult to throttle back. The returned UUID constitutes a pollable handle for the submission. Technically this UUID is now a submission ID rather than a workflow ID, though if the workflow is eventually validated that UUID could certainly be thought of as the workflow ID. When Cromwell is ready to validate a submission, the status of the WORKFLOW_EXECUTION can be moved to Validating and validation initiated. Assuming the validation succeeds the workflow would move to state Validated. Again when Cromwell is ready to run the workflow, it can be moved to state Running and the workflow would begin to run. Workflows that fail validation would get status Failed with text describing the nature of the failures in the FAILURE_MESSAGES table.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/564:130,error,error,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/564,2,"['error', 'failure']","['error', 'failures']"
Availability,"Provide a configurable knob which will limit the number of in flight calls per workflow. This includes scatters, e.g. if the limit is 5 and there's a 6-way scatter, at most 5 shards may be processed at once. This will likely be requested to be in place prior to or shortly after the GATK launch in early january. Edit: Some clarifications. As mentioned below this needs to be robust to the entirety of the root workflow, including all subworkflows. It also needs to be robust to restart, IOW on a restart of Cromwell if there were N jobs restarted the counter should start at MAX - N.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2846:376,robust,robust,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2846,2,['robust'],['robust']
Availability,"Put in testing for restarts. . A possible (and preferred by this author) solution would be to seed a database with a known state and make sure a launching cromwell picks things up properly. That sounds like it'd probably be in the Centaur-ish space, but perhaps not. Another possibility (and not preferred by this author) would be to have a system that submits stuff, shuts down that cromwell, launches another cromwell and goes to town.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1329:374,down,down,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329,1,['down'],['down']
Availability,Query endpoint `additionalQueryResultFields` parameter database error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115:64,error,error,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115,1,['error'],['error']
Availability,Query endpoint `includeSubworkflows` parameter database error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3873:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873,1,['error'],['error']
Availability,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:783,error,error,783,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,1,['error'],['error']
Availability,"Question; -----------------; Is there any plan on speeding up `java -jar $JAR run`? Pipeline developer would definitely benefits from a faster startup. Symptom; -----------------; ```bash; time java -jar cromwell-49.jar run sub-flow.wdl -i input.json; ```. ```; real	1m12.833s; user	1m12.148s; sys	0m7.644s; ```. ```; $ cat input.json ; {; ""hello_and_goodbye.hello_and_goodbye_input"":""test1""; }; ```. Detail; ----------; backend: local. File: `sub-flow.wdl`; ```wdl; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451:532,echo,echo,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451,1,['echo'],['echo']
Availability,Quieten down that localization switchover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1727:8,down,down,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1727,1,['down'],['down']
Availability,"Quite often I get ; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Docker image quay.io/biocontainers/star:2.5.3a--0@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b has an invalid syntax.""; }; ],; ```; Here is example of the task that caused this error. What is interested is that cromwell even did not start this task, so I suspect there is something wrong with docker parsing (I have this issue for this container both in develop and latest release of cromwell). ```; task star {. Int numberOfThreads = 8; File file; File genomeDir. command {; STAR --runThreadN ${numberOfThreads} --genomeDir ${genomeDir} --readFilesCommand gunzip -c --readFilesIn ${file}; }. runtime {; docker: ""quay.io/biocontainers/star:2.5.3a--0@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b""; }. output {; String result = ""STAR WORKS!""; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2254:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2254,2,"['error', 'failure']","['error', 'failures']"
Availability,Quotation marks when quoting source in error msg,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3725:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725,1,['error'],['error']
Availability,"REST API correctly gives error on unknown status value:. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=blah; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: blah""; }; ```. and correctly gives results for status with exact case-sensitive spelling (""Failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=Failed; {; ""results"": [{; ""id"": ""8b8ce542-9e4a-4549-8fd3-c72ea325f392"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:15:38.214+10:00"",; ""end"": ""2016-10-05T11:15:38.217+10:00""; }, {; ""id"": ""1784a44d-e623-42fd-bf7a-90b00d300017"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:16:38.273+10:00"",; ""end"": ""2016-10-05T11:16:38.275+10:00""; }]; }; ```. But returns neither error nor results when given a status value that is only case-insensitively correct (""failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=failed; {; ""results"": []; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1514:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1514,2,['error'],['error']
Availability,REST Endpoint to shut down Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2595:22,down,down,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2595,1,['down'],['down']
Availability,"ROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party kernel modules can be installed; [INFO 2020-08-04 23:40:07 UTC] Checking cached version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:3740,Down,Downloading,3740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['Down'],['Downloading']
Availability,"Ran a workflow (see below) with nested scatters on Cromwell v38. The metadata for the calls seems a bit odd... It would be helpful to find out:; 1. Why the name of the call is `ScatterAt9_13`; 2. Why aren't the shards numbered as they are for a single scatter?. <img width=""570"" alt=""Screen Shot 2019-04-10 at 2 44 12 PM"" src=""https://user-images.githubusercontent.com/14941133/55905173-75746200-5b9f-11e9-8388-4ca4d4e887b5.png"">. ```; version 1.0. workflow nested_scatter {. Array[Int] indices = [1,2,3]; Int y = 55. scatter(a in indices) {; scatter(b in indices) {; Int x = a + b; call add { ; 	input: foo = x; }; }; }; }. task add {; 	; 	input { Int foo }; 	. 	command { echo foo }. 	runtime {; 		docker: ""ubuntu:latest"". 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4820:674,echo,echo,674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4820,1,['echo'],['echo']
Availability,"Ran a workflow using V40 Cromwell on AWSBATCH that had as outputs (one outputfile.vcf for each of the shards in the workflow):. ```; output {; Array[File] outputs = task.outputvcf; }; ```. I used the following workflow options:; ```; {; ""workflow_failure_mode"": ""NoNewCalls"",; ""default_runtime_attributes"": {; ""maxRetries"": 1; },; ""final_workflow_outputs_dir"": ""s3://bucket/Cromwell/results"",; ""use_relative_output_paths"": ""false"",; ""final_workflow_log_dir"": ""s3://bucket/Cromwell/workflowLogs"",; ""final_call_logs_dir"": ""s3://bucket/Cromwell/workflowLogs""; }; ```. All calls of the workflow completed successfully but the workflow itself failed. . Error Message I got:; `""copying directories is not yet supported: s3://s3.amazonaws.com/bucket/Cromwell/results/workflowName/1ec38d0b-afc4-4cd5-90f1-f015395d6e36/call-task/shard-0/outputfile.vcf""`. Oddly enough, the correct prefixes for the output files were created in the correct S3 bucket, they just don't have an object there, and via the CLI they appear as directories. ??? . For the logs, a prefix was made that is empty, and the log file was written successfully to one level higher than the prefix it is supposed to be in. So instead of:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/<workflowid>.log; ```; there is:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/ (empty prefix); s3://bucket/Cromwell/workflowLogs/<workflowid>.log (successfully written file). ```; Thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982:648,Error,Error,648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982,1,['Error'],['Error']
Availability,"Rather than putting up with this error because their config is out of date:; ```; [ERROR] [04/19/2017 18:14:32.636] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService] Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where (""SUMMARY_TABLE_NAME"" = 'WORKFLOW_METADATA_SUMMARY_' at line 1; ```. We should have a nice error message that tells people what to do about it",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2186:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2186,5,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Re-enable and make the `""handle coercion failures gracefully""` test in `MaterializeWorkflowDescriptorActorSpec` work again",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1067:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1067,1,['failure'],['failures']
Availability,Readable error message for invalid indexing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3008:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3008,1,['error'],['error']
Availability,Recent change to JES travis test made it blind to failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1353:50,failure,failures,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1353,1,['failure'],['failures']
Availability,Recover from Docker image hash failures to fail the workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/333:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/333,2,"['Recover', 'failure']","['Recover', 'failures']"
Availability,Recover from database failures when reading the workflow store. Develop edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2150:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2150,2,"['Recover', 'failure']","['Recover', 'failures']"
Availability,Recover from database failures when reading the workflow store. Hotfix edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2149:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2149,2,"['Recover', 'failure']","['Recover', 'failures']"
Availability,Recover support for Local PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/666:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/666,1,['Recover'],['Recover']
Availability,Recovery functionality for HtCondor backend. Closes #1249.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1250:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250,1,['Recover'],['Recovery']
Availability,Recovery support for JES PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/751:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/751,1,['Recover'],['Recovery']
Availability,Recovery support for SGE Backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1162:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1162,1,['Recover'],['Recovery']
Availability,Refactored IO command creation to return errors instead of throw BT-8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6019:41,error,errors,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6019,1,['error'],['errors']
Availability,"Reinstates the ability to run at least the `simple_if` WDL from centaur:; ```wdl; task runMe {; command {; echo ""done""; }; output {; String s = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow simple_if {; if (true) {; call runMe as runMeTrue; }. if (false) {; call runMe as runMeFalse; }. output {; runMeTrue.s; runMeFalse.s; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2803:107,echo,echo,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2803,1,['echo'],['echo']
Availability,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050:246,echo,echo,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050,5,"['Echo', 'echo']","['Echo', 'echo']"
Availability,"Related to #1944 but with some options problems as well:. The following WDL prints nothing for `string_from_maybe_file` and a non-relativized path for `string_from_file` on JES, AWS, and Local (on Local the path is not relativized under the cromwell executions directory):. ```; task strings {; File file ; File? maybe_file. String string_from_file = file; String? string_from_maybe_file = maybe_file. command {; echo string_from_file: ${string_from_file} string_from_maybe_file: ${string_from_maybe_file}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; File file. call strings { input: file = file, maybe_file = file }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1945:413,echo,echo,413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945,1,['echo'],['echo']
Availability,"Relevant OpenWDL PR: ~~https://github.com/openwdl/wdl/pull/229~~ (rebased into) https://github.com/openwdl/wdl/pull/366. Adding a `sep` function to join arrays of string together. I've followed the general process from: https://github.com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:343,error,errors,343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,2,['error'],['errors']
Availability,Remove MetadataPutAcks & Failures. Closes #1811,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1903:25,Failure,Failures,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1903,1,['Failure'],['Failures']
Availability,"Remove link to Building (which was in any case broken), and replace it earlier in the page with a link to the releases page. Most users will want to download the latest release of Cromwell rather than build it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6984:149,down,download,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6984,1,['down'],['download']
Availability,Remove logging that could error with multiple compute environments [BT-429],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6547:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6547,1,['error'],['error']
Availability,Remove redundant WaitingForQueueSpace execution status [BA-6487 prereq],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5590:7,redundant,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5590,1,['redundant'],['redundant']
Availability,Remove redundant WaitingForQueueSpace status [BW-387],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6034:7,redundant,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6034,1,['redundant'],['redundant']
Availability,"Remove redundant nested /project directory, ignore BSP [no JIRA]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5931:7,redundant,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5931,1,['redundant'],['redundant']
Availability,Remove the 'Some' from an error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1899:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1899,1,['error'],['error']
Availability,Remove the 'Some' from an error message. Closes #1893,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1898:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1898,1,['error'],['error']
Availability,Removing Await.results: 1 down 6 to go.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/233:26,down,down,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/233,1,['down'],['down']
Availability,"Removing a duplicate write of workflow state that seemed to be responsible for some Tyburn failures. The WorkflowActor already handles the persistence of its own state, WorkflowManagerActor doesn't need to be doing this as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/240:91,failure,failures,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/240,1,['failure'],['failures']
